<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>cerca_optim_dnn</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>




<style type="text/css">
    pre { line-height: 125%; margin: 0; }
td.linenos pre { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }
span.linenos { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }
td.linenos pre.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>



<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/*
 * Webkit scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-corner {
  background: var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-thumb {
  background: rgb(var(--jp-scrollbar-thumb-color));
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-right: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-bottom: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar */

[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-corner,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-corner {
  background-color: transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid transparent;
  border-right: var(--jp-scrollbar-endpad) solid transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid transparent;
  border-bottom: var(--jp-scrollbar-endpad) solid transparent;
}

/*
 * Phosphor
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Widget, /* </DEPRECATED> */
.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  cursor: default;
}


/* <DEPRECATED> */ .p-Widget.p-mod-hidden, /* </DEPRECATED> */
.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-CommandPalette, /* </DEPRECATED> */
.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-CommandPalette-search, /* </DEPRECATED> */
.lm-CommandPalette-search {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-content, /* </DEPRECATED> */
.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-CommandPalette-header, /* </DEPRECATED> */
.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}


/* <DEPRECATED> */ .p-CommandPalette-item, /* </DEPRECATED> */
.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}


/* <DEPRECATED> */ .p-CommandPalette-itemIcon, /* </DEPRECATED> */
.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemContent, /* </DEPRECATED> */
.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}


/* <DEPRECATED> */ .p-CommandPalette-itemShortcut, /* </DEPRECATED> */
.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemLabel, /* </DEPRECATED> */
.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-DockPanel, /* </DEPRECATED> */
.lm-DockPanel {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-widget, /* </DEPRECATED> */
.lm-DockPanel-widget {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-tabBar, /* </DEPRECATED> */
.lm-DockPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-DockPanel-handle, /* </DEPRECATED> */
.lm-DockPanel-handle {
  z-index: 2;
}


/* <DEPRECATED> */ .p-DockPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-DockPanel-handle:after, /* </DEPRECATED> */
.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}


/* <DEPRECATED> */ .p-DockPanel-overlay, /* </DEPRECATED> */
.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}


/* <DEPRECATED> */ .p-DockPanel-overlay.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Menu, /* </DEPRECATED> */
.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-Menu-content, /* </DEPRECATED> */
.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-Menu-item, /* </DEPRECATED> */
.lm-Menu-item {
  display: table-row;
}


/* <DEPRECATED> */
.p-Menu-item.p-mod-hidden,
.p-Menu-item.p-mod-collapsed,
/* </DEPRECATED> */
.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}


/* <DEPRECATED> */
.p-Menu-itemIcon,
.p-Menu-itemSubmenuIcon,
/* </DEPRECATED> */
.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}


/* <DEPRECATED> */ .p-Menu-itemLabel, /* </DEPRECATED> */
.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}


/* <DEPRECATED> */ .p-Menu-itemShortcut, /* </DEPRECATED> */
.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-MenuBar, /* </DEPRECATED> */
.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-MenuBar-content, /* </DEPRECATED> */
.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}


/* <DEPRECATED> */ .p--MenuBar-item, /* </DEPRECATED> */
.lm-MenuBar-item {
  box-sizing: border-box;
}


/* <DEPRECATED> */
.p-MenuBar-itemIcon,
.p-MenuBar-itemLabel,
/* </DEPRECATED> */
.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-ScrollBar, /* </DEPRECATED> */
.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-ScrollBar-button, /* </DEPRECATED> */
.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-track, /* </DEPRECATED> */
.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-thumb, /* </DEPRECATED> */
.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-SplitPanel-child, /* </DEPRECATED> */
.lm-SplitPanel-child {
  z-index: 0;
}


/* <DEPRECATED> */ .p-SplitPanel-handle, /* </DEPRECATED> */
.lm-SplitPanel-handle {
  z-index: 1;
}


/* <DEPRECATED> */ .p-SplitPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-SplitPanel-handle:after, /* </DEPRECATED> */
.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabBar, /* </DEPRECATED> */
.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-TabBar-content, /* </DEPRECATED> */
.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='horizontal'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='vertical'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
}


/* <DEPRECATED> */
.p-TabBar-tabIcon,
.p-TabBar-tabCloseIcon,
/* </DEPRECATED> */
.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-TabBar-tabLabel, /* </DEPRECATED> */
.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}


/* <DEPRECATED> */ .p-TabBar-tab.p-mod-hidden, /* </DEPRECATED> */
.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='horizontal'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='vertical'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabPanel-tabBar, /* </DEPRECATED> */
.lm-TabPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-TabPanel-stackedPanel, /* </DEPRECATED> */
.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

@charset "UTF-8";
/*!

Copyright 2015-present Palantir Technologies, Inc. All rights reserved.
Licensed under the Apache License, Version 2.0.

*/
html{
  -webkit-box-sizing:border-box;
          box-sizing:border-box; }

*,
*::before,
*::after{
  -webkit-box-sizing:inherit;
          box-sizing:inherit; }

body{
  text-transform:none;
  line-height:1.28581;
  letter-spacing:0;
  font-size:14px;
  font-weight:400;
  color:#182026;
  font-family:-apple-system, "BlinkMacSystemFont", "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Open Sans", "Helvetica Neue", "Icons16", sans-serif; }

p{
  margin-top:0;
  margin-bottom:10px; }

small{
  font-size:12px; }

strong{
  font-weight:600; }

::-moz-selection{
  background:rgba(125, 188, 255, 0.6); }

::selection{
  background:rgba(125, 188, 255, 0.6); }
.bp3-heading{
  color:#182026;
  font-weight:600;
  margin:0 0 10px;
  padding:0; }
  .bp3-dark .bp3-heading{
    color:#f5f8fa; }

h1.bp3-heading, .bp3-running-text h1{
  line-height:40px;
  font-size:36px; }

h2.bp3-heading, .bp3-running-text h2{
  line-height:32px;
  font-size:28px; }

h3.bp3-heading, .bp3-running-text h3{
  line-height:25px;
  font-size:22px; }

h4.bp3-heading, .bp3-running-text h4{
  line-height:21px;
  font-size:18px; }

h5.bp3-heading, .bp3-running-text h5{
  line-height:19px;
  font-size:16px; }

h6.bp3-heading, .bp3-running-text h6{
  line-height:16px;
  font-size:14px; }
.bp3-ui-text{
  text-transform:none;
  line-height:1.28581;
  letter-spacing:0;
  font-size:14px;
  font-weight:400; }

.bp3-monospace-text{
  text-transform:none;
  font-family:monospace; }

.bp3-text-muted{
  color:#5c7080; }
  .bp3-dark .bp3-text-muted{
    color:#a7b6c2; }

.bp3-text-disabled{
  color:rgba(92, 112, 128, 0.6); }
  .bp3-dark .bp3-text-disabled{
    color:rgba(167, 182, 194, 0.6); }

.bp3-text-overflow-ellipsis{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal; }
.bp3-running-text{
  line-height:1.5;
  font-size:14px; }
  .bp3-running-text h1{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h1{
      color:#f5f8fa; }
  .bp3-running-text h2{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h2{
      color:#f5f8fa; }
  .bp3-running-text h3{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h3{
      color:#f5f8fa; }
  .bp3-running-text h4{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h4{
      color:#f5f8fa; }
  .bp3-running-text h5{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h5{
      color:#f5f8fa; }
  .bp3-running-text h6{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h6{
      color:#f5f8fa; }
  .bp3-running-text hr{
    margin:20px 0;
    border:none;
    border-bottom:1px solid rgba(16, 22, 26, 0.15); }
    .bp3-dark .bp3-running-text hr{
      border-color:rgba(255, 255, 255, 0.15); }
  .bp3-running-text p{
    margin:0 0 10px;
    padding:0; }

.bp3-text-large{
  font-size:16px; }

.bp3-text-small{
  font-size:12px; }
a{
  text-decoration:none;
  color:#106ba3; }
  a:hover{
    cursor:pointer;
    text-decoration:underline;
    color:#106ba3; }
  a .bp3-icon, a .bp3-icon-standard, a .bp3-icon-large{
    color:inherit; }
  a code,
  .bp3-dark a code{
    color:inherit; }
  .bp3-dark a,
  .bp3-dark a:hover{
    color:#48aff0; }
    .bp3-dark a .bp3-icon, .bp3-dark a .bp3-icon-standard, .bp3-dark a .bp3-icon-large,
    .bp3-dark a:hover .bp3-icon,
    .bp3-dark a:hover .bp3-icon-standard,
    .bp3-dark a:hover .bp3-icon-large{
      color:inherit; }
.bp3-running-text code, .bp3-code{
  text-transform:none;
  font-family:monospace;
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
  background:rgba(255, 255, 255, 0.7);
  padding:2px 5px;
  color:#5c7080;
  font-size:smaller; }
  .bp3-dark .bp3-running-text code, .bp3-running-text .bp3-dark code, .bp3-dark .bp3-code{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#a7b6c2; }
  .bp3-running-text a > code, a > .bp3-code{
    color:#137cbd; }
    .bp3-dark .bp3-running-text a > code, .bp3-running-text .bp3-dark a > code, .bp3-dark a > .bp3-code{
      color:inherit; }

.bp3-running-text pre, .bp3-code-block{
  text-transform:none;
  font-family:monospace;
  display:block;
  margin:10px 0;
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
  background:rgba(255, 255, 255, 0.7);
  padding:13px 15px 12px;
  line-height:1.4;
  color:#182026;
  font-size:13px;
  word-break:break-all;
  word-wrap:break-word; }
  .bp3-dark .bp3-running-text pre, .bp3-running-text .bp3-dark pre, .bp3-dark .bp3-code-block{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa; }
  .bp3-running-text pre > code, .bp3-code-block > code{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none;
    padding:0;
    color:inherit;
    font-size:inherit; }

.bp3-running-text kbd, .bp3-key{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  min-width:24px;
  height:24px;
  padding:3px 6px;
  vertical-align:middle;
  line-height:24px;
  color:#5c7080;
  font-family:inherit;
  font-size:12px; }
  .bp3-running-text kbd .bp3-icon, .bp3-key .bp3-icon, .bp3-running-text kbd .bp3-icon-standard, .bp3-key .bp3-icon-standard, .bp3-running-text kbd .bp3-icon-large, .bp3-key .bp3-icon-large{
    margin-right:5px; }
  .bp3-dark .bp3-running-text kbd, .bp3-running-text .bp3-dark kbd, .bp3-dark .bp3-key{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
    background:#394b59;
    color:#a7b6c2; }
.bp3-running-text blockquote, .bp3-blockquote{
  margin:0 0 10px;
  border-left:solid 4px rgba(167, 182, 194, 0.5);
  padding:0 20px; }
  .bp3-dark .bp3-running-text blockquote, .bp3-running-text .bp3-dark blockquote, .bp3-dark .bp3-blockquote{
    border-color:rgba(115, 134, 148, 0.5); }
.bp3-running-text ul,
.bp3-running-text ol, .bp3-list{
  margin:10px 0;
  padding-left:30px; }
  .bp3-running-text ul li:not(:last-child), .bp3-running-text ol li:not(:last-child), .bp3-list li:not(:last-child){
    margin-bottom:5px; }
  .bp3-running-text ul ol, .bp3-running-text ol ol, .bp3-list ol,
  .bp3-running-text ul ul,
  .bp3-running-text ol ul,
  .bp3-list ul{
    margin-top:5px; }

.bp3-list-unstyled{
  margin:0;
  padding:0;
  list-style:none; }
  .bp3-list-unstyled li{
    padding:0; }
.bp3-rtl{
  text-align:right; }

.bp3-dark{
  color:#f5f8fa; }

:focus{
  outline:rgba(19, 124, 189, 0.6) auto 2px;
  outline-offset:2px;
  -moz-outline-radius:6px; }

.bp3-focus-disabled :focus{
  outline:none !important; }
  .bp3-focus-disabled :focus ~ .bp3-control-indicator{
    outline:none !important; }

.bp3-alert{
  max-width:400px;
  padding:20px; }

.bp3-alert-body{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-alert-body .bp3-icon{
    margin-top:0;
    margin-right:20px;
    font-size:40px; }

.bp3-alert-footer{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:reverse;
      -ms-flex-direction:row-reverse;
          flex-direction:row-reverse;
  margin-top:10px; }
  .bp3-alert-footer .bp3-button{
    margin-left:10px; }
.bp3-breadcrumbs{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:wrap;
      flex-wrap:wrap;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  margin:0;
  cursor:default;
  height:30px;
  padding:0;
  list-style:none; }
  .bp3-breadcrumbs > li{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center; }
    .bp3-breadcrumbs > li::after{
      display:block;
      margin:0 5px;
      background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e");
      width:16px;
      height:16px;
      content:""; }
    .bp3-breadcrumbs > li:last-of-type::after{
      display:none; }

.bp3-breadcrumb,
.bp3-breadcrumb-current,
.bp3-breadcrumbs-collapsed{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  font-size:16px; }

.bp3-breadcrumb,
.bp3-breadcrumbs-collapsed{
  color:#5c7080; }

.bp3-breadcrumb:hover{
  text-decoration:none; }

.bp3-breadcrumb.bp3-disabled{
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-breadcrumb .bp3-icon{
  margin-right:5px; }

.bp3-breadcrumb-current{
  color:inherit;
  font-weight:600; }
  .bp3-breadcrumb-current .bp3-input{
    vertical-align:baseline;
    font-size:inherit;
    font-weight:inherit; }

.bp3-breadcrumbs-collapsed{
  margin-right:2px;
  border:none;
  border-radius:3px;
  background:#ced9e0;
  cursor:pointer;
  padding:1px 5px;
  vertical-align:text-bottom; }
  .bp3-breadcrumbs-collapsed::before{
    display:block;
    background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e") center no-repeat;
    width:16px;
    height:16px;
    content:""; }
  .bp3-breadcrumbs-collapsed:hover{
    background:#bfccd6;
    text-decoration:none;
    color:#182026; }

.bp3-dark .bp3-breadcrumb,
.bp3-dark .bp3-breadcrumbs-collapsed{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumbs > li::after{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumb.bp3-disabled{
  color:rgba(167, 182, 194, 0.6); }

.bp3-dark .bp3-breadcrumb-current{
  color:#f5f8fa; }

.bp3-dark .bp3-breadcrumbs-collapsed{
  background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-breadcrumbs-collapsed:hover{
    background:rgba(16, 22, 26, 0.6);
    color:#f5f8fa; }
.bp3-button{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  padding:5px 10px;
  vertical-align:middle;
  text-align:left;
  font-size:14px;
  min-width:30px;
  min-height:30px; }
  .bp3-button > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-button > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-button::before,
  .bp3-button > *{
    margin-right:7px; }
  .bp3-button:empty::before,
  .bp3-button > :last-child{
    margin-right:0; }
  .bp3-button:empty{
    padding:0 !important; }
  .bp3-button:disabled, .bp3-button.bp3-disabled{
    cursor:not-allowed; }
  .bp3-button.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button.bp3-align-right,
  .bp3-align-right .bp3-button{
    text-align:right; }
  .bp3-button.bp3-align-left,
  .bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-button:not([class*="bp3-intent-"]){
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    color:#182026; }
    .bp3-button:not([class*="bp3-intent-"]):hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
      background-clip:padding-box;
      background-color:#ebf1f5; }
    .bp3-button:not([class*="bp3-intent-"]):active, .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#d8e1e8;
      background-image:none; }
    .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      outline:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active:hover, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-button.bp3-intent-primary{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover, .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#106ba3; }
    .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#0e5a8a;
      background-image:none; }
    .bp3-button.bp3-intent-primary:disabled, .bp3-button.bp3-intent-primary.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(19, 124, 189, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-success{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#0f9960;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-success:hover, .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-success:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#0d8050; }
    .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#0a6640;
      background-image:none; }
    .bp3-button.bp3-intent-success:disabled, .bp3-button.bp3-intent-success.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(15, 153, 96, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-warning{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#d9822b;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover, .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#bf7326; }
    .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#a66321;
      background-image:none; }
    .bp3-button.bp3-intent-warning:disabled, .bp3-button.bp3-intent-warning.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(217, 130, 43, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-danger{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#db3737;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover, .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#c23030; }
    .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#a82a2a;
      background-image:none; }
    .bp3-button.bp3-intent-danger:disabled, .bp3-button.bp3-intent-danger.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(219, 55, 55, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
    stroke:#ffffff; }
  .bp3-button.bp3-large,
  .bp3-large .bp3-button{
    min-width:40px;
    min-height:40px;
    padding:5px 15px;
    font-size:16px; }
    .bp3-button.bp3-large::before,
    .bp3-button.bp3-large > *,
    .bp3-large .bp3-button::before,
    .bp3-large .bp3-button > *{
      margin-right:10px; }
    .bp3-button.bp3-large:empty::before,
    .bp3-button.bp3-large > :last-child,
    .bp3-large .bp3-button:empty::before,
    .bp3-large .bp3-button > :last-child{
      margin-right:0; }
  .bp3-button.bp3-small,
  .bp3-small .bp3-button{
    min-width:24px;
    min-height:24px;
    padding:0 7px; }
  .bp3-button.bp3-loading{
    position:relative; }
    .bp3-button.bp3-loading[class*="bp3-icon-"]::before{
      visibility:hidden; }
    .bp3-button.bp3-loading .bp3-button-spinner{
      position:absolute;
      margin:0; }
    .bp3-button.bp3-loading > :not(.bp3-button-spinner){
      visibility:hidden; }
  .bp3-button[class*="bp3-icon-"]::before{
    line-height:1;
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-weight:400;
    font-style:normal;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    color:#5c7080; }
  .bp3-button .bp3-icon, .bp3-button .bp3-icon-standard, .bp3-button .bp3-icon-large{
    color:#5c7080; }
    .bp3-button .bp3-icon.bp3-align-right, .bp3-button .bp3-icon-standard.bp3-align-right, .bp3-button .bp3-icon-large.bp3-align-right{
      margin-left:7px; }
  .bp3-button .bp3-icon:first-child:last-child,
  .bp3-button .bp3-spinner + .bp3-icon:last-child{
    margin:0 -7px; }
  .bp3-dark .bp3-button:not([class*="bp3-intent-"]){
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover, .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#30404d; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#202b33;
      background-image:none; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"])[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-large{
      color:#a7b6c2; }
  .bp3-dark .bp3-button[class*="bp3-intent-"]{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:active, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:disabled, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background-image:none;
      color:rgba(255, 255, 255, 0.3); }
    .bp3-dark .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
      stroke:#8a9ba8; }
  .bp3-button:disabled::before,
  .bp3-button:disabled .bp3-icon, .bp3-button:disabled .bp3-icon-standard, .bp3-button:disabled .bp3-icon-large, .bp3-button.bp3-disabled::before,
  .bp3-button.bp3-disabled .bp3-icon, .bp3-button.bp3-disabled .bp3-icon-standard, .bp3-button.bp3-disabled .bp3-icon-large, .bp3-button[class*="bp3-intent-"]::before,
  .bp3-button[class*="bp3-intent-"] .bp3-icon, .bp3-button[class*="bp3-intent-"] .bp3-icon-standard, .bp3-button[class*="bp3-intent-"] .bp3-icon-large{
    color:inherit !important; }
  .bp3-button.bp3-minimal{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none; }
    .bp3-button.bp3-minimal:hover{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(167, 182, 194, 0.3);
      text-decoration:none;
      color:#182026; }
    .bp3-button.bp3-minimal:active, .bp3-button.bp3-minimal.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(115, 134, 148, 0.3);
      color:#182026; }
    .bp3-button.bp3-minimal:disabled, .bp3-button.bp3-minimal:disabled:hover, .bp3-button.bp3-minimal.bp3-disabled, .bp3-button.bp3-minimal.bp3-disabled:hover{
      background:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button.bp3-minimal{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:inherit; }
      .bp3-dark .bp3-button.bp3-minimal:hover, .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none; }
      .bp3-dark .bp3-button.bp3-minimal:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button.bp3-minimal:disabled, .bp3-dark .bp3-button.bp3-minimal:disabled:hover, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{
        background:none;
        cursor:not-allowed;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover, .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-success{
      color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover, .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover, .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-danger{
      color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover, .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }

a.bp3-button{
  text-align:center;
  text-decoration:none;
  -webkit-transition:none;
  transition:none; }
  a.bp3-button, a.bp3-button:hover, a.bp3-button:active{
    color:#182026; }
  a.bp3-button.bp3-disabled{
    color:rgba(92, 112, 128, 0.6); }

.bp3-button-text{
  -webkit-box-flex:0;
      -ms-flex:0 1 auto;
          flex:0 1 auto; }

.bp3-button.bp3-align-left .bp3-button-text, .bp3-button.bp3-align-right .bp3-button-text,
.bp3-button-group.bp3-align-left .bp3-button-text,
.bp3-button-group.bp3-align-right .bp3-button-text{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto; }
.bp3-button-group{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex; }
  .bp3-button-group .bp3-button{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    position:relative;
    z-index:4; }
    .bp3-button-group .bp3-button:focus{
      z-index:5; }
    .bp3-button-group .bp3-button:hover{
      z-index:6; }
    .bp3-button-group .bp3-button:active, .bp3-button-group .bp3-button.bp3-active{
      z-index:7; }
    .bp3-button-group .bp3-button:disabled, .bp3-button-group .bp3-button.bp3-disabled{
      z-index:3; }
    .bp3-button-group .bp3-button[class*="bp3-intent-"]{
      z-index:9; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:focus{
        z-index:10; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:hover{
        z-index:11; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:active, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-active{
        z-index:12; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:disabled, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-disabled{
        z-index:8; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:first-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:first-child){
    border-top-left-radius:0;
    border-bottom-left-radius:0; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    margin-right:-1px;
    border-top-right-radius:0;
    border-bottom-right-radius:0; }
  .bp3-button-group.bp3-minimal .bp3-button{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none; }
    .bp3-button-group.bp3-minimal .bp3-button:hover{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(167, 182, 194, 0.3);
      text-decoration:none;
      color:#182026; }
    .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(115, 134, 148, 0.3);
      color:#182026; }
    .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
      background:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:inherit; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
        background:none;
        cursor:not-allowed;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
      color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
      color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
  .bp3-button-group .bp3-popover-wrapper,
  .bp3-button-group .bp3-popover-target{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button-group .bp3-button.bp3-fill,
  .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-vertical{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column;
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch;
    vertical-align:top; }
    .bp3-button-group.bp3-vertical.bp3-fill{
      width:unset;
      height:100%; }
    .bp3-button-group.bp3-vertical .bp3-button{
      margin-right:0 !important;
      width:100%; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:first-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:first-child{
      border-radius:3px 3px 0 0; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:last-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:last-child{
      border-radius:0 0 3px 3px; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:not(:last-child){
      margin-bottom:-1px; }
  .bp3-button-group.bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    margin-right:1px; }
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-button:not(:last-child){
    margin-bottom:1px; }
.bp3-callout{
  line-height:1.5;
  font-size:14px;
  position:relative;
  border-radius:3px;
  background-color:rgba(138, 155, 168, 0.15);
  width:100%;
  padding:10px 12px 9px; }
  .bp3-callout[class*="bp3-icon-"]{
    padding-left:40px; }
    .bp3-callout[class*="bp3-icon-"]::before{
      line-height:1;
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-weight:400;
      font-style:normal;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      position:absolute;
      top:10px;
      left:10px;
      color:#5c7080; }
  .bp3-callout.bp3-callout-icon{
    padding-left:40px; }
    .bp3-callout.bp3-callout-icon > .bp3-icon:first-child{
      position:absolute;
      top:10px;
      left:10px;
      color:#5c7080; }
  .bp3-callout .bp3-heading{
    margin-top:0;
    margin-bottom:5px;
    line-height:20px; }
    .bp3-callout .bp3-heading:last-child{
      margin-bottom:0; }
  .bp3-dark .bp3-callout{
    background-color:rgba(138, 155, 168, 0.2); }
    .bp3-dark .bp3-callout[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
  .bp3-callout.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15); }
    .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-primary .bp3-heading{
      color:#106ba3; }
    .bp3-dark .bp3-callout.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{
        color:#48aff0; }
  .bp3-callout.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15); }
    .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-success .bp3-heading{
      color:#0d8050; }
    .bp3-dark .bp3-callout.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{
        color:#3dcc91; }
  .bp3-callout.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15); }
    .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-warning .bp3-heading{
      color:#bf7326; }
    .bp3-dark .bp3-callout.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{
        color:#ffb366; }
  .bp3-callout.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15); }
    .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-danger .bp3-heading{
      color:#c23030; }
    .bp3-dark .bp3-callout.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{
        color:#ff7373; }
  .bp3-running-text .bp3-callout{
    margin:20px 0; }
.bp3-card{
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
  background-color:#ffffff;
  padding:20px;
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-card.bp3-dark,
  .bp3-dark .bp3-card{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
    background-color:#30404d; }

.bp3-elevation-0{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }
  .bp3-elevation-0.bp3-dark,
  .bp3-dark .bp3-elevation-0{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }

.bp3-elevation-1{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-1.bp3-dark,
  .bp3-dark .bp3-elevation-1{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-elevation-2{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-2.bp3-dark,
  .bp3-dark .bp3-elevation-2{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); }

.bp3-elevation-3{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-3.bp3-dark,
  .bp3-dark .bp3-elevation-3{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-elevation-4{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-4.bp3-dark,
  .bp3-dark .bp3-elevation-4{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:hover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  cursor:pointer; }
  .bp3-card.bp3-interactive:hover.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:hover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:active{
  opacity:0.9;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  -webkit-transition-duration:0;
          transition-duration:0; }
  .bp3-card.bp3-interactive:active.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:active{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-collapse{
  height:0;
  overflow-y:hidden;
  -webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-collapse .bp3-collapse-body{
    -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-collapse .bp3-collapse-body[aria-hidden="true"]{
      display:none; }

.bp3-context-menu .bp3-popover-target{
  display:block; }

.bp3-context-menu-popover-target{
  position:fixed; }

.bp3-divider{
  margin:5px;
  border-right:1px solid rgba(16, 22, 26, 0.15);
  border-bottom:1px solid rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-divider{
    border-color:rgba(16, 22, 26, 0.4); }
.bp3-dialog-container{
  opacity:1;
  -webkit-transform:scale(1);
          transform:scale(1);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  width:100%;
  min-height:100%;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-dialog-container.bp3-overlay-enter > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5); }
  .bp3-dialog-container.bp3-overlay-enter-active > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear-active > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-dialog-container.bp3-overlay-exit > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-dialog-container.bp3-overlay-exit-active > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5);
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }

.bp3-dialog{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:30px 0;
  border-radius:6px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  background:#ebf1f5;
  width:500px;
  padding-bottom:20px;
  pointer-events:all;
  -webkit-user-select:text;
     -moz-user-select:text;
      -ms-user-select:text;
          user-select:text; }
  .bp3-dialog:focus{
    outline:0; }
  .bp3-dialog.bp3-dark,
  .bp3-dark .bp3-dialog{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    background:#293742;
    color:#f5f8fa; }

.bp3-dialog-header{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border-radius:6px 6px 0 0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  background:#ffffff;
  min-height:40px;
  padding-right:5px;
  padding-left:20px; }
  .bp3-dialog-header .bp3-icon-large,
  .bp3-dialog-header .bp3-icon{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px;
    color:#5c7080; }
  .bp3-dialog-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    margin:0;
    line-height:inherit; }
    .bp3-dialog-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-dialog-header{
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
    background:#30404d; }
    .bp3-dark .bp3-dialog-header .bp3-icon-large,
    .bp3-dark .bp3-dialog-header .bp3-icon{
      color:#a7b6c2; }

.bp3-dialog-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  margin:20px;
  line-height:18px; }

.bp3-dialog-footer{
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  margin:0 20px; }

.bp3-dialog-footer-actions{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:end;
      -ms-flex-pack:end;
          justify-content:flex-end; }
  .bp3-dialog-footer-actions .bp3-button{
    margin-left:10px; }
.bp3-drawer{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  padding:0; }
  .bp3-drawer:focus{
    outline:0; }
  .bp3-drawer.bp3-position-top{
    top:0;
    right:0;
    left:0;
    height:50%; }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter, .bp3-drawer.bp3-position-top.bp3-overlay-appear{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%); }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter-active, .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-position-bottom{
    right:0;
    bottom:0;
    left:0;
    height:50%; }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-position-left{
    top:0;
    bottom:0;
    left:0;
    width:50%; }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter, .bp3-drawer.bp3-position-left.bp3-overlay-appear{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%); }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter-active, .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-position-right{
    top:0;
    right:0;
    bottom:0;
    width:50%; }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter, .bp3-drawer.bp3-position-right.bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter-active, .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right):not(.bp3-vertical){
    top:0;
    right:0;
    bottom:0;
    width:50%; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right).bp3-vertical{
    right:0;
    bottom:0;
    left:0;
    height:50%; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-dark,
  .bp3-dark .bp3-drawer{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    background:#30404d;
    color:#f5f8fa; }

.bp3-drawer-header{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  position:relative;
  border-radius:0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  min-height:40px;
  padding:5px;
  padding-left:20px; }
  .bp3-drawer-header .bp3-icon-large,
  .bp3-drawer-header .bp3-icon{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px;
    color:#5c7080; }
  .bp3-drawer-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    margin:0;
    line-height:inherit; }
    .bp3-drawer-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-drawer-header{
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-drawer-header .bp3-icon-large,
    .bp3-dark .bp3-drawer-header .bp3-icon{
      color:#a7b6c2; }

.bp3-drawer-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  overflow:auto;
  line-height:18px; }

.bp3-drawer-footer{
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  position:relative;
  -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
  padding:10px 20px; }
  .bp3-dark .bp3-drawer-footer{
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); }
.bp3-editable-text{
  display:inline-block;
  position:relative;
  cursor:text;
  max-width:100%;
  vertical-align:top;
  white-space:nowrap; }
  .bp3-editable-text::before{
    position:absolute;
    top:-3px;
    right:-3px;
    bottom:-3px;
    left:-3px;
    border-radius:3px;
    content:"";
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-editable-text.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
    background-color:#ffffff; }
  .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#137cbd; }
  .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); }
  .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#0f9960; }
  .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); }
  .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#d9822b; }
  .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); }
  .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#db3737; }
  .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); }
  .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); }
  .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background-color:rgba(16, 22, 26, 0.3); }
  .bp3-dark .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#48aff0; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4);
            box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#3dcc91; }
  .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4);
            box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#ffb366; }
  .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4);
            box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#ff7373; }
  .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4);
            box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-editable-text-input,
.bp3-editable-text-content{
  display:inherit;
  position:relative;
  min-width:inherit;
  max-width:inherit;
  vertical-align:top;
  text-transform:inherit;
  letter-spacing:inherit;
  color:inherit;
  font:inherit;
  resize:none; }

.bp3-editable-text-input{
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  background:none;
  width:100%;
  padding:0;
  white-space:pre-wrap; }
  .bp3-editable-text-input::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input:focus{
    outline:none; }
  .bp3-editable-text-input::-ms-clear{
    display:none; }

.bp3-editable-text-content{
  overflow:hidden;
  padding-right:2px;
  text-overflow:ellipsis;
  white-space:pre; }
  .bp3-editable-text-editing > .bp3-editable-text-content{
    position:absolute;
    left:0;
    visibility:hidden; }
  .bp3-editable-text-placeholder > .bp3-editable-text-content{
    color:rgba(92, 112, 128, 0.6); }
    .bp3-dark .bp3-editable-text-placeholder > .bp3-editable-text-content{
      color:rgba(167, 182, 194, 0.6); }

.bp3-editable-text.bp3-multiline{
  display:block; }
  .bp3-editable-text.bp3-multiline .bp3-editable-text-content{
    overflow:auto;
    white-space:pre-wrap;
    word-wrap:break-word; }
.bp3-control-group{
  -webkit-transform:translateZ(0);
          transform:translateZ(0);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:stretch;
      -ms-flex-align:stretch;
          align-items:stretch; }
  .bp3-control-group > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select,
  .bp3-control-group .bp3-input,
  .bp3-control-group .bp3-select{
    position:relative; }
  .bp3-control-group .bp3-input{
    z-index:2;
    border-radius:inherit; }
    .bp3-control-group .bp3-input:focus{
      z-index:14;
      border-radius:3px; }
    .bp3-control-group .bp3-input[class*="bp3-intent"]{
      z-index:13; }
      .bp3-control-group .bp3-input[class*="bp3-intent"]:focus{
        z-index:15; }
    .bp3-control-group .bp3-input[readonly], .bp3-control-group .bp3-input:disabled, .bp3-control-group .bp3-input.bp3-disabled{
      z-index:1; }
  .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input{
    z-index:13; }
    .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input:focus{
      z-index:15; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select select,
  .bp3-control-group .bp3-select select{
    -webkit-transform:translateZ(0);
            transform:translateZ(0);
    z-index:4;
    border-radius:inherit; }
    .bp3-control-group .bp3-button:focus,
    .bp3-control-group .bp3-html-select select:focus,
    .bp3-control-group .bp3-select select:focus{
      z-index:5; }
    .bp3-control-group .bp3-button:hover,
    .bp3-control-group .bp3-html-select select:hover,
    .bp3-control-group .bp3-select select:hover{
      z-index:6; }
    .bp3-control-group .bp3-button:active,
    .bp3-control-group .bp3-html-select select:active,
    .bp3-control-group .bp3-select select:active{
      z-index:7; }
    .bp3-control-group .bp3-button[readonly], .bp3-control-group .bp3-button:disabled, .bp3-control-group .bp3-button.bp3-disabled,
    .bp3-control-group .bp3-html-select select[readonly],
    .bp3-control-group .bp3-html-select select:disabled,
    .bp3-control-group .bp3-html-select select.bp3-disabled,
    .bp3-control-group .bp3-select select[readonly],
    .bp3-control-group .bp3-select select:disabled,
    .bp3-control-group .bp3-select select.bp3-disabled{
      z-index:3; }
    .bp3-control-group .bp3-button[class*="bp3-intent"],
    .bp3-control-group .bp3-html-select select[class*="bp3-intent"],
    .bp3-control-group .bp3-select select[class*="bp3-intent"]{
      z-index:9; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:focus{
        z-index:10; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:hover{
        z-index:11; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:active{
        z-index:12; }
      .bp3-control-group .bp3-button[class*="bp3-intent"][readonly], .bp3-control-group .bp3-button[class*="bp3-intent"]:disabled, .bp3-control-group .bp3-button[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"].bp3-disabled{
        z-index:8; }
  .bp3-control-group .bp3-input-group > .bp3-icon,
  .bp3-control-group .bp3-input-group > .bp3-button,
  .bp3-control-group .bp3-input-group > .bp3-input-action{
    z-index:16; }
  .bp3-control-group .bp3-select::after,
  .bp3-control-group .bp3-html-select::after,
  .bp3-control-group .bp3-select > .bp3-icon,
  .bp3-control-group .bp3-html-select > .bp3-icon{
    z-index:17; }
  .bp3-control-group:not(.bp3-vertical) > *{
    margin-right:-1px; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > *{
    margin-right:0; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > .bp3-button + .bp3-button{
    margin-left:1px; }
  .bp3-control-group .bp3-popover-wrapper,
  .bp3-control-group .bp3-popover-target{
    border-radius:inherit; }
  .bp3-control-group > :first-child{
    border-radius:3px 0 0 3px; }
  .bp3-control-group > :last-child{
    margin-right:0;
    border-radius:0 3px 3px 0; }
  .bp3-control-group > :only-child{
    margin-right:0;
    border-radius:3px; }
  .bp3-control-group .bp3-input-group .bp3-button{
    border-radius:3px; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-fill > *:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-vertical{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column; }
    .bp3-control-group.bp3-vertical > *{
      margin-top:-1px; }
    .bp3-control-group.bp3-vertical > :first-child{
      margin-top:0;
      border-radius:3px 3px 0 0; }
    .bp3-control-group.bp3-vertical > :last-child{
      border-radius:0 0 3px 3px; }
.bp3-control{
  display:block;
  position:relative;
  margin-bottom:10px;
  cursor:pointer;
  text-transform:none; }
  .bp3-control input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
  .bp3-control:hover input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#106ba3; }
  .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background:#0e5a8a; }
  .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(19, 124, 189, 0.5); }
  .bp3-dark .bp3-control input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control:hover input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#106ba3; }
  .bp3-dark .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#0e5a8a; }
  .bp3-dark .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(14, 90, 138, 0.5); }
  .bp3-control:not(.bp3-align-right){
    padding-left:26px; }
    .bp3-control:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-26px; }
  .bp3-control.bp3-align-right{
    padding-right:26px; }
    .bp3-control.bp3-align-right .bp3-control-indicator{
      margin-right:-26px; }
  .bp3-control.bp3-disabled{
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-control.bp3-inline{
    display:inline-block;
    margin-right:20px; }
  .bp3-control input{
    position:absolute;
    top:0;
    left:0;
    opacity:0;
    z-index:-1; }
  .bp3-control .bp3-control-indicator{
    display:inline-block;
    position:relative;
    margin-top:-3px;
    margin-right:10px;
    border:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    cursor:pointer;
    width:1em;
    height:1em;
    vertical-align:middle;
    font-size:16px;
    -webkit-user-select:none;
       -moz-user-select:none;
        -ms-user-select:none;
            user-select:none; }
    .bp3-control .bp3-control-indicator::before{
      display:block;
      width:1em;
      height:1em;
      content:""; }
  .bp3-control:hover .bp3-control-indicator{
    background-color:#ebf1f5; }
  .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background:#d8e1e8; }
  .bp3-control input:disabled ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(206, 217, 224, 0.5);
    cursor:not-allowed; }
  .bp3-control input:focus ~ .bp3-control-indicator{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:2px;
    -moz-outline-radius:6px; }
  .bp3-control.bp3-align-right .bp3-control-indicator{
    float:right;
    margin-top:1px;
    margin-left:10px; }
  .bp3-control.bp3-large{
    font-size:16px; }
    .bp3-control.bp3-large:not(.bp3-align-right){
      padding-left:30px; }
      .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
        margin-left:-30px; }
    .bp3-control.bp3-large.bp3-align-right{
      padding-right:30px; }
      .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
        margin-right:-30px; }
    .bp3-control.bp3-large .bp3-control-indicator{
      font-size:20px; }
    .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-top:0; }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
  .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#106ba3; }
  .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background:#0e5a8a; }
  .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(19, 124, 189, 0.5); }
  .bp3-dark .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#106ba3; }
  .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#0e5a8a; }
  .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(14, 90, 138, 0.5); }
  .bp3-control.bp3-checkbox .bp3-control-indicator{
    border-radius:3px; }
  .bp3-control.bp3-checkbox input:checked ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-radio .bp3-control-indicator{
    border-radius:50%; }
  .bp3-control.bp3-radio input:checked ~ .bp3-control-indicator::before{
    background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%); }
  .bp3-control.bp3-radio input:checked:disabled ~ .bp3-control-indicator::before{
    opacity:0.5; }
  .bp3-control.bp3-radio input:focus ~ .bp3-control-indicator{
    -moz-outline-radius:16px; }
  .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(167, 182, 194, 0.5); }
  .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(115, 134, 148, 0.5); }
  .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(92, 112, 128, 0.5); }
  .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(206, 217, 224, 0.5); }
    .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5); }
    .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch:not(.bp3-align-right){
    padding-left:38px; }
    .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-38px; }
  .bp3-control.bp3-switch.bp3-align-right{
    padding-right:38px; }
    .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{
      margin-right:-38px; }
  .bp3-control.bp3-switch .bp3-control-indicator{
    border:none;
    border-radius:1.75em;
    -webkit-box-shadow:none !important;
            box-shadow:none !important;
    width:auto;
    min-width:1.75em;
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-control.bp3-switch .bp3-control-indicator::before{
      position:absolute;
      left:0;
      margin:2px;
      border-radius:50%;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
      background:#ffffff;
      width:calc(1em - 4px);
      height:calc(1em - 4px);
      -webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
      transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    left:calc(100% - 1em); }
  .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){
    padding-left:45px; }
    .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-45px; }
  .bp3-control.bp3-switch.bp3-large.bp3-align-right{
    padding-right:45px; }
    .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-right:-45px; }
  .bp3-dark .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.7); }
  .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.9); }
  .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(57, 75, 89, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-dark .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background:#394b59; }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-control.bp3-switch .bp3-switch-inner-text{
    text-align:center;
    font-size:0.7em; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{
    visibility:hidden;
    margin-right:1.2em;
    margin-left:0.5em;
    line-height:0; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{
    visibility:visible;
    margin-right:0.5em;
    margin-left:1.2em;
    line-height:1em; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:first-child{
    visibility:visible;
    line-height:1em; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:last-child{
    visibility:hidden;
    line-height:0; }
  .bp3-dark .bp3-control{
    color:#f5f8fa; }
    .bp3-dark .bp3-control.bp3-disabled{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-control .bp3-control-indicator{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); }
    .bp3-dark .bp3-control:hover .bp3-control-indicator{
      background-color:#30404d; }
    .bp3-dark .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background:#202b33; }
    .bp3-dark .bp3-control input:disabled ~ .bp3-control-indicator{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      cursor:not-allowed; }
    .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked ~ .bp3-control-indicator, .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
      color:rgba(167, 182, 194, 0.6); }
.bp3-file-input{
  display:inline-block;
  position:relative;
  cursor:pointer;
  height:30px; }
  .bp3-file-input input{
    opacity:0;
    margin:0;
    min-width:200px; }
    .bp3-file-input input:disabled + .bp3-file-upload-input,
    .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(206, 217, 224, 0.5);
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6);
      resize:none; }
      .bp3-file-input input:disabled + .bp3-file-upload-input::after,
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
        outline:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        background-color:rgba(206, 217, 224, 0.5);
        background-image:none;
        cursor:not-allowed;
        color:rgba(92, 112, 128, 0.6); }
        .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active:hover,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active:hover{
          background:rgba(206, 217, 224, 0.7); }
      .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-dark
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:rgba(57, 75, 89, 0.5);
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-dark
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
          -webkit-box-shadow:none;
                  box-shadow:none;
          background-color:rgba(57, 75, 89, 0.5);
          background-image:none;
          color:rgba(167, 182, 194, 0.6); }
          .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-dark
          .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active{
            background:rgba(57, 75, 89, 0.7); }
  .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#182026; }
  .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#f5f8fa; }
  .bp3-file-input.bp3-fill{
    width:100%; }
  .bp3-file-input.bp3-large,
  .bp3-large .bp3-file-input{
    height:40px; }
  .bp3-file-input .bp3-file-upload-input-custom-text::after{
    content:attr(bp3-button-text); }

.bp3-file-upload-input{
  outline:none;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  height:30px;
  padding:0 10px;
  vertical-align:middle;
  line-height:30px;
  color:#182026;
  font-size:14px;
  font-weight:400;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none;
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  position:absolute;
  top:0;
  right:0;
  left:0;
  padding-right:80px;
  color:rgba(92, 112, 128, 0.6);
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-file-upload-input::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input:focus, .bp3-file-upload-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-file-upload-input[type="search"], .bp3-file-upload-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-file-upload-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-file-upload-input:disabled, .bp3-file-upload-input.bp3-disabled{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(206, 217, 224, 0.5);
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6);
    resize:none; }
  .bp3-file-upload-input::after{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    color:#182026;
    min-width:24px;
    min-height:24px;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    position:absolute;
    top:0;
    right:0;
    margin:3px;
    border-radius:3px;
    width:70px;
    text-align:center;
    line-height:24px;
    content:"Browse"; }
    .bp3-file-upload-input::after:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
      background-clip:padding-box;
      background-color:#ebf1f5; }
    .bp3-file-upload-input::after:active, .bp3-file-upload-input::after.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#d8e1e8;
      background-image:none; }
    .bp3-file-upload-input::after:disabled, .bp3-file-upload-input::after.bp3-disabled{
      outline:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-file-upload-input::after:disabled.bp3-active, .bp3-file-upload-input::after:disabled.bp3-active:hover, .bp3-file-upload-input::after.bp3-disabled.bp3-active, .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-file-upload-input:hover::after{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5; }
  .bp3-file-upload-input:active::after{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none; }
  .bp3-large .bp3-file-upload-input{
    height:40px;
    line-height:40px;
    font-size:16px;
    padding-right:95px; }
    .bp3-large .bp3-file-upload-input[type="search"], .bp3-large .bp3-file-upload-input.bp3-round{
      padding:0 15px; }
    .bp3-large .bp3-file-upload-input::after{
      min-width:30px;
      min-height:30px;
      margin:5px;
      width:85px;
      line-height:30px; }
  .bp3-dark .bp3-file-upload-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input:disabled, .bp3-dark .bp3-file-upload-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::after{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
      color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover, .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover{
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
        background-color:#30404d; }
      .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
        background-color:#202b33;
        background-image:none; }
      .bp3-dark .bp3-file-upload-input::after:disabled, .bp3-dark .bp3-file-upload-input::after.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none;
        background-color:rgba(57, 75, 89, 0.5);
        background-image:none;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active, .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{
          background:rgba(57, 75, 89, 0.7); }
      .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{
        background:rgba(16, 22, 26, 0.5);
        stroke:#8a9ba8; }
    .bp3-dark .bp3-file-upload-input:hover::after{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#30404d; }
    .bp3-dark .bp3-file-upload-input:active::after{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#202b33;
      background-image:none; }

.bp3-file-upload-input::after{
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
.bp3-form-group{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0 0 15px; }
  .bp3-form-group label.bp3-label{
    margin-bottom:5px; }
  .bp3-form-group .bp3-control{
    margin-top:7px; }
  .bp3-form-group .bp3-form-helper-text{
    margin-top:5px;
    color:#5c7080;
    font-size:12px; }
  .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#106ba3; }
  .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#0d8050; }
  .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#bf7326; }
  .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#c23030; }
  .bp3-form-group.bp3-inline{
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row;
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
    .bp3-form-group.bp3-inline.bp3-large label.bp3-label{
      margin:0 10px 0 0;
      line-height:40px; }
    .bp3-form-group.bp3-inline label.bp3-label{
      margin:0 10px 0 0;
      line-height:30px; }
  .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#48aff0; }
  .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#3dcc91; }
  .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#ffb366; }
  .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#ff7373; }
  .bp3-dark .bp3-form-group .bp3-form-helper-text{
    color:#a7b6c2; }
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(167, 182, 194, 0.6) !important; }
.bp3-input-group{
  display:block;
  position:relative; }
  .bp3-input-group .bp3-input{
    position:relative;
    width:100%; }
    .bp3-input-group .bp3-input:not(:first-child){
      padding-left:30px; }
    .bp3-input-group .bp3-input:not(:last-child){
      padding-right:30px; }
  .bp3-input-group .bp3-input-action,
  .bp3-input-group > .bp3-button,
  .bp3-input-group > .bp3-icon{
    position:absolute;
    top:0; }
    .bp3-input-group .bp3-input-action:first-child,
    .bp3-input-group > .bp3-button:first-child,
    .bp3-input-group > .bp3-icon:first-child{
      left:0; }
    .bp3-input-group .bp3-input-action:last-child,
    .bp3-input-group > .bp3-button:last-child,
    .bp3-input-group > .bp3-icon:last-child{
      right:0; }
  .bp3-input-group .bp3-button{
    min-width:24px;
    min-height:24px;
    margin:3px;
    padding:0 7px; }
    .bp3-input-group .bp3-button:empty{
      padding:0; }
  .bp3-input-group > .bp3-icon{
    z-index:1;
    color:#5c7080; }
    .bp3-input-group > .bp3-icon:empty{
      line-height:1;
      font-family:"Icons16", sans-serif;
      font-size:16px;
      font-weight:400;
      font-style:normal;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased; }
  .bp3-input-group > .bp3-icon,
  .bp3-input-group .bp3-input-action > .bp3-spinner{
    margin:7px; }
  .bp3-input-group .bp3-tag{
    margin:5px; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus),
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
    color:#5c7080; }
    .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-dark
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
      color:#a7b6c2; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{
      color:#5c7080; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled,
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled{
    color:rgba(92, 112, 128, 0.6) !important; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-input-group.bp3-disabled{
    cursor:not-allowed; }
    .bp3-input-group.bp3-disabled .bp3-icon{
      color:rgba(92, 112, 128, 0.6); }
  .bp3-input-group.bp3-large .bp3-button{
    min-width:30px;
    min-height:30px;
    margin:5px; }
  .bp3-input-group.bp3-large > .bp3-icon,
  .bp3-input-group.bp3-large .bp3-input-action > .bp3-spinner{
    margin:12px; }
  .bp3-input-group.bp3-large .bp3-input{
    height:40px;
    line-height:40px;
    font-size:16px; }
    .bp3-input-group.bp3-large .bp3-input[type="search"], .bp3-input-group.bp3-large .bp3-input.bp3-round{
      padding:0 15px; }
    .bp3-input-group.bp3-large .bp3-input:not(:first-child){
      padding-left:40px; }
    .bp3-input-group.bp3-large .bp3-input:not(:last-child){
      padding-right:40px; }
  .bp3-input-group.bp3-small .bp3-button{
    min-width:20px;
    min-height:20px;
    margin:2px; }
  .bp3-input-group.bp3-small .bp3-tag{
    min-width:20px;
    min-height:20px;
    margin:2px; }
  .bp3-input-group.bp3-small > .bp3-icon,
  .bp3-input-group.bp3-small .bp3-input-action > .bp3-spinner{
    margin:4px; }
  .bp3-input-group.bp3-small .bp3-input{
    height:24px;
    padding-right:8px;
    padding-left:8px;
    line-height:24px;
    font-size:12px; }
    .bp3-input-group.bp3-small .bp3-input[type="search"], .bp3-input-group.bp3-small .bp3-input.bp3-round{
      padding:0 12px; }
    .bp3-input-group.bp3-small .bp3-input:not(:first-child){
      padding-left:24px; }
    .bp3-input-group.bp3-small .bp3-input:not(:last-child){
      padding-right:24px; }
  .bp3-input-group.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-input-group.bp3-round .bp3-button,
  .bp3-input-group.bp3-round .bp3-input,
  .bp3-input-group.bp3-round .bp3-tag{
    border-radius:30px; }
  .bp3-dark .bp3-input-group .bp3-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-input-group.bp3-intent-primary .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input-group.bp3-intent-primary .bp3-input:disabled, .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-primary > .bp3-icon{
    color:#106ba3; }
    .bp3-dark .bp3-input-group.bp3-intent-primary > .bp3-icon{
      color:#48aff0; }
  .bp3-input-group.bp3-intent-success .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input-group.bp3-intent-success .bp3-input:disabled, .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-success > .bp3-icon{
    color:#0d8050; }
    .bp3-dark .bp3-input-group.bp3-intent-success > .bp3-icon{
      color:#3dcc91; }
  .bp3-input-group.bp3-intent-warning .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input-group.bp3-intent-warning .bp3-input:disabled, .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-warning > .bp3-icon{
    color:#bf7326; }
    .bp3-dark .bp3-input-group.bp3-intent-warning > .bp3-icon{
      color:#ffb366; }
  .bp3-input-group.bp3-intent-danger .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input-group.bp3-intent-danger .bp3-input:disabled, .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-danger > .bp3-icon{
    color:#c23030; }
    .bp3-dark .bp3-input-group.bp3-intent-danger > .bp3-icon{
      color:#ff7373; }
.bp3-input{
  outline:none;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  height:30px;
  padding:0 10px;
  vertical-align:middle;
  line-height:30px;
  color:#182026;
  font-size:14px;
  font-weight:400;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none; }
  .bp3-input::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input:focus, .bp3-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-input[type="search"], .bp3-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-input:disabled, .bp3-input.bp3-disabled{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(206, 217, 224, 0.5);
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6);
    resize:none; }
  .bp3-input.bp3-large{
    height:40px;
    line-height:40px;
    font-size:16px; }
    .bp3-input.bp3-large[type="search"], .bp3-input.bp3-large.bp3-round{
      padding:0 15px; }
  .bp3-input.bp3-small{
    height:24px;
    padding-right:8px;
    padding-left:8px;
    line-height:24px;
    font-size:12px; }
    .bp3-input.bp3-small[type="search"], .bp3-input.bp3-small.bp3-round{
      padding:0 12px; }
  .bp3-input.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-dark .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa; }
    .bp3-dark .bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input:disabled, .bp3-dark .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      color:rgba(167, 182, 194, 0.6); }
  .bp3-input.bp3-intent-primary{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input.bp3-intent-primary:disabled, .bp3-input.bp3-intent-primary.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary:focus{
        -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #137cbd;
                box-shadow:inset 0 0 0 1px #137cbd; }
      .bp3-dark .bp3-input.bp3-intent-primary:disabled, .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-success{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input.bp3-intent-success:disabled, .bp3-input.bp3-intent-success.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-success{
      -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success:focus{
        -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #0f9960;
                box-shadow:inset 0 0 0 1px #0f9960; }
      .bp3-dark .bp3-input.bp3-intent-success:disabled, .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-warning{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input.bp3-intent-warning:disabled, .bp3-input.bp3-intent-warning.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning:focus{
        -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #d9822b;
                box-shadow:inset 0 0 0 1px #d9822b; }
      .bp3-dark .bp3-input.bp3-intent-warning:disabled, .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-danger{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input.bp3-intent-danger:disabled, .bp3-input.bp3-intent-danger.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger:focus{
        -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #db3737;
                box-shadow:inset 0 0 0 1px #db3737; }
      .bp3-dark .bp3-input.bp3-intent-danger:disabled, .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input::-ms-clear{
    display:none; }
textarea.bp3-input{
  max-width:100%;
  padding:10px; }
  textarea.bp3-input, textarea.bp3-input.bp3-large, textarea.bp3-input.bp3-small{
    height:auto;
    line-height:inherit; }
  textarea.bp3-input.bp3-small{
    padding:8px; }
  .bp3-dark textarea.bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa; }
    .bp3-dark textarea.bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input:disabled, .bp3-dark textarea.bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      color:rgba(167, 182, 194, 0.6); }
label.bp3-label{
  display:block;
  margin-top:0;
  margin-bottom:15px; }
  label.bp3-label .bp3-html-select,
  label.bp3-label .bp3-input,
  label.bp3-label .bp3-select,
  label.bp3-label .bp3-slider,
  label.bp3-label .bp3-popover-wrapper{
    display:block;
    margin-top:5px;
    text-transform:none; }
  label.bp3-label .bp3-button-group{
    margin-top:5px; }
  label.bp3-label .bp3-select select,
  label.bp3-label .bp3-html-select select{
    width:100%;
    vertical-align:top;
    font-weight:400; }
  label.bp3-label.bp3-disabled,
  label.bp3-label.bp3-disabled .bp3-text-muted{
    color:rgba(92, 112, 128, 0.6); }
  label.bp3-label.bp3-inline{
    line-height:30px; }
    label.bp3-label.bp3-inline .bp3-html-select,
    label.bp3-label.bp3-inline .bp3-input,
    label.bp3-label.bp3-inline .bp3-input-group,
    label.bp3-label.bp3-inline .bp3-select,
    label.bp3-label.bp3-inline .bp3-popover-wrapper{
      display:inline-block;
      margin:0 0 0 5px;
      vertical-align:top; }
    label.bp3-label.bp3-inline .bp3-button-group{
      margin:0 0 0 5px; }
    label.bp3-label.bp3-inline .bp3-input-group .bp3-input{
      margin-left:0; }
    label.bp3-label.bp3-inline.bp3-large{
      line-height:40px; }
  label.bp3-label:not(.bp3-inline) .bp3-popover-target{
    display:block; }
  .bp3-dark label.bp3-label{
    color:#f5f8fa; }
    .bp3-dark label.bp3-label.bp3-disabled,
    .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{
      color:rgba(167, 182, 194, 0.6); }
.bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button{
  -webkit-box-flex:1;
      -ms-flex:1 1 14px;
          flex:1 1 14px;
  width:30px;
  min-height:0;
  padding:0; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:first-child{
    border-radius:0 3px 0 0; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:last-child{
    border-radius:0 0 3px 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:first-child{
  border-radius:3px 0 0 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:last-child{
  border-radius:0 0 0 3px; }

.bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical > .bp3-button{
  width:40px; }

form{
  display:block; }
.bp3-html-select select,
.bp3-select select{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  padding:5px 10px;
  vertical-align:middle;
  text-align:left;
  font-size:14px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  color:#182026;
  border-radius:3px;
  width:100%;
  height:30px;
  padding:0 25px 0 10px;
  -moz-appearance:none;
  -webkit-appearance:none; }
  .bp3-html-select select > *, .bp3-select select > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-html-select select > .bp3-fill, .bp3-select select > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-html-select select::before,
  .bp3-select select::before, .bp3-html-select select > *, .bp3-select select > *{
    margin-right:7px; }
  .bp3-html-select select:empty::before,
  .bp3-select select:empty::before,
  .bp3-html-select select > :last-child,
  .bp3-select select > :last-child{
    margin-right:0; }
  .bp3-html-select select:hover,
  .bp3-select select:hover{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5; }
  .bp3-html-select select:active,
  .bp3-select select:active, .bp3-html-select select.bp3-active,
  .bp3-select select.bp3-active{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none; }
  .bp3-html-select select:disabled,
  .bp3-select select:disabled, .bp3-html-select select.bp3-disabled,
  .bp3-select select.bp3-disabled{
    outline:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
    .bp3-html-select select:disabled.bp3-active,
    .bp3-select select:disabled.bp3-active, .bp3-html-select select:disabled.bp3-active:hover,
    .bp3-select select:disabled.bp3-active:hover, .bp3-html-select select.bp3-disabled.bp3-active,
    .bp3-select select.bp3-disabled.bp3-active, .bp3-html-select select.bp3-disabled.bp3-active:hover,
    .bp3-select select.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }

.bp3-html-select.bp3-minimal select,
.bp3-select.bp3-minimal select{
  -webkit-box-shadow:none;
          box-shadow:none;
  background:none; }
  .bp3-html-select.bp3-minimal select:hover,
  .bp3-select.bp3-minimal select:hover{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(167, 182, 194, 0.3);
    text-decoration:none;
    color:#182026; }
  .bp3-html-select.bp3-minimal select:active,
  .bp3-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal select.bp3-active,
  .bp3-select.bp3-minimal select.bp3-active{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(115, 134, 148, 0.3);
    color:#182026; }
  .bp3-html-select.bp3-minimal select:disabled,
  .bp3-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal select:disabled:hover,
  .bp3-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal select.bp3-disabled,
  .bp3-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal select.bp3-disabled:hover,
  .bp3-select.bp3-minimal select.bp3-disabled:hover{
    background:none;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
    .bp3-html-select.bp3-minimal select:disabled.bp3-active,
    .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{
      background:rgba(115, 134, 148, 0.3); }
  .bp3-dark .bp3-html-select.bp3-minimal select, .bp3-html-select.bp3-minimal .bp3-dark select,
  .bp3-dark .bp3-select.bp3-minimal select, .bp3-select.bp3-minimal .bp3-dark select{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none;
    color:inherit; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover{
      background:rgba(138, 155, 168, 0.15); }
    .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      background:rgba(138, 155, 168, 0.3);
      color:#f5f8fa; }
    .bp3-dark .bp3-html-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal .bp3-dark select:disabled,
    .bp3-dark .bp3-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{
      background:none;
      cursor:not-allowed;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{
        background:rgba(138, 155, 168, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-primary,
  .bp3-select.bp3-minimal select.bp3-intent-primary{
    color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover{
      background:rgba(19, 124, 189, 0.15);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      background:rgba(19, 124, 189, 0.3);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{
      background:none;
      color:rgba(16, 107, 163, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{
        background:rgba(19, 124, 189, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
      stroke:#106ba3; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{
      color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.2);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(72, 175, 240, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-success,
  .bp3-select.bp3-minimal select.bp3-intent-success{
    color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover{
      background:rgba(15, 153, 96, 0.15);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      background:rgba(15, 153, 96, 0.3);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{
      background:none;
      color:rgba(13, 128, 80, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{
        background:rgba(15, 153, 96, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
      stroke:#0d8050; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{
      color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.2);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(61, 204, 145, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-warning,
  .bp3-select.bp3-minimal select.bp3-intent-warning{
    color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover{
      background:rgba(217, 130, 43, 0.15);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      background:rgba(217, 130, 43, 0.3);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{
      background:none;
      color:rgba(191, 115, 38, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{
        background:rgba(217, 130, 43, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
      stroke:#bf7326; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{
      color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.2);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(255, 179, 102, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-danger,
  .bp3-select.bp3-minimal select.bp3-intent-danger{
    color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover{
      background:rgba(219, 55, 55, 0.15);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      background:rgba(219, 55, 55, 0.3);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{
      background:none;
      color:rgba(194, 48, 48, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{
        background:rgba(219, 55, 55, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
      stroke:#c23030; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{
      color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.2);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(255, 115, 115, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }

.bp3-html-select.bp3-large select,
.bp3-select.bp3-large select{
  height:40px;
  padding-right:35px;
  font-size:16px; }

.bp3-dark .bp3-html-select select, .bp3-dark .bp3-select select{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
  background-color:#394b59;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
  color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover, .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#30404d; }
  .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#202b33;
    background-image:none; }
  .bp3-dark .bp3-html-select select:disabled, .bp3-dark .bp3-select select:disabled, .bp3-dark .bp3-html-select select.bp3-disabled, .bp3-dark .bp3-select select.bp3-disabled{
    -webkit-box-shadow:none;
            box-shadow:none;
    background-color:rgba(57, 75, 89, 0.5);
    background-image:none;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-html-select select:disabled.bp3-active, .bp3-dark .bp3-select select:disabled.bp3-active, .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active, .bp3-dark .bp3-select select.bp3-disabled.bp3-active{
      background:rgba(57, 75, 89, 0.7); }
  .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head, .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{
    background:rgba(16, 22, 26, 0.5);
    stroke:#8a9ba8; }

.bp3-html-select select:disabled,
.bp3-select select:disabled{
  -webkit-box-shadow:none;
          box-shadow:none;
  background-color:rgba(206, 217, 224, 0.5);
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-html-select .bp3-icon,
.bp3-select .bp3-icon, .bp3-select::after{
  position:absolute;
  top:7px;
  right:7px;
  color:#5c7080;
  pointer-events:none; }
  .bp3-html-select .bp3-disabled.bp3-icon,
  .bp3-select .bp3-disabled.bp3-icon, .bp3-disabled.bp3-select::after{
    color:rgba(92, 112, 128, 0.6); }
.bp3-html-select,
.bp3-select{
  display:inline-block;
  position:relative;
  vertical-align:middle;
  letter-spacing:normal; }
  .bp3-html-select select::-ms-expand,
  .bp3-select select::-ms-expand{
    display:none; }
  .bp3-html-select .bp3-icon,
  .bp3-select .bp3-icon{
    color:#5c7080; }
    .bp3-html-select .bp3-icon:hover,
    .bp3-select .bp3-icon:hover{
      color:#182026; }
    .bp3-dark .bp3-html-select .bp3-icon, .bp3-dark
    .bp3-select .bp3-icon{
      color:#a7b6c2; }
      .bp3-dark .bp3-html-select .bp3-icon:hover, .bp3-dark
      .bp3-select .bp3-icon:hover{
        color:#f5f8fa; }
  .bp3-html-select.bp3-large::after,
  .bp3-html-select.bp3-large .bp3-icon,
  .bp3-select.bp3-large::after,
  .bp3-select.bp3-large .bp3-icon{
    top:12px;
    right:12px; }
  .bp3-html-select.bp3-fill,
  .bp3-html-select.bp3-fill select,
  .bp3-select.bp3-fill,
  .bp3-select.bp3-fill select{
    width:100%; }
  .bp3-dark .bp3-html-select option, .bp3-dark
  .bp3-select option{
    background-color:#30404d;
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select::after, .bp3-dark
  .bp3-select::after{
    color:#a7b6c2; }

.bp3-select::after{
  line-height:1;
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-weight:400;
  font-style:normal;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  content:""; }
.bp3-running-text table, table.bp3-html-table{
  border-spacing:0;
  font-size:14px; }
  .bp3-running-text table th, table.bp3-html-table th,
  .bp3-running-text table td,
  table.bp3-html-table td{
    padding:11px;
    vertical-align:top;
    text-align:left; }
  .bp3-running-text table th, table.bp3-html-table th{
    color:#182026;
    font-weight:600; }
  
  .bp3-running-text table td,
  table.bp3-html-table td{
    color:#182026; }
  .bp3-running-text table tbody tr:first-child th, table.bp3-html-table tbody tr:first-child th,
  .bp3-running-text table tbody tr:first-child td,
  table.bp3-html-table tbody tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-running-text table th, .bp3-running-text .bp3-dark table th, .bp3-dark table.bp3-html-table th{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table td, .bp3-running-text .bp3-dark table td, .bp3-dark table.bp3-html-table td{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table tbody tr:first-child th, .bp3-running-text .bp3-dark table tbody tr:first-child th, .bp3-dark table.bp3-html-table tbody tr:first-child th,
  .bp3-dark .bp3-running-text table tbody tr:first-child td,
  .bp3-running-text .bp3-dark table tbody tr:first-child td,
  .bp3-dark table.bp3-html-table tbody tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }

table.bp3-html-table.bp3-html-table-condensed th,
table.bp3-html-table.bp3-html-table-condensed td, table.bp3-html-table.bp3-small th,
table.bp3-html-table.bp3-small td{
  padding-top:6px;
  padding-bottom:6px; }

table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
  background:rgba(191, 204, 214, 0.15); }

table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
  -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered tbody tr td{
  -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){
    -webkit-box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
  -webkit-box-shadow:none;
          box-shadow:none; }
  table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){
    -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-interactive tbody tr:hover td{
  background-color:rgba(191, 204, 214, 0.3);
  cursor:pointer; }

table.bp3-html-table.bp3-interactive tbody tr:active td{
  background-color:rgba(191, 204, 214, 0.4); }

.bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
  background:rgba(92, 112, 128, 0.15); }

.bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
  -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }

.bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{
  -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
          box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){
    -webkit-box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); }

.bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
  -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{
    -webkit-box-shadow:none;
            box-shadow:none; }

.bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{
  background-color:rgba(92, 112, 128, 0.3);
  cursor:pointer; }

.bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{
  background-color:rgba(92, 112, 128, 0.4); }

.bp3-key-combo{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center; }
  .bp3-key-combo > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-key-combo > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-key-combo::before,
  .bp3-key-combo > *{
    margin-right:5px; }
  .bp3-key-combo:empty::before,
  .bp3-key-combo > :last-child{
    margin-right:0; }

.bp3-hotkey-dialog{
  top:40px;
  padding-bottom:0; }
  .bp3-hotkey-dialog .bp3-dialog-body{
    margin:0;
    padding:0; }
  .bp3-hotkey-dialog .bp3-hotkey-label{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1; }

.bp3-hotkey-column{
  margin:auto;
  max-height:80vh;
  overflow-y:auto;
  padding:30px; }
  .bp3-hotkey-column .bp3-heading{
    margin-bottom:20px; }
    .bp3-hotkey-column .bp3-heading:not(:first-child){
      margin-top:40px; }

.bp3-hotkey{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:justify;
      -ms-flex-pack:justify;
          justify-content:space-between;
  margin-right:0;
  margin-left:0; }
  .bp3-hotkey:not(:last-child){
    margin-bottom:10px; }
.bp3-icon{
  display:inline-block;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  vertical-align:text-bottom; }
  .bp3-icon:not(:empty)::before{
    content:"" !important;
    content:unset !important; }
  .bp3-icon > svg{
    display:block; }
    .bp3-icon > svg:not([fill]){
      fill:currentColor; }

.bp3-icon.bp3-intent-primary, .bp3-icon-standard.bp3-intent-primary, .bp3-icon-large.bp3-intent-primary{
  color:#106ba3; }
  .bp3-dark .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-icon-large.bp3-intent-primary{
    color:#48aff0; }

.bp3-icon.bp3-intent-success, .bp3-icon-standard.bp3-intent-success, .bp3-icon-large.bp3-intent-success{
  color:#0d8050; }
  .bp3-dark .bp3-icon.bp3-intent-success, .bp3-dark .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-icon-large.bp3-intent-success{
    color:#3dcc91; }

.bp3-icon.bp3-intent-warning, .bp3-icon-standard.bp3-intent-warning, .bp3-icon-large.bp3-intent-warning{
  color:#bf7326; }
  .bp3-dark .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-icon-large.bp3-intent-warning{
    color:#ffb366; }

.bp3-icon.bp3-intent-danger, .bp3-icon-standard.bp3-intent-danger, .bp3-icon-large.bp3-intent-danger{
  color:#c23030; }
  .bp3-dark .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-icon-large.bp3-intent-danger{
    color:#ff7373; }

span.bp3-icon-standard{
  line-height:1;
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-weight:400;
  font-style:normal;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon-large{
  line-height:1;
  font-family:"Icons20", sans-serif;
  font-size:20px;
  font-weight:400;
  font-style:normal;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon:empty{
  line-height:1;
  font-family:"Icons20";
  font-size:inherit;
  font-weight:400;
  font-style:normal; }
  span.bp3-icon:empty::before{
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased; }

.bp3-icon-add::before{
  content:""; }

.bp3-icon-add-column-left::before{
  content:""; }

.bp3-icon-add-column-right::before{
  content:""; }

.bp3-icon-add-row-bottom::before{
  content:""; }

.bp3-icon-add-row-top::before{
  content:""; }

.bp3-icon-add-to-artifact::before{
  content:""; }

.bp3-icon-add-to-folder::before{
  content:""; }

.bp3-icon-airplane::before{
  content:""; }

.bp3-icon-align-center::before{
  content:""; }

.bp3-icon-align-justify::before{
  content:""; }

.bp3-icon-align-left::before{
  content:""; }

.bp3-icon-align-right::before{
  content:""; }

.bp3-icon-alignment-bottom::before{
  content:""; }

.bp3-icon-alignment-horizontal-center::before{
  content:""; }

.bp3-icon-alignment-left::before{
  content:""; }

.bp3-icon-alignment-right::before{
  content:""; }

.bp3-icon-alignment-top::before{
  content:""; }

.bp3-icon-alignment-vertical-center::before{
  content:""; }

.bp3-icon-annotation::before{
  content:""; }

.bp3-icon-application::before{
  content:""; }

.bp3-icon-applications::before{
  content:""; }

.bp3-icon-archive::before{
  content:""; }

.bp3-icon-arrow-bottom-left::before{
  content:"↙"; }

.bp3-icon-arrow-bottom-right::before{
  content:"↘"; }

.bp3-icon-arrow-down::before{
  content:"↓"; }

.bp3-icon-arrow-left::before{
  content:"←"; }

.bp3-icon-arrow-right::before{
  content:"→"; }

.bp3-icon-arrow-top-left::before{
  content:"↖"; }

.bp3-icon-arrow-top-right::before{
  content:"↗"; }

.bp3-icon-arrow-up::before{
  content:"↑"; }

.bp3-icon-arrows-horizontal::before{
  content:"↔"; }

.bp3-icon-arrows-vertical::before{
  content:"↕"; }

.bp3-icon-asterisk::before{
  content:"*"; }

.bp3-icon-automatic-updates::before{
  content:""; }

.bp3-icon-badge::before{
  content:""; }

.bp3-icon-ban-circle::before{
  content:""; }

.bp3-icon-bank-account::before{
  content:""; }

.bp3-icon-barcode::before{
  content:""; }

.bp3-icon-blank::before{
  content:""; }

.bp3-icon-blocked-person::before{
  content:""; }

.bp3-icon-bold::before{
  content:""; }

.bp3-icon-book::before{
  content:""; }

.bp3-icon-bookmark::before{
  content:""; }

.bp3-icon-box::before{
  content:""; }

.bp3-icon-briefcase::before{
  content:""; }

.bp3-icon-bring-data::before{
  content:""; }

.bp3-icon-build::before{
  content:""; }

.bp3-icon-calculator::before{
  content:""; }

.bp3-icon-calendar::before{
  content:""; }

.bp3-icon-camera::before{
  content:""; }

.bp3-icon-caret-down::before{
  content:"⌄"; }

.bp3-icon-caret-left::before{
  content:"〈"; }

.bp3-icon-caret-right::before{
  content:"〉"; }

.bp3-icon-caret-up::before{
  content:"⌃"; }

.bp3-icon-cell-tower::before{
  content:""; }

.bp3-icon-changes::before{
  content:""; }

.bp3-icon-chart::before{
  content:""; }

.bp3-icon-chat::before{
  content:""; }

.bp3-icon-chevron-backward::before{
  content:""; }

.bp3-icon-chevron-down::before{
  content:""; }

.bp3-icon-chevron-forward::before{
  content:""; }

.bp3-icon-chevron-left::before{
  content:""; }

.bp3-icon-chevron-right::before{
  content:""; }

.bp3-icon-chevron-up::before{
  content:""; }

.bp3-icon-circle::before{
  content:""; }

.bp3-icon-circle-arrow-down::before{
  content:""; }

.bp3-icon-circle-arrow-left::before{
  content:""; }

.bp3-icon-circle-arrow-right::before{
  content:""; }

.bp3-icon-circle-arrow-up::before{
  content:""; }

.bp3-icon-citation::before{
  content:""; }

.bp3-icon-clean::before{
  content:""; }

.bp3-icon-clipboard::before{
  content:""; }

.bp3-icon-cloud::before{
  content:"☁"; }

.bp3-icon-cloud-download::before{
  content:""; }

.bp3-icon-cloud-upload::before{
  content:""; }

.bp3-icon-code::before{
  content:""; }

.bp3-icon-code-block::before{
  content:""; }

.bp3-icon-cog::before{
  content:""; }

.bp3-icon-collapse-all::before{
  content:""; }

.bp3-icon-column-layout::before{
  content:""; }

.bp3-icon-comment::before{
  content:""; }

.bp3-icon-comparison::before{
  content:""; }

.bp3-icon-compass::before{
  content:""; }

.bp3-icon-compressed::before{
  content:""; }

.bp3-icon-confirm::before{
  content:""; }

.bp3-icon-console::before{
  content:""; }

.bp3-icon-contrast::before{
  content:""; }

.bp3-icon-control::before{
  content:""; }

.bp3-icon-credit-card::before{
  content:""; }

.bp3-icon-cross::before{
  content:"✗"; }

.bp3-icon-crown::before{
  content:""; }

.bp3-icon-cube::before{
  content:""; }

.bp3-icon-cube-add::before{
  content:""; }

.bp3-icon-cube-remove::before{
  content:""; }

.bp3-icon-curved-range-chart::before{
  content:""; }

.bp3-icon-cut::before{
  content:""; }

.bp3-icon-dashboard::before{
  content:""; }

.bp3-icon-data-lineage::before{
  content:""; }

.bp3-icon-database::before{
  content:""; }

.bp3-icon-delete::before{
  content:""; }

.bp3-icon-delta::before{
  content:"Δ"; }

.bp3-icon-derive-column::before{
  content:""; }

.bp3-icon-desktop::before{
  content:""; }

.bp3-icon-diagram-tree::before{
  content:""; }

.bp3-icon-direction-left::before{
  content:""; }

.bp3-icon-direction-right::before{
  content:""; }

.bp3-icon-disable::before{
  content:""; }

.bp3-icon-document::before{
  content:""; }

.bp3-icon-document-open::before{
  content:""; }

.bp3-icon-document-share::before{
  content:""; }

.bp3-icon-dollar::before{
  content:"$"; }

.bp3-icon-dot::before{
  content:"•"; }

.bp3-icon-double-caret-horizontal::before{
  content:""; }

.bp3-icon-double-caret-vertical::before{
  content:""; }

.bp3-icon-double-chevron-down::before{
  content:""; }

.bp3-icon-double-chevron-left::before{
  content:""; }

.bp3-icon-double-chevron-right::before{
  content:""; }

.bp3-icon-double-chevron-up::before{
  content:""; }

.bp3-icon-doughnut-chart::before{
  content:""; }

.bp3-icon-download::before{
  content:""; }

.bp3-icon-drag-handle-horizontal::before{
  content:""; }

.bp3-icon-drag-handle-vertical::before{
  content:""; }

.bp3-icon-draw::before{
  content:""; }

.bp3-icon-drive-time::before{
  content:""; }

.bp3-icon-duplicate::before{
  content:""; }

.bp3-icon-edit::before{
  content:"✎"; }

.bp3-icon-eject::before{
  content:"⏏"; }

.bp3-icon-endorsed::before{
  content:""; }

.bp3-icon-envelope::before{
  content:"✉"; }

.bp3-icon-equals::before{
  content:""; }

.bp3-icon-eraser::before{
  content:""; }

.bp3-icon-error::before{
  content:""; }

.bp3-icon-euro::before{
  content:"€"; }

.bp3-icon-exchange::before{
  content:""; }

.bp3-icon-exclude-row::before{
  content:""; }

.bp3-icon-expand-all::before{
  content:""; }

.bp3-icon-export::before{
  content:""; }

.bp3-icon-eye-off::before{
  content:""; }

.bp3-icon-eye-on::before{
  content:""; }

.bp3-icon-eye-open::before{
  content:""; }

.bp3-icon-fast-backward::before{
  content:""; }

.bp3-icon-fast-forward::before{
  content:""; }

.bp3-icon-feed::before{
  content:""; }

.bp3-icon-feed-subscribed::before{
  content:""; }

.bp3-icon-film::before{
  content:""; }

.bp3-icon-filter::before{
  content:""; }

.bp3-icon-filter-keep::before{
  content:""; }

.bp3-icon-filter-list::before{
  content:""; }

.bp3-icon-filter-open::before{
  content:""; }

.bp3-icon-filter-remove::before{
  content:""; }

.bp3-icon-flag::before{
  content:"⚑"; }

.bp3-icon-flame::before{
  content:""; }

.bp3-icon-flash::before{
  content:""; }

.bp3-icon-floppy-disk::before{
  content:""; }

.bp3-icon-flow-branch::before{
  content:""; }

.bp3-icon-flow-end::before{
  content:""; }

.bp3-icon-flow-linear::before{
  content:""; }

.bp3-icon-flow-review::before{
  content:""; }

.bp3-icon-flow-review-branch::before{
  content:""; }

.bp3-icon-flows::before{
  content:""; }

.bp3-icon-folder-close::before{
  content:""; }

.bp3-icon-folder-new::before{
  content:""; }

.bp3-icon-folder-open::before{
  content:""; }

.bp3-icon-folder-shared::before{
  content:""; }

.bp3-icon-folder-shared-open::before{
  content:""; }

.bp3-icon-follower::before{
  content:""; }

.bp3-icon-following::before{
  content:""; }

.bp3-icon-font::before{
  content:""; }

.bp3-icon-fork::before{
  content:""; }

.bp3-icon-form::before{
  content:""; }

.bp3-icon-full-circle::before{
  content:""; }

.bp3-icon-full-stacked-chart::before{
  content:""; }

.bp3-icon-fullscreen::before{
  content:""; }

.bp3-icon-function::before{
  content:""; }

.bp3-icon-gantt-chart::before{
  content:""; }

.bp3-icon-geolocation::before{
  content:""; }

.bp3-icon-geosearch::before{
  content:""; }

.bp3-icon-git-branch::before{
  content:""; }

.bp3-icon-git-commit::before{
  content:""; }

.bp3-icon-git-merge::before{
  content:""; }

.bp3-icon-git-new-branch::before{
  content:""; }

.bp3-icon-git-pull::before{
  content:""; }

.bp3-icon-git-push::before{
  content:""; }

.bp3-icon-git-repo::before{
  content:""; }

.bp3-icon-glass::before{
  content:""; }

.bp3-icon-globe::before{
  content:""; }

.bp3-icon-globe-network::before{
  content:""; }

.bp3-icon-graph::before{
  content:""; }

.bp3-icon-graph-remove::before{
  content:""; }

.bp3-icon-greater-than::before{
  content:""; }

.bp3-icon-greater-than-or-equal-to::before{
  content:""; }

.bp3-icon-grid::before{
  content:""; }

.bp3-icon-grid-view::before{
  content:""; }

.bp3-icon-group-objects::before{
  content:""; }

.bp3-icon-grouped-bar-chart::before{
  content:""; }

.bp3-icon-hand::before{
  content:""; }

.bp3-icon-hand-down::before{
  content:""; }

.bp3-icon-hand-left::before{
  content:""; }

.bp3-icon-hand-right::before{
  content:""; }

.bp3-icon-hand-up::before{
  content:""; }

.bp3-icon-header::before{
  content:""; }

.bp3-icon-header-one::before{
  content:""; }

.bp3-icon-header-two::before{
  content:""; }

.bp3-icon-headset::before{
  content:""; }

.bp3-icon-heart::before{
  content:"♥"; }

.bp3-icon-heart-broken::before{
  content:""; }

.bp3-icon-heat-grid::before{
  content:""; }

.bp3-icon-heatmap::before{
  content:""; }

.bp3-icon-help::before{
  content:"?"; }

.bp3-icon-helper-management::before{
  content:""; }

.bp3-icon-highlight::before{
  content:""; }

.bp3-icon-history::before{
  content:""; }

.bp3-icon-home::before{
  content:"⌂"; }

.bp3-icon-horizontal-bar-chart::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-asc::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-desc::before{
  content:""; }

.bp3-icon-horizontal-distribution::before{
  content:""; }

.bp3-icon-id-number::before{
  content:""; }

.bp3-icon-image-rotate-left::before{
  content:""; }

.bp3-icon-image-rotate-right::before{
  content:""; }

.bp3-icon-import::before{
  content:""; }

.bp3-icon-inbox::before{
  content:""; }

.bp3-icon-inbox-filtered::before{
  content:""; }

.bp3-icon-inbox-geo::before{
  content:""; }

.bp3-icon-inbox-search::before{
  content:""; }

.bp3-icon-inbox-update::before{
  content:""; }

.bp3-icon-info-sign::before{
  content:"ℹ"; }

.bp3-icon-inheritance::before{
  content:""; }

.bp3-icon-inner-join::before{
  content:""; }

.bp3-icon-insert::before{
  content:""; }

.bp3-icon-intersection::before{
  content:""; }

.bp3-icon-ip-address::before{
  content:""; }

.bp3-icon-issue::before{
  content:""; }

.bp3-icon-issue-closed::before{
  content:""; }

.bp3-icon-issue-new::before{
  content:""; }

.bp3-icon-italic::before{
  content:""; }

.bp3-icon-join-table::before{
  content:""; }

.bp3-icon-key::before{
  content:""; }

.bp3-icon-key-backspace::before{
  content:""; }

.bp3-icon-key-command::before{
  content:""; }

.bp3-icon-key-control::before{
  content:""; }

.bp3-icon-key-delete::before{
  content:""; }

.bp3-icon-key-enter::before{
  content:""; }

.bp3-icon-key-escape::before{
  content:""; }

.bp3-icon-key-option::before{
  content:""; }

.bp3-icon-key-shift::before{
  content:""; }

.bp3-icon-key-tab::before{
  content:""; }

.bp3-icon-known-vehicle::before{
  content:""; }

.bp3-icon-label::before{
  content:""; }

.bp3-icon-layer::before{
  content:""; }

.bp3-icon-layers::before{
  content:""; }

.bp3-icon-layout::before{
  content:""; }

.bp3-icon-layout-auto::before{
  content:""; }

.bp3-icon-layout-balloon::before{
  content:""; }

.bp3-icon-layout-circle::before{
  content:""; }

.bp3-icon-layout-grid::before{
  content:""; }

.bp3-icon-layout-group-by::before{
  content:""; }

.bp3-icon-layout-hierarchy::before{
  content:""; }

.bp3-icon-layout-linear::before{
  content:""; }

.bp3-icon-layout-skew-grid::before{
  content:""; }

.bp3-icon-layout-sorted-clusters::before{
  content:""; }

.bp3-icon-learning::before{
  content:""; }

.bp3-icon-left-join::before{
  content:""; }

.bp3-icon-less-than::before{
  content:""; }

.bp3-icon-less-than-or-equal-to::before{
  content:""; }

.bp3-icon-lifesaver::before{
  content:""; }

.bp3-icon-lightbulb::before{
  content:""; }

.bp3-icon-link::before{
  content:""; }

.bp3-icon-list::before{
  content:"☰"; }

.bp3-icon-list-columns::before{
  content:""; }

.bp3-icon-list-detail-view::before{
  content:""; }

.bp3-icon-locate::before{
  content:""; }

.bp3-icon-lock::before{
  content:""; }

.bp3-icon-log-in::before{
  content:""; }

.bp3-icon-log-out::before{
  content:""; }

.bp3-icon-manual::before{
  content:""; }

.bp3-icon-manually-entered-data::before{
  content:""; }

.bp3-icon-map::before{
  content:""; }

.bp3-icon-map-create::before{
  content:""; }

.bp3-icon-map-marker::before{
  content:""; }

.bp3-icon-maximize::before{
  content:""; }

.bp3-icon-media::before{
  content:""; }

.bp3-icon-menu::before{
  content:""; }

.bp3-icon-menu-closed::before{
  content:""; }

.bp3-icon-menu-open::before{
  content:""; }

.bp3-icon-merge-columns::before{
  content:""; }

.bp3-icon-merge-links::before{
  content:""; }

.bp3-icon-minimize::before{
  content:""; }

.bp3-icon-minus::before{
  content:"−"; }

.bp3-icon-mobile-phone::before{
  content:""; }

.bp3-icon-mobile-video::before{
  content:""; }

.bp3-icon-moon::before{
  content:""; }

.bp3-icon-more::before{
  content:""; }

.bp3-icon-mountain::before{
  content:""; }

.bp3-icon-move::before{
  content:""; }

.bp3-icon-mugshot::before{
  content:""; }

.bp3-icon-multi-select::before{
  content:""; }

.bp3-icon-music::before{
  content:""; }

.bp3-icon-new-drawing::before{
  content:""; }

.bp3-icon-new-grid-item::before{
  content:""; }

.bp3-icon-new-layer::before{
  content:""; }

.bp3-icon-new-layers::before{
  content:""; }

.bp3-icon-new-link::before{
  content:""; }

.bp3-icon-new-object::before{
  content:""; }

.bp3-icon-new-person::before{
  content:""; }

.bp3-icon-new-prescription::before{
  content:""; }

.bp3-icon-new-text-box::before{
  content:""; }

.bp3-icon-ninja::before{
  content:""; }

.bp3-icon-not-equal-to::before{
  content:""; }

.bp3-icon-notifications::before{
  content:""; }

.bp3-icon-notifications-updated::before{
  content:""; }

.bp3-icon-numbered-list::before{
  content:""; }

.bp3-icon-numerical::before{
  content:""; }

.bp3-icon-office::before{
  content:""; }

.bp3-icon-offline::before{
  content:""; }

.bp3-icon-oil-field::before{
  content:""; }

.bp3-icon-one-column::before{
  content:""; }

.bp3-icon-outdated::before{
  content:""; }

.bp3-icon-page-layout::before{
  content:""; }

.bp3-icon-panel-stats::before{
  content:""; }

.bp3-icon-panel-table::before{
  content:""; }

.bp3-icon-paperclip::before{
  content:""; }

.bp3-icon-paragraph::before{
  content:""; }

.bp3-icon-path::before{
  content:""; }

.bp3-icon-path-search::before{
  content:""; }

.bp3-icon-pause::before{
  content:""; }

.bp3-icon-people::before{
  content:""; }

.bp3-icon-percentage::before{
  content:""; }

.bp3-icon-person::before{
  content:""; }

.bp3-icon-phone::before{
  content:"☎"; }

.bp3-icon-pie-chart::before{
  content:""; }

.bp3-icon-pin::before{
  content:""; }

.bp3-icon-pivot::before{
  content:""; }

.bp3-icon-pivot-table::before{
  content:""; }

.bp3-icon-play::before{
  content:""; }

.bp3-icon-plus::before{
  content:"+"; }

.bp3-icon-polygon-filter::before{
  content:""; }

.bp3-icon-power::before{
  content:""; }

.bp3-icon-predictive-analysis::before{
  content:""; }

.bp3-icon-prescription::before{
  content:""; }

.bp3-icon-presentation::before{
  content:""; }

.bp3-icon-print::before{
  content:"⎙"; }

.bp3-icon-projects::before{
  content:""; }

.bp3-icon-properties::before{
  content:""; }

.bp3-icon-property::before{
  content:""; }

.bp3-icon-publish-function::before{
  content:""; }

.bp3-icon-pulse::before{
  content:""; }

.bp3-icon-random::before{
  content:""; }

.bp3-icon-record::before{
  content:""; }

.bp3-icon-redo::before{
  content:""; }

.bp3-icon-refresh::before{
  content:""; }

.bp3-icon-regression-chart::before{
  content:""; }

.bp3-icon-remove::before{
  content:""; }

.bp3-icon-remove-column::before{
  content:""; }

.bp3-icon-remove-column-left::before{
  content:""; }

.bp3-icon-remove-column-right::before{
  content:""; }

.bp3-icon-remove-row-bottom::before{
  content:""; }

.bp3-icon-remove-row-top::before{
  content:""; }

.bp3-icon-repeat::before{
  content:""; }

.bp3-icon-reset::before{
  content:""; }

.bp3-icon-resolve::before{
  content:""; }

.bp3-icon-rig::before{
  content:""; }

.bp3-icon-right-join::before{
  content:""; }

.bp3-icon-ring::before{
  content:""; }

.bp3-icon-rotate-document::before{
  content:""; }

.bp3-icon-rotate-page::before{
  content:""; }

.bp3-icon-satellite::before{
  content:""; }

.bp3-icon-saved::before{
  content:""; }

.bp3-icon-scatter-plot::before{
  content:""; }

.bp3-icon-search::before{
  content:""; }

.bp3-icon-search-around::before{
  content:""; }

.bp3-icon-search-template::before{
  content:""; }

.bp3-icon-search-text::before{
  content:""; }

.bp3-icon-segmented-control::before{
  content:""; }

.bp3-icon-select::before{
  content:""; }

.bp3-icon-selection::before{
  content:"⦿"; }

.bp3-icon-send-to::before{
  content:""; }

.bp3-icon-send-to-graph::before{
  content:""; }

.bp3-icon-send-to-map::before{
  content:""; }

.bp3-icon-series-add::before{
  content:""; }

.bp3-icon-series-configuration::before{
  content:""; }

.bp3-icon-series-derived::before{
  content:""; }

.bp3-icon-series-filtered::before{
  content:""; }

.bp3-icon-series-search::before{
  content:""; }

.bp3-icon-settings::before{
  content:""; }

.bp3-icon-share::before{
  content:""; }

.bp3-icon-shield::before{
  content:""; }

.bp3-icon-shop::before{
  content:""; }

.bp3-icon-shopping-cart::before{
  content:""; }

.bp3-icon-signal-search::before{
  content:""; }

.bp3-icon-sim-card::before{
  content:""; }

.bp3-icon-slash::before{
  content:""; }

.bp3-icon-small-cross::before{
  content:""; }

.bp3-icon-small-minus::before{
  content:""; }

.bp3-icon-small-plus::before{
  content:""; }

.bp3-icon-small-tick::before{
  content:""; }

.bp3-icon-snowflake::before{
  content:""; }

.bp3-icon-social-media::before{
  content:""; }

.bp3-icon-sort::before{
  content:""; }

.bp3-icon-sort-alphabetical::before{
  content:""; }

.bp3-icon-sort-alphabetical-desc::before{
  content:""; }

.bp3-icon-sort-asc::before{
  content:""; }

.bp3-icon-sort-desc::before{
  content:""; }

.bp3-icon-sort-numerical::before{
  content:""; }

.bp3-icon-sort-numerical-desc::before{
  content:""; }

.bp3-icon-split-columns::before{
  content:""; }

.bp3-icon-square::before{
  content:""; }

.bp3-icon-stacked-chart::before{
  content:""; }

.bp3-icon-star::before{
  content:"★"; }

.bp3-icon-star-empty::before{
  content:"☆"; }

.bp3-icon-step-backward::before{
  content:""; }

.bp3-icon-step-chart::before{
  content:""; }

.bp3-icon-step-forward::before{
  content:""; }

.bp3-icon-stop::before{
  content:""; }

.bp3-icon-stopwatch::before{
  content:""; }

.bp3-icon-strikethrough::before{
  content:""; }

.bp3-icon-style::before{
  content:""; }

.bp3-icon-swap-horizontal::before{
  content:""; }

.bp3-icon-swap-vertical::before{
  content:""; }

.bp3-icon-symbol-circle::before{
  content:""; }

.bp3-icon-symbol-cross::before{
  content:""; }

.bp3-icon-symbol-diamond::before{
  content:""; }

.bp3-icon-symbol-square::before{
  content:""; }

.bp3-icon-symbol-triangle-down::before{
  content:""; }

.bp3-icon-symbol-triangle-up::before{
  content:""; }

.bp3-icon-tag::before{
  content:""; }

.bp3-icon-take-action::before{
  content:""; }

.bp3-icon-taxi::before{
  content:""; }

.bp3-icon-text-highlight::before{
  content:""; }

.bp3-icon-th::before{
  content:""; }

.bp3-icon-th-derived::before{
  content:""; }

.bp3-icon-th-disconnect::before{
  content:""; }

.bp3-icon-th-filtered::before{
  content:""; }

.bp3-icon-th-list::before{
  content:""; }

.bp3-icon-thumbs-down::before{
  content:""; }

.bp3-icon-thumbs-up::before{
  content:""; }

.bp3-icon-tick::before{
  content:"✓"; }

.bp3-icon-tick-circle::before{
  content:""; }

.bp3-icon-time::before{
  content:"⏲"; }

.bp3-icon-timeline-area-chart::before{
  content:""; }

.bp3-icon-timeline-bar-chart::before{
  content:""; }

.bp3-icon-timeline-events::before{
  content:""; }

.bp3-icon-timeline-line-chart::before{
  content:""; }

.bp3-icon-tint::before{
  content:""; }

.bp3-icon-torch::before{
  content:""; }

.bp3-icon-tractor::before{
  content:""; }

.bp3-icon-train::before{
  content:""; }

.bp3-icon-translate::before{
  content:""; }

.bp3-icon-trash::before{
  content:""; }

.bp3-icon-tree::before{
  content:""; }

.bp3-icon-trending-down::before{
  content:""; }

.bp3-icon-trending-up::before{
  content:""; }

.bp3-icon-truck::before{
  content:""; }

.bp3-icon-two-columns::before{
  content:""; }

.bp3-icon-unarchive::before{
  content:""; }

.bp3-icon-underline::before{
  content:"⎁"; }

.bp3-icon-undo::before{
  content:"⎌"; }

.bp3-icon-ungroup-objects::before{
  content:""; }

.bp3-icon-unknown-vehicle::before{
  content:""; }

.bp3-icon-unlock::before{
  content:""; }

.bp3-icon-unpin::before{
  content:""; }

.bp3-icon-unresolve::before{
  content:""; }

.bp3-icon-updated::before{
  content:""; }

.bp3-icon-upload::before{
  content:""; }

.bp3-icon-user::before{
  content:""; }

.bp3-icon-variable::before{
  content:""; }

.bp3-icon-vertical-bar-chart-asc::before{
  content:""; }

.bp3-icon-vertical-bar-chart-desc::before{
  content:""; }

.bp3-icon-vertical-distribution::before{
  content:""; }

.bp3-icon-video::before{
  content:""; }

.bp3-icon-volume-down::before{
  content:""; }

.bp3-icon-volume-off::before{
  content:""; }

.bp3-icon-volume-up::before{
  content:""; }

.bp3-icon-walk::before{
  content:""; }

.bp3-icon-warning-sign::before{
  content:""; }

.bp3-icon-waterfall-chart::before{
  content:""; }

.bp3-icon-widget::before{
  content:""; }

.bp3-icon-widget-button::before{
  content:""; }

.bp3-icon-widget-footer::before{
  content:""; }

.bp3-icon-widget-header::before{
  content:""; }

.bp3-icon-wrench::before{
  content:""; }

.bp3-icon-zoom-in::before{
  content:""; }

.bp3-icon-zoom-out::before{
  content:""; }

.bp3-icon-zoom-to-fit::before{
  content:""; }
.bp3-submenu > .bp3-popover-wrapper{
  display:block; }

.bp3-submenu .bp3-popover-target{
  display:block; }

.bp3-submenu.bp3-popover{
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0 5px; }
  .bp3-submenu.bp3-popover > .bp3-popover-content{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-submenu.bp3-popover, .bp3-submenu.bp3-popover.bp3-dark{
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-dark .bp3-submenu.bp3-popover > .bp3-popover-content, .bp3-submenu.bp3-popover.bp3-dark > .bp3-popover-content{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
.bp3-menu{
  margin:0;
  border-radius:3px;
  background:#ffffff;
  min-width:180px;
  padding:5px;
  list-style:none;
  text-align:left;
  color:#182026; }

.bp3-menu-divider{
  display:block;
  margin:5px;
  border-top:1px solid rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-menu-divider{
    border-top-color:rgba(255, 255, 255, 0.15); }

.bp3-menu-item{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  border-radius:2px;
  padding:5px 7px;
  text-decoration:none;
  line-height:20px;
  color:inherit;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-menu-item > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-menu-item > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-menu-item::before,
  .bp3-menu-item > *{
    margin-right:7px; }
  .bp3-menu-item:empty::before,
  .bp3-menu-item > :last-child{
    margin-right:0; }
  .bp3-menu-item > .bp3-fill{
    word-break:break-word; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    background-color:rgba(167, 182, 194, 0.3);
    cursor:pointer;
    text-decoration:none; }
  .bp3-menu-item.bp3-disabled{
    background-color:inherit;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-dark .bp3-menu-item{
    color:inherit; }
    .bp3-dark .bp3-menu-item:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
      background-color:rgba(138, 155, 168, 0.15);
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-disabled{
      background-color:inherit;
      color:rgba(167, 182, 194, 0.6); }
  .bp3-menu-item.bp3-intent-primary{
    color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-primary::before, .bp3-menu-item.bp3-intent-primary::after,
    .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
      color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary.bp3-active{
      background-color:#137cbd; }
    .bp3-menu-item.bp3-intent-primary:active{
      background-color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary:active, .bp3-menu-item.bp3-intent-primary:active::before, .bp3-menu-item.bp3-intent-primary:active::after,
    .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-menu-item.bp3-intent-primary.bp3-active::after,
    .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-success{
    color:#0d8050; }
    .bp3-menu-item.bp3-intent-success .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-success::before, .bp3-menu-item.bp3-intent-success::after,
    .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
      color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success.bp3-active{
      background-color:#0f9960; }
    .bp3-menu-item.bp3-intent-success:active{
      background-color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-menu-item.bp3-intent-success:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success:active, .bp3-menu-item.bp3-intent-success:active::before, .bp3-menu-item.bp3-intent-success:active::after,
    .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-menu-item.bp3-intent-success.bp3-active::after,
    .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-warning{
    color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-warning::before, .bp3-menu-item.bp3-intent-warning::after,
    .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
      color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning.bp3-active{
      background-color:#d9822b; }
    .bp3-menu-item.bp3-intent-warning:active{
      background-color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning:active, .bp3-menu-item.bp3-intent-warning:active::before, .bp3-menu-item.bp3-intent-warning:active::after,
    .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-menu-item.bp3-intent-warning.bp3-active::after,
    .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-danger{
    color:#c23030; }
    .bp3-menu-item.bp3-intent-danger .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-danger::before, .bp3-menu-item.bp3-intent-danger::after,
    .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
      color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger.bp3-active{
      background-color:#db3737; }
    .bp3-menu-item.bp3-intent-danger:active{
      background-color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger:active, .bp3-menu-item.bp3-intent-danger:active::before, .bp3-menu-item.bp3-intent-danger:active::after,
    .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-menu-item.bp3-intent-danger.bp3-active::after,
    .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item::before{
    line-height:1;
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-weight:400;
    font-style:normal;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    margin-right:7px; }
  .bp3-menu-item::before,
  .bp3-menu-item > .bp3-icon{
    margin-top:2px;
    color:#5c7080; }
  .bp3-menu-item .bp3-menu-item-label{
    color:#5c7080; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    color:inherit; }
  .bp3-menu-item.bp3-active, .bp3-menu-item:active{
    background-color:rgba(115, 134, 148, 0.3); }
  .bp3-menu-item.bp3-disabled{
    outline:none !important;
    background-color:inherit !important;
    cursor:not-allowed !important;
    color:rgba(92, 112, 128, 0.6) !important; }
    .bp3-menu-item.bp3-disabled::before,
    .bp3-menu-item.bp3-disabled > .bp3-icon,
    .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-large .bp3-menu-item{
    padding:9px 7px;
    line-height:22px;
    font-size:16px; }
    .bp3-large .bp3-menu-item .bp3-icon{
      margin-top:3px; }
    .bp3-large .bp3-menu-item::before{
      line-height:1;
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-weight:400;
      font-style:normal;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      margin-top:1px;
      margin-right:10px; }

button.bp3-menu-item{
  border:none;
  background:none;
  width:100%;
  text-align:left; }
.bp3-menu-header{
  display:block;
  margin:5px;
  border-top:1px solid rgba(16, 22, 26, 0.15);
  cursor:default;
  padding-left:2px; }
  .bp3-dark .bp3-menu-header{
    border-top-color:rgba(255, 255, 255, 0.15); }
  .bp3-menu-header:first-of-type{
    border-top:none; }
  .bp3-menu-header > h6{
    color:#182026;
    font-weight:600;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    margin:0;
    padding:10px 7px 0 1px;
    line-height:17px; }
    .bp3-dark .bp3-menu-header > h6{
      color:#f5f8fa; }
  .bp3-menu-header:first-of-type > h6{
    padding-top:0; }
  .bp3-large .bp3-menu-header > h6{
    padding-top:15px;
    padding-bottom:5px;
    font-size:18px; }
  .bp3-large .bp3-menu-header:first-of-type > h6{
    padding-top:0; }

.bp3-dark .bp3-menu{
  background:#30404d;
  color:#f5f8fa; }

.bp3-dark .bp3-menu-item.bp3-intent-primary{
  color:#48aff0; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary::before, .bp3-dark .bp3-menu-item.bp3-intent-primary::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
    color:#48aff0; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{
    background-color:#137cbd; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary:active{
    background-color:#106ba3; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary:active, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item.bp3-intent-success{
  color:#3dcc91; }
  .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-success::before, .bp3-dark .bp3-menu-item.bp3-intent-success::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
    color:#3dcc91; }
  .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{
    background-color:#0f9960; }
  .bp3-dark .bp3-menu-item.bp3-intent-success:active{
    background-color:#0d8050; }
  .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success:active, .bp3-dark .bp3-menu-item.bp3-intent-success:active::before, .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item.bp3-intent-warning{
  color:#ffb366; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning::before, .bp3-dark .bp3-menu-item.bp3-intent-warning::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
    color:#ffb366; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{
    background-color:#d9822b; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning:active{
    background-color:#bf7326; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning:active, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item.bp3-intent-danger{
  color:#ff7373; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger::before, .bp3-dark .bp3-menu-item.bp3-intent-danger::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
    color:#ff7373; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{
    background-color:#db3737; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger:active{
    background-color:#c23030; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger:active, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item::before,
.bp3-dark .bp3-menu-item > .bp3-icon{
  color:#a7b6c2; }

.bp3-dark .bp3-menu-item .bp3-menu-item-label{
  color:#a7b6c2; }

.bp3-dark .bp3-menu-item.bp3-active, .bp3-dark .bp3-menu-item:active{
  background-color:rgba(138, 155, 168, 0.3); }

.bp3-dark .bp3-menu-item.bp3-disabled{
  color:rgba(167, 182, 194, 0.6) !important; }
  .bp3-dark .bp3-menu-item.bp3-disabled::before,
  .bp3-dark .bp3-menu-item.bp3-disabled > .bp3-icon,
  .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
    color:rgba(167, 182, 194, 0.6) !important; }

.bp3-dark .bp3-menu-divider,
.bp3-dark .bp3-menu-header{
  border-color:rgba(255, 255, 255, 0.15); }

.bp3-dark .bp3-menu-header > h6{
  color:#f5f8fa; }

.bp3-label .bp3-menu{
  margin-top:5px; }
.bp3-navbar{
  position:relative;
  z-index:10;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  background-color:#ffffff;
  width:100%;
  height:50px;
  padding:0 15px; }
  .bp3-navbar.bp3-dark,
  .bp3-dark .bp3-navbar{
    background-color:#394b59; }
  .bp3-navbar.bp3-dark{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-navbar{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-navbar.bp3-fixed-top{
    position:fixed;
    top:0;
    right:0;
    left:0; }

.bp3-navbar-heading{
  margin-right:15px;
  font-size:16px; }

.bp3-navbar-group{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  height:50px; }
  .bp3-navbar-group.bp3-align-left{
    float:left; }
  .bp3-navbar-group.bp3-align-right{
    float:right; }

.bp3-navbar-divider{
  margin:0 10px;
  border-left:1px solid rgba(16, 22, 26, 0.15);
  height:20px; }
  .bp3-dark .bp3-navbar-divider{
    border-left-color:rgba(255, 255, 255, 0.15); }
.bp3-non-ideal-state{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  width:100%;
  height:100%;
  text-align:center; }
  .bp3-non-ideal-state > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-non-ideal-state > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-non-ideal-state::before,
  .bp3-non-ideal-state > *{
    margin-bottom:20px; }
  .bp3-non-ideal-state:empty::before,
  .bp3-non-ideal-state > :last-child{
    margin-bottom:0; }
  .bp3-non-ideal-state > *{
    max-width:400px; }

.bp3-non-ideal-state-visual{
  color:rgba(92, 112, 128, 0.6);
  font-size:60px; }
  .bp3-dark .bp3-non-ideal-state-visual{
    color:rgba(167, 182, 194, 0.6); }

.bp3-overflow-list{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:nowrap;
      flex-wrap:nowrap;
  min-width:0; }

.bp3-overflow-list-spacer{
  -ms-flex-negative:1;
      flex-shrink:1;
  width:1px; }

body.bp3-overlay-open{
  overflow:hidden; }

.bp3-overlay{
  position:static;
  top:0;
  right:0;
  bottom:0;
  left:0;
  z-index:20; }
  .bp3-overlay:not(.bp3-overlay-open){
    pointer-events:none; }
  .bp3-overlay.bp3-overlay-container{
    position:fixed;
    overflow:hidden; }
    .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-scroll-container{
    position:fixed;
    overflow:auto; }
    .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-inline{
    display:inline;
    overflow:visible; }

.bp3-overlay-content{
  position:fixed;
  z-index:20; }
  .bp3-overlay-inline .bp3-overlay-content,
  .bp3-overlay-scroll-container .bp3-overlay-content{
    position:absolute; }

.bp3-overlay-backdrop{
  position:fixed;
  top:0;
  right:0;
  bottom:0;
  left:0;
  opacity:1;
  z-index:20;
  background-color:rgba(16, 22, 26, 0.7);
  overflow:auto;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-overlay-backdrop.bp3-overlay-enter, .bp3-overlay-backdrop.bp3-overlay-appear{
    opacity:0; }
  .bp3-overlay-backdrop.bp3-overlay-enter-active, .bp3-overlay-backdrop.bp3-overlay-appear-active{
    opacity:1;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-overlay-backdrop.bp3-overlay-exit{
    opacity:1; }
  .bp3-overlay-backdrop.bp3-overlay-exit-active{
    opacity:0;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-overlay-backdrop:focus{
    outline:none; }
  .bp3-overlay-inline .bp3-overlay-backdrop{
    position:absolute; }
.bp3-panel-stack{
  position:relative;
  overflow:hidden; }

.bp3-panel-stack-header{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-negative:0;
      flex-shrink:0;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  z-index:1;
  -webkit-box-shadow:0 1px rgba(16, 22, 26, 0.15);
          box-shadow:0 1px rgba(16, 22, 26, 0.15);
  height:30px; }
  .bp3-dark .bp3-panel-stack-header{
    -webkit-box-shadow:0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 1px rgba(255, 255, 255, 0.15); }
  .bp3-panel-stack-header > span{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1;
            flex:1;
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch; }
  .bp3-panel-stack-header .bp3-heading{
    margin:0 5px; }

.bp3-button.bp3-panel-stack-header-back{
  margin-left:5px;
  padding-left:0;
  white-space:nowrap; }
  .bp3-button.bp3-panel-stack-header-back .bp3-icon{
    margin:0 2px; }

.bp3-panel-stack-view{
  position:absolute;
  top:0;
  right:0;
  bottom:0;
  left:0;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin-right:-1px;
  border-right:1px solid rgba(16, 22, 26, 0.15);
  background-color:#ffffff;
  overflow-y:auto; }
  .bp3-dark .bp3-panel-stack-view{
    background-color:#30404d; }

.bp3-panel-stack-push .bp3-panel-stack-enter, .bp3-panel-stack-push .bp3-panel-stack-appear{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0; }

.bp3-panel-stack-push .bp3-panel-stack-enter-active, .bp3-panel-stack-push .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }

.bp3-panel-stack-push .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-push .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }

.bp3-panel-stack-pop .bp3-panel-stack-enter, .bp3-panel-stack-pop .bp3-panel-stack-appear{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0; }

.bp3-panel-stack-pop .bp3-panel-stack-enter-active, .bp3-panel-stack-pop .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }

.bp3-panel-stack-pop .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-pop .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }
.bp3-popover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1);
  display:inline-block;
  z-index:20;
  border-radius:3px; }
  .bp3-popover .bp3-popover-arrow{
    position:absolute;
    width:30px;
    height:30px; }
    .bp3-popover .bp3-popover-arrow::before{
      margin:5px;
      width:20px;
      height:20px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover{
    margin-top:-17px;
    margin-bottom:17px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
      bottom:-11px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover{
    margin-left:17px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
      left:-11px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover{
    margin-top:17px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
      top:-11px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover{
    margin-right:17px;
    margin-left:-17px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
      right:-11px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-popover > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-popover > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
    top:-0.3934px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
    right:-0.3934px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
    left:-0.3934px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
    bottom:-0.3934px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-popover .bp3-popover-content{
    background:#ffffff;
    color:inherit; }
  .bp3-popover .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-popover .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-popover .bp3-popover-arrow-fill{
    fill:#ffffff; }
  .bp3-popover-enter > .bp3-popover, .bp3-popover-appear > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3); }
  .bp3-popover-enter-active > .bp3-popover, .bp3-popover-appear-active > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-popover-exit > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-popover .bp3-popover-content{
    position:relative;
    border-radius:3px; }
  .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{
    max-width:350px;
    padding:20px; }
  .bp3-popover-target + .bp3-overlay .bp3-popover.bp3-popover-content-sizing{
    width:350px; }
  .bp3-popover.bp3-minimal{
    margin:0 !important; }
    .bp3-popover.bp3-minimal .bp3-popover-arrow{
      display:none; }
    .bp3-popover.bp3-minimal.bp3-popover{
      -webkit-transform:scale(1);
              transform:scale(1); }
      .bp3-popover-enter > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-enter-active > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
        -webkit-transition-delay:0;
                transition-delay:0; }
      .bp3-popover-exit > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-exit-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
        -webkit-transition-delay:0;
                transition-delay:0; }
  .bp3-popover.bp3-dark,
  .bp3-dark .bp3-popover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-popover .bp3-popover-content{
      background:#30404d;
      color:inherit; }
    .bp3-popover.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-popover .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-popover .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-popover.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-popover .bp3-popover-arrow-fill{
      fill:#30404d; }

.bp3-popover-arrow::before{
  display:block;
  position:absolute;
  -webkit-transform:rotate(45deg);
          transform:rotate(45deg);
  border-radius:2px;
  content:""; }

.bp3-tether-pinned .bp3-popover-arrow{
  display:none; }

.bp3-popover-backdrop{
  background:rgba(255, 255, 255, 0); }

.bp3-transition-container{
  opacity:1;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  z-index:20; }
  .bp3-transition-container.bp3-popover-enter, .bp3-transition-container.bp3-popover-appear{
    opacity:0; }
  .bp3-transition-container.bp3-popover-enter-active, .bp3-transition-container.bp3-popover-appear-active{
    opacity:1;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-transition-container.bp3-popover-exit{
    opacity:1; }
  .bp3-transition-container.bp3-popover-exit-active{
    opacity:0;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-transition-container:focus{
    outline:none; }
  .bp3-transition-container.bp3-popover-leave .bp3-popover-content{
    pointer-events:none; }
  .bp3-transition-container[data-x-out-of-boundaries]{
    display:none; }

span.bp3-popover-target{
  display:inline-block; }

.bp3-popover-wrapper.bp3-fill{
  width:100%; }

.bp3-portal{
  position:absolute;
  top:0;
  right:0;
  left:0; }
@-webkit-keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }
@keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }

.bp3-progress-bar{
  display:block;
  position:relative;
  border-radius:40px;
  background:rgba(92, 112, 128, 0.2);
  width:100%;
  height:8px;
  overflow:hidden; }
  .bp3-progress-bar .bp3-progress-meter{
    position:absolute;
    border-radius:40px;
    background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);
    background-color:rgba(92, 112, 128, 0.8);
    background-size:30px 30px;
    width:100%;
    height:100%;
    -webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{
    animation:linear-progress-bar-stripes 300ms linear infinite reverse; }
  .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{
    background-image:none; }

.bp3-dark .bp3-progress-bar{
  background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-progress-bar .bp3-progress-meter{
    background-color:#8a9ba8; }

.bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{
  background-color:#137cbd; }

.bp3-progress-bar.bp3-intent-success .bp3-progress-meter{
  background-color:#0f9960; }

.bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{
  background-color:#d9822b; }

.bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{
  background-color:#db3737; }
@-webkit-keyframes skeleton-glow{
  from{
    border-color:rgba(206, 217, 224, 0.2);
    background:rgba(206, 217, 224, 0.2); }
  to{
    border-color:rgba(92, 112, 128, 0.2);
    background:rgba(92, 112, 128, 0.2); } }
@keyframes skeleton-glow{
  from{
    border-color:rgba(206, 217, 224, 0.2);
    background:rgba(206, 217, 224, 0.2); }
  to{
    border-color:rgba(92, 112, 128, 0.2);
    background:rgba(92, 112, 128, 0.2); } }
.bp3-skeleton{
  border-color:rgba(206, 217, 224, 0.2) !important;
  border-radius:2px;
  -webkit-box-shadow:none !important;
          box-shadow:none !important;
  background:rgba(206, 217, 224, 0.2);
  background-clip:padding-box !important;
  cursor:default;
  color:transparent !important;
  -webkit-animation:1000ms linear infinite alternate skeleton-glow;
          animation:1000ms linear infinite alternate skeleton-glow;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-skeleton::before, .bp3-skeleton::after,
  .bp3-skeleton *{
    visibility:hidden !important; }
.bp3-slider{
  width:100%;
  min-width:150px;
  height:40px;
  position:relative;
  outline:none;
  cursor:default;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-slider:hover{
    cursor:pointer; }
  .bp3-slider:active{
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-slider.bp3-disabled{
    opacity:0.5;
    cursor:not-allowed; }
  .bp3-slider.bp3-slider-unlabeled{
    height:16px; }

.bp3-slider-track,
.bp3-slider-progress{
  top:5px;
  right:0;
  left:0;
  height:6px;
  position:absolute; }

.bp3-slider-track{
  border-radius:3px;
  overflow:hidden; }

.bp3-slider-progress{
  background:rgba(92, 112, 128, 0.2); }
  .bp3-dark .bp3-slider-progress{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-slider-progress.bp3-intent-primary{
    background-color:#137cbd; }
  .bp3-slider-progress.bp3-intent-success{
    background-color:#0f9960; }
  .bp3-slider-progress.bp3-intent-warning{
    background-color:#d9822b; }
  .bp3-slider-progress.bp3-intent-danger{
    background-color:#db3737; }

.bp3-slider-handle{
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  color:#182026;
  position:absolute;
  top:0;
  left:0;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
  cursor:pointer;
  width:16px;
  height:16px; }
  .bp3-slider-handle:hover{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5; }
  .bp3-slider-handle:active, .bp3-slider-handle.bp3-active{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none; }
  .bp3-slider-handle:disabled, .bp3-slider-handle.bp3-disabled{
    outline:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
    .bp3-slider-handle:disabled.bp3-active, .bp3-slider-handle:disabled.bp3-active:hover, .bp3-slider-handle.bp3-disabled.bp3-active, .bp3-slider-handle.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }
  .bp3-slider-handle:focus{
    z-index:1; }
  .bp3-slider-handle:hover{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5;
    z-index:2;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
    cursor:-webkit-grab;
    cursor:grab; }
  .bp3-slider-handle.bp3-active{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-disabled .bp3-slider-handle{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:#bfccd6;
    pointer-events:none; }
  .bp3-dark .bp3-slider-handle{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover, .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#30404d; }
    .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#202b33;
      background-image:none; }
    .bp3-dark .bp3-slider-handle:disabled, .bp3-dark .bp3-slider-handle.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-slider-handle:disabled.bp3-active, .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-slider-handle, .bp3-dark .bp3-slider-handle:hover{
      background-color:#394b59; }
    .bp3-dark .bp3-slider-handle.bp3-active{
      background-color:#293742; }
  .bp3-dark .bp3-disabled .bp3-slider-handle{
    border-color:#5c7080;
    -webkit-box-shadow:none;
            box-shadow:none;
    background:#5c7080; }
  .bp3-slider-handle .bp3-slider-label{
    margin-left:8px;
    border-radius:3px;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
    background:#394b59;
    color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle .bp3-slider-label{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
      background:#e1e8ed;
      color:#394b59; }
    .bp3-disabled .bp3-slider-handle .bp3-slider-label{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-slider-handle.bp3-start, .bp3-slider-handle.bp3-end{
    width:8px; }
  .bp3-slider-handle.bp3-start{
    border-top-right-radius:0;
    border-bottom-right-radius:0; }
  .bp3-slider-handle.bp3-end{
    margin-left:8px;
    border-top-left-radius:0;
    border-bottom-left-radius:0; }
    .bp3-slider-handle.bp3-end .bp3-slider-label{
      margin-left:0; }

.bp3-slider-label{
  -webkit-transform:translate(-50%, 20px);
          transform:translate(-50%, 20px);
  display:inline-block;
  position:absolute;
  padding:2px 5px;
  vertical-align:top;
  line-height:1;
  font-size:12px; }

.bp3-slider.bp3-vertical{
  width:40px;
  min-width:40px;
  height:150px; }
  .bp3-slider.bp3-vertical .bp3-slider-track,
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    top:0;
    bottom:0;
    left:5px;
    width:6px;
    height:auto; }
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    top:auto; }
  .bp3-slider.bp3-vertical .bp3-slider-label{
    -webkit-transform:translate(20px, 50%);
            transform:translate(20px, 50%); }
  .bp3-slider.bp3-vertical .bp3-slider-handle{
    top:auto; }
    .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{
      margin-top:-8px;
      margin-left:0; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end, .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      margin-left:0;
      width:16px;
      height:8px; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      border-top-left-radius:0;
      border-bottom-right-radius:3px; }
      .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{
        -webkit-transform:translate(20px);
                transform:translate(20px); }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{
      margin-bottom:8px;
      border-top-left-radius:3px;
      border-bottom-left-radius:0;
      border-bottom-right-radius:0; }

@-webkit-keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

@keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

.bp3-spinner{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  overflow:visible;
  vertical-align:middle; }
  .bp3-spinner svg{
    display:block; }
  .bp3-spinner path{
    fill-opacity:0; }
  .bp3-spinner .bp3-spinner-head{
    -webkit-transform-origin:center;
            transform-origin:center;
    -webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    stroke:rgba(92, 112, 128, 0.8);
    stroke-linecap:round; }
  .bp3-spinner .bp3-spinner-track{
    stroke:rgba(92, 112, 128, 0.2); }

.bp3-spinner-animation{
  -webkit-animation:pt-spinner-animation 500ms linear infinite;
          animation:pt-spinner-animation 500ms linear infinite; }
  .bp3-no-spin > .bp3-spinner-animation{
    -webkit-animation:none;
            animation:none; }

.bp3-dark .bp3-spinner .bp3-spinner-head{
  stroke:#8a9ba8; }

.bp3-dark .bp3-spinner .bp3-spinner-track{
  stroke:rgba(16, 22, 26, 0.5); }

.bp3-spinner.bp3-intent-primary .bp3-spinner-head{
  stroke:#137cbd; }

.bp3-spinner.bp3-intent-success .bp3-spinner-head{
  stroke:#0f9960; }

.bp3-spinner.bp3-intent-warning .bp3-spinner-head{
  stroke:#d9822b; }

.bp3-spinner.bp3-intent-danger .bp3-spinner-head{
  stroke:#db3737; }
.bp3-tabs.bp3-vertical{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-tabs.bp3-vertical > .bp3-tab-list{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column;
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab{
      border-radius:3px;
      width:100%;
      padding:0 10px; }
      .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab[aria-selected="true"]{
        -webkit-box-shadow:none;
                box-shadow:none;
        background-color:rgba(19, 124, 189, 0.2); }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{
      top:0;
      right:0;
      bottom:0;
      left:0;
      border-radius:3px;
      background-color:rgba(19, 124, 189, 0.2);
      height:auto; }
  .bp3-tabs.bp3-vertical > .bp3-tab-panel{
    margin-top:0;
    padding-left:20px; }

.bp3-tab-list{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  -webkit-box-align:end;
      -ms-flex-align:end;
          align-items:flex-end;
  position:relative;
  margin:0;
  border:none;
  padding:0;
  list-style:none; }
  .bp3-tab-list > *:not(:last-child){
    margin-right:20px; }

.bp3-tab{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  position:relative;
  cursor:pointer;
  max-width:100%;
  vertical-align:top;
  line-height:30px;
  color:#182026;
  font-size:14px; }
  .bp3-tab a{
    display:block;
    text-decoration:none;
    color:inherit; }
  .bp3-tab-indicator-wrapper ~ .bp3-tab{
    -webkit-box-shadow:none !important;
            box-shadow:none !important;
    background-color:transparent !important; }
  .bp3-tab[aria-disabled="true"]{
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-tab[aria-selected="true"]{
    border-radius:0;
    -webkit-box-shadow:inset 0 -3px 0 #106ba3;
            box-shadow:inset 0 -3px 0 #106ba3; }
  .bp3-tab[aria-selected="true"], .bp3-tab:not([aria-disabled="true"]):hover{
    color:#106ba3; }
  .bp3-tab:focus{
    -moz-outline-radius:0; }
  .bp3-large > .bp3-tab{
    line-height:40px;
    font-size:16px; }

.bp3-tab-panel{
  margin-top:20px; }
  .bp3-tab-panel[aria-hidden="true"]{
    display:none; }

.bp3-tab-indicator-wrapper{
  position:absolute;
  top:0;
  left:0;
  -webkit-transform:translateX(0), translateY(0);
          transform:translateX(0), translateY(0);
  -webkit-transition:height, width, -webkit-transform;
  transition:height, width, -webkit-transform;
  transition:height, transform, width;
  transition:height, transform, width, -webkit-transform;
  -webkit-transition-duration:200ms;
          transition-duration:200ms;
  -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
          transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
  pointer-events:none; }
  .bp3-tab-indicator-wrapper .bp3-tab-indicator{
    position:absolute;
    right:0;
    bottom:0;
    left:0;
    background-color:#106ba3;
    height:3px; }
  .bp3-tab-indicator-wrapper.bp3-no-animation{
    -webkit-transition:none;
    transition:none; }

.bp3-dark .bp3-tab{
  color:#f5f8fa; }
  .bp3-dark .bp3-tab[aria-disabled="true"]{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tab[aria-selected="true"]{
    -webkit-box-shadow:inset 0 -3px 0 #48aff0;
            box-shadow:inset 0 -3px 0 #48aff0; }
  .bp3-dark .bp3-tab[aria-selected="true"], .bp3-dark .bp3-tab:not([aria-disabled="true"]):hover{
    color:#48aff0; }

.bp3-dark .bp3-tab-indicator{
  background-color:#48aff0; }

.bp3-flex-expander{
  -webkit-box-flex:1;
      -ms-flex:1 1;
          flex:1 1; }
.bp3-tag{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  position:relative;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:none;
          box-shadow:none;
  background-color:#5c7080;
  min-width:20px;
  max-width:100%;
  min-height:20px;
  padding:2px 6px;
  line-height:16px;
  color:#f5f8fa;
  font-size:12px; }
  .bp3-tag.bp3-interactive{
    cursor:pointer; }
    .bp3-tag.bp3-interactive:hover{
      background-color:rgba(92, 112, 128, 0.85); }
    .bp3-tag.bp3-interactive.bp3-active, .bp3-tag.bp3-interactive:active{
      background-color:rgba(92, 112, 128, 0.7); }
  .bp3-tag > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag::before,
  .bp3-tag > *{
    margin-right:4px; }
  .bp3-tag:empty::before,
  .bp3-tag > :last-child{
    margin-right:0; }
  .bp3-tag:focus{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:0;
    -moz-outline-radius:6px; }
  .bp3-tag.bp3-round{
    border-radius:30px;
    padding-right:8px;
    padding-left:8px; }
  .bp3-dark .bp3-tag{
    background-color:#bfccd6;
    color:#182026; }
    .bp3-dark .bp3-tag.bp3-interactive{
      cursor:pointer; }
      .bp3-dark .bp3-tag.bp3-interactive:hover{
        background-color:rgba(191, 204, 214, 0.85); }
      .bp3-dark .bp3-tag.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-interactive:active{
        background-color:rgba(191, 204, 214, 0.7); }
    .bp3-dark .bp3-tag > .bp3-icon, .bp3-dark .bp3-tag .bp3-icon-standard, .bp3-dark .bp3-tag .bp3-icon-large{
      fill:currentColor; }
  .bp3-tag > .bp3-icon, .bp3-tag .bp3-icon-standard, .bp3-tag .bp3-icon-large{
    fill:#ffffff; }
  .bp3-tag.bp3-large,
  .bp3-large .bp3-tag{
    min-width:30px;
    min-height:30px;
    padding:0 10px;
    line-height:20px;
    font-size:14px; }
    .bp3-tag.bp3-large::before,
    .bp3-tag.bp3-large > *,
    .bp3-large .bp3-tag::before,
    .bp3-large .bp3-tag > *{
      margin-right:7px; }
    .bp3-tag.bp3-large:empty::before,
    .bp3-tag.bp3-large > :last-child,
    .bp3-large .bp3-tag:empty::before,
    .bp3-large .bp3-tag > :last-child{
      margin-right:0; }
    .bp3-tag.bp3-large.bp3-round,
    .bp3-large .bp3-tag.bp3-round{
      padding-right:12px;
      padding-left:12px; }
  .bp3-tag.bp3-intent-primary{
    background:#137cbd;
    color:#ffffff; }
    .bp3-tag.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.85); }
      .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.7); }
  .bp3-tag.bp3-intent-success{
    background:#0f9960;
    color:#ffffff; }
    .bp3-tag.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.85); }
      .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.7); }
  .bp3-tag.bp3-intent-warning{
    background:#d9822b;
    color:#ffffff; }
    .bp3-tag.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.85); }
      .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.7); }
  .bp3-tag.bp3-intent-danger{
    background:#db3737;
    color:#ffffff; }
    .bp3-tag.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.85); }
      .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.7); }
  .bp3-tag.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-tag.bp3-minimal > .bp3-icon, .bp3-tag.bp3-minimal .bp3-icon-standard, .bp3-tag.bp3-minimal .bp3-icon-large{
    fill:#5c7080; }
  .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
    background-color:rgba(138, 155, 168, 0.2);
    color:#182026; }
    .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
        background-color:rgba(92, 112, 128, 0.3); }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
        background-color:rgba(92, 112, 128, 0.4); }
    .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
      color:#f5f8fa; }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
          background-color:rgba(191, 204, 214, 0.3); }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
          background-color:rgba(191, 204, 214, 0.4); }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) > .bp3-icon, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-large{
        fill:#a7b6c2; }
  .bp3-tag.bp3-minimal.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15);
    color:#106ba3; }
    .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-primary > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{
      fill:#137cbd; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25);
      color:#48aff0; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
          background-color:rgba(19, 124, 189, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
          background-color:rgba(19, 124, 189, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15);
    color:#0d8050; }
    .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-success > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{
      fill:#0f9960; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25);
      color:#3dcc91; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
          background-color:rgba(15, 153, 96, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
          background-color:rgba(15, 153, 96, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15);
    color:#bf7326; }
    .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-warning > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{
      fill:#d9822b; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25);
      color:#ffb366; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
          background-color:rgba(217, 130, 43, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
          background-color:rgba(217, 130, 43, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15);
    color:#c23030; }
    .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-danger > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{
      fill:#db3737; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25);
      color:#ff7373; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
          background-color:rgba(219, 55, 55, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
          background-color:rgba(219, 55, 55, 0.45); }

.bp3-tag-remove{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  opacity:0.5;
  margin-top:-2px;
  margin-right:-6px !important;
  margin-bottom:-2px;
  border:none;
  background:none;
  cursor:pointer;
  padding:2px;
  padding-left:0;
  color:inherit; }
  .bp3-tag-remove:hover{
    opacity:0.8;
    background:none;
    text-decoration:none; }
  .bp3-tag-remove:active{
    opacity:1; }
  .bp3-tag-remove:empty::before{
    line-height:1;
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-weight:400;
    font-style:normal;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    content:""; }
  .bp3-large .bp3-tag-remove{
    margin-right:-10px !important;
    padding:5px;
    padding-left:0; }
    .bp3-large .bp3-tag-remove:empty::before{
      line-height:1;
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-weight:400;
      font-style:normal; }
.bp3-tag-input{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  cursor:text;
  height:auto;
  min-height:30px;
  padding-right:0;
  padding-left:5px;
  line-height:inherit; }
  .bp3-tag-input > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag-input > .bp3-tag-input-values{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag-input .bp3-tag-input-icon{
    margin-top:7px;
    margin-right:7px;
    margin-left:2px;
    color:#5c7080; }
  .bp3-tag-input .bp3-tag-input-values{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row;
    -ms-flex-wrap:wrap;
        flex-wrap:wrap;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    -ms-flex-item-align:stretch;
        align-self:stretch;
    margin-top:5px;
    margin-right:7px;
    min-width:0; }
    .bp3-tag-input .bp3-tag-input-values > *{
      -webkit-box-flex:0;
          -ms-flex-positive:0;
              flex-grow:0;
      -ms-flex-negative:0;
          flex-shrink:0; }
    .bp3-tag-input .bp3-tag-input-values > .bp3-fill{
      -webkit-box-flex:1;
          -ms-flex-positive:1;
              flex-grow:1;
      -ms-flex-negative:1;
          flex-shrink:1; }
    .bp3-tag-input .bp3-tag-input-values::before,
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-right:5px; }
    .bp3-tag-input .bp3-tag-input-values:empty::before,
    .bp3-tag-input .bp3-tag-input-values > :last-child{
      margin-right:0; }
    .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{
      padding-left:5px; }
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-bottom:5px; }
  .bp3-tag-input .bp3-tag{
    overflow-wrap:break-word; }
    .bp3-tag-input .bp3-tag.bp3-active{
      outline:rgba(19, 124, 189, 0.6) auto 2px;
      outline-offset:0;
      -moz-outline-radius:6px; }
  .bp3-tag-input .bp3-input-ghost{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:80px;
    line-height:20px; }
    .bp3-tag-input .bp3-input-ghost:disabled, .bp3-tag-input .bp3-input-ghost.bp3-disabled{
      cursor:not-allowed; }
  .bp3-tag-input .bp3-button,
  .bp3-tag-input .bp3-spinner{
    margin:3px;
    margin-left:0; }
  .bp3-tag-input .bp3-button{
    min-width:24px;
    min-height:24px;
    padding:0 7px; }
  .bp3-tag-input.bp3-large{
    height:auto;
    min-height:40px; }
    .bp3-tag-input.bp3-large::before,
    .bp3-tag-input.bp3-large > *{
      margin-right:10px; }
    .bp3-tag-input.bp3-large:empty::before,
    .bp3-tag-input.bp3-large > :last-child{
      margin-right:0; }
    .bp3-tag-input.bp3-large .bp3-tag-input-icon{
      margin-top:10px;
      margin-left:5px; }
    .bp3-tag-input.bp3-large .bp3-input-ghost{
      line-height:30px; }
    .bp3-tag-input.bp3-large .bp3-button{
      min-width:30px;
      min-height:30px;
      padding:5px 10px;
      margin:5px;
      margin-left:0; }
    .bp3-tag-input.bp3-large .bp3-spinner{
      margin:8px;
      margin-left:0; }
  .bp3-tag-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
    background-color:#ffffff; }
    .bp3-tag-input.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-tag-input .bp3-tag-input-icon, .bp3-tag-input.bp3-dark .bp3-tag-input-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-tag-input .bp3-input-ghost, .bp3-tag-input.bp3-dark .bp3-input-ghost{
    color:#f5f8fa; }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{
      color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tag-input.bp3-active, .bp3-tag-input.bp3-dark.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background-color:rgba(16, 22, 26, 0.3); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-input-ghost{
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  background:none;
  padding:0; }
  .bp3-input-ghost::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost:focus{
    outline:none !important; }
.bp3-toast{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  position:relative !important;
  margin:20px 0 0;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  background-color:#ffffff;
  min-width:300px;
  max-width:500px;
  pointer-events:all; }
  .bp3-toast.bp3-toast-enter, .bp3-toast.bp3-toast-appear{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active, .bp3-toast.bp3-toast-appear-active{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-toast.bp3-toast-enter ~ .bp3-toast, .bp3-toast.bp3-toast-appear ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active ~ .bp3-toast, .bp3-toast.bp3-toast-appear-active ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-toast.bp3-toast-exit{
    opacity:1;
    -webkit-filter:blur(0);
            filter:blur(0); }
  .bp3-toast.bp3-toast-exit-active{
    opacity:0;
    -webkit-filter:blur(10px);
            filter:blur(10px);
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:opacity, filter;
    transition-property:opacity, filter, -webkit-filter;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-toast.bp3-toast-exit ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0); }
  .bp3-toast.bp3-toast-exit-active ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:50ms;
            transition-delay:50ms; }
  .bp3-toast .bp3-button-group{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    padding:5px;
    padding-left:0; }
  .bp3-toast > .bp3-icon{
    margin:12px;
    margin-right:0;
    color:#5c7080; }
  .bp3-toast.bp3-dark,
  .bp3-dark .bp3-toast{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
    background-color:#394b59; }
    .bp3-toast.bp3-dark > .bp3-icon,
    .bp3-dark .bp3-toast > .bp3-icon{
      color:#a7b6c2; }
  .bp3-toast[class*="bp3-intent-"] a{
    color:rgba(255, 255, 255, 0.7); }
    .bp3-toast[class*="bp3-intent-"] a:hover{
      color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] > .bp3-icon{
    color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button, .bp3-toast[class*="bp3-intent-"] .bp3-button::before,
  .bp3-toast[class*="bp3-intent-"] .bp3-button .bp3-icon, .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    color:rgba(255, 255, 255, 0.7) !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:focus{
    outline-color:rgba(255, 255, 255, 0.5); }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:hover{
    background-color:rgba(255, 255, 255, 0.15) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    background-color:rgba(255, 255, 255, 0.3) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button::after{
    background:rgba(255, 255, 255, 0.3) !important; }
  .bp3-toast.bp3-intent-primary{
    background-color:#137cbd;
    color:#ffffff; }
  .bp3-toast.bp3-intent-success{
    background-color:#0f9960;
    color:#ffffff; }
  .bp3-toast.bp3-intent-warning{
    background-color:#d9822b;
    color:#ffffff; }
  .bp3-toast.bp3-intent-danger{
    background-color:#db3737;
    color:#ffffff; }

.bp3-toast-message{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  padding:11px;
  word-break:break-word; }

.bp3-toast-container{
  display:-webkit-box !important;
  display:-ms-flexbox !important;
  display:flex !important;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  position:fixed;
  right:0;
  left:0;
  z-index:40;
  overflow:hidden;
  padding:0 20px 20px;
  pointer-events:none; }
  .bp3-toast-container.bp3-toast-container-top{
    top:0;
    bottom:auto; }
  .bp3-toast-container.bp3-toast-container-bottom{
    -webkit-box-orient:vertical;
    -webkit-box-direction:reverse;
        -ms-flex-direction:column-reverse;
            flex-direction:column-reverse;
    top:auto;
    bottom:0; }
  .bp3-toast-container.bp3-toast-container-left{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
  .bp3-toast-container.bp3-toast-container-right{
    -webkit-box-align:end;
        -ms-flex-align:end;
            align-items:flex-end; }

.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active) ~ .bp3-toast,
.bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active ~ .bp3-toast{
  -webkit-transform:translateY(60px);
          transform:translateY(60px); }
.bp3-tooltip{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1); }
  .bp3-tooltip .bp3-popover-arrow{
    position:absolute;
    width:22px;
    height:22px; }
    .bp3-tooltip .bp3-popover-arrow::before{
      margin:4px;
      width:14px;
      height:14px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip{
    margin-top:-11px;
    margin-bottom:11px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
      bottom:-8px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip{
    margin-left:11px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
      left:-8px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip{
    margin-top:11px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
      top:-8px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip{
    margin-right:11px;
    margin-left:-11px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
      right:-8px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-tooltip > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-tooltip > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
    top:-0.22183px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
    right:-0.22183px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
    left:-0.22183px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
    bottom:-0.22183px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-tooltip .bp3-popover-content{
    background:#394b59;
    color:#f5f8fa; }
  .bp3-tooltip .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-tooltip .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-tooltip .bp3-popover-arrow-fill{
    fill:#394b59; }
  .bp3-popover-enter > .bp3-tooltip, .bp3-popover-appear > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8); }
  .bp3-popover-enter-active > .bp3-tooltip, .bp3-popover-appear-active > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-popover-exit > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-tooltip .bp3-popover-content{
    padding:10px 12px; }
  .bp3-tooltip.bp3-dark,
  .bp3-dark .bp3-tooltip{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-tooltip .bp3-popover-content{
      background:#e1e8ed;
      color:#394b59; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{
      fill:#e1e8ed; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-content{
    background:#137cbd;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{
    fill:#137cbd; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-content{
    background:#0f9960;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{
    fill:#0f9960; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-content{
    background:#d9822b;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{
    fill:#d9822b; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-content{
    background:#db3737;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{
    fill:#db3737; }

.bp3-tooltip-indicator{
  border-bottom:dotted 1px;
  cursor:help; }
.bp3-tree .bp3-icon, .bp3-tree .bp3-icon-standard, .bp3-tree .bp3-icon-large{
  color:#5c7080; }
  .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-tree .bp3-icon.bp3-intent-success, .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-tree-node-list{
  margin:0;
  padding-left:0;
  list-style:none; }

.bp3-tree-root{
  position:relative;
  background-color:transparent;
  cursor:default;
  padding-left:0; }

.bp3-tree-node-content-0{
  padding-left:0px; }

.bp3-tree-node-content-1{
  padding-left:23px; }

.bp3-tree-node-content-2{
  padding-left:46px; }

.bp3-tree-node-content-3{
  padding-left:69px; }

.bp3-tree-node-content-4{
  padding-left:92px; }

.bp3-tree-node-content-5{
  padding-left:115px; }

.bp3-tree-node-content-6{
  padding-left:138px; }

.bp3-tree-node-content-7{
  padding-left:161px; }

.bp3-tree-node-content-8{
  padding-left:184px; }

.bp3-tree-node-content-9{
  padding-left:207px; }

.bp3-tree-node-content-10{
  padding-left:230px; }

.bp3-tree-node-content-11{
  padding-left:253px; }

.bp3-tree-node-content-12{
  padding-left:276px; }

.bp3-tree-node-content-13{
  padding-left:299px; }

.bp3-tree-node-content-14{
  padding-left:322px; }

.bp3-tree-node-content-15{
  padding-left:345px; }

.bp3-tree-node-content-16{
  padding-left:368px; }

.bp3-tree-node-content-17{
  padding-left:391px; }

.bp3-tree-node-content-18{
  padding-left:414px; }

.bp3-tree-node-content-19{
  padding-left:437px; }

.bp3-tree-node-content-20{
  padding-left:460px; }

.bp3-tree-node-content{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  width:100%;
  height:30px;
  padding-right:5px; }
  .bp3-tree-node-content:hover{
    background-color:rgba(191, 204, 214, 0.4); }

.bp3-tree-node-caret,
.bp3-tree-node-caret-none{
  min-width:30px; }

.bp3-tree-node-caret{
  color:#5c7080;
  -webkit-transform:rotate(0deg);
          transform:rotate(0deg);
  cursor:pointer;
  padding:7px;
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tree-node-caret:hover{
    color:#182026; }
  .bp3-dark .bp3-tree-node-caret{
    color:#a7b6c2; }
    .bp3-dark .bp3-tree-node-caret:hover{
      color:#f5f8fa; }
  .bp3-tree-node-caret.bp3-tree-node-caret-open{
    -webkit-transform:rotate(90deg);
            transform:rotate(90deg); }
  .bp3-tree-node-caret.bp3-icon-standard::before{
    content:""; }

.bp3-tree-node-icon{
  position:relative;
  margin-right:7px; }

.bp3-tree-node-label{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  position:relative;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-label span{
    display:inline; }

.bp3-tree-node-secondary-label{
  padding:0 5px;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-secondary-label .bp3-popover-wrapper,
  .bp3-tree-node-secondary-label .bp3-popover-target{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center; }

.bp3-tree-node.bp3-disabled .bp3-tree-node-content{
  background-color:inherit;
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-tree-node.bp3-disabled .bp3-tree-node-caret,
.bp3-tree-node.bp3-disabled .bp3-tree-node-icon{
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content,
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-standard, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-large{
    color:#ffffff; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret::before{
    color:rgba(255, 255, 255, 0.7); }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret:hover::before{
    color:#ffffff; }

.bp3-dark .bp3-tree-node-content:hover{
  background-color:rgba(92, 112, 128, 0.3); }

.bp3-dark .bp3-tree .bp3-icon, .bp3-dark .bp3-tree .bp3-icon-standard, .bp3-dark .bp3-tree .bp3-icon-large{
  color:#a7b6c2; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-dark .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
/*!

Copyright 2017-present Palantir Technologies, Inc. All rights reserved.
Licensed under the Apache License, Version 2.0.

*/
.bp3-omnibar{
  -webkit-filter:blur(0);
          filter:blur(0);
  opacity:1;
  top:20vh;
  left:calc(50% - 250px);
  z-index:21;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  background-color:#ffffff;
  width:500px; }
  .bp3-omnibar.bp3-overlay-enter, .bp3-omnibar.bp3-overlay-appear{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2; }
  .bp3-omnibar.bp3-overlay-enter-active, .bp3-omnibar.bp3-overlay-appear-active{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-omnibar.bp3-overlay-exit{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1; }
  .bp3-omnibar.bp3-overlay-exit-active{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-omnibar .bp3-input{
    border-radius:0;
    background-color:transparent; }
    .bp3-omnibar .bp3-input, .bp3-omnibar .bp3-input:focus{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-omnibar .bp3-menu{
    border-radius:0;
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
    background-color:transparent;
    max-height:calc(60vh - 40px);
    overflow:auto; }
    .bp3-omnibar .bp3-menu:empty{
      display:none; }
  .bp3-dark .bp3-omnibar, .bp3-omnibar.bp3-dark{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    background-color:#30404d; }

.bp3-omnibar-overlay .bp3-overlay-backdrop{
  background-color:rgba(16, 22, 26, 0.2); }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-width:400px;
  max-height:300px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }

.bp3-multi-select{
  min-width:150px; }

.bp3-multi-select-popover .bp3-menu{
  max-width:400px;
  max-height:300px;
  overflow:auto; }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-width:400px;
  max-height:300px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);
  --jp-icon-listings-info: url(data:image/svg+xml;base64,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);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}
.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}
.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}
.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}
.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}
.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}
.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}
.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}
.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}
.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}
.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}
.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}
.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}
.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}
.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}
.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}
.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}
.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}
.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}
.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}
.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}
.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}
.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}
.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}
.jp-FileIcon {
  background-image: var(--jp-icon-file);
}
.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}
.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}
.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}
.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}
.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}
.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}
.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}
.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}
.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}
.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}
.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}
.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}
.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}
.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}
.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}
.jp-ListIcon {
  background-image: var(--jp-icon-list);
}
.jp-ListingsInfoIcon {
  background-image: var(--jp-icon-listings-info);
}
.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}
.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}
.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}
.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}
.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}
.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}
.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}
.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}
.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}
.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}
.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}
.jp-RunIcon {
  background-image: var(--jp-icon-run);
}
.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}
.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}
.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}
.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}
.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}
.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}
.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}
.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}
.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}
.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}
.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}
.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}
.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

:root {
  --jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
}

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}
/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}
/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}
/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}
.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}
.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}
.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}
.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}
.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}
.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}
.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}
.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}
/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}
.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}
.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}
.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}
.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}
.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}
.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}
/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

/* CSS for icons in selected items in the settings editor */
#setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
#setting-editor
  .jp-PluginList
  .jp-mod-selected
  .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected tabs in the sidebar tab manager */
#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill] {
  fill: #fff;
}

#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable[fill] {
  fill: var(--jp-brand-color1);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable-inverse[fill] {
  fill: #fff;
}

/**
 * TODO: come up with non css-hack solution for showing the busy icon on top
 *  of the close icon
 * CSS for complex behavior of close icon of tabs in the sidebar tab manager
 */
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-dirty.jp-mod-active
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: #fff;
}

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) svg {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

/* Override Blueprint's _reset.scss styles */
html {
  box-sizing: unset;
}

*,
*::before,
*::after {
  box-sizing: unset;
}

body {
  color: unset;
  font-family: var(--jp-ui-font-family);
}

p {
  margin-top: unset;
  margin-bottom: unset;
}

small {
  font-size: unset;
}

strong {
  font-weight: unset;
}

/* Override Blueprint's _typography.scss styles */
a {
  text-decoration: unset;
  color: unset;
}
a:hover {
  text-decoration: unset;
  color: unset;
}

/* Override Blueprint's _accessibility.scss styles */
:focus {
  outline: unset;
  outline-offset: unset;
  -moz-outline-radius: unset;
}

/* Styles for ui-components */
.jp-Button {
  border-radius: var(--jp-border-radius);
  padding: 0px 12px;
  font-size: var(--jp-ui-font-size1);
}

/* Use our own theme for hover styles */
button.jp-Button.bp3-button.bp3-minimal:hover {
  background-color: var(--jp-layout-color2);
}
.jp-Button.minimal {
  color: unset !important;
}

.jp-Button.jp-ToolbarButtonComponent {
  text-transform: none;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color3);
}

.jp-BPIcon {
  display: inline-block;
  vertical-align: middle;
  margin: auto;
}

/* Stop blueprint futzing with our icon fills */
.bp3-icon.jp-BPIcon > svg:not([fill]) {
  fill: var(--jp-inverse-layout-color3);
}

.jp-InputGroupAction {
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

/* Use our own theme for hover and option styles */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}
select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-top: 1px solid var(--jp-border-color2);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-Collapse-header {
  padding: 1px 12px;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size2);
}

.jp-Collapse-header:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Collapse-contents {
  padding: 0px 12px 0px 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0px;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0px 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.lm-CommandPalette-wrapper::after {
  content: ' ';
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  height: 30px;
  width: 10px;
  padding: 0px 10px;
  background-image: var(--jp-icon-search-white);
  background-size: 20px;
  background-repeat: no-repeat;
  background-position: center;
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color3);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0px;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color3);
}

.lm-CommandPalette-item.lm-mod-active {
  background: var(--jp-layout-color3);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  background: var(--jp-layout-color4);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color3);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.4;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty:after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0px 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0px;
  left: 0px;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px;
  padding-bottom: 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0px;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

.jp-Dialog-header {
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0px 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

.jp-HoverBox.jp-mod-outofview {
  display: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;

  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;

  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #aa00ff;

  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;

  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;

  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;

  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;

  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;

  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;

  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;

  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;

  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;

  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ffff00;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;

  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;

  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;

  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;

  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;

  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eeeeee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;

  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent:before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent:after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0px 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  height: 28px;
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  background-color: var(--jp-layout-color1);
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0px 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  height: 32px;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 1;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px;
  margin: 0px;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px 6px;
  margin: 0px;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent span {
  padding: 0px;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ body.p-mod-override-cursor *, /* </DEPRECATED> */
body.lm-mod-override-cursor * {
  cursor: inherit !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* BASICS */

.CodeMirror {
  /* Set height, width, borders, and global font properties here */
  font-family: monospace;
  height: 300px;
  color: black;
  direction: ltr;
}

/* PADDING */

.CodeMirror-lines {
  padding: 4px 0; /* Vertical padding around content */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  padding: 0 4px; /* Horizontal padding of content */
}

.CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  background-color: white; /* The little square between H and V scrollbars */
}

/* GUTTER */

.CodeMirror-gutters {
  border-right: 1px solid #ddd;
  background-color: #f7f7f7;
  white-space: nowrap;
}
.CodeMirror-linenumbers {}
.CodeMirror-linenumber {
  padding: 0 3px 0 5px;
  min-width: 20px;
  text-align: right;
  color: #999;
  white-space: nowrap;
}

.CodeMirror-guttermarker { color: black; }
.CodeMirror-guttermarker-subtle { color: #999; }

/* CURSOR */

.CodeMirror-cursor {
  border-left: 1px solid black;
  border-right: none;
  width: 0;
}
/* Shown when moving in bi-directional text */
.CodeMirror div.CodeMirror-secondarycursor {
  border-left: 1px solid silver;
}
.cm-fat-cursor .CodeMirror-cursor {
  width: auto;
  border: 0 !important;
  background: #7e7;
}
.cm-fat-cursor div.CodeMirror-cursors {
  z-index: 1;
}
.cm-fat-cursor-mark {
  background-color: rgba(20, 255, 20, 0.5);
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
}
.cm-animate-fat-cursor {
  width: auto;
  border: 0;
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
  background-color: #7e7;
}
@-moz-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@-webkit-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}

/* Can style cursor different in overwrite (non-insert) mode */
.CodeMirror-overwrite .CodeMirror-cursor {}

.cm-tab { display: inline-block; text-decoration: inherit; }

.CodeMirror-rulers {
  position: absolute;
  left: 0; right: 0; top: -50px; bottom: 0;
  overflow: hidden;
}
.CodeMirror-ruler {
  border-left: 1px solid #ccc;
  top: 0; bottom: 0;
  position: absolute;
}

/* DEFAULT THEME */

.cm-s-default .cm-header {color: blue;}
.cm-s-default .cm-quote {color: #090;}
.cm-negative {color: #d44;}
.cm-positive {color: #292;}
.cm-header, .cm-strong {font-weight: bold;}
.cm-em {font-style: italic;}
.cm-link {text-decoration: underline;}
.cm-strikethrough {text-decoration: line-through;}

.cm-s-default .cm-keyword {color: #708;}
.cm-s-default .cm-atom {color: #219;}
.cm-s-default .cm-number {color: #164;}
.cm-s-default .cm-def {color: #00f;}
.cm-s-default .cm-variable,
.cm-s-default .cm-punctuation,
.cm-s-default .cm-property,
.cm-s-default .cm-operator {}
.cm-s-default .cm-variable-2 {color: #05a;}
.cm-s-default .cm-variable-3, .cm-s-default .cm-type {color: #085;}
.cm-s-default .cm-comment {color: #a50;}
.cm-s-default .cm-string {color: #a11;}
.cm-s-default .cm-string-2 {color: #f50;}
.cm-s-default .cm-meta {color: #555;}
.cm-s-default .cm-qualifier {color: #555;}
.cm-s-default .cm-builtin {color: #30a;}
.cm-s-default .cm-bracket {color: #997;}
.cm-s-default .cm-tag {color: #170;}
.cm-s-default .cm-attribute {color: #00c;}
.cm-s-default .cm-hr {color: #999;}
.cm-s-default .cm-link {color: #00c;}

.cm-s-default .cm-error {color: #f00;}
.cm-invalidchar {color: #f00;}

.CodeMirror-composing { border-bottom: 2px solid; }

/* Default styles for common addons */

div.CodeMirror span.CodeMirror-matchingbracket {color: #0b0;}
div.CodeMirror span.CodeMirror-nonmatchingbracket {color: #a22;}
.CodeMirror-matchingtag { background: rgba(255, 150, 0, .3); }
.CodeMirror-activeline-background {background: #e8f2ff;}

/* STOP */

/* The rest of this file contains styles related to the mechanics of
   the editor. You probably shouldn't touch them. */

.CodeMirror {
  position: relative;
  overflow: hidden;
  background: white;
}

.CodeMirror-scroll {
  overflow: scroll !important; /* Things will break if this is overridden */
  /* 30px is the magic margin used to hide the element's real scrollbars */
  /* See overflow: hidden in .CodeMirror */
  margin-bottom: -30px; margin-right: -30px;
  padding-bottom: 30px;
  height: 100%;
  outline: none; /* Prevent dragging from highlighting the element */
  position: relative;
}
.CodeMirror-sizer {
  position: relative;
  border-right: 30px solid transparent;
}

/* The fake, visible scrollbars. Used to force redraw during scrolling
   before actual scrolling happens, thus preventing shaking and
   flickering artifacts. */
.CodeMirror-vscrollbar, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  position: absolute;
  z-index: 6;
  display: none;
}
.CodeMirror-vscrollbar {
  right: 0; top: 0;
  overflow-x: hidden;
  overflow-y: scroll;
}
.CodeMirror-hscrollbar {
  bottom: 0; left: 0;
  overflow-y: hidden;
  overflow-x: scroll;
}
.CodeMirror-scrollbar-filler {
  right: 0; bottom: 0;
}
.CodeMirror-gutter-filler {
  left: 0; bottom: 0;
}

.CodeMirror-gutters {
  position: absolute; left: 0; top: 0;
  min-height: 100%;
  z-index: 3;
}
.CodeMirror-gutter {
  white-space: normal;
  height: 100%;
  display: inline-block;
  vertical-align: top;
  margin-bottom: -30px;
}
.CodeMirror-gutter-wrapper {
  position: absolute;
  z-index: 4;
  background: none !important;
  border: none !important;
}
.CodeMirror-gutter-background {
  position: absolute;
  top: 0; bottom: 0;
  z-index: 4;
}
.CodeMirror-gutter-elt {
  position: absolute;
  cursor: default;
  z-index: 4;
}
.CodeMirror-gutter-wrapper ::selection { background-color: transparent }
.CodeMirror-gutter-wrapper ::-moz-selection { background-color: transparent }

.CodeMirror-lines {
  cursor: text;
  min-height: 1px; /* prevents collapsing before first draw */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  /* Reset some styles that the rest of the page might have set */
  -moz-border-radius: 0; -webkit-border-radius: 0; border-radius: 0;
  border-width: 0;
  background: transparent;
  font-family: inherit;
  font-size: inherit;
  margin: 0;
  white-space: pre;
  word-wrap: normal;
  line-height: inherit;
  color: inherit;
  z-index: 2;
  position: relative;
  overflow: visible;
  -webkit-tap-highlight-color: transparent;
  -webkit-font-variant-ligatures: contextual;
  font-variant-ligatures: contextual;
}
.CodeMirror-wrap pre.CodeMirror-line,
.CodeMirror-wrap pre.CodeMirror-line-like {
  word-wrap: break-word;
  white-space: pre-wrap;
  word-break: normal;
}

.CodeMirror-linebackground {
  position: absolute;
  left: 0; right: 0; top: 0; bottom: 0;
  z-index: 0;
}

.CodeMirror-linewidget {
  position: relative;
  z-index: 2;
  padding: 0.1px; /* Force widget margins to stay inside of the container */
}

.CodeMirror-widget {}

.CodeMirror-rtl pre { direction: rtl; }

.CodeMirror-code {
  outline: none;
}

/* Force content-box sizing for the elements where we expect it */
.CodeMirror-scroll,
.CodeMirror-sizer,
.CodeMirror-gutter,
.CodeMirror-gutters,
.CodeMirror-linenumber {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}

.CodeMirror-measure {
  position: absolute;
  width: 100%;
  height: 0;
  overflow: hidden;
  visibility: hidden;
}

.CodeMirror-cursor {
  position: absolute;
  pointer-events: none;
}
.CodeMirror-measure pre { position: static; }

div.CodeMirror-cursors {
  visibility: hidden;
  position: relative;
  z-index: 3;
}
div.CodeMirror-dragcursors {
  visibility: visible;
}

.CodeMirror-focused div.CodeMirror-cursors {
  visibility: visible;
}

.CodeMirror-selected { background: #d9d9d9; }
.CodeMirror-focused .CodeMirror-selected { background: #d7d4f0; }
.CodeMirror-crosshair { cursor: crosshair; }
.CodeMirror-line::selection, .CodeMirror-line > span::selection, .CodeMirror-line > span > span::selection { background: #d7d4f0; }
.CodeMirror-line::-moz-selection, .CodeMirror-line > span::-moz-selection, .CodeMirror-line > span > span::-moz-selection { background: #d7d4f0; }

.cm-searching {
  background-color: #ffa;
  background-color: rgba(255, 255, 0, .4);
}

/* Used to force a border model for a node */
.cm-force-border { padding-right: .1px; }

@media print {
  /* Hide the cursor when printing */
  .CodeMirror div.CodeMirror-cursors {
    visibility: hidden;
  }
}

/* See issue #2901 */
.cm-tab-wrap-hack:after { content: ''; }

/* Help users use markselection to safely style text background */
span.CodeMirror-selectedtext { background: none; }

.CodeMirror-dialog {
  position: absolute;
  left: 0; right: 0;
  background: inherit;
  z-index: 15;
  padding: .1em .8em;
  overflow: hidden;
  color: inherit;
}

.CodeMirror-dialog-top {
  border-bottom: 1px solid #eee;
  top: 0;
}

.CodeMirror-dialog-bottom {
  border-top: 1px solid #eee;
  bottom: 0;
}

.CodeMirror-dialog input {
  border: none;
  outline: none;
  background: transparent;
  width: 20em;
  color: inherit;
  font-family: monospace;
}

.CodeMirror-dialog button {
  font-size: 70%;
}

.CodeMirror-foldmarker {
  color: blue;
  text-shadow: #b9f 1px 1px 2px, #b9f -1px -1px 2px, #b9f 1px -1px 2px, #b9f -1px 1px 2px;
  font-family: arial;
  line-height: .3;
  cursor: pointer;
}
.CodeMirror-foldgutter {
  width: .7em;
}
.CodeMirror-foldgutter-open,
.CodeMirror-foldgutter-folded {
  cursor: pointer;
}
.CodeMirror-foldgutter-open:after {
  content: "\25BE";
}
.CodeMirror-foldgutter-folded:after {
  content: "\25B8";
}

/*
  Name:       material
  Author:     Mattia Astorino (http://github.com/equinusocio)
  Website:    https://material-theme.site/
*/

.cm-s-material.CodeMirror {
  background-color: #263238;
  color: #EEFFFF;
}

.cm-s-material .CodeMirror-gutters {
  background: #263238;
  color: #546E7A;
  border: none;
}

.cm-s-material .CodeMirror-guttermarker,
.cm-s-material .CodeMirror-guttermarker-subtle,
.cm-s-material .CodeMirror-linenumber {
  color: #546E7A;
}

.cm-s-material .CodeMirror-cursor {
  border-left: 1px solid #FFCC00;
}

.cm-s-material div.CodeMirror-selected {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material.CodeMirror-focused div.CodeMirror-selected {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material .CodeMirror-line::selection,
.cm-s-material .CodeMirror-line>span::selection,
.cm-s-material .CodeMirror-line>span>span::selection {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material .CodeMirror-line::-moz-selection,
.cm-s-material .CodeMirror-line>span::-moz-selection,
.cm-s-material .CodeMirror-line>span>span::-moz-selection {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material .CodeMirror-activeline-background {
  background: rgba(0, 0, 0, 0.5);
}

.cm-s-material .cm-keyword {
  color: #C792EA;
}

.cm-s-material .cm-operator {
  color: #89DDFF;
}

.cm-s-material .cm-variable-2 {
  color: #EEFFFF;
}

.cm-s-material .cm-variable-3,
.cm-s-material .cm-type {
  color: #f07178;
}

.cm-s-material .cm-builtin {
  color: #FFCB6B;
}

.cm-s-material .cm-atom {
  color: #F78C6C;
}

.cm-s-material .cm-number {
  color: #FF5370;
}

.cm-s-material .cm-def {
  color: #82AAFF;
}

.cm-s-material .cm-string {
  color: #C3E88D;
}

.cm-s-material .cm-string-2 {
  color: #f07178;
}

.cm-s-material .cm-comment {
  color: #546E7A;
}

.cm-s-material .cm-variable {
  color: #f07178;
}

.cm-s-material .cm-tag {
  color: #FF5370;
}

.cm-s-material .cm-meta {
  color: #FFCB6B;
}

.cm-s-material .cm-attribute {
  color: #C792EA;
}

.cm-s-material .cm-property {
  color: #C792EA;
}

.cm-s-material .cm-qualifier {
  color: #DECB6B;
}

.cm-s-material .cm-variable-3,
.cm-s-material .cm-type {
  color: #DECB6B;
}


.cm-s-material .cm-error {
  color: rgba(255, 255, 255, 1.0);
  background-color: #FF5370;
}

.cm-s-material .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: white !important;
}
/**
 * "
 *  Using Zenburn color palette from the Emacs Zenburn Theme
 *  https://github.com/bbatsov/zenburn-emacs/blob/master/zenburn-theme.el
 *
 *  Also using parts of https://github.com/xavi/coderay-lighttable-theme
 * "
 * From: https://github.com/wisenomad/zenburn-lighttable-theme/blob/master/zenburn.css
 */

.cm-s-zenburn .CodeMirror-gutters { background: #3f3f3f !important; }
.cm-s-zenburn .CodeMirror-foldgutter-open, .CodeMirror-foldgutter-folded { color: #999; }
.cm-s-zenburn .CodeMirror-cursor { border-left: 1px solid white; }
.cm-s-zenburn { background-color: #3f3f3f; color: #dcdccc; }
.cm-s-zenburn span.cm-builtin { color: #dcdccc; font-weight: bold; }
.cm-s-zenburn span.cm-comment { color: #7f9f7f; }
.cm-s-zenburn span.cm-keyword { color: #f0dfaf; font-weight: bold; }
.cm-s-zenburn span.cm-atom { color: #bfebbf; }
.cm-s-zenburn span.cm-def { color: #dcdccc; }
.cm-s-zenburn span.cm-variable { color: #dfaf8f; }
.cm-s-zenburn span.cm-variable-2 { color: #dcdccc; }
.cm-s-zenburn span.cm-string { color: #cc9393; }
.cm-s-zenburn span.cm-string-2 { color: #cc9393; }
.cm-s-zenburn span.cm-number { color: #dcdccc; }
.cm-s-zenburn span.cm-tag { color: #93e0e3; }
.cm-s-zenburn span.cm-property { color: #dfaf8f; }
.cm-s-zenburn span.cm-attribute { color: #dfaf8f; }
.cm-s-zenburn span.cm-qualifier { color: #7cb8bb; }
.cm-s-zenburn span.cm-meta { color: #f0dfaf; }
.cm-s-zenburn span.cm-header { color: #f0efd0; }
.cm-s-zenburn span.cm-operator { color: #f0efd0; }
.cm-s-zenburn span.CodeMirror-matchingbracket { box-sizing: border-box; background: transparent; border-bottom: 1px solid; }
.cm-s-zenburn span.CodeMirror-nonmatchingbracket { border-bottom: 1px solid; background: none; }
.cm-s-zenburn .CodeMirror-activeline { background: #000000; }
.cm-s-zenburn .CodeMirror-activeline-background { background: #000000; }
.cm-s-zenburn div.CodeMirror-selected { background: #545454; }
.cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected { background: #4f4f4f; }

.cm-s-abcdef.CodeMirror { background: #0f0f0f; color: #defdef; }
.cm-s-abcdef div.CodeMirror-selected { background: #515151; }
.cm-s-abcdef .CodeMirror-line::selection, .cm-s-abcdef .CodeMirror-line > span::selection, .cm-s-abcdef .CodeMirror-line > span > span::selection { background: rgba(56, 56, 56, 0.99); }
.cm-s-abcdef .CodeMirror-line::-moz-selection, .cm-s-abcdef .CodeMirror-line > span::-moz-selection, .cm-s-abcdef .CodeMirror-line > span > span::-moz-selection { background: rgba(56, 56, 56, 0.99); }
.cm-s-abcdef .CodeMirror-gutters { background: #555; border-right: 2px solid #314151; }
.cm-s-abcdef .CodeMirror-guttermarker { color: #222; }
.cm-s-abcdef .CodeMirror-guttermarker-subtle { color: azure; }
.cm-s-abcdef .CodeMirror-linenumber { color: #FFFFFF; }
.cm-s-abcdef .CodeMirror-cursor { border-left: 1px solid #00FF00; }

.cm-s-abcdef span.cm-keyword { color: darkgoldenrod; font-weight: bold; }
.cm-s-abcdef span.cm-atom { color: #77F; }
.cm-s-abcdef span.cm-number { color: violet; }
.cm-s-abcdef span.cm-def { color: #fffabc; }
.cm-s-abcdef span.cm-variable { color: #abcdef; }
.cm-s-abcdef span.cm-variable-2 { color: #cacbcc; }
.cm-s-abcdef span.cm-variable-3, .cm-s-abcdef span.cm-type { color: #def; }
.cm-s-abcdef span.cm-property { color: #fedcba; }
.cm-s-abcdef span.cm-operator { color: #ff0; }
.cm-s-abcdef span.cm-comment { color: #7a7b7c; font-style: italic;}
.cm-s-abcdef span.cm-string { color: #2b4; }
.cm-s-abcdef span.cm-meta { color: #C9F; }
.cm-s-abcdef span.cm-qualifier { color: #FFF700; }
.cm-s-abcdef span.cm-builtin { color: #30aabc; }
.cm-s-abcdef span.cm-bracket { color: #8a8a8a; }
.cm-s-abcdef span.cm-tag { color: #FFDD44; }
.cm-s-abcdef span.cm-attribute { color: #DDFF00; }
.cm-s-abcdef span.cm-error { color: #FF0000; }
.cm-s-abcdef span.cm-header { color: aquamarine; font-weight: bold; }
.cm-s-abcdef span.cm-link { color: blueviolet; }

.cm-s-abcdef .CodeMirror-activeline-background { background: #314151; }

/*

    Name:       Base16 Default Light
    Author:     Chris Kempson (http://chriskempson.com)

    CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror)
    Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16)

*/

.cm-s-base16-light.CodeMirror { background: #f5f5f5; color: #202020; }
.cm-s-base16-light div.CodeMirror-selected { background: #e0e0e0; }
.cm-s-base16-light .CodeMirror-line::selection, .cm-s-base16-light .CodeMirror-line > span::selection, .cm-s-base16-light .CodeMirror-line > span > span::selection { background: #e0e0e0; }
.cm-s-base16-light .CodeMirror-line::-moz-selection, .cm-s-base16-light .CodeMirror-line > span::-moz-selection, .cm-s-base16-light .CodeMirror-line > span > span::-moz-selection { background: #e0e0e0; }
.cm-s-base16-light .CodeMirror-gutters { background: #f5f5f5; border-right: 0px; }
.cm-s-base16-light .CodeMirror-guttermarker { color: #ac4142; }
.cm-s-base16-light .CodeMirror-guttermarker-subtle { color: #b0b0b0; }
.cm-s-base16-light .CodeMirror-linenumber { color: #b0b0b0; }
.cm-s-base16-light .CodeMirror-cursor { border-left: 1px solid #505050; }

.cm-s-base16-light span.cm-comment { color: #8f5536; }
.cm-s-base16-light span.cm-atom { color: #aa759f; }
.cm-s-base16-light span.cm-number { color: #aa759f; }

.cm-s-base16-light span.cm-property, .cm-s-base16-light span.cm-attribute { color: #90a959; }
.cm-s-base16-light span.cm-keyword { color: #ac4142; }
.cm-s-base16-light span.cm-string { color: #f4bf75; }

.cm-s-base16-light span.cm-variable { color: #90a959; }
.cm-s-base16-light span.cm-variable-2 { color: #6a9fb5; }
.cm-s-base16-light span.cm-def { color: #d28445; }
.cm-s-base16-light span.cm-bracket { color: #202020; }
.cm-s-base16-light span.cm-tag { color: #ac4142; }
.cm-s-base16-light span.cm-link { color: #aa759f; }
.cm-s-base16-light span.cm-error { background: #ac4142; color: #505050; }

.cm-s-base16-light .CodeMirror-activeline-background { background: #DDDCDC; }
.cm-s-base16-light .CodeMirror-matchingbracket { color: #f5f5f5 !important; background-color: #6A9FB5 !important}

/*

    Name:       Base16 Default Dark
    Author:     Chris Kempson (http://chriskempson.com)

    CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror)
    Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16)

*/

.cm-s-base16-dark.CodeMirror { background: #151515; color: #e0e0e0; }
.cm-s-base16-dark div.CodeMirror-selected { background: #303030; }
.cm-s-base16-dark .CodeMirror-line::selection, .cm-s-base16-dark .CodeMirror-line > span::selection, .cm-s-base16-dark .CodeMirror-line > span > span::selection { background: rgba(48, 48, 48, .99); }
.cm-s-base16-dark .CodeMirror-line::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(48, 48, 48, .99); }
.cm-s-base16-dark .CodeMirror-gutters { background: #151515; border-right: 0px; }
.cm-s-base16-dark .CodeMirror-guttermarker { color: #ac4142; }
.cm-s-base16-dark .CodeMirror-guttermarker-subtle { color: #505050; }
.cm-s-base16-dark .CodeMirror-linenumber { color: #505050; }
.cm-s-base16-dark .CodeMirror-cursor { border-left: 1px solid #b0b0b0; }

.cm-s-base16-dark span.cm-comment { color: #8f5536; }
.cm-s-base16-dark span.cm-atom { color: #aa759f; }
.cm-s-base16-dark span.cm-number { color: #aa759f; }

.cm-s-base16-dark span.cm-property, .cm-s-base16-dark span.cm-attribute { color: #90a959; }
.cm-s-base16-dark span.cm-keyword { color: #ac4142; }
.cm-s-base16-dark span.cm-string { color: #f4bf75; }

.cm-s-base16-dark span.cm-variable { color: #90a959; }
.cm-s-base16-dark span.cm-variable-2 { color: #6a9fb5; }
.cm-s-base16-dark span.cm-def { color: #d28445; }
.cm-s-base16-dark span.cm-bracket { color: #e0e0e0; }
.cm-s-base16-dark span.cm-tag { color: #ac4142; }
.cm-s-base16-dark span.cm-link { color: #aa759f; }
.cm-s-base16-dark span.cm-error { background: #ac4142; color: #b0b0b0; }

.cm-s-base16-dark .CodeMirror-activeline-background { background: #202020; }
.cm-s-base16-dark .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; }

/*

    Name:       dracula
    Author:     Michael Kaminsky (http://github.com/mkaminsky11)

    Original dracula color scheme by Zeno Rocha (https://github.com/zenorocha/dracula-theme)

*/


.cm-s-dracula.CodeMirror, .cm-s-dracula .CodeMirror-gutters {
  background-color: #282a36 !important;
  color: #f8f8f2 !important;
  border: none;
}
.cm-s-dracula .CodeMirror-gutters { color: #282a36; }
.cm-s-dracula .CodeMirror-cursor { border-left: solid thin #f8f8f0; }
.cm-s-dracula .CodeMirror-linenumber { color: #6D8A88; }
.cm-s-dracula .CodeMirror-selected { background: rgba(255, 255, 255, 0.10); }
.cm-s-dracula .CodeMirror-line::selection, .cm-s-dracula .CodeMirror-line > span::selection, .cm-s-dracula .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-dracula .CodeMirror-line::-moz-selection, .cm-s-dracula .CodeMirror-line > span::-moz-selection, .cm-s-dracula .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-dracula span.cm-comment { color: #6272a4; }
.cm-s-dracula span.cm-string, .cm-s-dracula span.cm-string-2 { color: #f1fa8c; }
.cm-s-dracula span.cm-number { color: #bd93f9; }
.cm-s-dracula span.cm-variable { color: #50fa7b; }
.cm-s-dracula span.cm-variable-2 { color: white; }
.cm-s-dracula span.cm-def { color: #50fa7b; }
.cm-s-dracula span.cm-operator { color: #ff79c6; }
.cm-s-dracula span.cm-keyword { color: #ff79c6; }
.cm-s-dracula span.cm-atom { color: #bd93f9; }
.cm-s-dracula span.cm-meta { color: #f8f8f2; }
.cm-s-dracula span.cm-tag { color: #ff79c6; }
.cm-s-dracula span.cm-attribute { color: #50fa7b; }
.cm-s-dracula span.cm-qualifier { color: #50fa7b; }
.cm-s-dracula span.cm-property { color: #66d9ef; }
.cm-s-dracula span.cm-builtin { color: #50fa7b; }
.cm-s-dracula span.cm-variable-3, .cm-s-dracula span.cm-type { color: #ffb86c; }

.cm-s-dracula .CodeMirror-activeline-background { background: rgba(255,255,255,0.1); }
.cm-s-dracula .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; }

/*

    Name:       Hopscotch
    Author:     Jan T. Sott

    CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror)
    Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16)

*/

.cm-s-hopscotch.CodeMirror {background: #322931; color: #d5d3d5;}
.cm-s-hopscotch div.CodeMirror-selected {background: #433b42 !important;}
.cm-s-hopscotch .CodeMirror-gutters {background: #322931; border-right: 0px;}
.cm-s-hopscotch .CodeMirror-linenumber {color: #797379;}
.cm-s-hopscotch .CodeMirror-cursor {border-left: 1px solid #989498 !important;}

.cm-s-hopscotch span.cm-comment {color: #b33508;}
.cm-s-hopscotch span.cm-atom {color: #c85e7c;}
.cm-s-hopscotch span.cm-number {color: #c85e7c;}

.cm-s-hopscotch span.cm-property, .cm-s-hopscotch span.cm-attribute {color: #8fc13e;}
.cm-s-hopscotch span.cm-keyword {color: #dd464c;}
.cm-s-hopscotch span.cm-string {color: #fdcc59;}

.cm-s-hopscotch span.cm-variable {color: #8fc13e;}
.cm-s-hopscotch span.cm-variable-2 {color: #1290bf;}
.cm-s-hopscotch span.cm-def {color: #fd8b19;}
.cm-s-hopscotch span.cm-error {background: #dd464c; color: #989498;}
.cm-s-hopscotch span.cm-bracket {color: #d5d3d5;}
.cm-s-hopscotch span.cm-tag {color: #dd464c;}
.cm-s-hopscotch span.cm-link {color: #c85e7c;}

.cm-s-hopscotch .CodeMirror-matchingbracket { text-decoration: underline; color: white !important;}
.cm-s-hopscotch .CodeMirror-activeline-background { background: #302020; }

/****************************************************************/
/*   Based on mbonaci's Brackets mbo theme                      */
/*   https://github.com/mbonaci/global/blob/master/Mbo.tmTheme  */
/*   Create your own: http://tmtheme-editor.herokuapp.com       */
/****************************************************************/

.cm-s-mbo.CodeMirror { background: #2c2c2c; color: #ffffec; }
.cm-s-mbo div.CodeMirror-selected { background: #716C62; }
.cm-s-mbo .CodeMirror-line::selection, .cm-s-mbo .CodeMirror-line > span::selection, .cm-s-mbo .CodeMirror-line > span > span::selection { background: rgba(113, 108, 98, .99); }
.cm-s-mbo .CodeMirror-line::-moz-selection, .cm-s-mbo .CodeMirror-line > span::-moz-selection, .cm-s-mbo .CodeMirror-line > span > span::-moz-selection { background: rgba(113, 108, 98, .99); }
.cm-s-mbo .CodeMirror-gutters { background: #4e4e4e; border-right: 0px; }
.cm-s-mbo .CodeMirror-guttermarker { color: white; }
.cm-s-mbo .CodeMirror-guttermarker-subtle { color: grey; }
.cm-s-mbo .CodeMirror-linenumber { color: #dadada; }
.cm-s-mbo .CodeMirror-cursor { border-left: 1px solid #ffffec; }

.cm-s-mbo span.cm-comment { color: #95958a; }
.cm-s-mbo span.cm-atom { color: #00a8c6; }
.cm-s-mbo span.cm-number { color: #00a8c6; }

.cm-s-mbo span.cm-property, .cm-s-mbo span.cm-attribute { color: #9ddfe9; }
.cm-s-mbo span.cm-keyword { color: #ffb928; }
.cm-s-mbo span.cm-string { color: #ffcf6c; }
.cm-s-mbo span.cm-string.cm-property { color: #ffffec; }

.cm-s-mbo span.cm-variable { color: #ffffec; }
.cm-s-mbo span.cm-variable-2 { color: #00a8c6; }
.cm-s-mbo span.cm-def { color: #ffffec; }
.cm-s-mbo span.cm-bracket { color: #fffffc; font-weight: bold; }
.cm-s-mbo span.cm-tag { color: #9ddfe9; }
.cm-s-mbo span.cm-link { color: #f54b07; }
.cm-s-mbo span.cm-error { border-bottom: #636363; color: #ffffec; }
.cm-s-mbo span.cm-qualifier { color: #ffffec; }

.cm-s-mbo .CodeMirror-activeline-background { background: #494b41; }
.cm-s-mbo .CodeMirror-matchingbracket { color: #ffb928 !important; }
.cm-s-mbo .CodeMirror-matchingtag { background: rgba(255, 255, 255, .37); }

/*
  MDN-LIKE Theme - Mozilla
  Ported to CodeMirror by Peter Kroon <plakroon@gmail.com>
  Report bugs/issues here: https://github.com/codemirror/CodeMirror/issues
  GitHub: @peterkroon

  The mdn-like theme is inspired on the displayed code examples at: https://developer.mozilla.org/en-US/docs/Web/CSS/animation

*/
.cm-s-mdn-like.CodeMirror { color: #999; background-color: #fff; }
.cm-s-mdn-like div.CodeMirror-selected { background: #cfc; }
.cm-s-mdn-like .CodeMirror-line::selection, .cm-s-mdn-like .CodeMirror-line > span::selection, .cm-s-mdn-like .CodeMirror-line > span > span::selection { background: #cfc; }
.cm-s-mdn-like .CodeMirror-line::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span > span::-moz-selection { background: #cfc; }

.cm-s-mdn-like .CodeMirror-gutters { background: #f8f8f8; border-left: 6px solid rgba(0,83,159,0.65); color: #333; }
.cm-s-mdn-like .CodeMirror-linenumber { color: #aaa; padding-left: 8px; }
.cm-s-mdn-like .CodeMirror-cursor { border-left: 2px solid #222; }

.cm-s-mdn-like .cm-keyword { color: #6262FF; }
.cm-s-mdn-like .cm-atom { color: #F90; }
.cm-s-mdn-like .cm-number { color:  #ca7841; }
.cm-s-mdn-like .cm-def { color: #8DA6CE; }
.cm-s-mdn-like span.cm-variable-2, .cm-s-mdn-like span.cm-tag { color: #690; }
.cm-s-mdn-like span.cm-variable-3, .cm-s-mdn-like span.cm-def, .cm-s-mdn-like span.cm-type { color: #07a; }

.cm-s-mdn-like .cm-variable { color: #07a; }
.cm-s-mdn-like .cm-property { color: #905; }
.cm-s-mdn-like .cm-qualifier { color: #690; }

.cm-s-mdn-like .cm-operator { color: #cda869; }
.cm-s-mdn-like .cm-comment { color:#777; font-weight:normal; }
.cm-s-mdn-like .cm-string { color:#07a; font-style:italic; }
.cm-s-mdn-like .cm-string-2 { color:#bd6b18; } /*?*/
.cm-s-mdn-like .cm-meta { color: #000; } /*?*/
.cm-s-mdn-like .cm-builtin { color: #9B7536; } /*?*/
.cm-s-mdn-like .cm-tag { color: #997643; }
.cm-s-mdn-like .cm-attribute { color: #d6bb6d; } /*?*/
.cm-s-mdn-like .cm-header { color: #FF6400; }
.cm-s-mdn-like .cm-hr { color: #AEAEAE; }
.cm-s-mdn-like .cm-link { color:#ad9361; font-style:italic; text-decoration:none; }
.cm-s-mdn-like .cm-error { border-bottom: 1px solid red; }

div.cm-s-mdn-like .CodeMirror-activeline-background { background: #efefff; }
div.cm-s-mdn-like span.CodeMirror-matchingbracket { outline:1px solid grey; color: inherit; }

.cm-s-mdn-like.CodeMirror { background-image: url(data:image/png;base64,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); }

/*

    Name:       seti
    Author:     Michael Kaminsky (http://github.com/mkaminsky11)

    Original seti color scheme by Jesse Weed (https://github.com/jesseweed/seti-syntax)

*/


.cm-s-seti.CodeMirror {
  background-color: #151718 !important;
  color: #CFD2D1 !important;
  border: none;
}
.cm-s-seti .CodeMirror-gutters {
  color: #404b53;
  background-color: #0E1112;
  border: none;
}
.cm-s-seti .CodeMirror-cursor { border-left: solid thin #f8f8f0; }
.cm-s-seti .CodeMirror-linenumber { color: #6D8A88; }
.cm-s-seti.CodeMirror-focused div.CodeMirror-selected { background: rgba(255, 255, 255, 0.10); }
.cm-s-seti .CodeMirror-line::selection, .cm-s-seti .CodeMirror-line > span::selection, .cm-s-seti .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-seti .CodeMirror-line::-moz-selection, .cm-s-seti .CodeMirror-line > span::-moz-selection, .cm-s-seti .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-seti span.cm-comment { color: #41535b; }
.cm-s-seti span.cm-string, .cm-s-seti span.cm-string-2 { color: #55b5db; }
.cm-s-seti span.cm-number { color: #cd3f45; }
.cm-s-seti span.cm-variable { color: #55b5db; }
.cm-s-seti span.cm-variable-2 { color: #a074c4; }
.cm-s-seti span.cm-def { color: #55b5db; }
.cm-s-seti span.cm-keyword { color: #ff79c6; }
.cm-s-seti span.cm-operator { color: #9fca56; }
.cm-s-seti span.cm-keyword { color: #e6cd69; }
.cm-s-seti span.cm-atom { color: #cd3f45; }
.cm-s-seti span.cm-meta { color: #55b5db; }
.cm-s-seti span.cm-tag { color: #55b5db; }
.cm-s-seti span.cm-attribute { color: #9fca56; }
.cm-s-seti span.cm-qualifier { color: #9fca56; }
.cm-s-seti span.cm-property { color: #a074c4; }
.cm-s-seti span.cm-variable-3, .cm-s-seti span.cm-type { color: #9fca56; }
.cm-s-seti span.cm-builtin { color: #9fca56; }
.cm-s-seti .CodeMirror-activeline-background { background: #101213; }
.cm-s-seti .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; }

/*
Solarized theme for code-mirror
http://ethanschoonover.com/solarized
*/

/*
Solarized color palette
http://ethanschoonover.com/solarized/img/solarized-palette.png
*/

.solarized.base03 { color: #002b36; }
.solarized.base02 { color: #073642; }
.solarized.base01 { color: #586e75; }
.solarized.base00 { color: #657b83; }
.solarized.base0 { color: #839496; }
.solarized.base1 { color: #93a1a1; }
.solarized.base2 { color: #eee8d5; }
.solarized.base3  { color: #fdf6e3; }
.solarized.solar-yellow  { color: #b58900; }
.solarized.solar-orange  { color: #cb4b16; }
.solarized.solar-red { color: #dc322f; }
.solarized.solar-magenta { color: #d33682; }
.solarized.solar-violet  { color: #6c71c4; }
.solarized.solar-blue { color: #268bd2; }
.solarized.solar-cyan { color: #2aa198; }
.solarized.solar-green { color: #859900; }

/* Color scheme for code-mirror */

.cm-s-solarized {
  line-height: 1.45em;
  color-profile: sRGB;
  rendering-intent: auto;
}
.cm-s-solarized.cm-s-dark {
  color: #839496;
  background-color: #002b36;
  text-shadow: #002b36 0 1px;
}
.cm-s-solarized.cm-s-light {
  background-color: #fdf6e3;
  color: #657b83;
  text-shadow: #eee8d5 0 1px;
}

.cm-s-solarized .CodeMirror-widget {
  text-shadow: none;
}

.cm-s-solarized .cm-header { color: #586e75; }
.cm-s-solarized .cm-quote { color: #93a1a1; }

.cm-s-solarized .cm-keyword { color: #cb4b16; }
.cm-s-solarized .cm-atom { color: #d33682; }
.cm-s-solarized .cm-number { color: #d33682; }
.cm-s-solarized .cm-def { color: #2aa198; }

.cm-s-solarized .cm-variable { color: #839496; }
.cm-s-solarized .cm-variable-2 { color: #b58900; }
.cm-s-solarized .cm-variable-3, .cm-s-solarized .cm-type { color: #6c71c4; }

.cm-s-solarized .cm-property { color: #2aa198; }
.cm-s-solarized .cm-operator { color: #6c71c4; }

.cm-s-solarized .cm-comment { color: #586e75; font-style:italic; }

.cm-s-solarized .cm-string { color: #859900; }
.cm-s-solarized .cm-string-2 { color: #b58900; }

.cm-s-solarized .cm-meta { color: #859900; }
.cm-s-solarized .cm-qualifier { color: #b58900; }
.cm-s-solarized .cm-builtin { color: #d33682; }
.cm-s-solarized .cm-bracket { color: #cb4b16; }
.cm-s-solarized .CodeMirror-matchingbracket { color: #859900; }
.cm-s-solarized .CodeMirror-nonmatchingbracket { color: #dc322f; }
.cm-s-solarized .cm-tag { color: #93a1a1; }
.cm-s-solarized .cm-attribute { color: #2aa198; }
.cm-s-solarized .cm-hr {
  color: transparent;
  border-top: 1px solid #586e75;
  display: block;
}
.cm-s-solarized .cm-link { color: #93a1a1; cursor: pointer; }
.cm-s-solarized .cm-special { color: #6c71c4; }
.cm-s-solarized .cm-em {
  color: #999;
  text-decoration: underline;
  text-decoration-style: dotted;
}
.cm-s-solarized .cm-error,
.cm-s-solarized .cm-invalidchar {
  color: #586e75;
  border-bottom: 1px dotted #dc322f;
}

.cm-s-solarized.cm-s-dark div.CodeMirror-selected { background: #073642; }
.cm-s-solarized.cm-s-dark.CodeMirror ::selection { background: rgba(7, 54, 66, 0.99); }
.cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection, .cm-s-dark .CodeMirror-line > span::-moz-selection, .cm-s-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(7, 54, 66, 0.99); }

.cm-s-solarized.cm-s-light div.CodeMirror-selected { background: #eee8d5; }
.cm-s-solarized.cm-s-light .CodeMirror-line::selection, .cm-s-light .CodeMirror-line > span::selection, .cm-s-light .CodeMirror-line > span > span::selection { background: #eee8d5; }
.cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection, .cm-s-ligh .CodeMirror-line > span::-moz-selection, .cm-s-ligh .CodeMirror-line > span > span::-moz-selection { background: #eee8d5; }

/* Editor styling */



/* Little shadow on the view-port of the buffer view */
.cm-s-solarized.CodeMirror {
  -moz-box-shadow: inset 7px 0 12px -6px #000;
  -webkit-box-shadow: inset 7px 0 12px -6px #000;
  box-shadow: inset 7px 0 12px -6px #000;
}

/* Remove gutter border */
.cm-s-solarized .CodeMirror-gutters {
  border-right: 0;
}

/* Gutter colors and line number styling based of color scheme (dark / light) */

/* Dark */
.cm-s-solarized.cm-s-dark .CodeMirror-gutters {
  background-color: #073642;
}

.cm-s-solarized.cm-s-dark .CodeMirror-linenumber {
  color: #586e75;
  text-shadow: #021014 0 -1px;
}

/* Light */
.cm-s-solarized.cm-s-light .CodeMirror-gutters {
  background-color: #eee8d5;
}

.cm-s-solarized.cm-s-light .CodeMirror-linenumber {
  color: #839496;
}

/* Common */
.cm-s-solarized .CodeMirror-linenumber {
  padding: 0 5px;
}
.cm-s-solarized .CodeMirror-guttermarker-subtle { color: #586e75; }
.cm-s-solarized.cm-s-dark .CodeMirror-guttermarker { color: #ddd; }
.cm-s-solarized.cm-s-light .CodeMirror-guttermarker { color: #cb4b16; }

.cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text {
  color: #586e75;
}

/* Cursor */
.cm-s-solarized .CodeMirror-cursor { border-left: 1px solid #819090; }

/* Fat cursor */
.cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor { background: #77ee77; }
.cm-s-solarized.cm-s-light .cm-animate-fat-cursor { background-color: #77ee77; }
.cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor { background: #586e75; }
.cm-s-solarized.cm-s-dark .cm-animate-fat-cursor { background-color: #586e75; }

/* Active line */
.cm-s-solarized.cm-s-dark .CodeMirror-activeline-background {
  background: rgba(255, 255, 255, 0.06);
}
.cm-s-solarized.cm-s-light .CodeMirror-activeline-background {
  background: rgba(0, 0, 0, 0.06);
}

.cm-s-the-matrix.CodeMirror { background: #000000; color: #00FF00; }
.cm-s-the-matrix div.CodeMirror-selected { background: #2D2D2D; }
.cm-s-the-matrix .CodeMirror-line::selection, .cm-s-the-matrix .CodeMirror-line > span::selection, .cm-s-the-matrix .CodeMirror-line > span > span::selection { background: rgba(45, 45, 45, 0.99); }
.cm-s-the-matrix .CodeMirror-line::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span > span::-moz-selection { background: rgba(45, 45, 45, 0.99); }
.cm-s-the-matrix .CodeMirror-gutters { background: #060; border-right: 2px solid #00FF00; }
.cm-s-the-matrix .CodeMirror-guttermarker { color: #0f0; }
.cm-s-the-matrix .CodeMirror-guttermarker-subtle { color: white; }
.cm-s-the-matrix .CodeMirror-linenumber { color: #FFFFFF; }
.cm-s-the-matrix .CodeMirror-cursor { border-left: 1px solid #00FF00; }

.cm-s-the-matrix span.cm-keyword { color: #008803; font-weight: bold; }
.cm-s-the-matrix span.cm-atom { color: #3FF; }
.cm-s-the-matrix span.cm-number { color: #FFB94F; }
.cm-s-the-matrix span.cm-def { color: #99C; }
.cm-s-the-matrix span.cm-variable { color: #F6C; }
.cm-s-the-matrix span.cm-variable-2 { color: #C6F; }
.cm-s-the-matrix span.cm-variable-3, .cm-s-the-matrix span.cm-type { color: #96F; }
.cm-s-the-matrix span.cm-property { color: #62FFA0; }
.cm-s-the-matrix span.cm-operator { color: #999; }
.cm-s-the-matrix span.cm-comment { color: #CCCCCC; }
.cm-s-the-matrix span.cm-string { color: #39C; }
.cm-s-the-matrix span.cm-meta { color: #C9F; }
.cm-s-the-matrix span.cm-qualifier { color: #FFF700; }
.cm-s-the-matrix span.cm-builtin { color: #30a; }
.cm-s-the-matrix span.cm-bracket { color: #cc7; }
.cm-s-the-matrix span.cm-tag { color: #FFBD40; }
.cm-s-the-matrix span.cm-attribute { color: #FFF700; }
.cm-s-the-matrix span.cm-error { color: #FF0000; }

.cm-s-the-matrix .CodeMirror-activeline-background { background: #040; }

/*
Copyright (C) 2011 by MarkLogic Corporation
Author: Mike Brevoort <mike@brevoort.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/
.cm-s-xq-light span.cm-keyword { line-height: 1em; font-weight: bold; color: #5A5CAD; }
.cm-s-xq-light span.cm-atom { color: #6C8CD5; }
.cm-s-xq-light span.cm-number { color: #164; }
.cm-s-xq-light span.cm-def { text-decoration:underline; }
.cm-s-xq-light span.cm-variable { color: black; }
.cm-s-xq-light span.cm-variable-2 { color:black; }
.cm-s-xq-light span.cm-variable-3, .cm-s-xq-light span.cm-type { color: black; }
.cm-s-xq-light span.cm-property {}
.cm-s-xq-light span.cm-operator {}
.cm-s-xq-light span.cm-comment { color: #0080FF; font-style: italic; }
.cm-s-xq-light span.cm-string { color: red; }
.cm-s-xq-light span.cm-meta { color: yellow; }
.cm-s-xq-light span.cm-qualifier { color: grey; }
.cm-s-xq-light span.cm-builtin { color: #7EA656; }
.cm-s-xq-light span.cm-bracket { color: #cc7; }
.cm-s-xq-light span.cm-tag { color: #3F7F7F; }
.cm-s-xq-light span.cm-attribute { color: #7F007F; }
.cm-s-xq-light span.cm-error { color: #f00; }

.cm-s-xq-light .CodeMirror-activeline-background { background: #e8f2ff; }
.cm-s-xq-light .CodeMirror-matchingbracket { outline:1px solid grey;color:black !important;background:yellow; }

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.CodeMirror {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;
  /* Changed to auto to autogrow */
}

.CodeMirror pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* This causes https://github.com/jupyter/jupyterlab/issues/522 */
/* May not cause it not because we changed it! */
.CodeMirror-lines {
  padding: var(--jp-code-padding) 0;
}

.CodeMirror-linenumber {
  padding: 0 8px;
}

.jp-CodeMirrorEditor-static {
  margin: var(--jp-code-padding);
}

.jp-CodeMirrorEditor,
.jp-CodeMirrorEditor-static {
  cursor: text;
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.CodeMirror.jp-mod-readOnly .CodeMirror-cursor {
  display: none;
}

.CodeMirror-gutters {
  border-right: 1px solid var(--jp-border-color2);
  background-color: var(--jp-layout-color0);
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.CodeMirror-selectedtext.cm-searching {
  background-color: var(--jp-search-selected-match-background-color) !important;
  color: var(--jp-search-selected-match-color) !important;
}

.cm-searching {
  background-color: var(
    --jp-search-unselected-match-background-color
  ) !important;
  color: var(--jp-search-unselected-match-color) !important;
}

.CodeMirror-focused .CodeMirror-selected {
  background-color: var(--jp-editor-selected-focused-background);
}

.CodeMirror-selected {
  background-color: var(--jp-editor-selected-background);
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/**
 * Here is our jupyter theme for CodeMirror syntax highlighting
 * This is used in our marked.js syntax highlighting and CodeMirror itself
 * The string "jupyter" is set in ../codemirror/widget.DEFAULT_CODEMIRROR_THEME
 * This came from the classic notebook, which came form highlight.js/GitHub
 */

/**
 * CodeMirror themes are handling the background/color in this way. This works
 * fine for CodeMirror editors outside the notebook, but the notebook styles
 * these things differently.
 */
.CodeMirror.cm-s-jupyter {
  background: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* In the notebook, we want this styling to be handled by its container */
.jp-CodeConsole .CodeMirror.cm-s-jupyter,
.jp-Notebook .CodeMirror.cm-s-jupyter {
  background: transparent;
}

.cm-s-jupyter .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}
.cm-s-jupyter span.cm-keyword {
  color: var(--jp-mirror-editor-keyword-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-atom {
  color: var(--jp-mirror-editor-atom-color);
}
.cm-s-jupyter span.cm-number {
  color: var(--jp-mirror-editor-number-color);
}
.cm-s-jupyter span.cm-def {
  color: var(--jp-mirror-editor-def-color);
}
.cm-s-jupyter span.cm-variable {
  color: var(--jp-mirror-editor-variable-color);
}
.cm-s-jupyter span.cm-variable-2 {
  color: var(--jp-mirror-editor-variable-2-color);
}
.cm-s-jupyter span.cm-variable-3 {
  color: var(--jp-mirror-editor-variable-3-color);
}
.cm-s-jupyter span.cm-punctuation {
  color: var(--jp-mirror-editor-punctuation-color);
}
.cm-s-jupyter span.cm-property {
  color: var(--jp-mirror-editor-property-color);
}
.cm-s-jupyter span.cm-operator {
  color: var(--jp-mirror-editor-operator-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-comment {
  color: var(--jp-mirror-editor-comment-color);
  font-style: italic;
}
.cm-s-jupyter span.cm-string {
  color: var(--jp-mirror-editor-string-color);
}
.cm-s-jupyter span.cm-string-2 {
  color: var(--jp-mirror-editor-string-2-color);
}
.cm-s-jupyter span.cm-meta {
  color: var(--jp-mirror-editor-meta-color);
}
.cm-s-jupyter span.cm-qualifier {
  color: var(--jp-mirror-editor-qualifier-color);
}
.cm-s-jupyter span.cm-builtin {
  color: var(--jp-mirror-editor-builtin-color);
}
.cm-s-jupyter span.cm-bracket {
  color: var(--jp-mirror-editor-bracket-color);
}
.cm-s-jupyter span.cm-tag {
  color: var(--jp-mirror-editor-tag-color);
}
.cm-s-jupyter span.cm-attribute {
  color: var(--jp-mirror-editor-attribute-color);
}
.cm-s-jupyter span.cm-header {
  color: var(--jp-mirror-editor-header-color);
}
.cm-s-jupyter span.cm-quote {
  color: var(--jp-mirror-editor-quote-color);
}
.cm-s-jupyter span.cm-link {
  color: var(--jp-mirror-editor-link-color);
}
.cm-s-jupyter span.cm-error {
  color: var(--jp-mirror-editor-error-color);
}
.cm-s-jupyter span.cm-hr {
  color: #999;
}

.cm-s-jupyter span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}

.cm-s-jupyter .CodeMirror-activeline-background,
.cm-s-jupyter .CodeMirror-gutter {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0px;
  padding: 0px;
  line-height: normal;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}
.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}
.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}
.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}
.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}
.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}
.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}
.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
}
.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
}
.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
}
.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
}
.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
}
.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
}
.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
}
.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}
.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}
.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}
.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}
.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}
.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}
.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}
.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
}
.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
}
.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
}
.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
}
.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
}
.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
}
.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
}
.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}
.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}
.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0em;
}

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: 12px;
  table-layout: fixed;
  margin-left: auto;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon table {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0px;
}

.jp-RenderedHTMLCommon p {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}
/* ...or leave it untouched if they don't */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-dark-background {
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-light-background {
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}
.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}
.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}
.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}
.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}
.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}
.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}
.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}
.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: 0.8em;
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser {
  display: flex;
  flex-direction: column;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  border-bottom: none;
  height: auto;
  margin: var(--jp-toolbar-header-margin);
  box-shadow: none;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 4px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0px 2px;
  padding: 0px 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0px;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar.jp-Toolbar {
  padding: 0px;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  justify-content: space-evenly;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item {
  flex: 1;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent {
  width: 100%;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px 12px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-item.jp-mod-selected {
  color: white;
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before {
  color: limegreen;
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0px;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-DirListing-deadSpace {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

.jp-FileDialog.jp-mod-conflict input {
  color: red;
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
}

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: flex;
  flex-direction: row;
}

.jp-OutputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-output {
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea-child .jp-OutputArea-output {
  flex-grow: 1;
  flex-shrink: 1;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0px;
  padding: 0px;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0px;
  flex: 1 1 auto;
}

.jp-OutputArea-executeResult.jp-RenderedText {
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-OutputArea-stdin {
  line-height: var(--jp-code-line-height);
  padding-top: var(--jp-code-padding);
  display: flex;
}

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;
  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0px;
  bottom: 0px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0px;
  width: 100%;
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: flex;
  flex-direction: row;
}

.jp-InputArea-editor {
  flex: 1 1 auto;
}

.jp-InputArea-editor {
  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0px;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: flex;
  flex-direction: row;
  flex: 1 1 auto;
}

.jp-Placeholder-prompt {
  box-sizing: border-box;
}

.jp-Placeholder-content {
  flex: 1 1 auto;
  border: none;
  background: transparent;
  height: 20px;
  box-sizing: border-box;
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0px;
  margin: 0px;
  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 200px;
  box-shadow: inset 0 0 6px 2px rgba(0, 0, 0, 0.3);
  margin-left: var(--jp-private-cell-scrolling-output-offset);
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  flex: 0 0
    calc(
      var(--jp-cell-prompt-width) -
        var(--jp-private-cell-scrolling-output-offset)
    );
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  flex: 1 1 auto;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: 2px;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: flex;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0px;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-NotebookTools-tool {
  padding: 0px 12px 0 12px;
}

.jp-ActiveCellTool {
  padding: 12px;
  background-color: var(--jp-layout-color1);
  border-top: none !important;
}

.jp-ActiveCellTool .jp-InputArea-prompt {
  flex: 0 0 auto;
  padding-left: 0px;
}

.jp-ActiveCellTool .jp-InputArea-editor {
  flex: 1 1 auto;
  background: var(--jp-cell-editor-background);
  border-color: var(--jp-cell-editor-border-color);
}

.jp-ActiveCellTool .jp-InputArea-editor .CodeMirror {
  background: transparent;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0px 12px 0px;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label {
  line-height: 1.4;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

</style>

    <style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color),
    0px 1px 1px 0px var(--jp-shadow-penumbra-color),
    0px 1px 3px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color),
    0px 2px 2px 0px var(--jp-shadow-penumbra-color),
    0px 1px 5px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color),
    0px 4px 5px 0px var(--jp-shadow-penumbra-color),
    0px 1px 10px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color),
    0px 6px 10px 0px var(--jp-shadow-penumbra-color),
    0px 1px 18px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color),
    0px 8px 10px 1px var(--jp-shadow-penumbra-color),
    0px 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color),
    0px 12px 17px 2px var(--jp-shadow-penumbra-color),
    0px 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color),
    0px 16px 24px 2px var(--jp-shadow-penumbra-color),
    0px 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color),
    0px 20px 31px 3px var(--jp-shadow-penumbra-color),
    0px 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color),
    0px 24px 38px 3px var(--jp-shadow-penumbra-color),
    0px 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;

  --jp-ui-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica,
    Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;

  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);

  --jp-content-link-color: var(--md-blue-700);

  --jp-content-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI',
    Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: Menlo, Consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-700);
  --jp-brand-color1: var(--md-blue-500);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);

  --jp-accent-color0: var(--md-green-700);
  --jp-accent-color1: var(--md-green-500);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-700);
  --jp-warn-color1: var(--md-orange-500);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);

  --jp-error-color0: var(--md-red-700);
  --jp-error-color1: var(--md-red-500);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);

  --jp-success-color0: var(--md-green-700);
  --jp-success-color1: var(--md-green-500);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);

  --jp-info-color0: var(--md-cyan-700);
  --jp-info-color1: var(--md-cyan-500);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;

  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;

  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);

  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: 'Source Code Pro', monospace;
  --jp-cell-prompt-letter-spacing: 0px;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);
  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;
  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0px 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-border-color1);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: #05a;
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #aa22ff;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #aa22ff;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 180px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);
}
</style>

<style type="text/css">
a.anchor-link {
   display: none;
}
.highlight  {
    margin: 0.4em;
}

/* Input area styling */
.jp-InputArea {
    overflow: hidden;
}

.jp-InputArea-editor {
    overflow: hidden;
}

@media print {
  body {
    margin: 0;
  }
}
</style>



<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML-full,Safe"> </script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: { 
                    automatic: true 
                    }
                },
                "HTML-CSS": {
                    linebreaks: { 
                    automatic: true 
                    }
                }
            });
        
            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
    <!-- End of mathjax configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">

<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="Cercador-model-&#242;ptim-de-DNN">Cercador model &#242;ptim de DNN<a class="anchor-link" href="#Cercador-model-&#242;ptim-de-DNN">&#182;</a></h1>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow.compat.v2</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">enable_v2_behavior</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">GRU</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">TimeDistributed</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">backend</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>C:\Users\dj_kr\anaconda3\lib\site-packages\statsmodels\tools\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
  import pandas.util.testing as tm
</pre>
</div>
</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="C&#224;rrega-de-les-dades">C&#224;rrega de les dades<a class="anchor-link" href="#C&#224;rrega-de-les-dades">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/SentDATA.csv&#39;</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Time&#39;</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="An&#224;lisis-estad&#237;stic">An&#224;lisis estad&#237;stic<a class="anchor-link" href="#An&#224;lisis-estad&#237;stic">&#182;</a></h2>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Transformaci&#243;-de-dades">Transformaci&#243; de dades<a class="anchor-link" href="#Transformaci&#243;-de-dades">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PM1&#39;</span><span class="p">,</span><span class="s1">&#39;PM25&#39;</span><span class="p">,</span><span class="s1">&#39;PM10&#39;</span><span class="p">,</span><span class="s1">&#39;PM1ATM&#39;</span><span class="p">,</span><span class="s1">&#39;PM25ATM&#39;</span><span class="p">,</span><span class="s1">&#39;PM10ATM&#39;</span><span class="p">]</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">();</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;PM 1&quot;</span><span class="p">:</span><span class="s2">&quot;PM1&quot;</span><span class="p">,</span><span class="s2">&quot;PM 2.5&quot;</span><span class="p">:</span><span class="s2">&quot;PM25&quot;</span><span class="p">,</span><span class="s2">&quot;PM 10&quot;</span><span class="p">:</span><span class="s2">&quot;PM10&quot;</span><span class="p">,</span><span class="s2">&quot;PM 1 ATM&quot;</span><span class="p">:</span><span class="s2">&quot;PM1ATM&quot;</span><span class="p">,</span><span class="s2">&quot;PM 2.5 ATM&quot;</span><span class="p">:</span><span class="s2">&quot;PM25ATM&quot;</span><span class="p">,</span><span class="s2">&quot;PM 10 ATM&quot;</span><span class="p">:</span><span class="s2">&quot;PM10ATM&quot;</span><span class="p">})</span>

<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM25&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM10&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 10&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM1ATM&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 1 ATM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM25ATM&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 2.5 ATM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM10ATM&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 10 ATM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df2</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Crear-dades-d'entrenament-i-de-test">Crear dades d'entrenament i de test<a class="anchor-link" href="#Crear-dades-d'entrenament-i-de-test">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">train_size</span><span class="p">],</span> <span class="n">df2</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_size</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">df2</span><span class="p">)]</span>
<span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[5]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>((3991, 7), (998, 7))</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Normalitzar-les-dades-d'entrenament">Normalitzar les dades d'entrenament<a class="anchor-link" href="#Normalitzar-les-dades-d'entrenament">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Standardize the data</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
    <span class="n">train</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">col</span><span class="p">]])</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>&lt;ipython-input-6-c8a1383fd1da&gt;:4: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  train[col] = scaler.fit_transform(train[[col]])
&lt;ipython-input-6-c8a1383fd1da&gt;:4: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  train[col] = scaler.fit_transform(train[[col]])
&lt;ipython-input-6-c8a1383fd1da&gt;:4: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  train[col] = scaler.fit_transform(train[[col]])
&lt;ipython-input-6-c8a1383fd1da&gt;:4: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  train[col] = scaler.fit_transform(train[[col]])
&lt;ipython-input-6-c8a1383fd1da&gt;:4: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  train[col] = scaler.fit_transform(train[[col]])
&lt;ipython-input-6-c8a1383fd1da&gt;:4: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  train[col] = scaler.fit_transform(train[[col]])
</pre>
</div>
</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Crear-finestra-de-temps-PM-2.5">Crear finestra de temps PM 2.5<a class="anchor-link" href="#Crear-finestra-de-temps-PM-2.5">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">TIME_STEPS</span><span class="o">=</span><span class="mi">144</span> <span class="c1">#6 registres hora x 24h x 3 --&gt; equival a una finestra d&#39;un dia</span>

<span class="k">def</span> <span class="nf">create_sequences</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">time_steps</span><span class="o">=</span><span class="n">TIME_STEPS</span><span class="p">):</span>
    <span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">-</span><span class="n">time_steps</span><span class="p">):</span>
        <span class="n">Xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="n">time_steps</span><span class="p">)]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">time_steps</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Xs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

<span class="n">X_train1h</span><span class="p">,</span> <span class="n">y_train1h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">6</span><span class="p">)</span> <span class="c1">#1 hour</span>

<span class="n">X_train3h</span><span class="p">,</span> <span class="n">y_train3h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">18</span><span class="p">)</span> <span class="c1">#3 hours</span>

<span class="n">X_train6h</span><span class="p">,</span> <span class="n">y_train6h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">36</span><span class="p">)</span> <span class="c1">#6 hours</span>

<span class="n">X_train12h</span><span class="p">,</span> <span class="n">y_train12h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">72</span><span class="p">)</span> <span class="c1">#12 hours</span>

<span class="n">X_train1d</span><span class="p">,</span> <span class="n">y_train1d</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">144</span><span class="p">)</span> <span class="c1">#1 day</span>

<span class="n">X_train3d</span><span class="p">,</span> <span class="n">y_train3d</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">432</span><span class="p">)</span> <span class="c1">#3 days</span>

<span class="n">X_train7d</span><span class="p">,</span> <span class="n">y_train7d</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">1008</span><span class="p">)</span> <span class="c1">#7 days</span>
<span class="c1">#X_test, y_test = create_sequences(test[[columns[1]]], test[columns[1]])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train1h shape: </span><span class="si">{</span><span class="n">X_train1d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train3d shape: </span><span class="si">{</span><span class="n">X_train3h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train6h shape: </span><span class="si">{</span><span class="n">X_train6h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train12h shape: </span><span class="si">{</span><span class="n">X_train12h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train1d shape: </span><span class="si">{</span><span class="n">X_train1d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train3d shape: </span><span class="si">{</span><span class="n">X_train3d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train7d shape: </span><span class="si">{</span><span class="n">X_train7d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>X_train1h shape: (3847, 144, 1)
X_train3d shape: (3973, 18, 1)
X_train6h shape: (3955, 36, 1)
X_train12h shape: (3919, 72, 1)
X_train1d shape: (3847, 144, 1)
X_train3d shape: (3559, 432, 1)
X_train7d shape: (2983, 1008, 1)
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_prediction</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">actual</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">model_name</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean Absolute Error: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mae</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Root Mean Square Error: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean Square Error: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mse</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mae</span><span class="p">,</span><span class="n">rmse</span><span class="p">,</span><span class="n">mse</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Cerca-dels-models-&#242;ptims">Cerca dels models &#242;ptims<a class="anchor-link" href="#Cerca-dels-models-&#242;ptims">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">_model</span><span class="p">(</span><span class="n">units</span><span class="p">,</span><span class="n">activationDense</span><span class="p">,</span><span class="n">dropout1</span><span class="p">,</span><span class="n">optimizer</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout1</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">activation</span><span class="o">=</span><span class="n">activationDense</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mae&#39;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1h&quot;</span><span class="p">,</span><span class="s2">&quot;3h&quot;</span><span class="p">,</span><span class="s2">&quot;6h&quot;</span><span class="p">,</span><span class="s2">&quot;12h&quot;</span><span class="p">,</span><span class="s2">&quot;1d&quot;</span><span class="p">,</span> <span class="s2">&quot;3d&quot;</span><span class="p">,</span> <span class="s2">&quot;7d&quot;</span><span class="p">]</span>
<span class="n">X_trains</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_train1h</span><span class="p">,</span><span class="n">X_train3h</span><span class="p">,</span> <span class="n">X_train6h</span><span class="p">,</span> <span class="n">X_train12h</span><span class="p">,</span><span class="n">X_train1d</span><span class="p">,</span><span class="n">X_train3d</span><span class="p">,</span> <span class="n">X_train7d</span><span class="p">]</span>
<span class="n">y_trains</span><span class="o">=</span> <span class="p">[</span><span class="n">y_train1h</span><span class="p">,</span><span class="n">y_train3h</span><span class="p">,</span> <span class="n">y_train6h</span><span class="p">,</span><span class="n">y_train12h</span><span class="p">,</span><span class="n">y_train1d</span><span class="p">,</span><span class="n">y_train3d</span><span class="p">,</span> <span class="n">y_train7d</span><span class="p">]</span>
<span class="n">activationsDense</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">]</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span><span class="s1">&#39;adadelta&#39;</span><span class="p">,</span><span class="s1">&#39;adamax&#39;</span><span class="p">]</span>
<span class="n">list_validationSplit</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">]</span>
<span class="n">list_dropout1</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">list_units</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">list_epochs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> 
<span class="n">list_batchsize</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>   

<span class="n">list_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="k">for</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">sequence</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_trains</span><span class="p">,</span><span class="n">y_trains</span><span class="p">,</span><span class="n">sequences</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
        <span class="c1">#for activation in activations:</span>
            <span class="k">for</span> <span class="n">activationDense</span> <span class="ow">in</span> <span class="n">activationsDense</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">validationsplit</span> <span class="ow">in</span> <span class="n">list_validationSplit</span><span class="p">:</span> 
                    <span class="k">for</span> <span class="n">units</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">batchsize</span><span class="p">,</span><span class="n">dropout1</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">list_units</span><span class="p">,</span><span class="n">list_epochs</span><span class="p">,</span><span class="n">list_batchsize</span><span class="p">,</span><span class="n">list_dropout1</span><span class="p">):</span> 
                        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;###########################</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MODEL: &quot;</span><span class="p">,</span> <span class="s2">&quot;DNN&quot;</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence: &#39;</span><span class="p">,</span><span class="n">sequence</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;units: &#39;</span><span class="p">,</span><span class="n">units</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dropout1: &#39;</span><span class="p">,</span><span class="n">dropout1</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer:&#39;</span><span class="p">,</span><span class="n">optimizer</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;activationDense:&#39;</span><span class="p">,</span><span class="n">activationDense</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epochs:&#39;</span><span class="p">,</span><span class="n">epochs</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;batchsize:&#39;</span><span class="p">,</span><span class="n">batchsize</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;validation_split:&#39;</span><span class="p">,</span><span class="n">validationsplit</span><span class="p">)</span>

                        <span class="n">model</span> <span class="o">=</span> <span class="n">_model</span><span class="p">(</span><span class="n">units</span><span class="p">,</span><span class="n">activationDense</span><span class="p">,</span><span class="n">dropout1</span><span class="p">,</span><span class="n">optimizer</span><span class="p">)</span>
                        <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="n">validationsplit</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
                        <span class="n">totalTime</span> <span class="o">=</span> <span class="n">end</span><span class="o">-</span><span class="n">start</span>
                        <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Execution time: &#39;</span><span class="p">,</span><span class="n">totalTime</span><span class="p">)</span>

                        <span class="n">X_train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                        <span class="n">mae</span><span class="p">,</span><span class="n">rmse</span><span class="p">,</span><span class="n">mse</span> <span class="o">=</span> <span class="n">evaluate_prediction</span><span class="p">(</span><span class="n">X_train_pred</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span><span class="s2">&quot;DNN&quot;</span><span class="p">)</span>

                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train RMSE: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rmse</span><span class="p">);</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train MSE: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mse</span><span class="p">);</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train MAE: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mae</span><span class="p">);</span>

                        <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
                                               <span class="c1">#&#39;activation&#39;:[activation],</span>
                                               <span class="s1">&#39;model&#39;</span><span class="p">:[</span><span class="s2">&quot;DNN&quot;</span><span class="p">],</span>
                                               <span class="s1">&#39;sequence&#39;</span><span class="p">:[</span><span class="n">sequence</span><span class="p">],</span>
                                               <span class="s1">&#39;activationDense&#39;</span><span class="p">:[</span><span class="n">activationDense</span><span class="p">],</span>
                                               <span class="s1">&#39;optimizer&#39;</span><span class="p">:[</span><span class="n">optimizer</span><span class="p">],</span>
                                               <span class="s1">&#39;dropout1&#39;</span><span class="p">:[</span><span class="n">dropout1</span><span class="p">],</span>
                                               <span class="s1">&#39;units&#39;</span><span class="p">:[</span><span class="n">units</span><span class="p">],</span>
                                               <span class="s1">&#39;epochs&#39;</span><span class="p">:[</span><span class="n">epochs</span><span class="p">],</span>
                                               <span class="s1">&#39;batchsize&#39;</span><span class="p">:[</span><span class="n">batchsize</span><span class="p">],</span>
                                               <span class="s1">&#39;validation_split&#39;</span><span class="p">:[</span><span class="n">validationsplit</span><span class="p">],</span>

                                               <span class="s1">&#39;RMSE&#39;</span><span class="p">:[</span><span class="n">rmse</span><span class="p">],</span>
                                               <span class="s1">&#39;MSE&#39;</span><span class="p">:[</span><span class="n">mse</span><span class="p">],</span>
                                               <span class="s1">&#39;MAE&#39;</span><span class="p">:[</span><span class="n">mae</span><span class="p">],</span>                            
                                               <span class="s1">&#39;Time&#39;</span><span class="p">:[</span><span class="n">totalTime</span><span class="p">]})</span>
                        <span class="n">list_results</span> <span class="o">=</span> <span class="n">list_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> 
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 6, 87)             174       
_________________________________________________________________
dense_1 (Dense)              (None, 6, 16)             1408      
_________________________________________________________________
dropout (Dropout)            (None, 6, 16)             0         
_________________________________________________________________
dense_2 (Dense)              (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 4ms/step - loss: 0.2079 - val_loss: 0.0340
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0899 - val_loss: 0.0164
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0754 - val_loss: 0.0086
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0591 - val_loss: 0.0058
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0441 - val_loss: 0.0131
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0187
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0224
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0101
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0105
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0115
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0109
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0116
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0098
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0110
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0107
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0106
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0109
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0113
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0105
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0108
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0107
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0113
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0129
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0107
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0136
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0117
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0129
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0105
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0114
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0134
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0117
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0144
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0143
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0153
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0145
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0149
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0131
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0138
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0162
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0137
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0149
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0145
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0120
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0135
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0139
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0155
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0134
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0166
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0156
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0155
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0152
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0132
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0120
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0145
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0146
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0148
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0165
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0159
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0156
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0145
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0125
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0135
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0159
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0162
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0150
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0172
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0137
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0134
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0127
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0145
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0134
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0158
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0130
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0136
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0154
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0149
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0172
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0156
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0173
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0149
Execution time:  17.23777174949646
DNN:
Mean Absolute Error: 0.0099
Root Mean Square Error: 0.0192
Mean Square Error: 0.0004

Train RMSE: 0.019
Train MSE: 0.000
Train MAE: 0.010
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_3 (Dense)              (None, 6, 80)             160       
_________________________________________________________________
dense_4 (Dense)              (None, 6, 16)             1296      
_________________________________________________________________
dropout_1 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
144/144 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.0033
Epoch 2/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0395 - val_loss: 0.0105
Epoch 3/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0366 - val_loss: 0.0035
Epoch 4/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 0.0041
Epoch 5/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0045
Epoch 6/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0042
Epoch 7/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0051
Epoch 8/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0034
Epoch 9/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0039
Epoch 10/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0042
Epoch 11/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0041
Epoch 12/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0054
Epoch 13/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0065
Epoch 14/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0062
Epoch 15/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0144
Epoch 16/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0176
Epoch 17/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0181
Epoch 18/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0171
Epoch 19/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0202
Epoch 20/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0215
Epoch 21/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0166
Epoch 22/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0188
Epoch 23/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0194
Epoch 24/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0191
Epoch 25/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0180 - val_loss: 0.0184
Epoch 26/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0174 - val_loss: 0.0183
Epoch 27/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0185
Epoch 28/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0170 - val_loss: 0.0172
Epoch 29/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0182
Epoch 30/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0172
Epoch 31/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0178
Epoch 32/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0185
Epoch 33/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0177
Epoch 34/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0180
Epoch 35/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0167
Epoch 36/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0183
Epoch 37/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0167
Epoch 38/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0166
Epoch 39/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0185
Epoch 40/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0185
Epoch 41/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0179
Epoch 42/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0174
Epoch 43/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0175
Epoch 44/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0166
Epoch 45/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0180
Epoch 46/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0174
Epoch 47/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0127
Epoch 48/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0107 - val_loss: 0.0116
Epoch 49/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0129
Epoch 50/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0158
Epoch 51/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0139
Epoch 52/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0139
Epoch 53/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0121
Epoch 54/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0122
Epoch 55/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0106 - val_loss: 0.0108
Epoch 56/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0112
Epoch 57/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0110
Epoch 58/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0120
Epoch 59/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0112
Epoch 60/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0149
Epoch 61/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0124
Epoch 62/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0147
Epoch 63/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0141
Epoch 64/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0122
Epoch 65/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0109 - val_loss: 0.0104
Epoch 66/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0104
Epoch 67/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0120
Epoch 68/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0146
Epoch 69/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0126
Epoch 70/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0139
Epoch 71/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0109 - val_loss: 0.0142
Epoch 72/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0156
Epoch 73/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0148
Epoch 74/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0144
Epoch 75/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0146
Epoch 76/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0122
Epoch 77/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0119
Epoch 78/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0132
Epoch 79/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0122
Epoch 80/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0126
Epoch 81/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0109
Epoch 82/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0110
Epoch 83/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0119
Epoch 84/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0108 - val_loss: 0.0116
Epoch 85/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0124
Epoch 86/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0142
Epoch 87/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0118
Epoch 88/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0108 - val_loss: 0.0143
Epoch 89/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0106 - val_loss: 0.0115
Epoch 90/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0115
Execution time:  22.27723526954651
DNN:
Mean Absolute Error: 0.0050
Root Mean Square Error: 0.0189
Mean Square Error: 0.0004

Train RMSE: 0.019
Train MSE: 0.000
Train MAE: 0.005
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_2&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 6, 12)             24        
_________________________________________________________________
dense_7 (Dense)              (None, 6, 16)             208       
_________________________________________________________________
dropout_2 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_8 (Dense)              (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 4ms/step - loss: 0.3349 - val_loss: 0.1542
Epoch 2/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 0.0164
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0999 - val_loss: 0.0045
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0906 - val_loss: 0.0093
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0811 - val_loss: 0.0138
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.0108
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0088
Epoch 8/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.0118
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0444 - val_loss: 0.0080
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 0.0114
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0096
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0082
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0081
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0087
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0097
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0101
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0100
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0104
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0103
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0100
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0105
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0100
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0103
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0101
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0102
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0102
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0102
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0103
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0103
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0104
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0101
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0103
Epoch 33/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0100
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0103
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0101
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0105
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0168
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0110
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0096
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0115
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0098
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0114
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0096
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0125
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0098
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0123
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0100
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0128
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0096
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0123
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0100
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0126
Execution time:  6.903104782104492
DNN:
Mean Absolute Error: 0.0114
Root Mean Square Error: 0.0177
Mean Square Error: 0.0003

Train RMSE: 0.018
Train MSE: 0.000
Train MAE: 0.011
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_3&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_9 (Dense)              (None, 6, 87)             174       
_________________________________________________________________
dense_10 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_11 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.1425 - val_loss: 0.0342
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0729 - val_loss: 0.0133
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0635 - val_loss: 0.0109
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0563 - val_loss: 0.0203
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0484 - val_loss: 0.0160
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0411 - val_loss: 0.0127
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0116
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0118
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0124
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0131
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0157
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0153
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0147
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0150
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0150
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0149
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0148
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0150
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0149
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0155
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0147
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0144
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0144
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0144
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0135
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0127
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0128
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0122
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0118
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0118
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0123
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0122
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0117
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0117
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0118
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0111
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0124
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0115
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0116
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0120
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0116
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0115
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0113
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0116
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0118
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0117
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0116
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0118
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0114
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0111
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0115
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0122
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0109
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0119
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0114
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0118
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0118
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0112
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0108
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0110
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0117
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0108
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0120
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0120
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0116
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0110
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0110
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0118
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0118
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0114
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0110
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0111
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0119
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0120
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0114
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0116
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0111
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0114
Execution time:  16.15279769897461
DNN:
Mean Absolute Error: 0.0136
Root Mean Square Error: 0.0213
Mean Square Error: 0.0005

Train RMSE: 0.021
Train MSE: 0.000
Train MAE: 0.014
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_4&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_12 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_13 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_4 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_14 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 0.0337
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0452 - val_loss: 0.0340
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0242
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0307
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0268
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0113
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0168
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0147
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0107
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0115
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0104
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0105
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0106
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0117
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0112
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0114
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0112
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0114
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0101
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0106
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0096
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0104
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0111
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0107
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0105
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0116
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0115
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0127
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0126
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0126
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0125
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0114
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0120
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0119
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0121
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0122
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0122
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0121
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0112
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0118
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0119
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0115
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0121
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0122
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0112
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0114
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0118
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0111
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0115
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0098
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0093
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0095
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0110
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0119
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0115
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0116
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0103
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0097
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0107
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0111
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0111
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0113
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0112
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0108
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0112
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0109
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0113
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0115
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0098
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0099
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0105
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0114
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0113
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0113
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0097
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0099
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0097
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0093
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0095
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0102
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0095
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0096
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0095
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0103
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0097
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0097
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0093
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0086
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0093
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0095
Execution time:  20.035281896591187
DNN:
Mean Absolute Error: 0.0086
Root Mean Square Error: 0.0210
Mean Square Error: 0.0004

Train RMSE: 0.021
Train MSE: 0.000
Train MAE: 0.009
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_5&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_15 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_16 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_5 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_17 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.3539 - val_loss: 0.2205
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1550 - val_loss: 0.0349
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.0175
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.0134
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0852 - val_loss: 0.0138
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.0144
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0697 - val_loss: 0.0142
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0617 - val_loss: 0.0135
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.0138
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0142
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.0141
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0153
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0161
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0171
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0163
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0145
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0162
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0160
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0162
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0161
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0163
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0158
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0155
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0154
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0153
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0150
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0148
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0150
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0147
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0146
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0143
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0146
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0139
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0147
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0138
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0145
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0137
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0141
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0134
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0136
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0130
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0136
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0129
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0132
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0128
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0127
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0125
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0130
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0122
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0118
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0119
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0120
Execution time:  6.411895751953125
DNN:
Mean Absolute Error: 0.0165
Root Mean Square Error: 0.0250
Mean Square Error: 0.0006

Train RMSE: 0.025
Train MSE: 0.001
Train MAE: 0.016
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_6&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_18 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_19 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_6 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_20 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.0256
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.0265
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0199
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.0203
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.0190
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0188
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0187
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0176
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0179
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0170
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0169
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0168
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0157
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0169
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0151
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0153
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0147
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0147
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0142
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0141
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0142
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0139
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0142
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0138
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0140
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0136
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0136
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0150
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0153
Epoch 30/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0156
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0165
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0154
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0140
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0132
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0134
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0135
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0128
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0128
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0123
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0124
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0119
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0114
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0124
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0118
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0116
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0108
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0107
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0102
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0104
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0106
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0102
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0079
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0088
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0089
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0085
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0086
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0075
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0074
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0090
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0089
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0096
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0079
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0077
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0070
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0085
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0074
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0082
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0077
Epoch 69/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0071
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0094
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0086
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0097
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0083
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0076
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0079
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0085
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0100
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0071
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0059
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0085
Execution time:  16.1406090259552
DNN:
Mean Absolute Error: 0.0100
Root Mean Square Error: 0.0187
Mean Square Error: 0.0004

Train RMSE: 0.019
Train MSE: 0.000
Train MAE: 0.010
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_7&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_21 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_22 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_7 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_23 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
144/144 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.0358
Epoch 2/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 0.0289
Epoch 3/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 0.0238
Epoch 4/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0203
Epoch 5/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0344 - val_loss: 0.0204
Epoch 6/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 0.0186
Epoch 7/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0326 - val_loss: 0.0191
Epoch 8/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0320 - val_loss: 0.0189
Epoch 9/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0311 - val_loss: 0.0177
Epoch 10/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0303 - val_loss: 0.0164
Epoch 11/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0169
Epoch 12/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0292 - val_loss: 0.0167
Epoch 13/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0162
Epoch 14/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0267 - val_loss: 0.0170
Epoch 15/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0255 - val_loss: 0.0161
Epoch 16/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0240 - val_loss: 0.0159
Epoch 17/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0148
Epoch 18/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0177
Epoch 19/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0174
Epoch 20/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0183
Epoch 21/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0169
Epoch 22/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0155
Epoch 23/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0118
Epoch 24/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0088
Epoch 25/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0076
Epoch 26/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0086
Epoch 27/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0071
Epoch 28/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0084
Epoch 29/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0074
Epoch 30/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0071
Epoch 31/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0075
Epoch 32/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0082
Epoch 33/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0066
Epoch 34/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0068
Epoch 35/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0053
Epoch 36/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0074
Epoch 37/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0073
Epoch 38/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0064
Epoch 39/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0068
Epoch 40/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0073
Epoch 41/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0065
Epoch 42/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0069
Epoch 43/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0072
Epoch 44/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0074
Epoch 45/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0069
Epoch 46/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0058
Epoch 47/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0066
Epoch 48/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0078
Epoch 49/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0076
Epoch 50/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0074
Epoch 51/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0064
Epoch 52/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0062
Epoch 53/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0064
Epoch 54/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0065
Epoch 55/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0059
Epoch 56/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0064
Epoch 57/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0060
Epoch 58/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0059
Epoch 59/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0072
Epoch 60/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0072
Epoch 61/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0063
Epoch 62/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0064
Epoch 63/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0061
Epoch 64/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0061
Epoch 65/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0059
Epoch 66/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0064
Epoch 67/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0065
Epoch 68/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0064
Epoch 69/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0058
Epoch 70/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0062
Epoch 71/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0058
Epoch 72/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0063
Epoch 73/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0056
Epoch 74/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0063
Epoch 75/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0059
Epoch 76/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0060
Epoch 77/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0060
Epoch 78/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0058
Epoch 79/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0056
Epoch 80/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0057
Epoch 81/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0065
Epoch 82/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0058
Epoch 83/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0061
Epoch 84/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0062
Epoch 85/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0059
Epoch 86/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0060
Epoch 87/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0061
Epoch 88/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0059
Epoch 89/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0060
Epoch 90/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0056
Execution time:  20.425318241119385
DNN:
Mean Absolute Error: 0.0055
Root Mean Square Error: 0.0201
Mean Square Error: 0.0004

Train RMSE: 0.020
Train MSE: 0.000
Train MAE: 0.005
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_8&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_24 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_25 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_8 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_26 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0439
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.0398
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0352
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.0338
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0388 - val_loss: 0.0339
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0331
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0319
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0315
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0297
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0267
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0242
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0237
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0227
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0221
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0213
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0208
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0202
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0194
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0187
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0181
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0177
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0171
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0167
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0160
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0156
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0151
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0147
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0142
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0138
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0136
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0125
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0120
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0114
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0113
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0107
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0107
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0100
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0102
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0099
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0103
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0099
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0103
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0097
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0090
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0100
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0095
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0095
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0096
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0093
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0095
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0097
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0095
Execution time:  6.711838960647583
DNN:
Mean Absolute Error: 0.0109
Root Mean Square Error: 0.0194
Mean Square Error: 0.0004

Train RMSE: 0.019
Train MSE: 0.000
Train MAE: 0.011
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_9&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_27 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_28 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_9 (Dropout)          (None, 6, 16)             0         
_________________________________________________________________
dense_29 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 1s 5ms/step - loss: 0.0493 - val_loss: 0.0275
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0197
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0211
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0217
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0218
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0211
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0205
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0195
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0191
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0188
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0181
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0176
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0169
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0166
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0160
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0154
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0151
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0145
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0142
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0140
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0139
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0138
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0138
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0137
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0137
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0136
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0137
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0137
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0139
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0140
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0140
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0138
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0139
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0137
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0135
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0133
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0134
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0132
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0132
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0151
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0144
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0140
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0140
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0131
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0127
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0142
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0140
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0143
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0140
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0143
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0136
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0143
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0142
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0137
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0133
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0143
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0151
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0143
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0136
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0144
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0139
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0142
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0141
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0144
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0131
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0139
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0136
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0129
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0137
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0138
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0153
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0140
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0124
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0150
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0134
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0142
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0140
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0136
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0145
Execution time:  15.500949144363403
DNN:
Mean Absolute Error: 0.0142
Root Mean Square Error: 0.0282
Mean Square Error: 0.0008

Train RMSE: 0.028
Train MSE: 0.001
Train MAE: 0.014
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_10&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_30 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_31 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_10 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_32 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.0180
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0185
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0185
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.0183
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.0187
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 0.0178
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0178
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0181
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0167
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0176
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0174
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0174
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0171
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0169
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0168
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.0166
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0166
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0158
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0163
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0161
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0158
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0156
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0150
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0144
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0143
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0140
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0141
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0137
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0135
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0126
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0128
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0123
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0119
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0115
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0109
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0109
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0108
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0109
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0107
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0091
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0117
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0098
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0111
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0089
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0098
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0095
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0094
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0110
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0109
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0114
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0083
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0113
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0109
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0105
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0106
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0107
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0098
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0116
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0123
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0117
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0114
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0097
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0117
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0094
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0100
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0103
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0108
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0110
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0088
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0082
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0087
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0078
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0081
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0081
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0084
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0082
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0087
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0085
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0087
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0083
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0085
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0091
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0085
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0083
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0084
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0086
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0084
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0085
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0087
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0083
Execution time:  19.581538677215576
DNN:
Mean Absolute Error: 0.0070
Root Mean Square Error: 0.0230
Mean Square Error: 0.0005

Train RMSE: 0.023
Train MSE: 0.001
Train MAE: 0.007
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_11&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_33 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_34 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_11 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_35 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.0495 - val_loss: 0.0381
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.0382
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0353
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0351
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.0338
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.0327
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.0329
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0324
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.0272
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 0.0263
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0258
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0256
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 0.0244
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0244
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0243
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0238
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0237
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0229
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0225
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0222
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0217
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0206
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0201
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0200
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0194
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0191
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0183
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0179
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0177
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0171
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0167
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0165
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0162
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0160
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0156
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0152
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0149
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0148
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0145
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0140
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0140
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0135
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0132
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0131
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0127
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0123
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0123
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0120
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0118
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0116
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0115
Execution time:  6.61554479598999
DNN:
Mean Absolute Error: 0.0144
Root Mean Square Error: 0.0247
Mean Square Error: 0.0006

Train RMSE: 0.025
Train MSE: 0.001
Train MAE: 0.014
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_12&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_36 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_37 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_12 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_38 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 3ms/step - loss: 0.4534 - val_loss: 0.4112
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4496 - val_loss: 0.4076
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4458 - val_loss: 0.4037
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.3997
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4376 - val_loss: 0.3955
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4333 - val_loss: 0.3910
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4288 - val_loss: 0.3864
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4237 - val_loss: 0.3816
Epoch 9/80
116/116 [==============================] - 0s 1ms/step - loss: 0.4188 - val_loss: 0.3767
Epoch 10/80
116/116 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.3715
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4076 - val_loss: 0.3662
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4014 - val_loss: 0.3607
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3961 - val_loss: 0.3550
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3898 - val_loss: 0.3492
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3431
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3780 - val_loss: 0.3370
Epoch 17/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.3307
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3242
Epoch 19/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3179
Epoch 20/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3116
Epoch 21/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.3052
Epoch 22/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.2987
Epoch 23/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.2920
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3243 - val_loss: 0.2853
Epoch 25/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.2784
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3093 - val_loss: 0.2714
Epoch 27/80
116/116 [==============================] - 0s 1ms/step - loss: 0.3024 - val_loss: 0.2642
Epoch 28/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2948 - val_loss: 0.2570
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2878 - val_loss: 0.2508
Epoch 30/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2809 - val_loss: 0.2449
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2750 - val_loss: 0.2390
Epoch 32/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2682 - val_loss: 0.2330
Epoch 33/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2619 - val_loss: 0.2269
Epoch 34/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2555 - val_loss: 0.2207
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2493 - val_loss: 0.2143
Epoch 36/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2422 - val_loss: 0.2079
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2355 - val_loss: 0.2023
Epoch 38/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2293 - val_loss: 0.1968
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2236 - val_loss: 0.1912
Epoch 40/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2178 - val_loss: 0.1855
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2125 - val_loss: 0.1797
Epoch 42/80
116/116 [==============================] - 0s 1ms/step - loss: 0.2057 - val_loss: 0.1738
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1987 - val_loss: 0.1677
Epoch 44/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1926 - val_loss: 0.1614
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 0.1551
Epoch 46/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1801 - val_loss: 0.1486
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1736 - val_loss: 0.1419
Epoch 48/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1676 - val_loss: 0.1352
Epoch 49/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1595 - val_loss: 0.1284
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1548 - val_loss: 0.1218
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1464 - val_loss: 0.1152
Epoch 52/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1412 - val_loss: 0.1088
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1359 - val_loss: 0.1029
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1310 - val_loss: 0.0974
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1279 - val_loss: 0.0920
Epoch 56/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1237 - val_loss: 0.0870
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1214 - val_loss: 0.0824
Epoch 58/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1176 - val_loss: 0.0783
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1158 - val_loss: 0.0746
Epoch 60/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1150 - val_loss: 0.0711
Epoch 61/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1131 - val_loss: 0.0679
Epoch 62/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1118 - val_loss: 0.0648
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1089 - val_loss: 0.0621
Epoch 64/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1101 - val_loss: 0.0595
Epoch 65/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1078 - val_loss: 0.0572
Epoch 66/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1080 - val_loss: 0.0551
Epoch 67/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1062 - val_loss: 0.0531
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.0512
Epoch 69/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1050 - val_loss: 0.0495
Epoch 70/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1061 - val_loss: 0.0479
Epoch 71/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1036 - val_loss: 0.0464
Epoch 72/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1044 - val_loss: 0.0450
Epoch 73/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1031 - val_loss: 0.0437
Epoch 74/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1026 - val_loss: 0.0424
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1024 - val_loss: 0.0411
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 0.0400
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 0.0388
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1019 - val_loss: 0.0376
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 0.0366
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1003 - val_loss: 0.0356
Execution time:  15.81882357597351
DNN:
Mean Absolute Error: 0.0558
Root Mean Square Error: 0.0616
Mean Square Error: 0.0038

Train RMSE: 0.062
Train MSE: 0.004
Train MAE: 0.056
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_13&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_39 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_40 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_13 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_41 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
144/144 [==============================] - 0s 3ms/step - loss: 0.3523 - val_loss: 0.3190
Epoch 2/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3156
Epoch 3/90
144/144 [==============================] - 0s 2ms/step - loss: 0.3450 - val_loss: 0.3118
Epoch 4/90
144/144 [==============================] - 0s 2ms/step - loss: 0.3410 - val_loss: 0.3077
Epoch 5/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3032
Epoch 6/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3317 - val_loss: 0.2985
Epoch 7/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3267 - val_loss: 0.2935
Epoch 8/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3216 - val_loss: 0.2883
Epoch 9/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.2828
Epoch 10/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3098 - val_loss: 0.2771
Epoch 11/90
144/144 [==============================] - 0s 1ms/step - loss: 0.3040 - val_loss: 0.2714
Epoch 12/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.2663
Epoch 13/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2927 - val_loss: 0.2613
Epoch 14/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2875 - val_loss: 0.2563
Epoch 15/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2822 - val_loss: 0.2513
Epoch 16/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2769 - val_loss: 0.2464
Epoch 17/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2718 - val_loss: 0.2413
Epoch 18/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2668 - val_loss: 0.2361
Epoch 19/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2610 - val_loss: 0.2308
Epoch 20/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2558 - val_loss: 0.2253
Epoch 21/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2499 - val_loss: 0.2198
Epoch 22/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2441 - val_loss: 0.2141
Epoch 23/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2378 - val_loss: 0.2082
Epoch 24/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2314 - val_loss: 0.2023
Epoch 25/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2253 - val_loss: 0.1964
Epoch 26/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2190 - val_loss: 0.1904
Epoch 27/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2125 - val_loss: 0.1842
Epoch 28/90
144/144 [==============================] - 0s 1ms/step - loss: 0.2066 - val_loss: 0.1779
Epoch 29/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1995 - val_loss: 0.1714
Epoch 30/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1929 - val_loss: 0.1648
Epoch 31/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1862 - val_loss: 0.1581
Epoch 32/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1794 - val_loss: 0.1512
Epoch 33/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1722 - val_loss: 0.1442
Epoch 34/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1649 - val_loss: 0.1370
Epoch 35/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1570 - val_loss: 0.1296
Epoch 36/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1493 - val_loss: 0.1221
Epoch 37/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1417 - val_loss: 0.1144
Epoch 38/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1336 - val_loss: 0.1065
Epoch 39/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1256 - val_loss: 0.0984
Epoch 40/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1167 - val_loss: 0.0902
Epoch 41/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1082 - val_loss: 0.0818
Epoch 42/90
144/144 [==============================] - 0s 1ms/step - loss: 0.1002 - val_loss: 0.0733
Epoch 43/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.0646
Epoch 44/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0558
Epoch 45/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0739 - val_loss: 0.0468
Epoch 46/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0637 - val_loss: 0.0379
Epoch 47/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0572 - val_loss: 0.0320
Epoch 48/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0546 - val_loss: 0.0281
Epoch 49/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0526 - val_loss: 0.0259
Epoch 50/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0526 - val_loss: 0.0242
Epoch 51/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0524 - val_loss: 0.0231
Epoch 52/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0514 - val_loss: 0.0223
Epoch 53/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0524 - val_loss: 0.0215
Epoch 54/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0210
Epoch 55/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0509 - val_loss: 0.0205
Epoch 56/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0516 - val_loss: 0.0200
Epoch 57/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0510 - val_loss: 0.0195
Epoch 58/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0514 - val_loss: 0.0191
Epoch 59/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0516 - val_loss: 0.0188
Epoch 60/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0508 - val_loss: 0.0185
Epoch 61/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0505 - val_loss: 0.0182
Epoch 62/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0510 - val_loss: 0.0180
Epoch 63/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0507 - val_loss: 0.0177
Epoch 64/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0174
Epoch 65/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0507 - val_loss: 0.0172
Epoch 66/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0169
Epoch 67/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0507 - val_loss: 0.0167
Epoch 68/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0165
Epoch 69/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0514 - val_loss: 0.0163
Epoch 70/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0508 - val_loss: 0.0161
Epoch 71/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.0159
Epoch 72/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0157
Epoch 73/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0155
Epoch 74/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0509 - val_loss: 0.0153
Epoch 75/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0507 - val_loss: 0.0151
Epoch 76/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0506 - val_loss: 0.0150
Epoch 77/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0149
Epoch 78/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0147
Epoch 79/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0508 - val_loss: 0.0146
Epoch 80/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0145
Epoch 81/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0507 - val_loss: 0.0144
Epoch 82/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0501 - val_loss: 0.0143
Epoch 83/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0505 - val_loss: 0.0143
Epoch 84/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0508 - val_loss: 0.0142
Epoch 85/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0506 - val_loss: 0.0141
Epoch 86/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0141
Epoch 87/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0140
Epoch 88/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0140
Epoch 89/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0139
Epoch 90/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0139
Execution time:  19.60874104499817
DNN:
Mean Absolute Error: 0.0310
Root Mean Square Error: 0.0381
Mean Square Error: 0.0015

Train RMSE: 0.038
Train MSE: 0.001
Train MAE: 0.031
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_14&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_42 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_43 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_14 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_44 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 3ms/step - loss: 0.3766 - val_loss: 0.3423
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3761 - val_loss: 0.3419
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3414
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3749 - val_loss: 0.3409
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3747 - val_loss: 0.3403
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3739 - val_loss: 0.3398
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3737 - val_loss: 0.3392
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3728 - val_loss: 0.3386
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 0.3380
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3374
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3367
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3699 - val_loss: 0.3360
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3694 - val_loss: 0.3353
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3689 - val_loss: 0.3346
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 0.3339
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3666 - val_loss: 0.3332
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3664 - val_loss: 0.3324
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3662 - val_loss: 0.3316
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3308
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3640 - val_loss: 0.3300
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3631 - val_loss: 0.3292
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3623 - val_loss: 0.3284
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3275
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3611 - val_loss: 0.3267
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3258
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3249
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3585 - val_loss: 0.3240
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3572 - val_loss: 0.3231
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3561 - val_loss: 0.3221
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3212
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3540 - val_loss: 0.3202
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3193
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3183
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3515 - val_loss: 0.3173
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3508 - val_loss: 0.3163
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3491 - val_loss: 0.3152
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3478 - val_loss: 0.3142
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3132
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3121
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3448 - val_loss: 0.3111
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3440 - val_loss: 0.3100
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3428 - val_loss: 0.3089
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3425 - val_loss: 0.3078
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3405 - val_loss: 0.3067
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3392 - val_loss: 0.3056
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3374 - val_loss: 0.3044
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3365 - val_loss: 0.3033
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3357 - val_loss: 0.3021
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3349 - val_loss: 0.3009
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3336 - val_loss: 0.2998
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.2986
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3312 - val_loss: 0.2974
Execution time:  6.342407703399658
DNN:
Mean Absolute Error: 0.3266
Root Mean Square Error: 0.3286
Mean Square Error: 0.1080

Train RMSE: 0.329
Train MSE: 0.108
Train MAE: 0.327
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_15&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_45 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_46 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_15 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_47 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3971 - val_loss: 0.3740
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3943 - val_loss: 0.3714
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3920 - val_loss: 0.3685
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3893 - val_loss: 0.3654
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3853 - val_loss: 0.3622
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.3588
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3790 - val_loss: 0.3552
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3744 - val_loss: 0.3514
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3708 - val_loss: 0.3475
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3669 - val_loss: 0.3435
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3629 - val_loss: 0.3393
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3580 - val_loss: 0.3350
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3305
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3492 - val_loss: 0.3260
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3440 - val_loss: 0.3215
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3392 - val_loss: 0.3169
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3350 - val_loss: 0.3122
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3304 - val_loss: 0.3076
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3258 - val_loss: 0.3030
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3207 - val_loss: 0.2983
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3160 - val_loss: 0.2935
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3104 - val_loss: 0.2887
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3066 - val_loss: 0.2839
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3005 - val_loss: 0.2790
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2957 - val_loss: 0.2740
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2913 - val_loss: 0.2690
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2859 - val_loss: 0.2640
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2809 - val_loss: 0.2588
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2758 - val_loss: 0.2537
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2691 - val_loss: 0.2485
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.2432
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2592 - val_loss: 0.2379
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2548 - val_loss: 0.2325
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2473 - val_loss: 0.2271
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 0.2217
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2368 - val_loss: 0.2169
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2320 - val_loss: 0.2123
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2273 - val_loss: 0.2078
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2241 - val_loss: 0.2032
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2196 - val_loss: 0.1986
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2136 - val_loss: 0.1940
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2091 - val_loss: 0.1893
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2041 - val_loss: 0.1845
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1995 - val_loss: 0.1797
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1942 - val_loss: 0.1748
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1904 - val_loss: 0.1699
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1849 - val_loss: 0.1650
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1799 - val_loss: 0.1599
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1742 - val_loss: 0.1548
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 0.1497
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1642 - val_loss: 0.1445
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1600 - val_loss: 0.1396
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 0.1349
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1504 - val_loss: 0.1305
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1481 - val_loss: 0.1264
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1452 - val_loss: 0.1224
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1417 - val_loss: 0.1185
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1389 - val_loss: 0.1147
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1361 - val_loss: 0.1110
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1340 - val_loss: 0.1074
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1322 - val_loss: 0.1039
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.1005
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1273 - val_loss: 0.0972
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.0940
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1235 - val_loss: 0.0910
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1211 - val_loss: 0.0881
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1206 - val_loss: 0.0854
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1195 - val_loss: 0.0829
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1182 - val_loss: 0.0805
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 0.0782
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1162 - val_loss: 0.0760
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.0738
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1157 - val_loss: 0.0717
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.0697
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1133 - val_loss: 0.0679
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1122 - val_loss: 0.0661
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.0643
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 0.0625
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1100 - val_loss: 0.0607
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1092 - val_loss: 0.0590
Execution time:  15.335560083389282
DNN:
Mean Absolute Error: 0.0694
Root Mean Square Error: 0.0736
Mean Square Error: 0.0054

Train RMSE: 0.074
Train MSE: 0.005
Train MAE: 0.069
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_16&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_48 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_49 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_16 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_50 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.4001 - val_loss: 0.3766
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3970 - val_loss: 0.3737
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3939 - val_loss: 0.3705
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3906 - val_loss: 0.3670
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3869 - val_loss: 0.3633
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3830 - val_loss: 0.3594
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3791 - val_loss: 0.3553
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3745 - val_loss: 0.3509
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 0.3464
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3653 - val_loss: 0.3417
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3604 - val_loss: 0.3367
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3556 - val_loss: 0.3317
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3502 - val_loss: 0.3265
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3445 - val_loss: 0.3211
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3387 - val_loss: 0.3155
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3330 - val_loss: 0.3098
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.3040
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3212 - val_loss: 0.2979
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.2917
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.2854
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3016 - val_loss: 0.2789
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2954 - val_loss: 0.2732
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2898 - val_loss: 0.2680
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2839 - val_loss: 0.2629
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2794 - val_loss: 0.2577
Epoch 26/90
128/128 [==============================] - 0s 1ms/step - loss: 0.2738 - val_loss: 0.2524
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2686 - val_loss: 0.2471
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2626 - val_loss: 0.2417
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2570 - val_loss: 0.2362
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2518 - val_loss: 0.2306
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2458 - val_loss: 0.2249
Epoch 32/90
128/128 [==============================] - 0s 1ms/step - loss: 0.2400 - val_loss: 0.2192
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2342 - val_loss: 0.2135
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2280 - val_loss: 0.2077
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2218 - val_loss: 0.2017
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 0.1957
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2104 - val_loss: 0.1896
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2044 - val_loss: 0.1834
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1974 - val_loss: 0.1772
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1910 - val_loss: 0.1708
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1846 - val_loss: 0.1643
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1779 - val_loss: 0.1577
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1716 - val_loss: 0.1510
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1640 - val_loss: 0.1443
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1587 - val_loss: 0.1374
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1511 - val_loss: 0.1304
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1431 - val_loss: 0.1234
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1362 - val_loss: 0.1162
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1295 - val_loss: 0.1090
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 0.1017
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1135 - val_loss: 0.0943
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 0.0869
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0998 - val_loss: 0.0794
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0928 - val_loss: 0.0718
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0845 - val_loss: 0.0641
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0776 - val_loss: 0.0564
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0704 - val_loss: 0.0487
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0619 - val_loss: 0.0420
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.0393
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0373
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0357
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.0348
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.0341
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0572 - val_loss: 0.0335
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0331
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0573 - val_loss: 0.0327
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.0324
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0573 - val_loss: 0.0321
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0564 - val_loss: 0.0318
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.0315
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 0.0313
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.0313
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0560 - val_loss: 0.0312
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0312
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.0311
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0560 - val_loss: 0.0311
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0558 - val_loss: 0.0311
Epoch 78/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0567 - val_loss: 0.0310
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0310
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0568 - val_loss: 0.0310
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0568 - val_loss: 0.0309
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0309
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0561 - val_loss: 0.0309
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.0309
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0308
Epoch 86/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0564 - val_loss: 0.0308
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0309
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.0308
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.0308
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0564 - val_loss: 0.0308
Execution time:  19.509649991989136
DNN:
Mean Absolute Error: 0.0414
Root Mean Square Error: 0.0483
Mean Square Error: 0.0023

Train RMSE: 0.048
Train MSE: 0.002
Train MAE: 0.041
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_17&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_51 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_52 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_17 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_53 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.3856 - val_loss: 0.3645
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3857 - val_loss: 0.3638
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 0.3632
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 0.3625
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3834 - val_loss: 0.3618
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.3610
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3819 - val_loss: 0.3603
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.3595
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.3586
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3788 - val_loss: 0.3578
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3778 - val_loss: 0.3569
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3777 - val_loss: 0.3560
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3764 - val_loss: 0.3551
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 0.3542
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.3532
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3742 - val_loss: 0.3523
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3721 - val_loss: 0.3513
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3718 - val_loss: 0.3503
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3493
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3699 - val_loss: 0.3482
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3693 - val_loss: 0.3472
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3678 - val_loss: 0.3461
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3667 - val_loss: 0.3450
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3658 - val_loss: 0.3439
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.3428
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3632 - val_loss: 0.3416
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3623 - val_loss: 0.3404
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3605 - val_loss: 0.3393
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.3382
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3581 - val_loss: 0.3371
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3571 - val_loss: 0.3359
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 0.3347
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3336
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3539 - val_loss: 0.3324
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3312
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.3300
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3503 - val_loss: 0.3288
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3486 - val_loss: 0.3275
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3480 - val_loss: 0.3263
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3456 - val_loss: 0.3250
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3452 - val_loss: 0.3237
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3437 - val_loss: 0.3224
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3426 - val_loss: 0.3211
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3413 - val_loss: 0.3198
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3395 - val_loss: 0.3185
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3381 - val_loss: 0.3171
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3376 - val_loss: 0.3158
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3362 - val_loss: 0.3144
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 0.3130
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3325 - val_loss: 0.3116
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3309 - val_loss: 0.3102
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3304 - val_loss: 0.3088
Execution time:  6.494782447814941
DNN:
Mean Absolute Error: 0.3250
Root Mean Square Error: 0.3271
Mean Square Error: 0.1070

Train RMSE: 0.327
Train MSE: 0.107
Train MAE: 0.325
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_18&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_54 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_55 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_18 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_56 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 3ms/step - loss: 0.1214 - val_loss: 0.1534
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1204 - val_loss: 0.1524
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.1513
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1181 - val_loss: 0.1501
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.1488
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1154 - val_loss: 0.1475
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.1462
Epoch 8/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1128 - val_loss: 0.1449
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.1435
Epoch 10/80
116/116 [==============================] - 0s 1ms/step - loss: 0.1100 - val_loss: 0.1421
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1085 - val_loss: 0.1406
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.1391
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.1376
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.1361
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1026 - val_loss: 0.1348
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1011 - val_loss: 0.1335
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.1323
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0987 - val_loss: 0.1310
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1297
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.1285
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0949 - val_loss: 0.1272
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0937 - val_loss: 0.1259
Epoch 23/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0924 - val_loss: 0.1245
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0910 - val_loss: 0.1232
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0896 - val_loss: 0.1219
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.1206
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0871 - val_loss: 0.1193
Epoch 28/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1180
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.1166
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0834 - val_loss: 0.1153
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0821 - val_loss: 0.1140
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0809 - val_loss: 0.1127
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0796 - val_loss: 0.1114
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0782 - val_loss: 0.1100
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.1087
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0759 - val_loss: 0.1074
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0747 - val_loss: 0.1061
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 0.1048
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0724 - val_loss: 0.1036
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.1023
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0705 - val_loss: 0.1011
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0691 - val_loss: 0.0999
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0681 - val_loss: 0.0988
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0673 - val_loss: 0.0977
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0664 - val_loss: 0.0966
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.0955
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0647 - val_loss: 0.0945
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0639 - val_loss: 0.0936
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0632 - val_loss: 0.0929
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0922
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0621 - val_loss: 0.0915
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0615 - val_loss: 0.0908
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0612 - val_loss: 0.0901
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0606 - val_loss: 0.0894
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0888
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0596 - val_loss: 0.0881
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0591 - val_loss: 0.0875
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0586 - val_loss: 0.0869
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0584 - val_loss: 0.0863
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0856
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.0851
Epoch 62/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0572 - val_loss: 0.0845
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.0839
Epoch 64/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0561 - val_loss: 0.0833
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0558 - val_loss: 0.0827
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0821
Epoch 67/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0549 - val_loss: 0.0816
Epoch 68/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0547 - val_loss: 0.0810
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0805
Epoch 70/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0543 - val_loss: 0.0799
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.0794
Epoch 72/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0536 - val_loss: 0.0789
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.0784
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0525 - val_loss: 0.0778
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0522 - val_loss: 0.0773
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0521 - val_loss: 0.0768
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0764
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0517 - val_loss: 0.0759
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0754
Epoch 80/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0749
Execution time:  16.37921118736267
DNN:
Mean Absolute Error: 0.0510
Root Mean Square Error: 0.0591
Mean Square Error: 0.0035

Train RMSE: 0.059
Train MSE: 0.003
Train MAE: 0.051
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_19&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_57 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_58 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_19 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_59 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
144/144 [==============================] - 0s 3ms/step - loss: 0.0916 - val_loss: 0.1254
Epoch 2/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0908 - val_loss: 0.1246
Epoch 3/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 0.1237
Epoch 4/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0891 - val_loss: 0.1227
Epoch 5/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.1217
Epoch 6/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 0.1206
Epoch 7/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.1195
Epoch 8/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0850 - val_loss: 0.1183
Epoch 9/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.1171
Epoch 10/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.1159
Epoch 11/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.1147
Epoch 12/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.1136
Epoch 13/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0794 - val_loss: 0.1125
Epoch 14/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0784 - val_loss: 0.1114
Epoch 15/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0773 - val_loss: 0.1103
Epoch 16/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0763 - val_loss: 0.1091
Epoch 17/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0753 - val_loss: 0.1080
Epoch 18/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0743 - val_loss: 0.1068
Epoch 19/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0733 - val_loss: 0.1057
Epoch 20/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0722 - val_loss: 0.1045
Epoch 21/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0711 - val_loss: 0.1033
Epoch 22/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0701 - val_loss: 0.1021
Epoch 23/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0690 - val_loss: 0.1009
Epoch 24/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0681 - val_loss: 0.0998
Epoch 25/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0670 - val_loss: 0.0986
Epoch 26/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0661 - val_loss: 0.0974
Epoch 27/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0651 - val_loss: 0.0963
Epoch 28/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0640 - val_loss: 0.0951
Epoch 29/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0632 - val_loss: 0.0939
Epoch 30/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0623 - val_loss: 0.0928
Epoch 31/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0614 - val_loss: 0.0916
Epoch 32/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0604 - val_loss: 0.0905
Epoch 33/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0595 - val_loss: 0.0893
Epoch 34/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0882
Epoch 35/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0870
Epoch 36/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0568 - val_loss: 0.0859
Epoch 37/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0561 - val_loss: 0.0849
Epoch 38/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0553 - val_loss: 0.0839
Epoch 39/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0546 - val_loss: 0.0829
Epoch 40/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0539 - val_loss: 0.0819
Epoch 41/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0533 - val_loss: 0.0810
Epoch 42/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0801
Epoch 43/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0520 - val_loss: 0.0792
Epoch 44/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0514 - val_loss: 0.0783
Epoch 45/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.0774
Epoch 46/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0765
Epoch 47/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0756
Epoch 48/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0493 - val_loss: 0.0748
Epoch 49/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0487 - val_loss: 0.0740
Epoch 50/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0482 - val_loss: 0.0732
Epoch 51/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0478 - val_loss: 0.0724
Epoch 52/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0473 - val_loss: 0.0716
Epoch 53/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0469 - val_loss: 0.0708
Epoch 54/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0464 - val_loss: 0.0700
Epoch 55/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0460 - val_loss: 0.0693
Epoch 56/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0455 - val_loss: 0.0685
Epoch 57/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0452 - val_loss: 0.0678
Epoch 58/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0448 - val_loss: 0.0671
Epoch 59/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0445 - val_loss: 0.0664
Epoch 60/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0441 - val_loss: 0.0657
Epoch 61/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0437 - val_loss: 0.0650
Epoch 62/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0433 - val_loss: 0.0643
Epoch 63/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0430 - val_loss: 0.0636
Epoch 64/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0425 - val_loss: 0.0629
Epoch 65/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0423 - val_loss: 0.0622
Epoch 66/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0615
Epoch 67/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0416 - val_loss: 0.0608
Epoch 68/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0413 - val_loss: 0.0601
Epoch 69/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0411 - val_loss: 0.0595
Epoch 70/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0405 - val_loss: 0.0588
Epoch 71/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0404 - val_loss: 0.0582
Epoch 72/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0399 - val_loss: 0.0575
Epoch 73/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0397 - val_loss: 0.0569
Epoch 74/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0395 - val_loss: 0.0562
Epoch 75/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0394 - val_loss: 0.0556
Epoch 76/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0390 - val_loss: 0.0550
Epoch 77/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0387 - val_loss: 0.0544
Epoch 78/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0386 - val_loss: 0.0539
Epoch 79/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0384 - val_loss: 0.0533
Epoch 80/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0381 - val_loss: 0.0528
Epoch 81/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0379 - val_loss: 0.0522
Epoch 82/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0376 - val_loss: 0.0517
Epoch 83/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0376 - val_loss: 0.0511
Epoch 84/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0372 - val_loss: 0.0506
Epoch 85/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0371 - val_loss: 0.0501
Epoch 86/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0371 - val_loss: 0.0496
Epoch 87/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0367 - val_loss: 0.0491
Epoch 88/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0367 - val_loss: 0.0486
Epoch 89/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 0.0481
Epoch 90/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0362 - val_loss: 0.0476
Execution time:  19.777557611465454
DNN:
Mean Absolute Error: 0.0366
Root Mean Square Error: 0.0470
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.037
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_20&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_60 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_61 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_20 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_62 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 4ms/step - loss: 0.0778 - val_loss: 0.1108
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0776 - val_loss: 0.1106
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0777 - val_loss: 0.1104
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0772 - val_loss: 0.1102
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.1100
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0768 - val_loss: 0.1098
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0767 - val_loss: 0.1095
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0763 - val_loss: 0.1093
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0763 - val_loss: 0.1091
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0762 - val_loss: 0.1089
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0760 - val_loss: 0.1086
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0756 - val_loss: 0.1084
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.1081
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0755 - val_loss: 0.1079
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.1076
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.1074
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.1071
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0745 - val_loss: 0.1069
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0742 - val_loss: 0.1066
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0741 - val_loss: 0.1063
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0741 - val_loss: 0.1061
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 0.1058
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0734 - val_loss: 0.1055
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0730 - val_loss: 0.1053
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0729 - val_loss: 0.1050
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0728 - val_loss: 0.1047
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0722 - val_loss: 0.1044
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0722 - val_loss: 0.1041
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.1039
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.1036
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0716 - val_loss: 0.1033
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.1030
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0710 - val_loss: 0.1027
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0708 - val_loss: 0.1024
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.1022
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0704 - val_loss: 0.1019
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0702 - val_loss: 0.1016
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0700 - val_loss: 0.1013
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0698 - val_loss: 0.1010
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0694 - val_loss: 0.1007
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0693 - val_loss: 0.1005
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0688 - val_loss: 0.1002
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0689 - val_loss: 0.1000
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0686 - val_loss: 0.0998
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0685 - val_loss: 0.0996
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0684 - val_loss: 0.0994
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0681 - val_loss: 0.0992
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0683 - val_loss: 0.0990
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.0988
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0675 - val_loss: 0.0986
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0678 - val_loss: 0.0984
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0674 - val_loss: 0.0982
Execution time:  6.700023889541626
DNN:
Mean Absolute Error: 0.0691
Root Mean Square Error: 0.0770
Mean Square Error: 0.0059

Train RMSE: 0.077
Train MSE: 0.006
Train MAE: 0.069
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_21&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_63 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_64 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_21 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_65 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1092 - val_loss: 0.1292
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1088 - val_loss: 0.1289
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.1284
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1079 - val_loss: 0.1280
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.1276
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1070 - val_loss: 0.1271
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1066 - val_loss: 0.1266
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1060 - val_loss: 0.1261
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.1256
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1250
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.1245
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1039 - val_loss: 0.1240
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1033 - val_loss: 0.1234
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1027 - val_loss: 0.1228
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1019 - val_loss: 0.1222
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1014 - val_loss: 0.1216
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.1210
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.1204
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0995 - val_loss: 0.1198
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.1192
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0983 - val_loss: 0.1186
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.1181
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0970 - val_loss: 0.1175
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0965 - val_loss: 0.1170
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0959 - val_loss: 0.1164
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0953 - val_loss: 0.1159
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0948 - val_loss: 0.1153
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0942 - val_loss: 0.1147
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0935 - val_loss: 0.1142
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0929 - val_loss: 0.1136
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0924 - val_loss: 0.1130
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0918 - val_loss: 0.1124
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0912 - val_loss: 0.1118
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0906 - val_loss: 0.1113
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0899 - val_loss: 0.1107
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.1101
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0887 - val_loss: 0.1094
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0880 - val_loss: 0.1085
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0870 - val_loss: 0.1075
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0861 - val_loss: 0.1065
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0849 - val_loss: 0.1055
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0840 - val_loss: 0.1045
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0829 - val_loss: 0.1035
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0821 - val_loss: 0.1025
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0809 - val_loss: 0.1016
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.1006
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0790 - val_loss: 0.0996
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0781 - val_loss: 0.0986
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0770 - val_loss: 0.0976
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0761 - val_loss: 0.0966
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0956
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0742 - val_loss: 0.0947
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0937
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0722 - val_loss: 0.0927
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.0918
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0704 - val_loss: 0.0908
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0697 - val_loss: 0.0899
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0686 - val_loss: 0.0890
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0679 - val_loss: 0.0882
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0670 - val_loss: 0.0875
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0665 - val_loss: 0.0867
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0658 - val_loss: 0.0860
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0651 - val_loss: 0.0853
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0644 - val_loss: 0.0846
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0839
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0633 - val_loss: 0.0833
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0628 - val_loss: 0.0826
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0620 - val_loss: 0.0820
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0617 - val_loss: 0.0813
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0610 - val_loss: 0.0807
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0605 - val_loss: 0.0800
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0602 - val_loss: 0.0794
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0597 - val_loss: 0.0788
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0593 - val_loss: 0.0782
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0587 - val_loss: 0.0777
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0584 - val_loss: 0.0771
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0765
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 0.0760
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 0.0754
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0568 - val_loss: 0.0749
Execution time:  15.50702428817749
DNN:
Mean Absolute Error: 0.0588
Root Mean Square Error: 0.0666
Mean Square Error: 0.0044

Train RMSE: 0.067
Train MSE: 0.004
Train MAE: 0.059
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_22&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_66 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_67 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_22 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_68 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.0888 - val_loss: 0.1101
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0881 - val_loss: 0.1094
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0873 - val_loss: 0.1087
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0865 - val_loss: 0.1079
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.1070
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0847 - val_loss: 0.1061
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0838 - val_loss: 0.1052
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0828 - val_loss: 0.1042
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0818 - val_loss: 0.1031
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0807 - val_loss: 0.1021
Epoch 11/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0796 - val_loss: 0.1010
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0786 - val_loss: 0.1000
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0776 - val_loss: 0.0990
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0764 - val_loss: 0.0979
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0755 - val_loss: 0.0968
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0744 - val_loss: 0.0958
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0732 - val_loss: 0.0947
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0722 - val_loss: 0.0936
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0925
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0702 - val_loss: 0.0914
Epoch 21/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0690 - val_loss: 0.0903
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0680 - val_loss: 0.0891
Epoch 23/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0670 - val_loss: 0.0880
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0660 - val_loss: 0.0869
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0649 - val_loss: 0.0858
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0639 - val_loss: 0.0848
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0629 - val_loss: 0.0837
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0620 - val_loss: 0.0827
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0610 - val_loss: 0.0816
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0602 - val_loss: 0.0806
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0593 - val_loss: 0.0795
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0583 - val_loss: 0.0785
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.0775
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.0765
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0560 - val_loss: 0.0757
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0748
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0740
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0540 - val_loss: 0.0732
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.0724
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0529 - val_loss: 0.0717
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.0709
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0520 - val_loss: 0.0702
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0516 - val_loss: 0.0695
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0508 - val_loss: 0.0688
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.0680
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0673
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0666
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.0659
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0653
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0481 - val_loss: 0.0646
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.0640
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0634
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 0.0628
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.0621
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0462 - val_loss: 0.0615
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.0609
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0603
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.0598
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0448 - val_loss: 0.0592
Epoch 60/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0445 - val_loss: 0.0586
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.0580
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.0575
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0438 - val_loss: 0.0569
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0564
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0558
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0553
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0547
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0542
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.0537
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0532
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.0526
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0521
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0516
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0511
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.0506
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0404 - val_loss: 0.0501
Epoch 77/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0401 - val_loss: 0.0496
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.0491
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 0.0487
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0482
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0394 - val_loss: 0.0478
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0390 - val_loss: 0.0474
Epoch 83/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0391 - val_loss: 0.0469
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0388 - val_loss: 0.0465
Epoch 85/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0387 - val_loss: 0.0462
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0458
Epoch 87/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0381 - val_loss: 0.0454
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.0450
Epoch 89/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0383 - val_loss: 0.0446
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.0442
Execution time:  19.163029432296753
DNN:
Mean Absolute Error: 0.0381
Root Mean Square Error: 0.0480
Mean Square Error: 0.0023

Train RMSE: 0.048
Train MSE: 0.002
Train MAE: 0.038
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_23&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_69 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_70 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_23 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_71 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.0992 - val_loss: 0.1196
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.1194
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.1192
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0984 - val_loss: 0.1190
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0982 - val_loss: 0.1187
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0977 - val_loss: 0.1184
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0977 - val_loss: 0.1181
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0973 - val_loss: 0.1178
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0972 - val_loss: 0.1175
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.1171
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0965 - val_loss: 0.1168
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0958 - val_loss: 0.1165
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0954 - val_loss: 0.1161
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.1158
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0953 - val_loss: 0.1154
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0946 - val_loss: 0.1151
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.1147
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0940 - val_loss: 0.1143
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0935 - val_loss: 0.1140
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.1136
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0924 - val_loss: 0.1132
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0922 - val_loss: 0.1128
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0918 - val_loss: 0.1124
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0916 - val_loss: 0.1120
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0913 - val_loss: 0.1117
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0906 - val_loss: 0.1113
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0904 - val_loss: 0.1109
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0901 - val_loss: 0.1105
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.1101
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0892 - val_loss: 0.1097
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0889 - val_loss: 0.1093
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0883 - val_loss: 0.1088
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0879 - val_loss: 0.1084
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0875 - val_loss: 0.1080
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0870 - val_loss: 0.1076
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.1072
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0861 - val_loss: 0.1068
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0859 - val_loss: 0.1064
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0855 - val_loss: 0.1060
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0851 - val_loss: 0.1055
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0848 - val_loss: 0.1051
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.1047
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0839 - val_loss: 0.1043
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0835 - val_loss: 0.1039
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0832 - val_loss: 0.1034
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0827 - val_loss: 0.1030
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0820 - val_loss: 0.1026
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0816 - val_loss: 0.1022
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0816 - val_loss: 0.1017
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0811 - val_loss: 0.1013
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0806 - val_loss: 0.1009
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.1004
Execution time:  6.310053586959839
DNN:
Mean Absolute Error: 0.0832
Root Mean Square Error: 0.0902
Mean Square Error: 0.0081

Train RMSE: 0.090
Train MSE: 0.008
Train MAE: 0.083
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_24&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_72 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_73 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_24 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_74 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 0.0079
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0826 - val_loss: 0.0171
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0766 - val_loss: 0.0164
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0697 - val_loss: 0.0093
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0073
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0560 - val_loss: 0.0046
Epoch 7/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0496 - val_loss: 0.0045
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0433 - val_loss: 0.0055
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0388 - val_loss: 0.0071
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0084
Epoch 11/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0314 - val_loss: 0.0128
Epoch 12/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0151
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0159
Epoch 14/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0258 - val_loss: 0.0149
Epoch 15/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.0102
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0106
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0104
Epoch 18/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0234 - val_loss: 0.0113
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0101
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0102
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0105
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0115
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0121
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0127
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0140
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0136
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0145
Epoch 28/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0142
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0140
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0135
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0134
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0119
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0135
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0124
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0117
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0135
Epoch 37/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0133
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0126
Epoch 39/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0125
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0124
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0137
Epoch 42/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0128
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0134
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0130
Epoch 45/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0131
Epoch 46/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0123
Epoch 47/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0123
Epoch 48/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0123
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0119
Epoch 50/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0122
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0120
Epoch 52/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0124
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0133
Epoch 54/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0136
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0135
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0120
Epoch 57/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0130
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0128
Epoch 59/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0136
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0124
Epoch 61/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0134
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0127
Epoch 63/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0124
Epoch 64/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0131
Epoch 65/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0121
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0128
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0128
Epoch 68/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0123
Epoch 69/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0129
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0127
Epoch 71/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0134
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0128
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0132
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0137
Epoch 75/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0124
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0132
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0123
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0123
Epoch 79/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0124
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0135
Execution time:  15.828001737594604
DNN:
Mean Absolute Error: 0.0071
Root Mean Square Error: 0.0136
Mean Square Error: 0.0002

Train RMSE: 0.014
Train MSE: 0.000
Train MAE: 0.007
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_25&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_75 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_76 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_25 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_77 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
144/144 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 0.0149
Epoch 2/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0507 - val_loss: 0.0158
Epoch 3/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0496 - val_loss: 0.0158
Epoch 4/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0473 - val_loss: 0.0139
Epoch 5/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0462 - val_loss: 0.0098
Epoch 6/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0444 - val_loss: 0.0071
Epoch 7/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0419 - val_loss: 0.0075
Epoch 8/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0410 - val_loss: 0.0059
Epoch 9/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0387 - val_loss: 0.0047
Epoch 10/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 0.0040
Epoch 11/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 0.0040
Epoch 12/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 0.0050
Epoch 13/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0320 - val_loss: 0.0053
Epoch 14/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0302 - val_loss: 0.0061
Epoch 15/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0291 - val_loss: 0.0069
Epoch 16/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0069
Epoch 17/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0257 - val_loss: 0.0086
Epoch 18/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0235 - val_loss: 0.0079
Epoch 19/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0082
Epoch 20/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0115
Epoch 21/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0108
Epoch 22/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0108
Epoch 23/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0133
Epoch 24/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0128
Epoch 25/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0116
Epoch 26/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0160
Epoch 27/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0144
Epoch 28/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0140
Epoch 29/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0137
Epoch 30/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0138
Epoch 31/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0148
Epoch 32/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0149
Epoch 33/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0134
Epoch 34/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0132
Epoch 35/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0133
Epoch 36/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0134
Epoch 37/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0136
Epoch 38/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0135
Epoch 39/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0126
Epoch 40/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0130
Epoch 41/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0137
Epoch 42/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0136
Epoch 43/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0124
Epoch 44/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0129
Epoch 45/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0138
Epoch 46/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0133
Epoch 47/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0113
Epoch 48/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0126
Epoch 49/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0143
Epoch 50/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0130
Epoch 51/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0133
Epoch 52/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0128
Epoch 53/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0138
Epoch 54/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0133
Epoch 55/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0136
Epoch 56/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0130
Epoch 57/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0130
Epoch 58/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0140
Epoch 59/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0141
Epoch 60/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0139
Epoch 61/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0139
Epoch 62/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0141
Epoch 63/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0144
Epoch 64/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0137
Epoch 65/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0140
Epoch 66/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0136
Epoch 67/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0141
Epoch 68/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0138
Epoch 69/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0138
Epoch 70/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0139
Epoch 71/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0134
Epoch 72/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0134
Epoch 73/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0129
Epoch 74/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0139
Epoch 75/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0135
Epoch 76/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0143
Epoch 77/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0138
Epoch 78/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0144
Epoch 79/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0142
Epoch 80/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0139
Epoch 81/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0131
Epoch 82/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0138
Epoch 83/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0134
Epoch 84/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0137
Epoch 85/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0138
Epoch 86/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0141
Epoch 87/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0140
Epoch 88/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0144
Epoch 89/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0132
Epoch 90/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0146
Execution time:  19.506383895874023
DNN:
Mean Absolute Error: 0.0072
Root Mean Square Error: 0.0150
Mean Square Error: 0.0002

Train RMSE: 0.015
Train MSE: 0.000
Train MAE: 0.007
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_26&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_78 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_79 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_26 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_80 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 6ms/step - loss: 0.1391 - val_loss: 0.0224
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 0.0217
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0985 - val_loss: 0.0211
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0887 - val_loss: 0.0191
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0821 - val_loss: 0.0150
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.0179
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0706 - val_loss: 0.0179
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0654 - val_loss: 0.0150
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0609 - val_loss: 0.0135
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0559 - val_loss: 0.0117
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0084
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0057
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.0039
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0048
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0049
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0059
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0061
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0058
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0064
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0072
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0081
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0092
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0098
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0101
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0101
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0099
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0099
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0100
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0101
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0102
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0099
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0102
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0102
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0101
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0104
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0100
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0110
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0104
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0110
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0103
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0116
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0103
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0118
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0108
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0109
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0114
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0107
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0115
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0108
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0115
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0109
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0114
Execution time:  6.807473659515381
DNN:
Mean Absolute Error: 0.0080
Root Mean Square Error: 0.0137
Mean Square Error: 0.0002

Train RMSE: 0.014
Train MSE: 0.000
Train MAE: 0.008
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_27&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_81 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_82 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_27 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_83 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.2657 - val_loss: 0.0575
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1043 - val_loss: 0.0383
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.0345
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0776 - val_loss: 0.0312
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0668 - val_loss: 0.0251
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0244
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0515 - val_loss: 0.0180
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0155
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.0143
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0139
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0149
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0146
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0155
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0161
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0162
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0159
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0158
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0158
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0162
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0154
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0148
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0152
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0148
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0143
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0146
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0144
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0145
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0141
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0142
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0140
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0139
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0139
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0139
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0136
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0134
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0131
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0128
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0124
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0120
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0112
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0118
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0101
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0110
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0103
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0104
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0105
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0108
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0104
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0096
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0109
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0098
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0104
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0097
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0110
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0094
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0113
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0104
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0098
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0111
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0097
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0100
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0104
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0114
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0095
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0105
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0108
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0100
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0111
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0103
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0104
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0107
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0100
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0107
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0102
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0103
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0101
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0114
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0103
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0108
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0099
Execution time:  15.186566591262817
DNN:
Mean Absolute Error: 0.0098
Root Mean Square Error: 0.0176
Mean Square Error: 0.0003

Train RMSE: 0.018
Train MSE: 0.000
Train MAE: 0.010
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_28&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_84 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_85 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_28 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_86 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0244
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.0218
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0139
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0146
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.0107
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.0093
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0098
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0095
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0092
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0096
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0097
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0121
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0133
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0113
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0124
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0096
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0133
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0091
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0084
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0127
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0080
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0089
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0093
Epoch 24/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0078
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0077
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0086
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0078
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0081
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0076
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0076
Epoch 31/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0071
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0071
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0068
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0069
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0065
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0066
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0063
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0062
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0063
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0068
Epoch 41/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0070
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0074
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0075
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0079
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0081
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0084
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0078
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0080
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0090
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0101
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0094
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0094
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0106
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0098
Epoch 55/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0103
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0100
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0097
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0098
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0093
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0097
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0100
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0096
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0100
Epoch 64/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0092
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0095
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0097
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0107
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0098
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0097
Epoch 70/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0094
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0095
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0104
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0095
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0103
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0095
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0099
Epoch 77/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0102
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0094
Epoch 79/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0095
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0097
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0098
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0090
Epoch 83/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0097
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0096
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0098
Epoch 86/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0095
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0097
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0098
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0092
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0096
Execution time:  19.297134399414062
DNN:
Mean Absolute Error: 0.0092
Root Mean Square Error: 0.0147
Mean Square Error: 0.0002

Train RMSE: 0.015
Train MSE: 0.000
Train MAE: 0.009
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_29&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_87 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_88 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_29 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_89 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 8ms/step - loss: 0.3054 - val_loss: 0.1414
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1549 - val_loss: 0.0785
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1389 - val_loss: 0.0396
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1230 - val_loss: 0.0123
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1131 - val_loss: 0.0140
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 0.0142
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.0144
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0844 - val_loss: 0.0144
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0146
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0631 - val_loss: 0.0144
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0140
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0446 - val_loss: 0.0143
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0145
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0146
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0158
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0159
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0155
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0161
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0161
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0163
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0164
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0167
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0168
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0170
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0170
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0169
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0165
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0161
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0160
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0159
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0159
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0158
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0157
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0156
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0155
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0153
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0152
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0153
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0152
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0151
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0148
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0146
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0144
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0143
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0142
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0141
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0142
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0135
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0141
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0136
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0138
Execution time:  6.577101230621338
DNN:
Mean Absolute Error: 0.0197
Root Mean Square Error: 0.0324
Mean Square Error: 0.0011

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_30&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_90 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_91 (Dense)             (None, 6, 16)             1408      
_________________________________________________________________
dropout_30 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_92 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.0366
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0333
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0319
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.0297
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.0293
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0276
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.0246
Epoch 8/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 0.0234
Epoch 9/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0347 - val_loss: 0.0231
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0216
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0212
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0209
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0199
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0192
Epoch 15/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0305 - val_loss: 0.0184
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0175
Epoch 17/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0297 - val_loss: 0.0172
Epoch 18/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0293 - val_loss: 0.0177
Epoch 19/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0292 - val_loss: 0.0169
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0163
Epoch 21/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0280 - val_loss: 0.0158
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0155
Epoch 23/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0149
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0145
Epoch 25/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0271 - val_loss: 0.0142
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0144
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0138
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0136
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0130
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0131
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0128
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0127
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0114
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0118
Epoch 35/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0254 - val_loss: 0.0121
Epoch 36/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0251 - val_loss: 0.0119
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0121
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0121
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0120
Epoch 40/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0237 - val_loss: 0.0122
Epoch 41/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0123
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0121
Epoch 43/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0122
Epoch 44/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0125
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0124
Epoch 46/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0209 - val_loss: 0.0130
Epoch 47/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0202 - val_loss: 0.0133
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0129
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0133
Epoch 50/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0138
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0151
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0150
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0145
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0141
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0124
Epoch 56/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0117
Epoch 57/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0105
Epoch 58/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0099
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0090
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0091
Epoch 61/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0081
Epoch 62/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0105
Epoch 63/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0097
Epoch 64/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0104
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0096
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0091
Epoch 67/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0095
Epoch 68/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0095
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0097
Epoch 70/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0089
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0089
Epoch 72/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0092
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0098
Epoch 74/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0092
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0092
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0100
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0093
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0092
Epoch 79/80
116/116 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0085
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0096
Execution time:  15.967062711715698
DNN:
Mean Absolute Error: 0.0095
Root Mean Square Error: 0.0194
Mean Square Error: 0.0004

Train RMSE: 0.019
Train MSE: 0.000
Train MAE: 0.010
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_31&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_93 (Dense)             (None, 6, 80)             160       
_________________________________________________________________
dense_94 (Dense)             (None, 6, 16)             1296      
_________________________________________________________________
dropout_31 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_95 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
144/144 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.0259
Epoch 2/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 0.0242
Epoch 3/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0247
Epoch 4/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 0.0245
Epoch 5/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 0.0221
Epoch 6/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0342 - val_loss: 0.0225
Epoch 7/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 0.0223
Epoch 8/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0334 - val_loss: 0.0220
Epoch 9/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0330 - val_loss: 0.0218
Epoch 10/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0326 - val_loss: 0.0197
Epoch 11/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0324 - val_loss: 0.0192
Epoch 12/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0318 - val_loss: 0.0205
Epoch 13/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0314 - val_loss: 0.0189
Epoch 14/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 0.0191
Epoch 15/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 0.0191
Epoch 16/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0300 - val_loss: 0.0185
Epoch 17/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0299 - val_loss: 0.0172
Epoch 18/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0296 - val_loss: 0.0169
Epoch 19/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0290 - val_loss: 0.0174
Epoch 20/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0288 - val_loss: 0.0160
Epoch 21/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0285 - val_loss: 0.0162
Epoch 22/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0279 - val_loss: 0.0169
Epoch 23/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0166
Epoch 24/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0273 - val_loss: 0.0157
Epoch 25/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0269 - val_loss: 0.0166
Epoch 26/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0265 - val_loss: 0.0159
Epoch 27/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0259 - val_loss: 0.0162
Epoch 28/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0255 - val_loss: 0.0160
Epoch 29/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0248 - val_loss: 0.0160
Epoch 30/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0155
Epoch 31/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 0.0163
Epoch 32/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0154
Epoch 33/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0164
Epoch 34/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0163
Epoch 35/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0163
Epoch 36/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0208 - val_loss: 0.0163
Epoch 37/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 0.0167
Epoch 38/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0163
Epoch 39/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0194 - val_loss: 0.0162
Epoch 40/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0174
Epoch 41/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0182
Epoch 42/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0174 - val_loss: 0.0173
Epoch 43/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0167 - val_loss: 0.0129
Epoch 44/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0126
Epoch 45/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0103
Epoch 46/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0094
Epoch 47/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0090
Epoch 48/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0087
Epoch 49/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0093
Epoch 50/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0072
Epoch 51/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0074
Epoch 52/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0067
Epoch 53/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0079
Epoch 54/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0068
Epoch 55/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0079
Epoch 56/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0063
Epoch 57/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0069
Epoch 58/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0081
Epoch 59/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0076
Epoch 60/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0073
Epoch 61/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0063
Epoch 62/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0078
Epoch 63/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0076
Epoch 64/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0090
Epoch 65/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0071
Epoch 66/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0080
Epoch 67/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0076
Epoch 68/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0075
Epoch 69/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0077
Epoch 70/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0081
Epoch 71/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0074
Epoch 72/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0076
Epoch 73/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0073
Epoch 74/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0085
Epoch 75/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0073
Epoch 76/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0081
Epoch 77/90
144/144 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0075
Epoch 78/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0074
Epoch 79/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0079
Epoch 80/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0065
Epoch 81/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0076
Epoch 82/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0074
Epoch 83/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0073
Epoch 84/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0079
Epoch 85/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0075
Epoch 86/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0086
Epoch 87/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0078
Epoch 88/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0081
Epoch 89/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0081
Epoch 90/90
144/144 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0074
Execution time:  19.883842706680298
DNN:
Mean Absolute Error: 0.0076
Root Mean Square Error: 0.0236
Mean Square Error: 0.0006

Train RMSE: 0.024
Train MSE: 0.001
Train MAE: 0.008
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_32&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_96 (Dense)             (None, 6, 12)             24        
_________________________________________________________________
dense_97 (Dense)             (None, 6, 16)             208       
_________________________________________________________________
dropout_32 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_98 (Dense)             (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 0.0422
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.0377
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0363
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0403 - val_loss: 0.0367
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0364
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0352
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.0337
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.0332
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.0327
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0318
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.0313
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0312
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0303
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0299
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0287
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 0.0282
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0276
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0269
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0267
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0261
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0253
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0250
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0250
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0242
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0237
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0234
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0230
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0226
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0224
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0222
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0220
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0212
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0209
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0205
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0199
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0195
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0188
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0183
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0178
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0178
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0175
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0168
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0166
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0162
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0161
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0155
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0152
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0148
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0144
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0141
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0139
Execution time:  6.661231994628906
DNN:
Mean Absolute Error: 0.0229
Root Mean Square Error: 0.0385
Mean Square Error: 0.0015

Train RMSE: 0.039
Train MSE: 0.001
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  1h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_33&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_99 (Dense)             (None, 6, 87)             174       
_________________________________________________________________
dense_100 (Dense)            (None, 6, 16)             1408      
_________________________________________________________________
dropout_33 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_101 (Dense)            (None, 6, 1)              17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.0286
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.0276
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0273
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.0262
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0394 - val_loss: 0.0255
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.0249
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0241
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 0.0235
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0230
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.0218
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0218
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0214
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0213
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0207
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0200
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0191
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0187
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0186
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0182
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0177
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0172
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0169
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0164
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0161
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0156
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0152
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0145
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0140
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0138
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0131
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0129
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0128
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0126
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0124
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0123
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0121
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0119
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0116
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0115
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0113
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0110
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0106
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0104
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0100
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0099
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0095
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0095
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0094
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0096
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0096
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0095
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0096
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0096
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0094
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0095
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0095
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0094
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0094
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0092
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0092
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0093
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0089
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0089
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0088
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0089
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0085
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0084
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0087
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0087
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0085
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0085
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0086
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0084
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0083
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0083
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0085
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0084
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0083
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0084
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0083
Execution time:  15.485010385513306
DNN:
Mean Absolute Error: 0.0070
Root Mean Square Error: 0.0135
Mean Square Error: 0.0002

Train RMSE: 0.013
Train MSE: 0.000
Train MAE: 0.007
###########################

MODEL:  DNN
sequence:  1h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_34&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_102 (Dense)            (None, 6, 80)             160       
_________________________________________________________________
dense_103 (Dense)            (None, 6, 16)             1296      
_________________________________________________________________
dropout_34 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_104 (Dense)            (None, 6, 1)              17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.0204
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.0202
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.0200
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 0.0195
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0195
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0194
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0190
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0190
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.0189
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0187
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 0.0185
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0185
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.0185
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0183
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0181
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0179
Epoch 17/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0324 - val_loss: 0.0181
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0178
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0178
Epoch 20/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0314 - val_loss: 0.0177
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0175
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.0175
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0171
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0166
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0161
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0161
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0157
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0157
Epoch 29/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0288 - val_loss: 0.0155
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0153
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0149
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0147
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0142
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0139
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0138
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0134
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0133
Epoch 38/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0131
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0129
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0127
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0124
Epoch 43/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0122
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0119
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0115
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0112
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0108
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0105
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0101
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0097
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0093
Epoch 52/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0083
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0083
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0081
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0081
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0079
Epoch 57/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0095
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0099
Epoch 59/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0098
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0087
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0095
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0094
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0083
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0097
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0097
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0093
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0084
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0096
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0088
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0090
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0096
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0094
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0093
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0084
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0091
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0092
Epoch 77/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0090
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0090
Epoch 79/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0094
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0089
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0086
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0087
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0084
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0091
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0089
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0084
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0089
Epoch 88/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0088
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0082
Epoch 90/90
128/128 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0087
Execution time:  19.100332498550415
DNN:
Mean Absolute Error: 0.0055
Root Mean Square Error: 0.0134
Mean Square Error: 0.0002

Train RMSE: 0.013
Train MSE: 0.000
Train MAE: 0.006
###########################

MODEL:  DNN
sequence:  1h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_35&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_105 (Dense)            (None, 6, 12)             24        
_________________________________________________________________
dense_106 (Dense)            (None, 6, 16)             208       
_________________________________________________________________
dropout_35 (Dropout)         (None, 6, 16)             0         
_________________________________________________________________
dense_107 (Dense)            (None, 6, 1)              17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 8ms/step - loss: 0.0942 - val_loss: 0.0898
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0632
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0494
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0444 - val_loss: 0.0451
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0430
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0411
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0390 - val_loss: 0.0395
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0380
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0366
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 0.0350
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0337
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 0.0325
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0314
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0303
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0294
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0285
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0275
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0268
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0257
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0254
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0249
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0241
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0238
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0228
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0228
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0222
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0218
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0213
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0211
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0207
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0205
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0201
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0198
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0197
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0192
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0188
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0186
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0184
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0182
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0180
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0178
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0176
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0175
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0172
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0169
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0168
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0166
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0164
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0163
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0161
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0160
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0159
Execution time:  6.896315574645996
DNN:
Mean Absolute Error: 0.0209
Root Mean Square Error: 0.0341
Mean Square Error: 0.0012

Train RMSE: 0.034
Train MSE: 0.001
Train MAE: 0.021
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_36&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_108 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_109 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_36 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_110 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 4ms/step - loss: 0.1344 - val_loss: 0.0059
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0786 - val_loss: 0.0118
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0665 - val_loss: 0.0056
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0069
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.0130
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0135
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0129
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0229
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0121
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0110
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0100
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0107
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0109
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0108
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0109
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0112
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0106
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0111
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0105
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0108
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0110
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0107
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0107
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0105
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0112
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0106
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0109
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0109
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0135
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0105
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0122
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0107
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0104
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0122
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0108
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0123
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0118
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0105
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0122
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0114
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0117
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0114
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0112
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0118
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0115
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0122
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0120
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0116
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0121
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0115
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0123
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0120
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0116
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0110
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0123
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0124
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0119
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0133
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0116
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0116
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0115
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0122
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0115
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0121
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0113
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0121
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0115
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0125
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0116
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0120
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0115
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0122
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0133
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0116
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0117
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0118
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0125
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0122
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0125
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0120
Execution time:  21.342922687530518
DNN:
Mean Absolute Error: 0.0145
Root Mean Square Error: 0.0233
Mean Square Error: 0.0005

Train RMSE: 0.023
Train MSE: 0.001
Train MAE: 0.014
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_37&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_111 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_112 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_37 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_113 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 0s 3ms/step - loss: 0.1376 - val_loss: 0.0187
Epoch 2/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0516 - val_loss: 0.0201
Epoch 3/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0490 - val_loss: 0.0175
Epoch 4/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0464 - val_loss: 0.0124
Epoch 5/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0436 - val_loss: 0.0087
Epoch 6/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.0054
Epoch 7/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.0056
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0068
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0106
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0114
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0130
Epoch 12/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0166
Epoch 13/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0123
Epoch 14/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0190
Epoch 15/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0258 - val_loss: 0.0210
Epoch 16/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0253 - val_loss: 0.0210
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0205
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0205
Epoch 19/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0237 - val_loss: 0.0202
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0200
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0185
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0194
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0188
Epoch 24/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0156
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0124
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0163
Epoch 27/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0173
Epoch 28/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0188
Epoch 29/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0167
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0165
Epoch 31/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0129
Epoch 32/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0133
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0142
Epoch 34/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0137
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0133
Epoch 36/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0148
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0128
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0146
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0155
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0138
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0150
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0132
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0162
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0141
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0141
Epoch 46/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0146
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0130
Epoch 48/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0158
Epoch 49/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0132
Epoch 50/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0132
Epoch 51/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0130
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0158
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0150
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0142
Epoch 55/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0150
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0143
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0149
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0132
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0146
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0151
Epoch 61/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0151
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0146
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0132
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0142
Epoch 65/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0145
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0156
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0145
Epoch 68/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0161
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0163
Epoch 70/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0146
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0154
Epoch 72/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0156
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0164
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0164
Epoch 75/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0165
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0148
Epoch 77/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0139
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0151
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0157
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0138
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0146
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0169
Epoch 83/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0147
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0157
Epoch 85/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0159
Epoch 86/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0147
Epoch 87/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0140
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0140
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0159
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0156
Execution time:  21.208261013031006
DNN:
Mean Absolute Error: 0.0132
Root Mean Square Error: 0.0230
Mean Square Error: 0.0005

Train RMSE: 0.023
Train MSE: 0.001
Train MAE: 0.013
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_38&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_114 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_115 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_38 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_116 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.3404 - val_loss: 0.1351
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.0072
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0927 - val_loss: 0.0068
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.0071
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0744 - val_loss: 0.0093
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0680 - val_loss: 0.0071
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0612 - val_loss: 0.0068
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.0064
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0063
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.0069
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0073
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 0.0085
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0093
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0093
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0098
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0099
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0105
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0099
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0106
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0094
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0107
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0098
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0111
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0099
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0110
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0121
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0100
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0101
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0100
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0100
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0099
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0102
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0102
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0101
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0101
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0100
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0101
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0099
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0102
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0098
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0100
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0097
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0104
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0097
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0101
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0100
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0104
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0099
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0099
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0110
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0100
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0105
Execution time:  7.914307594299316
DNN:
Mean Absolute Error: 0.0172
Root Mean Square Error: 0.0261
Mean Square Error: 0.0007

Train RMSE: 0.026
Train MSE: 0.001
Train MAE: 0.017
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_39&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_117 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_118 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_39 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_119 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.1393 - val_loss: 0.0494
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0861 - val_loss: 0.0249
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0684 - val_loss: 0.0136
Epoch 4/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.0130
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0466 - val_loss: 0.0122
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.0129
Epoch 7/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0144
Epoch 8/80
103/103 [==============================] - ETA: 0s - loss: 0.031 - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0148
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0156
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0160
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0157
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0170
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0156
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0171
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0166
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0165
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0164
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0163
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0162
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0160
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0160
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0159
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0159
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0157
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0157
Epoch 26/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0162
Epoch 27/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0158
Epoch 28/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0155
Epoch 29/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0158
Epoch 30/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0153
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0155
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0154
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0154
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0152
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0155
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0150
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0156
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0149
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0151
Epoch 40/80
103/103 [==============================] - ETA: 0s - loss: 0.028 - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0149
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0151
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0148
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0155
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0148
Epoch 45/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0144
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0146
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0144
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0144
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0144
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0141
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0139
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0138
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0135
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0137
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0134
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0129
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0135
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0127
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0135
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0130
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0132
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0132
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0132
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0131
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0130
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0132
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0127
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0134
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0128
Epoch 70/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0131
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0132
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0133
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0128
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0133
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0130
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0130
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0133
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0129
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0132
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0125
Execution time:  20.461792469024658
DNN:
Mean Absolute Error: 0.0169
Root Mean Square Error: 0.0274
Mean Square Error: 0.0008

Train RMSE: 0.027
Train MSE: 0.001
Train MAE: 0.017
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_40&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_120 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_121 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_40 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_122 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 4ms/step - loss: 0.1082 - val_loss: 0.0327
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0511 - val_loss: 0.0342
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.0342
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.0225
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0186
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0395 - val_loss: 0.0173
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0160
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0121
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0140
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0128
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0116
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0116
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0117
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0119
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0121
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0121
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0129
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0134
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0131
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0133
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0134
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0129
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0137
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0135
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0128
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0144
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0141
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0141
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0144
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0143
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0138
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0144
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0134
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0136
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0136
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0133
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0133
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0133
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0130
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0146
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0135
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0134
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0131
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0130
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0131
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0142
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0132
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0135
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0124
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0131
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0128
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0129
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0131
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0129
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0130
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0128
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0128
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0141
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0132
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0126
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0131
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0126
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0130
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0125
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0126
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0127
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0128
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0130
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0126
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0129
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0129
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0133
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0129
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0129
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0129
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0129
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0129
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0120
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0129
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0127
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0127
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0129
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0128
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0128
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0125
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0128
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0130
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0128
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0128
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0127
Execution time:  25.262813091278076
DNN:
Mean Absolute Error: 0.0149
Root Mean Square Error: 0.0276
Mean Square Error: 0.0008

Train RMSE: 0.028
Train MSE: 0.001
Train MAE: 0.015
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_41&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_123 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_124 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_41 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_125 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 9ms/step - loss: 0.3221 - val_loss: 0.0279
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1179 - val_loss: 0.0173
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.0273
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0910 - val_loss: 0.0356
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0835 - val_loss: 0.0313
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0755 - val_loss: 0.0220
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0641 - val_loss: 0.0139
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.0138
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0500 - val_loss: 0.0137
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0448 - val_loss: 0.0132
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0143
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0133
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0131
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0131
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0131
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0135
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0143
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0146
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0141
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0140
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0135
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0138
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0135
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0138
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0137
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0136
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0139
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0136
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0135
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0128
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0125
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0129
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0129
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0124
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0128
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0128
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0128
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0122
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0127
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0124
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0126
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0126
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0126
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0125
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0126
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0126
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0124
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0127
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0120
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0127
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0125
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0125
Execution time:  7.537402868270874
DNN:
Mean Absolute Error: 0.0161
Root Mean Square Error: 0.0249
Mean Square Error: 0.0006

Train RMSE: 0.025
Train MSE: 0.001
Train MAE: 0.016
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_42&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_126 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_127 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_42 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_128 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 4ms/step - loss: 0.0448 - val_loss: 0.0260
Epoch 2/80
116/116 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.0261
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.0247
Epoch 4/80
116/116 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0239
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0234
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.0225
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 0.0224
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0213
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0204
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0199
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0195
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0185
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0180
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0171
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0164
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0160
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0156
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0147
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0149
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0146
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0141
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0137
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0138
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0139
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0136
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0137
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0135
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0134
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0136
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0136
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0138
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0138
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0139
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0139
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0140
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0138
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0137
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0130
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0130
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0131
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0126
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0129
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0129
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0129
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0130
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0131
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0133
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0137
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0142
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0135
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0136
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0141
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0143
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0141
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0148
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0141
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0153
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0141
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0153
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0157
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0136
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0180
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0146
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0166
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0154
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0165
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0156
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0159
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0147
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0153
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0154
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0138
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0148
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0152
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0150
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0144
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0157
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0159
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0143
Execution time:  21.58287000656128
DNN:
Mean Absolute Error: 0.0136
Root Mean Square Error: 0.0236
Mean Square Error: 0.0006

Train RMSE: 0.024
Train MSE: 0.001
Train MAE: 0.014
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_43&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_129 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_130 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_43 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_131 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.0277
Epoch 2/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 0.0250
Epoch 3/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.0242
Epoch 4/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0218
Epoch 5/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0226
Epoch 6/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0202
Epoch 7/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 0.0191
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0191
Epoch 9/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0321 - val_loss: 0.0183
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0276
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0174
Epoch 12/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0170
Epoch 13/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0168
Epoch 14/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0155
Epoch 15/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0159
Epoch 16/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0295 - val_loss: 0.0155
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0161
Epoch 18/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0288 - val_loss: 0.0150
Epoch 19/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0149
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0147
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0232
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0138
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0145
Epoch 24/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0263 - val_loss: 0.0145
Epoch 25/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0258 - val_loss: 0.0147
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0147
Epoch 27/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0136
Epoch 28/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0141
Epoch 29/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0141
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0136
Epoch 31/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0135
Epoch 32/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0139
Epoch 33/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0146
Epoch 34/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0148
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0155
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0144
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0138
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0139
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0126
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0109
Epoch 41/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0101
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0102
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0101
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0098
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0100
Epoch 46/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0095
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0092
Epoch 48/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0101
Epoch 49/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0098
Epoch 50/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0107
Epoch 51/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0103
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0109
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0104
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0108
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0113
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0108
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0109
Epoch 58/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0107
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0118
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0118
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0107
Epoch 62/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0122
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0124
Epoch 64/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0132
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0109
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0106
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0105
Epoch 68/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0115
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0112
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0116
Epoch 71/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0108
Epoch 72/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0111
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0109
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0104
Epoch 75/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0106
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0121
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0111
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0111
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0105
Epoch 80/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0111
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0109
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0110
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0105
Epoch 84/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0109
Epoch 85/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0110
Epoch 86/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0108
Epoch 87/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0109
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0109
Epoch 89/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0110
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0111
Execution time:  21.16887092590332
DNN:
Mean Absolute Error: 0.0079
Root Mean Square Error: 0.0168
Mean Square Error: 0.0003

Train RMSE: 0.017
Train MSE: 0.000
Train MAE: 0.008
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_44&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_132 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_133 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_44 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_134 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.0548 - val_loss: 0.0476
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.0439
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0441 - val_loss: 0.0421
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0370
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.0327
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0321
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0315
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.0307
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0266
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0260
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0263
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0253
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0244
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0238
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0228
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0221
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0218
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0207
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0202
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0196
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0191
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0185
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0180
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0173
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0169
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0162
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0157
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0153
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0144
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0138
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0136
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0132
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0129
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0127
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0127
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0124
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0127
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0123
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0124
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0121
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0121
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0124
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0121
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0125
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0125
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0123
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0124
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0123
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0120
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0123
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0121
Execution time:  7.899348974227905
DNN:
Mean Absolute Error: 0.0188
Root Mean Square Error: 0.0325
Mean Square Error: 0.0011

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_45&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_135 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_136 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_45 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_137 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.0475 - val_loss: 0.0247
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0437 - val_loss: 0.0241
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0239
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.0239
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0228
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0225
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0220
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0211
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0171
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0177
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0197
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0195
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0186
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0185
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0181
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0175
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0173
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0168
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0161
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0159
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0155
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0152
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0149
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0145
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0144
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0142
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0141
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0141
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0141
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0140
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0142
Epoch 32/80
103/103 [==============================] - ETA: 0s - loss: 0.027 - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0142
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0142
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0142
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0142
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0142
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0144
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0144
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0143
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0143
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0143
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0143
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0143
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0142
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0138
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0136
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0137
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0137
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0137
Epoch 50/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0134
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0136
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0137
Epoch 53/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0136
Epoch 54/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0132
Epoch 55/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0137
Epoch 56/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0141
Epoch 57/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0141
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0140
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0140
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0142
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0141
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0139
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0140
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0141
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0137
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0140
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0142
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0140
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0140
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0137
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0140
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0139
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0140
Epoch 74/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0134
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0136
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0135
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0134
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0136
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0128
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0143
Execution time:  20.633455991744995
DNN:
Mean Absolute Error: 0.0129
Root Mean Square Error: 0.0278
Mean Square Error: 0.0008

Train RMSE: 0.028
Train MSE: 0.001
Train MAE: 0.013
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_46&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_138 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_139 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_46 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_140 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 4ms/step - loss: 0.0384 - val_loss: 0.0183
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0388 - val_loss: 0.0192
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.0180
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0185
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0175
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0173
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0170
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0168
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0166
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0168
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0169
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0167
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0166
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0165
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0164
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0166
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0164
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0164
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0162
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0161
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0159
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0156
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0153
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0149
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0148
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0148
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0145
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0143
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0142
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0139
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0138
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0137
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0134
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0131
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0126
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0122
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0121
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0118
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0119
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0122
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0119
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0121
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0121
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0119
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0108
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0106
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0112
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0115
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0104
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0106
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0108
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0104
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0103
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0110
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0109
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0104
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0117
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0106
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0109
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0109
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0106
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0108
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0106
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0110
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0109
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0106
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0104
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0111
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0106
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0104
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0111
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0108
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0105
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0109
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0106
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0103
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0107
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0106
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0104
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0111
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0104
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0108
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0102
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0104
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0106
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0105
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0108
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0105
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0110
Execution time:  25.066622257232666
DNN:
Mean Absolute Error: 0.0082
Root Mean Square Error: 0.0238
Mean Square Error: 0.0006

Train RMSE: 0.024
Train MSE: 0.001
Train MAE: 0.008
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_47&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_141 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_142 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_47 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_143 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.0582 - val_loss: 0.0473
Epoch 2/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.0404
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.0380
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0363
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.0348
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0343
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0326
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.0317
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.0306
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0298
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0290
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0283
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0277
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0272
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0269
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0264
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0260
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0258
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0252
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0252
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0245
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0240
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0238
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0233
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0229
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0224
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0222
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0218
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0215
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0213
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0211
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0207
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0205
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0203
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0200
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0197
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0194
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0191
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0189
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0186
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0184
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0182
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0180
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0178
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0176
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0174
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0172
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0170
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0170
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0168
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0166
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0165
Execution time:  7.578886032104492
DNN:
Mean Absolute Error: 0.0232
Root Mean Square Error: 0.0398
Mean Square Error: 0.0016

Train RMSE: 0.040
Train MSE: 0.002
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_48&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_144 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_145 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_48 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_146 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 4ms/step - loss: 0.4326 - val_loss: 0.3923
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4296 - val_loss: 0.3894
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4264 - val_loss: 0.3862
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4229 - val_loss: 0.3828
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4192 - val_loss: 0.3791
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4153 - val_loss: 0.3753
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4112 - val_loss: 0.3712
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4069 - val_loss: 0.3670
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.4027 - val_loss: 0.3627
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3582
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3931 - val_loss: 0.3535
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3883 - val_loss: 0.3488
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3830 - val_loss: 0.3441
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3783 - val_loss: 0.3395
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.3349
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3686 - val_loss: 0.3302
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.3254
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3205
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.3160
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 0.3114
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3441 - val_loss: 0.3077
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3402 - val_loss: 0.3046
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3365 - val_loss: 0.3014
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3333 - val_loss: 0.2982
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3296 - val_loss: 0.2949
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3263 - val_loss: 0.2916
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3224 - val_loss: 0.2881
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3190 - val_loss: 0.2846
Epoch 29/80
116/116 [==============================] - 0s 3ms/step - loss: 0.3154 - val_loss: 0.2809
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3115 - val_loss: 0.2772
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3078 - val_loss: 0.2735
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.3038 - val_loss: 0.2696
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2995 - val_loss: 0.2656
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2952 - val_loss: 0.2615
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2910 - val_loss: 0.2573
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2867 - val_loss: 0.2530
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2819 - val_loss: 0.2486
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2772 - val_loss: 0.2441
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2732 - val_loss: 0.2394
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2678 - val_loss: 0.2347
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2630 - val_loss: 0.2298
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2582 - val_loss: 0.2248
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2531 - val_loss: 0.2197
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2484 - val_loss: 0.2145
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 0.2091
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2366 - val_loss: 0.2036
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2306 - val_loss: 0.1980
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2250 - val_loss: 0.1922
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 0.1863
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2135 - val_loss: 0.1802
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2069 - val_loss: 0.1740
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.2007 - val_loss: 0.1677
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1942 - val_loss: 0.1612
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1876 - val_loss: 0.1546
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1822 - val_loss: 0.1478
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1748 - val_loss: 0.1409
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1676 - val_loss: 0.1340
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1610 - val_loss: 0.1273
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1565 - val_loss: 0.1222
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1536 - val_loss: 0.1179
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.1143
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1495 - val_loss: 0.1110
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1474 - val_loss: 0.1082
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1451 - val_loss: 0.1056
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1451 - val_loss: 0.1032
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1431 - val_loss: 0.1010
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1426 - val_loss: 0.0989
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1421 - val_loss: 0.0970
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.0951
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1400 - val_loss: 0.0933
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1397 - val_loss: 0.0916
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1397 - val_loss: 0.0902
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1380 - val_loss: 0.0890
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1374 - val_loss: 0.0879
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1385 - val_loss: 0.0868
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1374 - val_loss: 0.0857
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1365 - val_loss: 0.0847
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1365 - val_loss: 0.0837
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1368 - val_loss: 0.0828
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1356 - val_loss: 0.0819
Execution time:  21.224387168884277
DNN:
Mean Absolute Error: 0.1045
Root Mean Square Error: 0.1084
Mean Square Error: 0.0117

Train RMSE: 0.108
Train MSE: 0.012
Train MAE: 0.105
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_49&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_147 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_148 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_49 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_149 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3116 - val_loss: 0.2811
Epoch 2/90
143/143 [==============================] - 0s 2ms/step - loss: 0.3065 - val_loss: 0.2756
Epoch 3/90
143/143 [==============================] - 0s 2ms/step - loss: 0.3005 - val_loss: 0.2696
Epoch 4/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2939 - val_loss: 0.2631
Epoch 5/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2868 - val_loss: 0.2560
Epoch 6/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2791 - val_loss: 0.2484
Epoch 7/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2709 - val_loss: 0.2401
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2621 - val_loss: 0.2317
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 0.2233
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2444 - val_loss: 0.2146
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2353 - val_loss: 0.2056
Epoch 12/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2256 - val_loss: 0.1963
Epoch 13/90
143/143 [==============================] - 0s 1ms/step - loss: 0.2158 - val_loss: 0.1868
Epoch 14/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2055 - val_loss: 0.1770
Epoch 15/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1957 - val_loss: 0.1670
Epoch 16/90
143/143 [==============================] - 0s 1ms/step - loss: 0.1851 - val_loss: 0.1567
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1740 - val_loss: 0.1461
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1634 - val_loss: 0.1359
Epoch 19/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1529 - val_loss: 0.1261
Epoch 20/90
143/143 [==============================] - 0s 1ms/step - loss: 0.1428 - val_loss: 0.1167
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1326 - val_loss: 0.1071
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1226 - val_loss: 0.0973
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1126 - val_loss: 0.0873
Epoch 24/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1026 - val_loss: 0.0772
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0921 - val_loss: 0.0670
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0818 - val_loss: 0.0568
Epoch 27/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0713 - val_loss: 0.0466
Epoch 28/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0618 - val_loss: 0.0369
Epoch 29/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0310
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0525 - val_loss: 0.0272
Epoch 31/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0517 - val_loss: 0.0246
Epoch 32/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0228
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0214
Epoch 34/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0499 - val_loss: 0.0203
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.0193
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0183
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0492 - val_loss: 0.0174
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.0167
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0160
Epoch 40/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0484 - val_loss: 0.0154
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0148
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0143
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0138
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0133
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0128
Epoch 46/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0484 - val_loss: 0.0124
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0120
Epoch 48/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0481 - val_loss: 0.0117
Epoch 49/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0484 - val_loss: 0.0113
Epoch 50/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0110
Epoch 51/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0480 - val_loss: 0.0107
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0479 - val_loss: 0.0104
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0481 - val_loss: 0.0102
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0481 - val_loss: 0.0099
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0479 - val_loss: 0.0097
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0094
Epoch 57/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0479 - val_loss: 0.0092
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0090
Epoch 59/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0478 - val_loss: 0.0088
Epoch 60/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0478 - val_loss: 0.0086
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0085
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0083
Epoch 63/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0477 - val_loss: 0.0082
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0081
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0079
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0078
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0078
Epoch 68/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0484 - val_loss: 0.0077
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0076
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0075
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0074
Epoch 72/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.0074
Epoch 73/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0477 - val_loss: 0.0073
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0072
Epoch 75/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0072
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0071
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0071
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0071
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.0070
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0070
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0069
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.0069
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0069
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0069
Epoch 85/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0472 - val_loss: 0.0069
Epoch 86/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0474 - val_loss: 0.0068
Epoch 87/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0068
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0068
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0067
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0067
Execution time:  21.23253107070923
DNN:
Mean Absolute Error: 0.0203
Root Mean Square Error: 0.0279
Mean Square Error: 0.0008

Train RMSE: 0.028
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_50&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_150 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_151 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_50 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_152 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.4140 - val_loss: 0.3765
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4134 - val_loss: 0.3760
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4127 - val_loss: 0.3754
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4122 - val_loss: 0.3748
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4114 - val_loss: 0.3742
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4108 - val_loss: 0.3736
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4102 - val_loss: 0.3729
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4095 - val_loss: 0.3722
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4087 - val_loss: 0.3715
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4082 - val_loss: 0.3708
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4072 - val_loss: 0.3700
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4066 - val_loss: 0.3693
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4058 - val_loss: 0.3685
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4050 - val_loss: 0.3676
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4042 - val_loss: 0.3668
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4032 - val_loss: 0.3660
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4022 - val_loss: 0.3651
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4017 - val_loss: 0.3642
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.4007 - val_loss: 0.3633
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3994 - val_loss: 0.3623
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3983 - val_loss: 0.3614
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3974 - val_loss: 0.3604
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3964 - val_loss: 0.3595
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.3585
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3946 - val_loss: 0.3574
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3933 - val_loss: 0.3564
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.3554
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3913 - val_loss: 0.3543
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3899 - val_loss: 0.3532
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3895 - val_loss: 0.3521
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3880 - val_loss: 0.3510
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3868 - val_loss: 0.3499
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3856 - val_loss: 0.3488
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3847 - val_loss: 0.3476
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3834 - val_loss: 0.3465
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3453
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3810 - val_loss: 0.3441
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.3429
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3786 - val_loss: 0.3417
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3773 - val_loss: 0.3404
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.3392
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.3379
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3735 - val_loss: 0.3366
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3353
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3340
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3327
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3679 - val_loss: 0.3314
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3667 - val_loss: 0.3300
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3654 - val_loss: 0.3287
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3638 - val_loss: 0.3273
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3625 - val_loss: 0.3260
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3246
Execution time:  7.709161043167114
DNN:
Mean Absolute Error: 0.3567
Root Mean Square Error: 0.3590
Mean Square Error: 0.1289

Train RMSE: 0.359
Train MSE: 0.129
Train MAE: 0.357
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_51&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_153 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_154 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_51 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_155 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.4200 - val_loss: 0.3957
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.4175 - val_loss: 0.3930
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.4143 - val_loss: 0.3900
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.4113 - val_loss: 0.3868
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.4080 - val_loss: 0.3834
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.4045 - val_loss: 0.3799
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.4006 - val_loss: 0.3761
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3964 - val_loss: 0.3723
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3929 - val_loss: 0.3688
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3894 - val_loss: 0.3651
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3854 - val_loss: 0.3613
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3814 - val_loss: 0.3574
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3773 - val_loss: 0.3533
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3730 - val_loss: 0.3491
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3687 - val_loss: 0.3448
Epoch 16/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3643 - val_loss: 0.3403
Epoch 17/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3598 - val_loss: 0.3358
Epoch 18/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3550 - val_loss: 0.3311
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3498 - val_loss: 0.3262
Epoch 20/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3451 - val_loss: 0.3213
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3399 - val_loss: 0.3162
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3347 - val_loss: 0.3114
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3303 - val_loss: 0.3071
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3255 - val_loss: 0.3029
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3213 - val_loss: 0.2986
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3169 - val_loss: 0.2943
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.2899
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3077 - val_loss: 0.2855
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.3031 - val_loss: 0.2810
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2988 - val_loss: 0.2765
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 0.2719
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2891 - val_loss: 0.2672
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2846 - val_loss: 0.2625
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2794 - val_loss: 0.2577
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 0.2528
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2693 - val_loss: 0.2479
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2651 - val_loss: 0.2429
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2591 - val_loss: 0.2378
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2541 - val_loss: 0.2327
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2488 - val_loss: 0.2276
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2439 - val_loss: 0.2223
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2380 - val_loss: 0.2171
Epoch 43/80
103/103 [==============================] - ETA: 0s - loss: 0.238 - 0s 2ms/step - loss: 0.2336 - val_loss: 0.2123
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2280 - val_loss: 0.2077
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2236 - val_loss: 0.2031
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2184 - val_loss: 0.1983
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2135 - val_loss: 0.1935
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2086 - val_loss: 0.1887
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.2033 - val_loss: 0.1837
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 0.1787
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1929 - val_loss: 0.1735
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1885 - val_loss: 0.1683
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1832 - val_loss: 0.1630
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1777 - val_loss: 0.1576
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1725 - val_loss: 0.1521
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1672 - val_loss: 0.1465
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1619 - val_loss: 0.1410
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1568 - val_loss: 0.1361
Epoch 59/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1528 - val_loss: 0.1314
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.1269
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1465 - val_loss: 0.1226
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1433 - val_loss: 0.1184
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1399 - val_loss: 0.1143
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1371 - val_loss: 0.1102
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1349 - val_loss: 0.1062
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1326 - val_loss: 0.1022
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.0982
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1272 - val_loss: 0.0942
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1248 - val_loss: 0.0902
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.0863
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.0827
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1183 - val_loss: 0.0792
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.0759
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1147 - val_loss: 0.0728
Epoch 75/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 0.0701
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 0.0678
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1117 - val_loss: 0.0657
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 0.0637
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 0.0623
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 0.0610
Execution time:  20.530303955078125
DNN:
Mean Absolute Error: 0.0722
Root Mean Square Error: 0.0768
Mean Square Error: 0.0059

Train RMSE: 0.077
Train MSE: 0.006
Train MAE: 0.072
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_52&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_156 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_157 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_52 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_158 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.3752 - val_loss: 0.3535
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3728 - val_loss: 0.3509
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3701 - val_loss: 0.3483
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3673 - val_loss: 0.3455
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3644 - val_loss: 0.3426
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3394
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3579 - val_loss: 0.3361
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.3326
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3290
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3471 - val_loss: 0.3253
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3432 - val_loss: 0.3214
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3390 - val_loss: 0.3173
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3351 - val_loss: 0.3138
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3314 - val_loss: 0.3103
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3277 - val_loss: 0.3067
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3240 - val_loss: 0.3030
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3203 - val_loss: 0.2993
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3164 - val_loss: 0.2956
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3126 - val_loss: 0.2918
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3087 - val_loss: 0.2879
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3044 - val_loss: 0.2839
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.3004 - val_loss: 0.2799
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2964 - val_loss: 0.2757
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2923 - val_loss: 0.2715
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2876 - val_loss: 0.2672
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2831 - val_loss: 0.2628
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2785 - val_loss: 0.2583
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2741 - val_loss: 0.2537
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2693 - val_loss: 0.2490
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.2442
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2596 - val_loss: 0.2394
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2545 - val_loss: 0.2344
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2493 - val_loss: 0.2293
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2442 - val_loss: 0.2241
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2387 - val_loss: 0.2188
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2333 - val_loss: 0.2134
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2279 - val_loss: 0.2080
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2222 - val_loss: 0.2024
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2164 - val_loss: 0.1968
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2108 - val_loss: 0.1911
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.2046 - val_loss: 0.1853
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1986 - val_loss: 0.1794
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1926 - val_loss: 0.1734
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 0.1673
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1805 - val_loss: 0.1611
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1744 - val_loss: 0.1548
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1671 - val_loss: 0.1483
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.1417
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1544 - val_loss: 0.1350
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1476 - val_loss: 0.1282
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1405 - val_loss: 0.1213
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.1143
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1266 - val_loss: 0.1072
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1191 - val_loss: 0.0999
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.0925
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1046 - val_loss: 0.0850
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0969 - val_loss: 0.0775
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0891 - val_loss: 0.0699
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0818 - val_loss: 0.0622
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0742 - val_loss: 0.0545
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0671 - val_loss: 0.0470
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0603 - val_loss: 0.0406
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0573 - val_loss: 0.0378
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0355
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0564 - val_loss: 0.0340
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0563 - val_loss: 0.0328
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0319
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0311
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0305
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0553 - val_loss: 0.0301
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.0298
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0549 - val_loss: 0.0296
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0545 - val_loss: 0.0293
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0291
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0553 - val_loss: 0.0288
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0286
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0284
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0282
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0280
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0279
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0549 - val_loss: 0.0278
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0276
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0549 - val_loss: 0.0275
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.0274
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0273
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0272
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0271
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0270
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0269
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0268
Execution time:  25.251575708389282
DNN:
Mean Absolute Error: 0.0371
Root Mean Square Error: 0.0441
Mean Square Error: 0.0019

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.037
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_53&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_159 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_160 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_53 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_161 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.4560 - val_loss: 0.4313
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4559 - val_loss: 0.4304
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4540 - val_loss: 0.4295
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4523 - val_loss: 0.4285
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4528 - val_loss: 0.4275
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4520 - val_loss: 0.4265
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4488 - val_loss: 0.4254
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4475 - val_loss: 0.4243
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4484 - val_loss: 0.4231
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4464 - val_loss: 0.4219
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4454 - val_loss: 0.4207
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4436 - val_loss: 0.4194
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4429 - val_loss: 0.4181
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4409 - val_loss: 0.4168
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4400 - val_loss: 0.4155
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4386 - val_loss: 0.4141
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4366 - val_loss: 0.4127
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4372 - val_loss: 0.4112
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4337 - val_loss: 0.4098
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.4083
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.4068
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4297 - val_loss: 0.4053
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4293 - val_loss: 0.4037
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4266 - val_loss: 0.4021
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4256 - val_loss: 0.4005
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4235 - val_loss: 0.3989
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4221 - val_loss: 0.3972
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4210 - val_loss: 0.3955
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4187 - val_loss: 0.3938
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4164 - val_loss: 0.3921
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4145 - val_loss: 0.3904
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4129 - val_loss: 0.3886
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4118 - val_loss: 0.3868
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4111 - val_loss: 0.3850
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4076 - val_loss: 0.3832
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4061 - val_loss: 0.3814
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4045 - val_loss: 0.3795
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4018 - val_loss: 0.3776
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.4007 - val_loss: 0.3757
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3991 - val_loss: 0.3738
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3963 - val_loss: 0.3719
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3937 - val_loss: 0.3699
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3927 - val_loss: 0.3679
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3913 - val_loss: 0.3660
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.3890 - val_loss: 0.3640
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.3874 - val_loss: 0.3619
Epoch 47/52
56/56 [==============================] - 0s 3ms/step - loss: 0.3850 - val_loss: 0.3599
Epoch 48/52
56/56 [==============================] - 0s 3ms/step - loss: 0.3823 - val_loss: 0.3578
Epoch 49/52
56/56 [==============================] - 0s 3ms/step - loss: 0.3812 - val_loss: 0.3558
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.3785 - val_loss: 0.3537
Epoch 51/52
56/56 [==============================] - 0s 3ms/step - loss: 0.3766 - val_loss: 0.3516
Epoch 52/52
56/56 [==============================] - 0s 3ms/step - loss: 0.3744 - val_loss: 0.3494
Execution time:  7.529304504394531
DNN:
Mean Absolute Error: 0.3681
Root Mean Square Error: 0.3706
Mean Square Error: 0.1374

Train RMSE: 0.371
Train MSE: 0.137
Train MAE: 0.368
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_54&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_162 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_163 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_54 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_164 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 1s 5ms/step - loss: 0.0875 - val_loss: 0.1214
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0873 - val_loss: 0.1210
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.1205
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.1200
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0859 - val_loss: 0.1195
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0854 - val_loss: 0.1190
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0848 - val_loss: 0.1184
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0844 - val_loss: 0.1178
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0838 - val_loss: 0.1172
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0832 - val_loss: 0.1166
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0826 - val_loss: 0.1159
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0819 - val_loss: 0.1153
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0815 - val_loss: 0.1146
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0807 - val_loss: 0.1139
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.1132
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0795 - val_loss: 0.1125
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0788 - val_loss: 0.1117
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0781 - val_loss: 0.1110
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.1102
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0768 - val_loss: 0.1094
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0762 - val_loss: 0.1087
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0754 - val_loss: 0.1079
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.1071
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.1063
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.1055
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0728 - val_loss: 0.1047
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0720 - val_loss: 0.1040
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.1033
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.1026
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.1019
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0698 - val_loss: 0.1013
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0692 - val_loss: 0.1006
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0688 - val_loss: 0.0999
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0681 - val_loss: 0.0993
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0677 - val_loss: 0.0986
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0670 - val_loss: 0.0980
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0668 - val_loss: 0.0974
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0660 - val_loss: 0.0967
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0657 - val_loss: 0.0961
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0653 - val_loss: 0.0955
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0648 - val_loss: 0.0949
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0645 - val_loss: 0.0943
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0641 - val_loss: 0.0937
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0636 - val_loss: 0.0931
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0632 - val_loss: 0.0925
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.0919
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0623 - val_loss: 0.0914
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0621 - val_loss: 0.0908
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0615 - val_loss: 0.0903
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0612 - val_loss: 0.0897
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0607 - val_loss: 0.0892
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0604 - val_loss: 0.0887
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0882
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0597 - val_loss: 0.0876
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0592 - val_loss: 0.0871
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0592 - val_loss: 0.0866
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0588 - val_loss: 0.0861
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0584 - val_loss: 0.0856
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0851
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0846
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.0842
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0573 - val_loss: 0.0837
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0832
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.0827
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0563 - val_loss: 0.0823
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0559 - val_loss: 0.0818
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0558 - val_loss: 0.0814
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0555 - val_loss: 0.0809
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.0805
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0800
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0796
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0792
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0543 - val_loss: 0.0787
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0541 - val_loss: 0.0783
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0537 - val_loss: 0.0779
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0775
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0771
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0766
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0528 - val_loss: 0.0762
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0528 - val_loss: 0.0758
Execution time:  21.527066469192505
DNN:
Mean Absolute Error: 0.0519
Root Mean Square Error: 0.0601
Mean Square Error: 0.0036

Train RMSE: 0.060
Train MSE: 0.004
Train MAE: 0.052
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_55&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_165 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_166 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_55 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_167 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 0.1154
Epoch 2/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0809 - val_loss: 0.1145
Epoch 3/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0800 - val_loss: 0.1136
Epoch 4/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0791 - val_loss: 0.1125
Epoch 5/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0781 - val_loss: 0.1114
Epoch 6/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.1103
Epoch 7/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0760 - val_loss: 0.1091
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.1080
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0741 - val_loss: 0.1070
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0732 - val_loss: 0.1060
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0722 - val_loss: 0.1050
Epoch 12/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0713 - val_loss: 0.1040
Epoch 13/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0704 - val_loss: 0.1029
Epoch 14/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0695 - val_loss: 0.1018
Epoch 15/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0685 - val_loss: 0.1007
Epoch 16/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.0996
Epoch 17/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0666 - val_loss: 0.0985
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0657 - val_loss: 0.0974
Epoch 19/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0648 - val_loss: 0.0963
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0639 - val_loss: 0.0952
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0629 - val_loss: 0.0940
Epoch 22/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0621 - val_loss: 0.0929
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0611 - val_loss: 0.0918
Epoch 24/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0602 - val_loss: 0.0906
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0593 - val_loss: 0.0895
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.0883
Epoch 27/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0872
Epoch 28/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.0861
Epoch 29/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0560 - val_loss: 0.0850
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0840
Epoch 31/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0545 - val_loss: 0.0829
Epoch 32/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0537 - val_loss: 0.0819
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0809
Epoch 34/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0524 - val_loss: 0.0799
Epoch 35/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0518 - val_loss: 0.0789
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0511 - val_loss: 0.0779
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.0770
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.0760
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0492 - val_loss: 0.0751
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.0742
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0733
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0724
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0715
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0466 - val_loss: 0.0707
Epoch 45/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0462 - val_loss: 0.0698
Epoch 46/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0458 - val_loss: 0.0690
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.0682
Epoch 48/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0449 - val_loss: 0.0674
Epoch 49/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.0666
Epoch 50/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0441 - val_loss: 0.0658
Epoch 51/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0436 - val_loss: 0.0650
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0433 - val_loss: 0.0642
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0635
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0627
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.0619
Epoch 56/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 0.0612
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0604
Epoch 58/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0412 - val_loss: 0.0597
Epoch 59/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0408 - val_loss: 0.0590
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0404 - val_loss: 0.0582
Epoch 61/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0401 - val_loss: 0.0575
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 0.0568
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0394 - val_loss: 0.0561
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0554
Epoch 65/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0389 - val_loss: 0.0548
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0541
Epoch 67/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0384 - val_loss: 0.0535
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0528
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.0522
Epoch 70/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0376 - val_loss: 0.0516
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0510
Epoch 72/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0373 - val_loss: 0.0504
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.0498
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.0493
Epoch 75/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0366 - val_loss: 0.0487
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0481
Epoch 77/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 0.0476
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0470
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0465
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0459
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.0454
Epoch 82/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 0.0449
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0444
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0439
Epoch 85/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.0435
Epoch 86/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0347 - val_loss: 0.0430
Epoch 87/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0347 - val_loss: 0.0426
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0422
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0418
Epoch 90/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0343 - val_loss: 0.0413
Execution time:  21.147061347961426
DNN:
Mean Absolute Error: 0.0344
Root Mean Square Error: 0.0466
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.034
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_56&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_168 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_169 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_56 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_170 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.1188 - val_loss: 0.1508
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1183 - val_loss: 0.1506
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1183 - val_loss: 0.1503
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 0.1501
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1176 - val_loss: 0.1498
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1174 - val_loss: 0.1495
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.1492
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1167 - val_loss: 0.1489
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.1486
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1482
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1155 - val_loss: 0.1479
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1153 - val_loss: 0.1476
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.1472
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.1468
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.1465
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.1461
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1133 - val_loss: 0.1457
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1130 - val_loss: 0.1453
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1126 - val_loss: 0.1449
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1123 - val_loss: 0.1445
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1121 - val_loss: 0.1441
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1113 - val_loss: 0.1436
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 0.1432
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 0.1428
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1103 - val_loss: 0.1423
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.1419
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1093 - val_loss: 0.1414
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1091 - val_loss: 0.1410
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.1405
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1080 - val_loss: 0.1401
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.1396
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1070 - val_loss: 0.1392
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1066 - val_loss: 0.1388
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.1383
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.1379
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1053 - val_loss: 0.1374
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1370
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.1365
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1040 - val_loss: 0.1360
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.1356
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.1351
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.1346
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1022 - val_loss: 0.1342
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.1337
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1013 - val_loss: 0.1332
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.1327
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1003 - val_loss: 0.1323
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1000 - val_loss: 0.1318
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0994 - val_loss: 0.1313
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.1308
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0985 - val_loss: 0.1303
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0979 - val_loss: 0.1298
Execution time:  7.990282773971558
DNN:
Mean Absolute Error: 0.1000
Root Mean Square Error: 0.1060
Mean Square Error: 0.0112

Train RMSE: 0.106
Train MSE: 0.011
Train MAE: 0.100
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_57&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_171 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_172 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_57 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_173 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.0883 - val_loss: 0.1094
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.1088
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0871 - val_loss: 0.1082
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.1075
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.1068
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0848 - val_loss: 0.1060
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0842 - val_loss: 0.1052
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0833 - val_loss: 0.1044
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0825 - val_loss: 0.1036
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0817 - val_loss: 0.1028
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0808 - val_loss: 0.1019
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0799 - val_loss: 0.1010
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0792 - val_loss: 0.1001
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0783 - val_loss: 0.0991
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.0982
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0764 - val_loss: 0.0973
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0755 - val_loss: 0.0963
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0746 - val_loss: 0.0954
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0737 - val_loss: 0.0944
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0727 - val_loss: 0.0934
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.0925
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.0915
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0700 - val_loss: 0.0905
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0692 - val_loss: 0.0895
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0685 - val_loss: 0.0886
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0675 - val_loss: 0.0876
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0667 - val_loss: 0.0867
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0659 - val_loss: 0.0858
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0651 - val_loss: 0.0849
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0646 - val_loss: 0.0841
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0834
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0634 - val_loss: 0.0826
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0819
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0622 - val_loss: 0.0812
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0615 - val_loss: 0.0805
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0611 - val_loss: 0.0798
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0605 - val_loss: 0.0792
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0785
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0595 - val_loss: 0.0779
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.0773
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0587 - val_loss: 0.0767
Epoch 42/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0583 - val_loss: 0.0761
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0755
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.0749
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0744
Epoch 46/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.0738
Epoch 47/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.0733
Epoch 48/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.0727
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0722
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0717
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0712
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0707
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0702
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0541 - val_loss: 0.0697
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0539 - val_loss: 0.0692
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.0687
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0682
Epoch 58/80
103/103 [==============================] - ETA: 0s - loss: 0.049 - 0s 2ms/step - loss: 0.0529 - val_loss: 0.0678
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0524 - val_loss: 0.0673
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0522 - val_loss: 0.0668
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0664
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0520 - val_loss: 0.0660
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0514 - val_loss: 0.0655
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0651
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0646
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0642
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.0638
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0634
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0501 - val_loss: 0.0630
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0626
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0622
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0618
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0614
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0491 - val_loss: 0.0610
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0490 - val_loss: 0.0607
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0603
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0599
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0596
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0481 - val_loss: 0.0592
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0588
Execution time:  19.68028450012207
DNN:
Mean Absolute Error: 0.0467
Root Mean Square Error: 0.0551
Mean Square Error: 0.0030

Train RMSE: 0.055
Train MSE: 0.003
Train MAE: 0.047
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_58&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_174 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_175 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_58 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_176 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.1157 - val_loss: 0.1349
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1147 - val_loss: 0.1338
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1135 - val_loss: 0.1327
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1123 - val_loss: 0.1315
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.1302
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1096 - val_loss: 0.1288
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1081 - val_loss: 0.1274
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1066 - val_loss: 0.1259
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1049 - val_loss: 0.1243
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1033 - val_loss: 0.1227
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1210
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0998 - val_loss: 0.1193
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0981 - val_loss: 0.1179
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0967 - val_loss: 0.1166
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0954 - val_loss: 0.1154
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0940 - val_loss: 0.1141
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0927 - val_loss: 0.1128
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0913 - val_loss: 0.1115
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0899 - val_loss: 0.1101
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0885 - val_loss: 0.1088
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0871 - val_loss: 0.1074
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.1061
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0843 - val_loss: 0.1047
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0830 - val_loss: 0.1033
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0816 - val_loss: 0.1019
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.1005
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0786 - val_loss: 0.0991
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.0977
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0759 - val_loss: 0.0963
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0745 - val_loss: 0.0949
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0731 - val_loss: 0.0935
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0717 - val_loss: 0.0921
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0704 - val_loss: 0.0908
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0690 - val_loss: 0.0894
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0677 - val_loss: 0.0880
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0664 - val_loss: 0.0865
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0650 - val_loss: 0.0852
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0838
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0625 - val_loss: 0.0824
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0613 - val_loss: 0.0811
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0602 - val_loss: 0.0798
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.0785
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0772
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.0759
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0557 - val_loss: 0.0747
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0549 - val_loss: 0.0735
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0723
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0529 - val_loss: 0.0712
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0522 - val_loss: 0.0701
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0514 - val_loss: 0.0691
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0507 - val_loss: 0.0681
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0500 - val_loss: 0.0671
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0661
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.0652
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0643
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0634
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 0.0625
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0617
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.0609
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.0601
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0594
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0446 - val_loss: 0.0586
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0441 - val_loss: 0.0579
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0437 - val_loss: 0.0572
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.0565
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0558
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0551
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0545
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.0538
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0532
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.0526
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0520
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.0514
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.0508
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.0503
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0498
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.0493
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0488
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.0483
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0479
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.0474
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0470
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0465
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0461
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0382 - val_loss: 0.0457
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0453
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0449
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0446
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0442
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0438
Execution time:  25.082218647003174
DNN:
Mean Absolute Error: 0.0375
Root Mean Square Error: 0.0468
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.037
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_59&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_177 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_178 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_59 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_179 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.1027 - val_loss: 0.1233
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1026 - val_loss: 0.1232
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.1231
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1024 - val_loss: 0.1230
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1023 - val_loss: 0.1229
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1021 - val_loss: 0.1227
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.1226
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1019 - val_loss: 0.1225
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.1223
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 0.1222
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1220
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1013 - val_loss: 0.1219
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1011 - val_loss: 0.1217
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1009 - val_loss: 0.1216
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.1214
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1007 - val_loss: 0.1212
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1005 - val_loss: 0.1211
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1002 - val_loss: 0.1209
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1002 - val_loss: 0.1207
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1000 - val_loss: 0.1206
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0997 - val_loss: 0.1204
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0996 - val_loss: 0.1202
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0994 - val_loss: 0.1200
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0992 - val_loss: 0.1198
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.1196
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.1194
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0986 - val_loss: 0.1193
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0984 - val_loss: 0.1191
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0982 - val_loss: 0.1189
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0981 - val_loss: 0.1187
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.1185
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0976 - val_loss: 0.1183
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1181
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0972 - val_loss: 0.1178
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0970 - val_loss: 0.1176
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0968 - val_loss: 0.1174
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.1172
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.1170
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.1168
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0958 - val_loss: 0.1166
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0957 - val_loss: 0.1163
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.1161
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0952 - val_loss: 0.1159
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0950 - val_loss: 0.1157
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0948 - val_loss: 0.1155
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0946 - val_loss: 0.1152
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.1150
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.1148
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0938 - val_loss: 0.1145
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0936 - val_loss: 0.1143
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0934 - val_loss: 0.1141
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0932 - val_loss: 0.1138
Execution time:  7.3392980098724365
DNN:
Mean Absolute Error: 0.0970
Root Mean Square Error: 0.1032
Mean Square Error: 0.0107

Train RMSE: 0.103
Train MSE: 0.011
Train MAE: 0.097
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_60&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_180 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_181 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_60 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_182 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
  1/116 [..............................] - ETA: 0s - loss: 0.4556WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0124s). Check your callbacks.
116/116 [==============================] - 0s 4ms/step - loss: 0.2122 - val_loss: 0.0837
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1231 - val_loss: 0.0507
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.0309
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.0128
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0685 - val_loss: 0.0062
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0522 - val_loss: 0.0157
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0073
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0086
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0100
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0115
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0114
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0122
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0122
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0128
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0130
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0144
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0144
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0116
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0107
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0138
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0155
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0116
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0137
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0124
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0123
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0115
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0110
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0119
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0107
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0109
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0105
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0107
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0105
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0108
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0107
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0107
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0106
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0105
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0105
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0103
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0105
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0101
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0106
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0103
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0107
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0119
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0108
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0105
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0113
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0114
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0117
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0145
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0102
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0116
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0118
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0110
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0118
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0112
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0113
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0110
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0152
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0126
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0113
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0111
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0116
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0122
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0115
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0118
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0110
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0117
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0118
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0119
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0118
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0117
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0118
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0119
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0110
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0117
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0118
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0117
Execution time:  21.28831934928894
DNN:
Mean Absolute Error: 0.0124
Root Mean Square Error: 0.0214
Mean Square Error: 0.0005

Train RMSE: 0.021
Train MSE: 0.000
Train MAE: 0.012
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_61&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_183 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_184 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_61 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_185 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 1s 5ms/step - loss: 0.0938 - val_loss: 0.0126
Epoch 2/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0132
Epoch 3/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.0140
Epoch 4/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0102
Epoch 5/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0423 - val_loss: 0.0108
Epoch 6/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0090
Epoch 7/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0392 - val_loss: 0.0056
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 0.0047
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0066
Epoch 10/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0340 - val_loss: 0.0077
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0080
Epoch 12/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0095
Epoch 13/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0304 - val_loss: 0.0095
Epoch 14/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0106
Epoch 15/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0113
Epoch 16/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0112
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0127
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0124
Epoch 19/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0123
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0119
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0126
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0111
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0121
Epoch 24/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0209 - val_loss: 0.0117
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0103
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0092
Epoch 27/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0115
Epoch 28/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0119
Epoch 29/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0180 - val_loss: 0.0105
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0122
Epoch 31/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0174 - val_loss: 0.0133
Epoch 32/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0137
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0136
Epoch 34/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0136
Epoch 35/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0141
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0151
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0147
Epoch 38/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0142
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0138
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0147
Epoch 41/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0146
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0132
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0157
Epoch 44/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0126
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0131
Epoch 46/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0138
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0146
Epoch 48/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0131
Epoch 49/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0136
Epoch 50/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0137
Epoch 51/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0137
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0137
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0142
Epoch 54/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0135
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0138
Epoch 56/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0123
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0130
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0123
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0145
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0147
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0151
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0140
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0135
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0135
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0137
Epoch 66/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0138
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0130
Epoch 68/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0141
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0122
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0132
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0121
Epoch 72/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0134
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0134
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0152
Epoch 75/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0121
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0141
Epoch 77/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0116
Epoch 78/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0139
Epoch 79/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0120
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0138
Epoch 81/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0119
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0116
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0136
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0130
Epoch 85/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0129
Epoch 86/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0136
Epoch 87/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0147
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0136
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0124
Epoch 90/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0139
Execution time:  21.43317699432373
DNN:
Mean Absolute Error: 0.0098
Root Mean Square Error: 0.0160
Mean Square Error: 0.0003

Train RMSE: 0.016
Train MSE: 0.000
Train MAE: 0.010
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_62&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_186 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_187 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_62 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_188 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.2113 - val_loss: 0.0525
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1138 - val_loss: 0.0060
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0971 - val_loss: 0.0071
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0861 - val_loss: 0.0084
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0788 - val_loss: 0.0061
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0064
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0686 - val_loss: 0.0069
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0641 - val_loss: 0.0055
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0049
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0555 - val_loss: 0.0061
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0514 - val_loss: 0.0061
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0077
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.0089
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.0096
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.0093
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.0093
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0099
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0104
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0094
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0088
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0093
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0096
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0102
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0101
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0105
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0105
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0105
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0102
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0106
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0101
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0101
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0100
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0105
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0101
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0102
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0103
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0101
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0103
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0103
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0101
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0101
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0099
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0100
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0099
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0101
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0099
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0099
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0100
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0100
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0101
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0099
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0100
Execution time:  7.91131591796875
DNN:
Mean Absolute Error: 0.0188
Root Mean Square Error: 0.0300
Mean Square Error: 0.0009

Train RMSE: 0.030
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_63&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_189 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_190 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_63 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_191 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.1202 - val_loss: 0.0254
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0885 - val_loss: 0.0378
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0791 - val_loss: 0.0319
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0737 - val_loss: 0.0297
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0675 - val_loss: 0.0293
Epoch 6/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0602 - val_loss: 0.0230
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0192
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.0187
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0194
Epoch 10/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0175
Epoch 11/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0136
Epoch 12/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0131
Epoch 13/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0124
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0140
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0156
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0144
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0152
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0151
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0147
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0151
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0150
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0151
Epoch 24/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0148
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0146
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0146
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0145
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0143
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0142
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0141
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0141
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0139
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0140
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0138
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0139
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0139
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0137
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0137
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0134
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0134
Epoch 41/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0132
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0133
Epoch 43/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0132
Epoch 44/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0129
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0128
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0130
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0130
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0128
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0127
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0127
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0128
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0128
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0127
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0125
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0128
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0128
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0128
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0127
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0127
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0126
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0125
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0126
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0126
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0129
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0127
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0125
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0127
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0127
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0125
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0126
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0127
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0128
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0127
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0128
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0126
Epoch 76/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0176 - val_loss: 0.0125
Epoch 77/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0125
Epoch 78/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0124
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0127
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0126
Execution time:  20.29176354408264
DNN:
Mean Absolute Error: 0.0156
Root Mean Square Error: 0.0240
Mean Square Error: 0.0006

Train RMSE: 0.024
Train MSE: 0.001
Train MAE: 0.016
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_64&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_192 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_193 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_64 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_194 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 0.0433
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0380
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0378
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.0348
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0324
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0433 - val_loss: 0.0280
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0253
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0223
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0177
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 0.0227
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0192
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0232
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0234
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0231
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0193
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0183
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0177
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0144
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0140
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0151
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0123
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0114
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0111
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0108
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0106
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0103
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0106
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0103
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0102
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0100
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0096
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0095
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0095
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0096
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0096
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0095
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0102
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0104
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0109
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0111
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0111
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0111
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0117
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0118
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0121
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0126
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0125
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0125
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0125
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0123
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0125
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0123
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0124
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0123
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0126
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0126
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0124
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0125
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0125
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0124
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0127
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0126
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0124
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0125
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0122
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0127
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0126
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0125
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0125
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0123
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0125
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0125
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0124
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0123
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0125
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0124
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0126
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0124
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0128
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0127
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0128
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0126
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0128
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0126
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0126
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0128
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0121
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0125
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0127
Execution time:  24.604466676712036
DNN:
Mean Absolute Error: 0.0143
Root Mean Square Error: 0.0230
Mean Square Error: 0.0005

Train RMSE: 0.023
Train MSE: 0.001
Train MAE: 0.014
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_65&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_195 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_196 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_65 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_197 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 5ms/step - loss: 0.4759 - val_loss: 0.2638
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1996 - val_loss: 0.0840
Epoch 3/52
56/56 [==============================] - 0s 3ms/step - loss: 0.1209 - val_loss: 0.0278
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.0294
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0957 - val_loss: 0.0303
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0873 - val_loss: 0.0308
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0805 - val_loss: 0.0284
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0242
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0664 - val_loss: 0.0192
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0598 - val_loss: 0.0158
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.0141
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0140
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.0155
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0147
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0151
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0156
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0157
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0155
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0151
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0147
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0146
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0146
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0148
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0147
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0147
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0146
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0143
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0142
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0143
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0143
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0141
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0141
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0139
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0139
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0138
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0135
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0135
Epoch 38/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0132
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0131
Epoch 40/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0128
Epoch 41/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0128
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0125
Epoch 43/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0124
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0124
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0126
Epoch 46/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0124
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0122
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0121
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0117
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0117
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0116
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0115
Execution time:  7.554470777511597
DNN:
Mean Absolute Error: 0.0106
Root Mean Square Error: 0.0198
Mean Square Error: 0.0004

Train RMSE: 0.020
Train MSE: 0.000
Train MAE: 0.011
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_66&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_198 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_199 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_66 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_200 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
116/116 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0418
Epoch 2/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0377
Epoch 3/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.0332
Epoch 4/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.0309
Epoch 5/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0292
Epoch 6/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0266
Epoch 7/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0242
Epoch 8/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0226
Epoch 9/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0218
Epoch 10/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0208
Epoch 11/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0200
Epoch 12/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0190
Epoch 13/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0179
Epoch 14/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0176
Epoch 15/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0170
Epoch 16/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0164
Epoch 17/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0160
Epoch 18/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0156
Epoch 19/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0153
Epoch 20/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0149
Epoch 21/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0145
Epoch 22/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0143
Epoch 23/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0139
Epoch 24/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0134
Epoch 25/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0131
Epoch 26/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0128
Epoch 27/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0126
Epoch 28/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0124
Epoch 29/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0125
Epoch 30/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0122
Epoch 31/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0122
Epoch 32/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0119
Epoch 33/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0121
Epoch 34/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0120
Epoch 35/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0121
Epoch 36/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0118
Epoch 37/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0119
Epoch 38/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0120
Epoch 39/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0121
Epoch 40/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0122
Epoch 41/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0122
Epoch 42/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0121
Epoch 43/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0122
Epoch 44/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0121
Epoch 45/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0123
Epoch 46/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0121
Epoch 47/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0126
Epoch 48/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0125
Epoch 49/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0126
Epoch 50/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0124
Epoch 51/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0126
Epoch 52/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0124
Epoch 53/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0121
Epoch 54/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0129
Epoch 55/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0128
Epoch 56/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0125
Epoch 57/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0127
Epoch 58/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0127
Epoch 59/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0126
Epoch 60/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0131
Epoch 61/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0127
Epoch 62/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0126
Epoch 63/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0125
Epoch 64/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0125
Epoch 65/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0123
Epoch 66/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0125
Epoch 67/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0120
Epoch 68/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0122
Epoch 69/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0121
Epoch 70/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0122
Epoch 71/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0119
Epoch 72/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0120
Epoch 73/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0121
Epoch 74/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0119
Epoch 75/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0118
Epoch 76/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0118
Epoch 77/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0118
Epoch 78/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0116
Epoch 79/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0120
Epoch 80/80
116/116 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0118
Execution time:  21.08873200416565
DNN:
Mean Absolute Error: 0.0108
Root Mean Square Error: 0.0199
Mean Square Error: 0.0004

Train RMSE: 0.020
Train MSE: 0.000
Train MAE: 0.011
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_67&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_201 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_202 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_67 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_203 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.0236
Epoch 2/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0203
Epoch 3/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0359 - val_loss: 0.0191
Epoch 4/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 0.0187
Epoch 5/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.0189
Epoch 6/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0344 - val_loss: 0.0189
Epoch 7/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0180
Epoch 8/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 0.0185
Epoch 9/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0327 - val_loss: 0.0182
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0171
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0185
Epoch 12/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0173
Epoch 13/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0168
Epoch 14/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 0.0169
Epoch 15/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0164
Epoch 16/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0298 - val_loss: 0.0164
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0165
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 19/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0279 - val_loss: 0.0164
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0162
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0148
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0161
Epoch 23/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0253 - val_loss: 0.0156
Epoch 24/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0157
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0148
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0140
Epoch 27/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0135
Epoch 28/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0136
Epoch 29/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0148
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0158
Epoch 31/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0145
Epoch 32/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0139
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0137
Epoch 34/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0147
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0128
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0121
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0115
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0113
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0112
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0100
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0094
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0093
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0082
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0088
Epoch 45/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0087
Epoch 46/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0078
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0084
Epoch 48/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0078
Epoch 49/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0086
Epoch 50/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0074
Epoch 51/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0082
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0080
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0080
Epoch 54/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0075
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0086
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0074
Epoch 57/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0074
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0074
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0087
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0077
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0075
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0079
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0084
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0076
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0087
Epoch 66/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0078
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0089
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0091
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0079
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0089
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0091
Epoch 72/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0089
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0094
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0093
Epoch 75/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0085
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0089
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0084
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0092
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0086
Epoch 80/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0084
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0084
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0084
Epoch 83/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0083
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0093
Epoch 85/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0083
Epoch 86/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0092
Epoch 87/90
143/143 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0097
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0094
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0092
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0095
Execution time:  21.09965682029724
DNN:
Mean Absolute Error: 0.0066
Root Mean Square Error: 0.0163
Mean Square Error: 0.0003

Train RMSE: 0.016
Train MSE: 0.000
Train MAE: 0.007
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_68&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_204 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_205 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_68 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_206 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.0548 - val_loss: 0.0596
Epoch 2/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.0534
Epoch 3/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0502
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0462 - val_loss: 0.0480
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0446 - val_loss: 0.0463
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0448
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0439
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0427
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0423
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0412
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0403
Epoch 12/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0394
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0385
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0376
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0365
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0354
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0332
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.0321
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0307
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0297
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0290
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0281
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0274
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0266
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0261
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0256
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0250
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0248
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0242
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0240
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0236
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0232
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0229
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0226
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0222
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0221
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0218
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0218
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0219
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0217
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0206
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0206
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0205
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0203
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0202
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0201
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0200
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0199
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0196
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0196
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0194
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0198
Execution time:  8.262229681015015
DNN:
Mean Absolute Error: 0.0262
Root Mean Square Error: 0.0430
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  3h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_69&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_207 (Dense)            (None, 18, 87)            174       
_________________________________________________________________
dense_208 (Dense)            (None, 18, 16)            1408      
_________________________________________________________________
dropout_69 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_209 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 4ms/step - loss: 0.0480 - val_loss: 0.0365
Epoch 2/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.0339
Epoch 3/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0317
Epoch 4/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0296
Epoch 5/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.0276
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.0263
Epoch 7/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0254
Epoch 8/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.0249
Epoch 9/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0235
Epoch 10/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0226
Epoch 11/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0221
Epoch 12/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0215
Epoch 13/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0210
Epoch 14/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0205
Epoch 15/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0202
Epoch 16/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0195
Epoch 17/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0194
Epoch 18/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0189
Epoch 19/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0188
Epoch 20/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0183
Epoch 21/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0183
Epoch 22/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0180
Epoch 23/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0176
Epoch 24/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0174
Epoch 25/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0172
Epoch 26/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0168
Epoch 27/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0165
Epoch 28/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0162
Epoch 29/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0159
Epoch 30/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0157
Epoch 31/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0156
Epoch 32/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0147
Epoch 33/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 34/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0151
Epoch 35/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0147
Epoch 36/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0144
Epoch 37/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0138
Epoch 38/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0137
Epoch 39/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0137
Epoch 40/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0137
Epoch 41/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0136
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0134
Epoch 43/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0134
Epoch 44/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0133
Epoch 45/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0133
Epoch 46/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0132
Epoch 47/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0131
Epoch 48/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0130
Epoch 49/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0129
Epoch 50/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0127
Epoch 51/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0126
Epoch 52/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0125
Epoch 53/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0124
Epoch 54/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0124
Epoch 55/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0122
Epoch 56/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0119
Epoch 57/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0117
Epoch 58/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0117
Epoch 59/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0116
Epoch 60/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0115
Epoch 61/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0114
Epoch 62/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0114
Epoch 63/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0113
Epoch 64/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0112
Epoch 65/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0111
Epoch 66/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0112
Epoch 67/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0111
Epoch 68/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0110
Epoch 69/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0109
Epoch 70/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0108
Epoch 71/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0107
Epoch 72/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0110
Epoch 73/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0109
Epoch 74/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0113
Epoch 75/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0110
Epoch 76/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0109
Epoch 77/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0110
Epoch 78/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0108
Epoch 79/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0108
Epoch 80/80
103/103 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0110
Execution time:  20.035155773162842
DNN:
Mean Absolute Error: 0.0096
Root Mean Square Error: 0.0269
Mean Square Error: 0.0007

Train RMSE: 0.027
Train MSE: 0.001
Train MAE: 0.010
###########################

MODEL:  DNN
sequence:  3h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_70&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_210 (Dense)            (None, 18, 80)            160       
_________________________________________________________________
dense_211 (Dense)            (None, 18, 16)            1296      
_________________________________________________________________
dropout_70 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_212 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
128/128 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.0194
Epoch 2/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.0194
Epoch 3/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 0.0191
Epoch 4/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0191
Epoch 5/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0188
Epoch 6/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0188
Epoch 7/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0187
Epoch 8/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0187
Epoch 9/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0186
Epoch 10/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0185
Epoch 11/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0182
Epoch 12/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0184
Epoch 13/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.0182
Epoch 14/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0181
Epoch 15/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0178
Epoch 16/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0179
Epoch 17/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0178
Epoch 18/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0175
Epoch 19/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0164
Epoch 20/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0164
Epoch 21/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0172
Epoch 22/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0169
Epoch 23/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0169
Epoch 24/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0169
Epoch 25/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0166
Epoch 26/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0166
Epoch 27/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0164
Epoch 28/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0165
Epoch 29/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0162
Epoch 30/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 31/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0160
Epoch 32/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0158
Epoch 33/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0155
Epoch 34/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0150
Epoch 35/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0148
Epoch 36/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0145
Epoch 37/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0142
Epoch 38/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0140
Epoch 39/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0138
Epoch 40/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0137
Epoch 41/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0136
Epoch 42/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0135
Epoch 43/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0133
Epoch 44/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0131
Epoch 45/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0129
Epoch 46/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0129
Epoch 47/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0128
Epoch 48/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0127
Epoch 49/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0126
Epoch 50/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0124
Epoch 51/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0123
Epoch 52/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0122
Epoch 53/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0119
Epoch 54/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0118
Epoch 55/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0116
Epoch 56/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0115
Epoch 57/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0113
Epoch 58/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0110
Epoch 59/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0109
Epoch 60/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0106
Epoch 61/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0105
Epoch 62/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0106
Epoch 63/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0103
Epoch 64/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0102
Epoch 65/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0105
Epoch 66/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0102
Epoch 67/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0100
Epoch 68/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0105
Epoch 69/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0105
Epoch 70/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0100
Epoch 71/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0100
Epoch 72/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0098
Epoch 73/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0106
Epoch 74/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0100
Epoch 75/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0099
Epoch 76/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0103
Epoch 77/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0100
Epoch 78/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0103
Epoch 79/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0099
Epoch 80/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0101
Epoch 81/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0097
Epoch 82/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0099
Epoch 83/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0098
Epoch 84/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0099
Epoch 85/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0099
Epoch 86/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0098
Epoch 87/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0098
Epoch 88/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0097
Epoch 89/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0098
Epoch 90/90
128/128 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0098
Execution time:  25.04784393310547
DNN:
Mean Absolute Error: 0.0066
Root Mean Square Error: 0.0214
Mean Square Error: 0.0005

Train RMSE: 0.021
Train MSE: 0.000
Train MAE: 0.007
###########################

MODEL:  DNN
sequence:  3h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_71&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_213 (Dense)            (None, 18, 12)            24        
_________________________________________________________________
dense_214 (Dense)            (None, 18, 16)            208       
_________________________________________________________________
dropout_71 (Dropout)         (None, 18, 16)            0         
_________________________________________________________________
dense_215 (Dense)            (None, 18, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 1s 9ms/step - loss: 0.0492 - val_loss: 0.0416
Epoch 2/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0443 - val_loss: 0.0383
Epoch 3/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0370
Epoch 4/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0363
Epoch 5/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0353
Epoch 6/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.0346
Epoch 7/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0339
Epoch 8/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0394 - val_loss: 0.0332
Epoch 9/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0327
Epoch 10/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0321
Epoch 11/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0317
Epoch 12/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0311
Epoch 13/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.0308
Epoch 14/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.0308
Epoch 15/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0301
Epoch 16/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0299
Epoch 17/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0296
Epoch 18/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.0294
Epoch 19/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0290
Epoch 20/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 0.0287
Epoch 21/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0286
Epoch 22/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0285
Epoch 23/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0282
Epoch 24/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0279
Epoch 25/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0276
Epoch 26/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0273
Epoch 27/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0269
Epoch 28/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0267
Epoch 29/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0263
Epoch 30/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0258
Epoch 31/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0254
Epoch 32/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0249
Epoch 33/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0246
Epoch 34/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0242
Epoch 35/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0238
Epoch 36/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0235
Epoch 37/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0232
Epoch 38/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0228
Epoch 39/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0226
Epoch 40/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0223
Epoch 41/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0220
Epoch 42/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0218
Epoch 43/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0215
Epoch 44/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0210
Epoch 45/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0206
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0204
Epoch 47/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0201
Epoch 48/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0199
Epoch 49/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0197
Epoch 50/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0195
Epoch 51/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0194
Epoch 52/52
56/56 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0189
Execution time:  7.5913403034210205
DNN:
Mean Absolute Error: 0.0239
Root Mean Square Error: 0.0387
Mean Square Error: 0.0015

Train RMSE: 0.039
Train MSE: 0.001
Train MAE: 0.024
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_72&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_216 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_217 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_72 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_218 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
115/115 [==============================] - 0s 4ms/step - loss: 0.1246 - val_loss: 0.0171
Epoch 2/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0781 - val_loss: 0.0158
Epoch 3/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0654 - val_loss: 0.0096
Epoch 4/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.0063
Epoch 5/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.0112
Epoch 6/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0126
Epoch 7/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0202
Epoch 8/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0099
Epoch 9/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0102
Epoch 10/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0103
Epoch 11/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0103
Epoch 12/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0105
Epoch 13/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0105
Epoch 14/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0107
Epoch 15/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0109
Epoch 16/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0114
Epoch 17/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0114
Epoch 18/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0118
Epoch 19/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0118
Epoch 20/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0120
Epoch 21/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0120
Epoch 22/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0118
Epoch 23/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0120
Epoch 24/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0116
Epoch 25/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0116
Epoch 26/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0116
Epoch 27/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0114
Epoch 28/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0112
Epoch 29/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0111
Epoch 30/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0113
Epoch 31/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0112
Epoch 32/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0112
Epoch 33/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0109
Epoch 34/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0107
Epoch 35/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0107
Epoch 36/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0105
Epoch 37/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0105
Epoch 38/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0107
Epoch 39/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0106
Epoch 40/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0105
Epoch 41/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0106
Epoch 42/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0105
Epoch 43/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0105
Epoch 44/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0106
Epoch 45/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0105
Epoch 46/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0106
Epoch 47/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0105
Epoch 48/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0107
Epoch 49/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0106
Epoch 50/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0110
Epoch 51/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0114
Epoch 52/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0099
Epoch 53/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0105
Epoch 54/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0102
Epoch 55/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0101
Epoch 56/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0094
Epoch 57/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0113
Epoch 58/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0099
Epoch 59/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0103
Epoch 60/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0099
Epoch 61/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0104
Epoch 62/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0098
Epoch 63/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0101
Epoch 64/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0099
Epoch 65/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0099
Epoch 66/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0104
Epoch 67/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0097
Epoch 68/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0102
Epoch 69/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0104
Epoch 70/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0098
Epoch 71/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0104
Epoch 72/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0098
Epoch 73/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0097
Epoch 74/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0102
Epoch 75/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0098
Epoch 76/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0103
Epoch 77/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0098
Epoch 78/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0111
Epoch 79/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0098
Epoch 80/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0101
Execution time:  26.171632766723633
DNN:
Mean Absolute Error: 0.0212
Root Mean Square Error: 0.0330
Mean Square Error: 0.0011

Train RMSE: 0.033
Train MSE: 0.001
Train MAE: 0.021
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_73&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_219 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_220 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_73 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_221 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 1s 4ms/step - loss: 0.0766 - val_loss: 0.0160
Epoch 2/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.0113
Epoch 3/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0128
Epoch 4/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0051
Epoch 5/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0054
Epoch 6/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.0077
Epoch 7/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0093
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0068
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0078
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0089
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0094
Epoch 12/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0101
Epoch 13/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0113
Epoch 14/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0144
Epoch 15/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0125
Epoch 16/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0133
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0116
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0119
Epoch 19/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0123
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0129
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0144
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0171
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0209
Epoch 24/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0213
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0204
Epoch 26/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0167
Epoch 27/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0159
Epoch 28/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0124
Epoch 29/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0118
Epoch 30/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0117
Epoch 31/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0101
Epoch 32/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0096
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0124
Epoch 34/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0102
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0124
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0096
Epoch 37/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0103
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0106
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0112
Epoch 40/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0113
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0146
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0156
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0147
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0158
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0148
Epoch 46/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0164
Epoch 47/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0157
Epoch 48/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0166
Epoch 49/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0159
Epoch 50/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0169
Epoch 51/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0175
Epoch 52/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0174
Epoch 53/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0173
Epoch 54/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0171
Epoch 55/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0156
Epoch 56/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0136
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0149
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0151
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0174
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0152
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0153
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0150
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0131
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0126
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0142
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0130
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0150
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0131
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0131
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0134
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0125
Epoch 72/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0131
Epoch 73/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0152
Epoch 74/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0125
Epoch 75/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0146
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0126
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0145
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0145
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0126
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0146
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0134
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0128
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0126
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0135
Epoch 85/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0151
Epoch 86/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0150
Epoch 87/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0151
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0150
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0146
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0152
Execution time:  33.37441921234131
DNN:
Mean Absolute Error: 0.0178
Root Mean Square Error: 0.0301
Mean Square Error: 0.0009

Train RMSE: 0.030
Train MSE: 0.001
Train MAE: 0.018
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_74&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_222 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_223 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_74 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_224 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.1848 - val_loss: 0.0090
Epoch 2/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 0.0150
Epoch 3/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 0.0055
Epoch 4/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.0102
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0723 - val_loss: 0.0086
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0636 - val_loss: 0.0084
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0543 - val_loss: 0.0055
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0470 - val_loss: 0.0066
Epoch 9/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.0057
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0057
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0062
Epoch 12/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0071
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0085
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0096
Epoch 15/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0097
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0097
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0096
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0102
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0097
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0100
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0099
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0098
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0098
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0110
Epoch 25/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0099
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0099
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0103
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0101
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0098
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0098
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0100
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0098
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0107
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0096
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0109
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0097
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0107
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0097
Epoch 39/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0106
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0097
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0111
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0099
Epoch 43/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0105
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0099
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0104
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0098
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0105
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0098
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0104
Epoch 50/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0100
Epoch 51/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0097
Epoch 52/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0098
Execution time:  9.253444910049438
DNN:
Mean Absolute Error: 0.0188
Root Mean Square Error: 0.0288
Mean Square Error: 0.0008

Train RMSE: 0.029
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_75&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_225 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_226 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_75 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_227 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 5ms/step - loss: 0.1451 - val_loss: 0.0381
Epoch 2/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 0.0128
Epoch 3/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0130
Epoch 4/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0594 - val_loss: 0.0153
Epoch 5/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.0161
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0144
Epoch 7/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0149
Epoch 8/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0174
Epoch 9/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0160
Epoch 10/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0156
Epoch 11/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0156
Epoch 12/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0156
Epoch 13/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0160
Epoch 14/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0159
Epoch 15/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0161
Epoch 16/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0160
Epoch 17/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0160
Epoch 18/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0160
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0159
Epoch 20/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0161
Epoch 21/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0161
Epoch 22/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0158
Epoch 23/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0162
Epoch 24/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0157
Epoch 25/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0160
Epoch 26/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0157
Epoch 27/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0161
Epoch 28/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0156
Epoch 29/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0159
Epoch 30/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0155
Epoch 31/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0159
Epoch 32/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0154
Epoch 33/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0160
Epoch 34/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0154
Epoch 35/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0158
Epoch 36/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0154
Epoch 37/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0160
Epoch 38/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0154
Epoch 39/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0161
Epoch 40/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153
Epoch 41/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0158
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0153
Epoch 43/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0160
Epoch 44/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0153
Epoch 45/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0156
Epoch 46/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0152
Epoch 47/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0158
Epoch 48/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 49/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0154
Epoch 50/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0150
Epoch 51/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0150
Epoch 52/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0149
Epoch 53/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0148
Epoch 54/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0147
Epoch 55/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0149
Epoch 56/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0146
Epoch 57/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0148
Epoch 58/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0147
Epoch 59/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0148
Epoch 60/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0146
Epoch 61/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0147
Epoch 62/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0145
Epoch 63/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0150
Epoch 64/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0146
Epoch 65/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0148
Epoch 66/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0147
Epoch 67/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0148
Epoch 68/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0147
Epoch 69/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0148
Epoch 70/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0147
Epoch 71/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0147
Epoch 72/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0147
Epoch 73/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0148
Epoch 74/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0147
Epoch 75/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0147
Epoch 76/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0147
Epoch 77/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0147
Epoch 78/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0147
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0148
Epoch 80/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0147
Execution time:  25.21869421005249
DNN:
Mean Absolute Error: 0.0218
Root Mean Square Error: 0.0335
Mean Square Error: 0.0011

Train RMSE: 0.033
Train MSE: 0.001
Train MAE: 0.022
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_76&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_228 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_229 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_76 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_230 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
127/127 [==============================] - 0s 4ms/step - loss: 0.1364 - val_loss: 0.0269
Epoch 2/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.0304
Epoch 3/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.0259
Epoch 4/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0244
Epoch 5/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.0247
Epoch 6/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.0214
Epoch 7/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.0235
Epoch 8/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0231
Epoch 9/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.0241
Epoch 10/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.0147
Epoch 11/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0223
Epoch 12/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0174
Epoch 13/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0180
Epoch 14/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0161
Epoch 15/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0160
Epoch 16/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0157
Epoch 17/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0156
Epoch 18/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0154
Epoch 19/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0154
Epoch 20/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0147
Epoch 21/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0165
Epoch 22/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0156
Epoch 23/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0159
Epoch 24/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0153
Epoch 25/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0159
Epoch 26/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0154
Epoch 27/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0156
Epoch 28/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0154
Epoch 29/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0157
Epoch 30/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0154
Epoch 31/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0156
Epoch 32/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0153
Epoch 33/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0156
Epoch 34/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0152
Epoch 35/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0156
Epoch 36/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0151
Epoch 37/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0155
Epoch 38/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0153
Epoch 39/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0153
Epoch 40/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 41/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0154
Epoch 42/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0151
Epoch 43/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0153
Epoch 44/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0151
Epoch 45/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0153
Epoch 46/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0152
Epoch 47/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0151
Epoch 48/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0151
Epoch 49/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0150
Epoch 50/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0151
Epoch 51/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0150
Epoch 52/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0150
Epoch 53/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0149
Epoch 54/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0150
Epoch 55/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0147
Epoch 56/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0146
Epoch 57/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0148
Epoch 58/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0149
Epoch 59/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 0.0149
Epoch 60/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0150
Epoch 61/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0150
Epoch 62/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0148
Epoch 63/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0147
Epoch 64/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0146
Epoch 65/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0147
Epoch 66/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0150
Epoch 67/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0147
Epoch 68/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0150
Epoch 69/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0146
Epoch 70/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0147
Epoch 71/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0149
Epoch 72/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0146
Epoch 73/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0147
Epoch 74/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0146
Epoch 75/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0149
Epoch 76/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0146
Epoch 77/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0146
Epoch 78/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0145
Epoch 79/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0146
Epoch 80/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0146
Epoch 81/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0150
Epoch 82/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0146
Epoch 83/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0146
Epoch 84/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0146
Epoch 85/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0147
Epoch 86/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0146
Epoch 87/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0150
Epoch 88/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0146
Epoch 89/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0146
Epoch 90/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0149
Execution time:  31.463290214538574
DNN:
Mean Absolute Error: 0.0164
Root Mean Square Error: 0.0300
Mean Square Error: 0.0009

Train RMSE: 0.030
Train MSE: 0.001
Train MAE: 0.016
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_77&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_231 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_232 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_77 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_233 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 6ms/step - loss: 0.2332 - val_loss: 0.0476
Epoch 2/52
56/56 [==============================] - 0s 3ms/step - loss: 0.1069 - val_loss: 0.0161
Epoch 3/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0888 - val_loss: 0.0176
Epoch 4/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0816 - val_loss: 0.0189
Epoch 5/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0744 - val_loss: 0.0161
Epoch 6/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0674 - val_loss: 0.0136
Epoch 7/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.0135
Epoch 8/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.0138
Epoch 9/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.0138
Epoch 10/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0461 - val_loss: 0.0137
Epoch 11/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.0135
Epoch 12/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.0135
Epoch 13/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0137
Epoch 14/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0135
Epoch 15/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0137
Epoch 16/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0142
Epoch 17/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0140
Epoch 18/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0143
Epoch 19/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0146
Epoch 20/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0147
Epoch 21/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 22/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0150
Epoch 23/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0147
Epoch 24/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0146
Epoch 25/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0147
Epoch 26/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0150
Epoch 27/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0146
Epoch 28/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0145
Epoch 29/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0146
Epoch 30/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0144
Epoch 31/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0145
Epoch 32/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0144
Epoch 33/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0145
Epoch 34/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0142
Epoch 35/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0144
Epoch 36/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0141
Epoch 37/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0144
Epoch 38/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0140
Epoch 39/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0145
Epoch 40/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0141
Epoch 41/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0144
Epoch 42/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0141
Epoch 43/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0145
Epoch 44/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0140
Epoch 45/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0143
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0140
Epoch 47/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0142
Epoch 48/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0141
Epoch 49/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0142
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0142
Epoch 51/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0140
Epoch 52/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0143
Execution time:  9.001029968261719
DNN:
Mean Absolute Error: 0.0178
Root Mean Square Error: 0.0268
Mean Square Error: 0.0007

Train RMSE: 0.027
Train MSE: 0.001
Train MAE: 0.018
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_78&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_234 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_235 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_78 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_236 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
115/115 [==============================] - 0s 4ms/step - loss: 0.0459 - val_loss: 0.0261
Epoch 2/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.0221
Epoch 3/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.0218
Epoch 4/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.0204
Epoch 5/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0199
Epoch 6/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0190
Epoch 7/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0191
Epoch 8/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0180
Epoch 9/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0177
Epoch 10/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0174
Epoch 11/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0171
Epoch 12/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0167
Epoch 13/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0160
Epoch 14/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0157
Epoch 15/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0175
Epoch 16/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0153
Epoch 17/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0144
Epoch 18/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0145
Epoch 19/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0141
Epoch 20/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0139
Epoch 21/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0137
Epoch 22/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0136
Epoch 23/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0134
Epoch 24/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0132
Epoch 25/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0131
Epoch 26/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0129
Epoch 27/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0128
Epoch 28/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 29/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0126
Epoch 30/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0126
Epoch 31/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0126
Epoch 32/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0125
Epoch 33/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0122
Epoch 34/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0123
Epoch 35/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0125
Epoch 36/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0123
Epoch 37/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0123
Epoch 38/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0125
Epoch 39/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0129
Epoch 40/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0126
Epoch 41/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0127
Epoch 42/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0127
Epoch 43/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0121
Epoch 44/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0124
Epoch 45/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0119
Epoch 46/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0120
Epoch 47/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0125
Epoch 48/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0124
Epoch 49/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0129
Epoch 50/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0128
Epoch 51/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0132
Epoch 52/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0124
Epoch 53/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0136
Epoch 54/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0130
Epoch 55/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0133
Epoch 56/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0135
Epoch 57/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0131
Epoch 58/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0138
Epoch 59/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0127
Epoch 60/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0137
Epoch 61/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0140
Epoch 62/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0135
Epoch 63/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0138
Epoch 64/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0135
Epoch 65/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0144
Epoch 66/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0134
Epoch 67/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0143
Epoch 68/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0146
Epoch 69/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0140
Epoch 70/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0148
Epoch 71/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0139
Epoch 72/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0146
Epoch 73/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0145
Epoch 74/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0146
Epoch 75/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0144
Epoch 76/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0141
Epoch 77/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0145
Epoch 78/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0149
Epoch 79/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0172
Epoch 80/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0139
Execution time:  26.19046950340271
DNN:
Mean Absolute Error: 0.0178
Root Mean Square Error: 0.0315
Mean Square Error: 0.0010

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.018
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_79&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_237 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_238 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_79 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_239 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 1s 4ms/step - loss: 0.0385 - val_loss: 0.0292
Epoch 2/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0229
Epoch 3/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0222
Epoch 4/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0199
Epoch 5/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0199
Epoch 6/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0211
Epoch 7/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0248
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 0.0223
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0211
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0187
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0183
Epoch 12/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0174
Epoch 13/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0175
Epoch 14/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0165
Epoch 15/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0165
Epoch 16/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0163
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0154
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0155
Epoch 19/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0240
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0151
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0144
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0152
Epoch 23/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0168
Epoch 24/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0143
Epoch 25/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0138
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0143
Epoch 27/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0140
Epoch 28/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0138
Epoch 29/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0137
Epoch 30/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0132
Epoch 31/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0135
Epoch 32/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0132
Epoch 33/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0130
Epoch 34/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0134
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0136
Epoch 36/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0135
Epoch 37/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0129
Epoch 38/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0133
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0133
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0134
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0129
Epoch 42/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0134
Epoch 43/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0138
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0129
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0133
Epoch 46/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0136
Epoch 47/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0134
Epoch 48/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0128
Epoch 49/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0128
Epoch 50/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0135
Epoch 51/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0176
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0160
Epoch 53/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0154
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0156
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0152
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0155
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0158
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0165
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0160
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0160
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0200
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0160
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0158
Epoch 64/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0196
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0158
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0158
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0168
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0191
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0165
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0167
Epoch 71/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0158
Epoch 72/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0171
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0158
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0159
Epoch 75/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0163
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0160
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0155
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0163
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0167
Epoch 80/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0165
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0155
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0158
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0156
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0156
Epoch 85/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0164
Epoch 86/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0156
Epoch 87/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0155
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0162
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0156
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0158
Execution time:  33.45890426635742
DNN:
Mean Absolute Error: 0.0162
Root Mean Square Error: 0.0258
Mean Square Error: 0.0007

Train RMSE: 0.026
Train MSE: 0.001
Train MAE: 0.016
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_80&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_240 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_241 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_80 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_242 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 7ms/step - loss: 0.0782 - val_loss: 0.0818
Epoch 2/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.0383
Epoch 3/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.0346
Epoch 4/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.0331
Epoch 5/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.0325
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.0317
Epoch 7/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0309
Epoch 8/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.0295
Epoch 9/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0276
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0264
Epoch 11/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0254
Epoch 12/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0245
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0235
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0226
Epoch 15/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0217
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0212
Epoch 17/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0204
Epoch 18/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0196
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0190
Epoch 20/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 21/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0175
Epoch 22/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0170
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0164
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0157
Epoch 25/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0150
Epoch 26/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0147
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0143
Epoch 28/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0141
Epoch 29/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0137
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0137
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0133
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0131
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0129
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0128
Epoch 35/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0126
Epoch 36/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0125
Epoch 37/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0126
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0123
Epoch 39/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0122
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0128
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0129
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0128
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0125
Epoch 44/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0123
Epoch 45/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0125
Epoch 46/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0126
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0128
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0126
Epoch 50/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0124
Epoch 51/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0124
Epoch 52/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0122
Execution time:  9.482357501983643
DNN:
Mean Absolute Error: 0.0212
Root Mean Square Error: 0.0367
Mean Square Error: 0.0013

Train RMSE: 0.037
Train MSE: 0.001
Train MAE: 0.021
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_81&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_243 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_244 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_81 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_245 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 5ms/step - loss: 0.0485 - val_loss: 0.0283
Epoch 2/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.0188
Epoch 3/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.0273
Epoch 4/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0260
Epoch 5/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.0251
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.0243
Epoch 7/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0236
Epoch 8/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0234
Epoch 9/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0223
Epoch 10/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0220
Epoch 11/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0214
Epoch 12/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0210
Epoch 13/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0207
Epoch 14/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0205
Epoch 15/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0201
Epoch 16/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0185
Epoch 17/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0194
Epoch 18/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0191
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0190
Epoch 20/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0188
Epoch 21/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0186
Epoch 22/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0184
Epoch 23/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0182
Epoch 24/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0181
Epoch 25/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0189
Epoch 26/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0180
Epoch 27/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0177
Epoch 28/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0175
Epoch 29/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0173
Epoch 30/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0171
Epoch 31/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0170
Epoch 32/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0169
Epoch 33/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0169
Epoch 34/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0168
Epoch 35/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0167
Epoch 36/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0167
Epoch 37/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0166
Epoch 38/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0166
Epoch 39/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0162
Epoch 40/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0166
Epoch 41/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0166
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0166
Epoch 43/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0162
Epoch 44/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0165
Epoch 45/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0165
Epoch 46/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0164
Epoch 47/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0164
Epoch 48/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0164
Epoch 49/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0164
Epoch 50/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0164
Epoch 51/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0164
Epoch 52/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0164
Epoch 53/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0164
Epoch 54/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0164
Epoch 55/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0165
Epoch 56/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0154
Epoch 57/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0165
Epoch 58/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0166
Epoch 59/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0167
Epoch 60/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0167
Epoch 61/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0168
Epoch 62/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0167
Epoch 63/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0167
Epoch 64/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0165
Epoch 65/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0165
Epoch 66/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0165
Epoch 67/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0164
Epoch 68/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0165
Epoch 69/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0165
Epoch 70/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0164
Epoch 71/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0165
Epoch 72/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0163
Epoch 73/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0163
Epoch 74/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0166
Epoch 75/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0165
Epoch 76/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0166
Epoch 77/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0167
Epoch 78/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0169
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0169
Epoch 80/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0169
Execution time:  24.69905686378479
DNN:
Mean Absolute Error: 0.0187
Root Mean Square Error: 0.0328
Mean Square Error: 0.0011

Train RMSE: 0.033
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_82&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_246 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_247 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_82 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_248 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
127/127 [==============================] - 1s 4ms/step - loss: 0.0422 - val_loss: 0.0180
Epoch 2/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.0185
Epoch 3/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.0178
Epoch 4/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0180
Epoch 5/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 0.0180
Epoch 6/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0183
Epoch 7/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0182
Epoch 8/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0179
Epoch 9/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0180
Epoch 10/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0178
Epoch 11/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0175
Epoch 12/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0176
Epoch 13/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0176
Epoch 14/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0175
Epoch 15/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0175
Epoch 16/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0175
Epoch 17/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0174
Epoch 18/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0175
Epoch 19/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0309 - val_loss: 0.0174
Epoch 20/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0173
Epoch 21/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0171
Epoch 22/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0171
Epoch 23/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0171
Epoch 24/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0170
Epoch 25/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0169
Epoch 26/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0168
Epoch 27/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0167
Epoch 28/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0166
Epoch 29/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0165
Epoch 30/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0164
Epoch 31/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0164
Epoch 32/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0162
Epoch 33/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0161
Epoch 34/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0159
Epoch 35/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0157
Epoch 36/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0156
Epoch 37/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0153
Epoch 38/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0152
Epoch 39/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0150
Epoch 40/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0151
Epoch 41/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0149
Epoch 42/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 43/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0147
Epoch 44/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0147
Epoch 45/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0147
Epoch 46/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0146
Epoch 47/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0145
Epoch 48/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0146
Epoch 49/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0144
Epoch 50/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0144
Epoch 51/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0144
Epoch 52/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0144
Epoch 53/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0143
Epoch 54/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0142
Epoch 55/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0141
Epoch 56/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0141
Epoch 57/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0141
Epoch 58/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0141
Epoch 59/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0142
Epoch 60/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0142
Epoch 61/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0143
Epoch 62/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0142
Epoch 63/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0144
Epoch 64/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0146
Epoch 65/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0201 - val_loss: 0.0151
Epoch 66/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 0.0151
Epoch 67/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0158
Epoch 68/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0145
Epoch 69/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0197 - val_loss: 0.0146
Epoch 70/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0157
Epoch 71/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0157
Epoch 72/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0157
Epoch 73/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0155
Epoch 74/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0158
Epoch 75/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0156
Epoch 76/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0146
Epoch 77/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0153
Epoch 78/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0148
Epoch 79/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0149
Epoch 80/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0151
Epoch 81/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0157
Epoch 82/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0154
Epoch 83/90
127/127 [==============================] - ETA: 0s - loss: 0.020 - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0151
Epoch 84/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0164
Epoch 85/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0164
Epoch 86/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0153
Epoch 87/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0153
Epoch 88/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0148
Epoch 89/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0165
Epoch 90/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0157
Execution time:  32.04720616340637
DNN:
Mean Absolute Error: 0.0146
Root Mean Square Error: 0.0283
Mean Square Error: 0.0008

Train RMSE: 0.028
Train MSE: 0.001
Train MAE: 0.015
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_83&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_249 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_250 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_83 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_251 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 6ms/step - loss: 0.0646 - val_loss: 0.0628
Epoch 2/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.0530
Epoch 3/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.0431
Epoch 4/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0379
Epoch 5/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.0344
Epoch 6/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.0321
Epoch 7/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.0262
Epoch 8/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0214
Epoch 9/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0246
Epoch 10/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0245
Epoch 11/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0244
Epoch 12/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0241
Epoch 13/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0239
Epoch 14/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0235
Epoch 15/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0231
Epoch 16/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0229
Epoch 17/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0223
Epoch 18/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0219
Epoch 19/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0213
Epoch 20/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0208
Epoch 21/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0204
Epoch 22/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0200
Epoch 23/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0196
Epoch 24/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0193
Epoch 25/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0189
Epoch 26/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0186
Epoch 27/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0183
Epoch 28/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0181
Epoch 29/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0177
Epoch 30/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0175
Epoch 31/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0173
Epoch 32/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0171
Epoch 33/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0170
Epoch 34/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0168
Epoch 35/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0167
Epoch 36/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0165
Epoch 37/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0164
Epoch 38/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0162
Epoch 39/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0156
Epoch 40/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0156
Epoch 41/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0155
Epoch 42/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0154
Epoch 43/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0153
Epoch 44/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0153
Epoch 45/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0152
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 47/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0151
Epoch 48/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0150
Epoch 49/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0150
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0149
Epoch 51/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0149
Epoch 52/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0148
Execution time:  8.913937091827393
DNN:
Mean Absolute Error: 0.0198
Root Mean Square Error: 0.0366
Mean Square Error: 0.0013

Train RMSE: 0.037
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_84&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_252 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_253 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_84 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_254 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
115/115 [==============================] - 0s 4ms/step - loss: 0.3305 - val_loss: 0.2991
Epoch 2/80
115/115 [==============================] - 0s 3ms/step - loss: 0.3272 - val_loss: 0.2957
Epoch 3/80
115/115 [==============================] - 0s 3ms/step - loss: 0.3234 - val_loss: 0.2919
Epoch 4/80
115/115 [==============================] - 0s 3ms/step - loss: 0.3189 - val_loss: 0.2879
Epoch 5/80
115/115 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 0.2835
Epoch 6/80
115/115 [==============================] - 0s 3ms/step - loss: 0.3103 - val_loss: 0.2790
Epoch 7/80
115/115 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 0.2742
Epoch 8/80
115/115 [==============================] - 0s 3ms/step - loss: 0.3004 - val_loss: 0.2691
Epoch 9/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2947 - val_loss: 0.2638
Epoch 10/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2891 - val_loss: 0.2583
Epoch 11/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2837 - val_loss: 0.2526
Epoch 12/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2777 - val_loss: 0.2467
Epoch 13/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2715 - val_loss: 0.2406
Epoch 14/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2647 - val_loss: 0.2344
Epoch 15/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2586 - val_loss: 0.2283
Epoch 16/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2521 - val_loss: 0.2221
Epoch 17/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2455 - val_loss: 0.2158
Epoch 18/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2390 - val_loss: 0.2092
Epoch 19/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2321 - val_loss: 0.2025
Epoch 20/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2249 - val_loss: 0.1956
Epoch 21/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2180 - val_loss: 0.1895
Epoch 22/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2116 - val_loss: 0.1833
Epoch 23/80
115/115 [==============================] - 0s 3ms/step - loss: 0.2051 - val_loss: 0.1770
Epoch 24/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1983 - val_loss: 0.1705
Epoch 25/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1916 - val_loss: 0.1640
Epoch 26/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1850 - val_loss: 0.1573
Epoch 27/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1782 - val_loss: 0.1505
Epoch 28/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1712 - val_loss: 0.1436
Epoch 29/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1639 - val_loss: 0.1368
Epoch 30/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1572 - val_loss: 0.1304
Epoch 31/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1508 - val_loss: 0.1240
Epoch 32/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 0.1177
Epoch 33/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1401 - val_loss: 0.1115
Epoch 34/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1345 - val_loss: 0.1055
Epoch 35/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 0.0997
Epoch 36/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 0.0942
Epoch 37/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1216 - val_loss: 0.0888
Epoch 38/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1175 - val_loss: 0.0839
Epoch 39/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1147 - val_loss: 0.0791
Epoch 40/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 0.0746
Epoch 41/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1095 - val_loss: 0.0703
Epoch 42/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1071 - val_loss: 0.0663
Epoch 43/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 0.0625
Epoch 44/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.0590
Epoch 45/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 0.0558
Epoch 46/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1010 - val_loss: 0.0528
Epoch 47/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 0.0501
Epoch 48/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 0.0475
Epoch 49/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.0451
Epoch 50/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 0.0429
Epoch 51/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.0409
Epoch 52/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 0.0390
Epoch 53/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.0372
Epoch 54/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0946 - val_loss: 0.0356
Epoch 55/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0944 - val_loss: 0.0341
Epoch 56/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 0.0327
Epoch 57/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 0.0313
Epoch 58/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0939 - val_loss: 0.0301
Epoch 59/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 0.0289
Epoch 60/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 0.0277
Epoch 61/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0930 - val_loss: 0.0267
Epoch 62/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0927 - val_loss: 0.0258
Epoch 63/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 0.0248
Epoch 64/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 0.0239
Epoch 65/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.0230
Epoch 66/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.0222
Epoch 67/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0917 - val_loss: 0.0215
Epoch 68/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 0.0208
Epoch 69/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 0.0201
Epoch 70/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0909 - val_loss: 0.0195
Epoch 71/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0909 - val_loss: 0.0189
Epoch 72/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 0.0184
Epoch 73/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 0.0178
Epoch 74/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 0.0173
Epoch 75/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 0.0168
Epoch 76/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 0.0164
Epoch 77/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 0.0160
Epoch 78/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 0.0156
Epoch 79/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0900 - val_loss: 0.0153
Epoch 80/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 0.0149
Execution time:  25.81791377067566
DNN:
Mean Absolute Error: 0.0310
Root Mean Square Error: 0.0374
Mean Square Error: 0.0014

Train RMSE: 0.037
Train MSE: 0.001
Train MAE: 0.031
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_85&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_255 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_256 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_85 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_257 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 1s 4ms/step - loss: 0.3511 - val_loss: 0.3186
Epoch 2/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3489 - val_loss: 0.3163
Epoch 3/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3464 - val_loss: 0.3138
Epoch 4/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3435 - val_loss: 0.3110
Epoch 5/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 0.3080
Epoch 6/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3373 - val_loss: 0.3047
Epoch 7/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3339 - val_loss: 0.3013
Epoch 8/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3303 - val_loss: 0.2977
Epoch 9/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 0.2940
Epoch 10/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3225 - val_loss: 0.2903
Epoch 11/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3186 - val_loss: 0.2865
Epoch 12/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3146 - val_loss: 0.2826
Epoch 13/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3104 - val_loss: 0.2785
Epoch 14/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3061 - val_loss: 0.2742
Epoch 15/90
143/143 [==============================] - 0s 3ms/step - loss: 0.3016 - val_loss: 0.2698
Epoch 16/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2969 - val_loss: 0.2652
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2921 - val_loss: 0.2605
Epoch 18/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2871 - val_loss: 0.2556
Epoch 19/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2819 - val_loss: 0.2506
Epoch 20/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 0.2454
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2711 - val_loss: 0.2400
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2655 - val_loss: 0.2345
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2595 - val_loss: 0.2287
Epoch 24/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2537 - val_loss: 0.2228
Epoch 25/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2474 - val_loss: 0.2166
Epoch 26/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2408 - val_loss: 0.2103
Epoch 27/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2343 - val_loss: 0.2037
Epoch 28/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 0.1969
Epoch 29/90
143/143 [==============================] - 0s 3ms/step - loss: 0.2203 - val_loss: 0.1900
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2129 - val_loss: 0.1828
Epoch 31/90
143/143 [==============================] - 0s 2ms/step - loss: 0.2053 - val_loss: 0.1754
Epoch 32/90
143/143 [==============================] - 0s 3ms/step - loss: 0.1975 - val_loss: 0.1678
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1895 - val_loss: 0.1598
Epoch 34/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1812 - val_loss: 0.1516
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1726 - val_loss: 0.1431
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1638 - val_loss: 0.1344
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.1253
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1450 - val_loss: 0.1160
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1354 - val_loss: 0.1063
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.0963
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.0860
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.1043 - val_loss: 0.0755
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0936 - val_loss: 0.0649
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.0540
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0723 - val_loss: 0.0432
Epoch 46/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0624 - val_loss: 0.0333
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0564 - val_loss: 0.0274
Epoch 48/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0541 - val_loss: 0.0238
Epoch 49/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0214
Epoch 50/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.0197
Epoch 51/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.0185
Epoch 52/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.0175
Epoch 53/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0168
Epoch 54/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.0161
Epoch 55/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.0155
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0512 - val_loss: 0.0149
Epoch 57/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.0144
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0512 - val_loss: 0.0139
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0134
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.0129
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.0125
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0508 - val_loss: 0.0121
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0117
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0113
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0507 - val_loss: 0.0109
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.0106
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.0102
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0508 - val_loss: 0.0099
Epoch 69/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.0096
Epoch 70/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.0093
Epoch 71/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0090
Epoch 72/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.0088
Epoch 73/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.0085
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.0083
Epoch 75/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.0081
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0501 - val_loss: 0.0079
Epoch 77/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0077
Epoch 78/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.0075
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.0073
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.0072
Epoch 81/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0070
Epoch 82/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0069
Epoch 83/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.0067
Epoch 84/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.0066
Epoch 85/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0064
Epoch 86/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.0063
Epoch 87/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.0062
Epoch 88/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.0061
Epoch 89/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.0060
Epoch 90/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.0058
Execution time:  33.890835762023926
DNN:
Mean Absolute Error: 0.0229
Root Mean Square Error: 0.0328
Mean Square Error: 0.0011

Train RMSE: 0.033
Train MSE: 0.001
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_86&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_258 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_259 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_86 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_260 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
 1/63 [..............................] - ETA: 0s - loss: 0.2465WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0131s). Check your callbacks.
63/63 [==============================] - 0s 6ms/step - loss: 0.2205 - val_loss: 0.1973
Epoch 2/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2202 - val_loss: 0.1967
Epoch 3/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2191 - val_loss: 0.1960
Epoch 4/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2184 - val_loss: 0.1952
Epoch 5/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 0.1945
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.2173 - val_loss: 0.1937
Epoch 7/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 0.1929
Epoch 8/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2157 - val_loss: 0.1920
Epoch 9/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 0.1911
Epoch 10/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2140 - val_loss: 0.1902
Epoch 11/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2138 - val_loss: 0.1893
Epoch 12/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2122 - val_loss: 0.1884
Epoch 13/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 0.1874
Epoch 14/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2108 - val_loss: 0.1864
Epoch 15/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 0.1854
Epoch 16/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2093 - val_loss: 0.1844
Epoch 17/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2079 - val_loss: 0.1833
Epoch 18/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2070 - val_loss: 0.1822
Epoch 19/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2058 - val_loss: 0.1811
Epoch 20/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2051 - val_loss: 0.1800
Epoch 21/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2036 - val_loss: 0.1789
Epoch 22/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2034 - val_loss: 0.1777
Epoch 23/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2020 - val_loss: 0.1765
Epoch 24/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2005 - val_loss: 0.1754
Epoch 25/52
63/63 [==============================] - 0s 3ms/step - loss: 0.2002 - val_loss: 0.1742
Epoch 26/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1986 - val_loss: 0.1730
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1977 - val_loss: 0.1718
Epoch 28/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1968 - val_loss: 0.1705
Epoch 29/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 0.1693
Epoch 30/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1952 - val_loss: 0.1680
Epoch 31/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1936 - val_loss: 0.1668
Epoch 32/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 0.1655
Epoch 33/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1916 - val_loss: 0.1643
Epoch 34/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1908 - val_loss: 0.1630
Epoch 35/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1900 - val_loss: 0.1617
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1887 - val_loss: 0.1604
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.1591
Epoch 38/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1862 - val_loss: 0.1578
Epoch 39/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1856 - val_loss: 0.1565
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1847 - val_loss: 0.1552
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1838 - val_loss: 0.1538
Epoch 42/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1823 - val_loss: 0.1525
Epoch 43/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1820 - val_loss: 0.1512
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1811 - val_loss: 0.1498
Epoch 45/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1797 - val_loss: 0.1485
Epoch 46/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1785 - val_loss: 0.1471
Epoch 47/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1779 - val_loss: 0.1457
Epoch 48/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1775 - val_loss: 0.1444
Epoch 49/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1762 - val_loss: 0.1430
Epoch 50/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1753 - val_loss: 0.1416
Epoch 51/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1735 - val_loss: 0.1401
Epoch 52/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1726 - val_loss: 0.1385
Execution time:  9.555951118469238
DNN:
Mean Absolute Error: 0.1553
Root Mean Square Error: 0.1567
Mean Square Error: 0.0246

Train RMSE: 0.157
Train MSE: 0.025
Train MAE: 0.155
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_87&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_261 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_262 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_87 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_263 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 5ms/step - loss: 0.3936 - val_loss: 0.3709
Epoch 2/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3911 - val_loss: 0.3684
Epoch 3/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3884 - val_loss: 0.3657
Epoch 4/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3857 - val_loss: 0.3628
Epoch 5/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3826 - val_loss: 0.3597
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3794 - val_loss: 0.3565
Epoch 7/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3758 - val_loss: 0.3530
Epoch 8/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3723 - val_loss: 0.3495
Epoch 9/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3685 - val_loss: 0.3458
Epoch 10/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3649 - val_loss: 0.3423
Epoch 11/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3611 - val_loss: 0.3388
Epoch 12/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3576 - val_loss: 0.3351
Epoch 13/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3538 - val_loss: 0.3317
Epoch 14/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 0.3281
Epoch 15/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3465 - val_loss: 0.3245
Epoch 16/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3429 - val_loss: 0.3208
Epoch 17/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3392 - val_loss: 0.3171
Epoch 18/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3355 - val_loss: 0.3132
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3314 - val_loss: 0.3093
Epoch 20/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3273 - val_loss: 0.3053
Epoch 21/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3231 - val_loss: 0.3013
Epoch 22/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3191 - val_loss: 0.2971
Epoch 23/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3144 - val_loss: 0.2929
Epoch 24/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3102 - val_loss: 0.2885
Epoch 25/80
103/103 [==============================] - ETA: 0s - loss: 0.310 - 0s 3ms/step - loss: 0.3057 - val_loss: 0.2841
Epoch 26/80
103/103 [==============================] - 0s 3ms/step - loss: 0.3014 - val_loss: 0.2796
Epoch 27/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2964 - val_loss: 0.2739
Epoch 28/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2901 - val_loss: 0.2678
Epoch 29/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2841 - val_loss: 0.2616
Epoch 30/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2774 - val_loss: 0.2553
Epoch 31/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2712 - val_loss: 0.2491
Epoch 32/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2649 - val_loss: 0.2428
Epoch 33/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2582 - val_loss: 0.2365
Epoch 34/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2516 - val_loss: 0.2300
Epoch 35/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2448 - val_loss: 0.2234
Epoch 36/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2381 - val_loss: 0.2166
Epoch 37/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2318 - val_loss: 0.2098
Epoch 38/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2242 - val_loss: 0.2027
Epoch 39/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 0.1956
Epoch 40/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2102 - val_loss: 0.1882
Epoch 41/80
103/103 [==============================] - 0s 3ms/step - loss: 0.2026 - val_loss: 0.1807
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1951 - val_loss: 0.1731
Epoch 43/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 0.1653
Epoch 44/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1796 - val_loss: 0.1573
Epoch 45/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1711 - val_loss: 0.1491
Epoch 46/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1633 - val_loss: 0.1411
Epoch 47/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 0.1337
Epoch 48/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1496 - val_loss: 0.1265
Epoch 49/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1437 - val_loss: 0.1195
Epoch 50/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1385 - val_loss: 0.1129
Epoch 51/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 0.1068
Epoch 52/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 0.1011
Epoch 53/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 0.0959
Epoch 54/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.0910
Epoch 55/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1211 - val_loss: 0.0867
Epoch 56/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 0.0827
Epoch 57/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1166 - val_loss: 0.0791
Epoch 58/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 0.0758
Epoch 59/80
103/103 [==============================] - ETA: 0s - loss: 0.117 - 0s 3ms/step - loss: 0.1146 - val_loss: 0.0726
Epoch 60/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 0.0698
Epoch 61/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 0.0671
Epoch 62/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1112 - val_loss: 0.0646
Epoch 63/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 0.0622
Epoch 64/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1094 - val_loss: 0.0599
Epoch 65/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1092 - val_loss: 0.0578
Epoch 66/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 0.0558
Epoch 67/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1073 - val_loss: 0.0539
Epoch 68/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1071 - val_loss: 0.0521
Epoch 69/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 0.0504
Epoch 70/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 0.0488
Epoch 71/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 0.0473
Epoch 72/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 0.0459
Epoch 73/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1050 - val_loss: 0.0446
Epoch 74/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 0.0433
Epoch 75/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 0.0421
Epoch 76/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1036 - val_loss: 0.0409
Epoch 77/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 0.0399
Epoch 78/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1030 - val_loss: 0.0390
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.0382
Epoch 80/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 0.0374
Execution time:  24.300387620925903
DNN:
Mean Absolute Error: 0.0479
Root Mean Square Error: 0.0539
Mean Square Error: 0.0029

Train RMSE: 0.054
Train MSE: 0.003
Train MAE: 0.048
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_88&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_264 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_265 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_88 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_266 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
127/127 [==============================] - 0s 4ms/step - loss: 0.2590 - val_loss: 0.2422
Epoch 2/90
127/127 [==============================] - 0s 3ms/step - loss: 0.2543 - val_loss: 0.2374
Epoch 3/90
127/127 [==============================] - 0s 3ms/step - loss: 0.2490 - val_loss: 0.2321
Epoch 4/90
127/127 [==============================] - 0s 3ms/step - loss: 0.2436 - val_loss: 0.2263
Epoch 5/90
127/127 [==============================] - 0s 3ms/step - loss: 0.2374 - val_loss: 0.2202
Epoch 6/90
127/127 [==============================] - 0s 3ms/step - loss: 0.2310 - val_loss: 0.2136
Epoch 7/90
127/127 [==============================] - 0s 3ms/step - loss: 0.2239 - val_loss: 0.2067
Epoch 8/90
127/127 [==============================] - 0s 2ms/step - loss: 0.2170 - val_loss: 0.1994
Epoch 9/90
127/127 [==============================] - 0s 2ms/step - loss: 0.2094 - val_loss: 0.1918
Epoch 10/90
127/127 [==============================] - 0s 3ms/step - loss: 0.2015 - val_loss: 0.1839
Epoch 11/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1933 - val_loss: 0.1757
Epoch 12/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 0.1672
Epoch 13/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1761 - val_loss: 0.1585
Epoch 14/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 0.1501
Epoch 15/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1584 - val_loss: 0.1416
Epoch 16/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1498 - val_loss: 0.1329
Epoch 17/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1409 - val_loss: 0.1240
Epoch 18/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 0.1148
Epoch 19/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 0.1054
Epoch 20/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1128 - val_loss: 0.0957
Epoch 21/90
127/127 [==============================] - 0s 3ms/step - loss: 0.1030 - val_loss: 0.0859
Epoch 22/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0759
Epoch 23/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0833 - val_loss: 0.0658
Epoch 24/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.0559
Epoch 25/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0650 - val_loss: 0.0466
Epoch 26/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.0396
Epoch 27/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0541 - val_loss: 0.0340
Epoch 28/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.0296
Epoch 29/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0261
Epoch 30/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0484 - val_loss: 0.0235
Epoch 31/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.0214
Epoch 32/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.0197
Epoch 33/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.0182
Epoch 34/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0462 - val_loss: 0.0170
Epoch 35/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.0160
Epoch 36/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.0151
Epoch 37/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.0144
Epoch 38/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.0136
Epoch 39/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.0131
Epoch 40/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.0126
Epoch 41/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0121
Epoch 42/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0120
Epoch 43/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.0119
Epoch 44/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.0119
Epoch 45/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.0119
Epoch 46/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0118
Epoch 47/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.0118
Epoch 48/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0118
Epoch 49/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.0118
Epoch 50/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0118
Epoch 51/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0118
Epoch 52/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0117
Epoch 53/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0117
Epoch 54/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0117
Epoch 55/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0117
Epoch 56/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0117
Epoch 57/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0117
Epoch 58/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0117
Epoch 59/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0117
Epoch 60/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0117
Epoch 61/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0117
Epoch 62/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0117
Epoch 63/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0117
Epoch 64/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0117
Epoch 65/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0117
Epoch 66/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0117
Epoch 67/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0117
Epoch 68/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0117
Epoch 69/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0117
Epoch 70/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0117
Epoch 71/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0117
Epoch 72/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0117
Epoch 73/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0117
Epoch 74/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0117
Epoch 75/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0117
Epoch 76/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0117
Epoch 77/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0117
Epoch 78/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0117
Epoch 79/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.0117
Epoch 80/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0117
Epoch 81/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.0117
Epoch 82/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.0117
Epoch 83/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.0117
Epoch 84/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0117
Epoch 85/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.0117
Epoch 86/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0117
Epoch 87/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0117
Epoch 88/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.0117
Epoch 89/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.0117
Epoch 90/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0117
Execution time:  31.515806436538696
DNN:
Mean Absolute Error: 0.0121
Root Mean Square Error: 0.0214
Mean Square Error: 0.0005

Train RMSE: 0.021
Train MSE: 0.000
Train MAE: 0.012
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_89&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_267 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_268 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_89 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_269 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 6ms/step - loss: 0.5175 - val_loss: 0.4909
Epoch 2/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5167 - val_loss: 0.4903
Epoch 3/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5164 - val_loss: 0.4896
Epoch 4/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5152 - val_loss: 0.4889
Epoch 5/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5148 - val_loss: 0.4881
Epoch 6/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5141 - val_loss: 0.4873
Epoch 7/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5125 - val_loss: 0.4865
Epoch 8/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5113 - val_loss: 0.4856
Epoch 9/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5114 - val_loss: 0.4848
Epoch 10/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5104 - val_loss: 0.4838
Epoch 11/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5100 - val_loss: 0.4829
Epoch 12/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5085 - val_loss: 0.4820
Epoch 13/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5073 - val_loss: 0.4810
Epoch 14/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5065 - val_loss: 0.4800
Epoch 15/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5055 - val_loss: 0.4790
Epoch 16/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5048 - val_loss: 0.4779
Epoch 17/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5031 - val_loss: 0.4768
Epoch 18/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5027 - val_loss: 0.4757
Epoch 19/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5017 - val_loss: 0.4746
Epoch 20/52
56/56 [==============================] - 0s 3ms/step - loss: 0.5001 - val_loss: 0.4735
Epoch 21/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4995 - val_loss: 0.4723
Epoch 22/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4976 - val_loss: 0.4712
Epoch 23/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4974 - val_loss: 0.4700
Epoch 24/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4956 - val_loss: 0.4688
Epoch 25/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4942 - val_loss: 0.4675
Epoch 26/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4932 - val_loss: 0.4663
Epoch 27/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4912 - val_loss: 0.4650
Epoch 28/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4899 - val_loss: 0.4637
Epoch 29/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4896 - val_loss: 0.4624
Epoch 30/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4877 - val_loss: 0.4611
Epoch 31/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4867 - val_loss: 0.4597
Epoch 32/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4851 - val_loss: 0.4584
Epoch 33/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4834 - val_loss: 0.4570
Epoch 34/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4827 - val_loss: 0.4556
Epoch 35/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4806 - val_loss: 0.4542
Epoch 36/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4796 - val_loss: 0.4528
Epoch 37/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4780 - val_loss: 0.4513
Epoch 38/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4772 - val_loss: 0.4498
Epoch 39/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4754 - val_loss: 0.4484
Epoch 40/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4731 - val_loss: 0.4469
Epoch 41/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4729 - val_loss: 0.4454
Epoch 42/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4701 - val_loss: 0.4438
Epoch 43/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4689 - val_loss: 0.4423
Epoch 44/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4676 - val_loss: 0.4407
Epoch 45/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4658 - val_loss: 0.4392
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4650 - val_loss: 0.4376
Epoch 47/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4630 - val_loss: 0.4360
Epoch 48/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4613 - val_loss: 0.4343
Epoch 49/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4593 - val_loss: 0.4327
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4582 - val_loss: 0.4311
Epoch 51/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4565 - val_loss: 0.4294
Epoch 52/52
56/56 [==============================] - 0s 3ms/step - loss: 0.4545 - val_loss: 0.4277
Execution time:  9.281136751174927
DNN:
Mean Absolute Error: 0.4495
Root Mean Square Error: 0.4523
Mean Square Error: 0.2046

Train RMSE: 0.452
Train MSE: 0.205
Train MAE: 0.449
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_90&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_270 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_271 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_90 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_272 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
115/115 [==============================] - 0s 4ms/step - loss: 0.0944 - val_loss: 0.1280
Epoch 2/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 0.1274
Epoch 3/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 0.1267
Epoch 4/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0925 - val_loss: 0.1259
Epoch 5/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0917 - val_loss: 0.1252
Epoch 6/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 0.1244
Epoch 7/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 0.1235
Epoch 8/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 0.1226
Epoch 9/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 0.1217
Epoch 10/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 0.1208
Epoch 11/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 0.1198
Epoch 12/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0857 - val_loss: 0.1188
Epoch 13/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0848 - val_loss: 0.1178
Epoch 14/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.1167
Epoch 15/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 0.1157
Epoch 16/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 0.1146
Epoch 17/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0808 - val_loss: 0.1135
Epoch 18/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 0.1124
Epoch 19/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0788 - val_loss: 0.1114
Epoch 20/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 0.1104
Epoch 21/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0770 - val_loss: 0.1095
Epoch 22/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 0.1087
Epoch 23/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.1078
Epoch 24/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0747 - val_loss: 0.1070
Epoch 25/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.1061
Epoch 26/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 0.1053
Epoch 27/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.1045
Epoch 28/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0718 - val_loss: 0.1036
Epoch 29/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.1028
Epoch 30/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.1020
Epoch 31/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.1012
Epoch 32/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.1004
Epoch 33/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.0997
Epoch 34/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.0989
Epoch 35/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0672 - val_loss: 0.0981
Epoch 36/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.0974
Epoch 37/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.0966
Epoch 38/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.0959
Epoch 39/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0650 - val_loss: 0.0951
Epoch 40/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.0944
Epoch 41/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.0936
Epoch 42/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.0929
Epoch 43/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.0922
Epoch 44/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.0915
Epoch 45/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.0908
Epoch 46/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.0901
Epoch 47/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.0894
Epoch 48/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0888
Epoch 49/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0881
Epoch 50/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.0875
Epoch 51/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0868
Epoch 52/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.0862
Epoch 53/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.0856
Epoch 54/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.0850
Epoch 55/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.0844
Epoch 56/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.0838
Epoch 57/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.0832
Epoch 58/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0564 - val_loss: 0.0826
Epoch 59/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.0820
Epoch 60/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.0814
Epoch 61/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.0809
Epoch 62/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.0803
Epoch 63/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.0797
Epoch 64/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.0792
Epoch 65/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.0787
Epoch 66/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.0781
Epoch 67/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.0776
Epoch 68/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.0771
Epoch 69/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.0766
Epoch 70/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.0761
Epoch 71/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.0756
Epoch 72/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.0751
Epoch 73/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.0746
Epoch 74/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.0741
Epoch 75/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.0736
Epoch 76/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.0732
Epoch 77/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0508 - val_loss: 0.0727
Epoch 78/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.0722
Epoch 79/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.0718
Epoch 80/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0713
Execution time:  25.624168395996094
DNN:
Mean Absolute Error: 0.0491
Root Mean Square Error: 0.0573
Mean Square Error: 0.0033

Train RMSE: 0.057
Train MSE: 0.003
Train MAE: 0.049
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_91&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_273 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_274 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_91 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_275 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 1s 4ms/step - loss: 0.0901 - val_loss: 0.1238
Epoch 2/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 0.1231
Epoch 3/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0886 - val_loss: 0.1222
Epoch 4/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0878 - val_loss: 0.1214
Epoch 5/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0870 - val_loss: 0.1204
Epoch 6/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0860 - val_loss: 0.1195
Epoch 7/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0851 - val_loss: 0.1184
Epoch 8/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0841 - val_loss: 0.1173
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0830 - val_loss: 0.1162
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0819 - val_loss: 0.1152
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0810 - val_loss: 0.1143
Epoch 12/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.1134
Epoch 13/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0792 - val_loss: 0.1125
Epoch 14/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.1115
Epoch 15/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.1106
Epoch 16/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0764 - val_loss: 0.1091
Epoch 17/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0751 - val_loss: 0.1077
Epoch 18/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0738 - val_loss: 0.1063
Epoch 19/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0726 - val_loss: 0.1049
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0713 - val_loss: 0.1036
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0701 - val_loss: 0.1022
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0690 - val_loss: 0.1008
Epoch 23/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.0995
Epoch 24/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.0982
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0656 - val_loss: 0.0969
Epoch 26/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.0956
Epoch 27/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.0943
Epoch 28/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0623 - val_loss: 0.0929
Epoch 29/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0611 - val_loss: 0.0914
Epoch 30/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0898
Epoch 31/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0588 - val_loss: 0.0883
Epoch 32/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.0869
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0854
Epoch 34/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0555 - val_loss: 0.0840
Epoch 35/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.0827
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.0813
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0526 - val_loss: 0.0800
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0517 - val_loss: 0.0788
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0775
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0501 - val_loss: 0.0763
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.0751
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0739
Epoch 43/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.0728
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0717
Epoch 45/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.0706
Epoch 46/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.0695
Epoch 47/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.0685
Epoch 48/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0675
Epoch 49/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0665
Epoch 50/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0655
Epoch 51/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.0645
Epoch 52/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.0635
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.0626
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0616
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0607
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0598
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.0589
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0580
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0571
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0562
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.0554
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0545
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0537
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.0529
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0522
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 0.0514
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.0507
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.0500
Epoch 69/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0493
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.0486
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0479
Epoch 72/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0472
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0466
Epoch 74/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0459
Epoch 75/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0453
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0447
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0441
Epoch 78/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0436
Epoch 79/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431
Epoch 80/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0426
Epoch 81/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0421
Epoch 82/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0416
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0411
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0407
Epoch 85/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0402
Epoch 86/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0398
Epoch 87/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.0394
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0390
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0386
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.0383
Execution time:  33.28885006904602
DNN:
Mean Absolute Error: 0.0330
Root Mean Square Error: 0.0457
Mean Square Error: 0.0021

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.033
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_92&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_276 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_277 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_92 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_278 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 6ms/step - loss: 0.1066 - val_loss: 0.1391
Epoch 2/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.1389
Epoch 3/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1064 - val_loss: 0.1388
Epoch 4/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 0.1386
Epoch 5/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 0.1384
Epoch 6/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1059 - val_loss: 0.1382
Epoch 7/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1057 - val_loss: 0.1380
Epoch 8/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 0.1378
Epoch 9/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 0.1376
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1052 - val_loss: 0.1374
Epoch 11/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1047 - val_loss: 0.1372
Epoch 12/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1047 - val_loss: 0.1370
Epoch 13/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 0.1368
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.1365
Epoch 15/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 0.1363
Epoch 16/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1037 - val_loss: 0.1360
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 0.1358
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.1355
Epoch 19/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.1353
Epoch 20/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1027 - val_loss: 0.1350
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1026 - val_loss: 0.1348
Epoch 22/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1022 - val_loss: 0.1345
Epoch 23/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.1342
Epoch 24/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 0.1340
Epoch 25/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1014 - val_loss: 0.1337
Epoch 26/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1011 - val_loss: 0.1334
Epoch 27/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1009 - val_loss: 0.1331
Epoch 28/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1007 - val_loss: 0.1328
Epoch 29/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1003 - val_loss: 0.1326
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.1323
Epoch 31/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0998 - val_loss: 0.1320
Epoch 32/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0995 - val_loss: 0.1317
Epoch 33/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0993 - val_loss: 0.1314
Epoch 34/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.1311
Epoch 35/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0987 - val_loss: 0.1308
Epoch 36/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0984 - val_loss: 0.1305
Epoch 37/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 0.1302
Epoch 38/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.1299
Epoch 39/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 0.1295
Epoch 40/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0972 - val_loss: 0.1292
Epoch 41/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0969 - val_loss: 0.1289
Epoch 42/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.1286
Epoch 43/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.1283
Epoch 44/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.1280
Epoch 45/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0956 - val_loss: 0.1276
Epoch 46/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0952 - val_loss: 0.1273
Epoch 47/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.1270
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0947 - val_loss: 0.1267
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0946 - val_loss: 0.1263
Epoch 50/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 0.1260
Epoch 51/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0939 - val_loss: 0.1257
Epoch 52/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0934 - val_loss: 0.1253
Execution time:  9.504527568817139
DNN:
Mean Absolute Error: 0.0951
Root Mean Square Error: 0.1015
Mean Square Error: 0.0103

Train RMSE: 0.102
Train MSE: 0.010
Train MAE: 0.095
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_93&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_279 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_280 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_93 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_281 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 5ms/step - loss: 0.1103 - val_loss: 0.1302
Epoch 2/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1099 - val_loss: 0.1298
Epoch 3/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1095 - val_loss: 0.1294
Epoch 4/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 0.1289
Epoch 5/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 0.1285
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1082 - val_loss: 0.1280
Epoch 7/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1076 - val_loss: 0.1275
Epoch 8/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1071 - val_loss: 0.1270
Epoch 9/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.1264
Epoch 10/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1059 - val_loss: 0.1259
Epoch 11/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 0.1253
Epoch 12/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 0.1247
Epoch 13/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 0.1241
Epoch 14/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1035 - val_loss: 0.1235
Epoch 15/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.1228
Epoch 16/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1022 - val_loss: 0.1222
Epoch 17/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 0.1215
Epoch 18/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1208
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 0.1204
Epoch 20/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 0.1200
Epoch 21/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0993 - val_loss: 0.1197
Epoch 22/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 0.1194
Epoch 23/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 0.1190
Epoch 24/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 0.1187
Epoch 25/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 0.1183
Epoch 26/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.1180
Epoch 27/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0971 - val_loss: 0.1176
Epoch 28/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0968 - val_loss: 0.1173
Epoch 29/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.1169
Epoch 30/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.1166
Epoch 31/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0956 - val_loss: 0.1162
Epoch 32/80
103/103 [==============================] - ETA: 0s - loss: 0.090 - 0s 3ms/step - loss: 0.0953 - val_loss: 0.1158
Epoch 33/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0949 - val_loss: 0.1155
Epoch 34/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 0.1151
Epoch 35/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 0.1147
Epoch 36/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0937 - val_loss: 0.1144
Epoch 37/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 0.1140
Epoch 38/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 0.1138
Epoch 39/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0928 - val_loss: 0.1136
Epoch 40/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0925 - val_loss: 0.1133
Epoch 41/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0922 - val_loss: 0.1131
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 0.1128
Epoch 43/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0917 - val_loss: 0.1126
Epoch 44/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 0.1123
Epoch 45/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 0.1121
Epoch 46/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 0.1118
Epoch 47/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 0.1116
Epoch 48/80
103/103 [==============================] - ETA: 0s - loss: 0.085 - 0s 3ms/step - loss: 0.0904 - val_loss: 0.1113
Epoch 49/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 0.1111
Epoch 50/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 0.1108
Epoch 51/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 0.1105
Epoch 52/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 0.1102
Epoch 53/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0891 - val_loss: 0.1100
Epoch 54/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0888 - val_loss: 0.1097
Epoch 55/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 0.1094
Epoch 56/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0882 - val_loss: 0.1091
Epoch 57/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 0.1088
Epoch 58/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 0.1086
Epoch 59/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0874 - val_loss: 0.1083
Epoch 60/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0871 - val_loss: 0.1080
Epoch 61/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 0.1077
Epoch 62/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0865 - val_loss: 0.1074
Epoch 63/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 0.1071
Epoch 64/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 0.1068
Epoch 65/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0856 - val_loss: 0.1065
Epoch 66/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 0.1062
Epoch 67/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 0.1059
Epoch 68/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.1056
Epoch 69/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0843 - val_loss: 0.1052
Epoch 70/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 0.1049
Epoch 71/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 0.1046
Epoch 72/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0833 - val_loss: 0.1043
Epoch 73/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.1040
Epoch 74/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 0.1036
Epoch 75/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0823 - val_loss: 0.1033
Epoch 76/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 0.1030
Epoch 77/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0816 - val_loss: 0.1026
Epoch 78/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0813 - val_loss: 0.1023
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0810 - val_loss: 0.1020
Epoch 80/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0806 - val_loss: 0.1017
Execution time:  24.770567655563354
DNN:
Mean Absolute Error: 0.0845
Root Mean Square Error: 0.0914
Mean Square Error: 0.0084

Train RMSE: 0.091
Train MSE: 0.008
Train MAE: 0.085
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_94&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_282 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_283 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_94 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_284 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
127/127 [==============================] - 1s 4ms/step - loss: 0.0906 - val_loss: 0.1117
Epoch 2/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0900 - val_loss: 0.1111
Epoch 3/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 0.1104
Epoch 4/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0886 - val_loss: 0.1096
Epoch 5/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0878 - val_loss: 0.1089
Epoch 6/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0870 - val_loss: 0.1081
Epoch 7/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 0.1072
Epoch 8/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0853 - val_loss: 0.1064
Epoch 9/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 0.1056
Epoch 10/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0836 - val_loss: 0.1048
Epoch 11/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 0.1039
Epoch 12/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0820 - val_loss: 0.1033
Epoch 13/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.1028
Epoch 14/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.1022
Epoch 15/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 0.1016
Epoch 16/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.1010
Epoch 17/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.1004
Epoch 18/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0997
Epoch 19/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 0.0991
Epoch 20/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0770 - val_loss: 0.0985
Epoch 21/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0978
Epoch 22/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 0.0972
Epoch 23/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 0.0965
Epoch 24/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0744 - val_loss: 0.0958
Epoch 25/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 0.0951
Epoch 26/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0731 - val_loss: 0.0944
Epoch 27/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0724 - val_loss: 0.0937
Epoch 28/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0930
Epoch 29/90
127/127 [==============================] - ETA: 0s - loss: 0.068 - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0923
Epoch 30/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.0916
Epoch 31/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.0909
Epoch 32/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0690 - val_loss: 0.0901
Epoch 33/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.0894
Epoch 34/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.0886
Epoch 35/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.0879
Epoch 36/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.0871
Epoch 37/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0654 - val_loss: 0.0864
Epoch 38/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.0856
Epoch 39/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0640 - val_loss: 0.0848
Epoch 40/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.0841
Epoch 41/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.0833
Epoch 42/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.0826
Epoch 43/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.0819
Epoch 44/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0812
Epoch 45/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0602 - val_loss: 0.0805
Epoch 46/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0596 - val_loss: 0.0798
Epoch 47/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.0791
Epoch 48/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0784
Epoch 49/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0777
Epoch 50/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.0770
Epoch 51/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.0763
Epoch 52/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.0757
Epoch 53/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.0750
Epoch 54/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.0743
Epoch 55/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.0737
Epoch 56/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.0731
Epoch 57/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.0724
Epoch 58/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.0718
Epoch 59/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0712
Epoch 60/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.0707
Epoch 61/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0701
Epoch 62/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0515 - val_loss: 0.0695
Epoch 63/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.0689
Epoch 64/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0507 - val_loss: 0.0684
Epoch 65/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.0678
Epoch 66/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0673
Epoch 67/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.0667
Epoch 68/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.0662
Epoch 69/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.0657
Epoch 70/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.0652
Epoch 71/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.0646
Epoch 72/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.0641
Epoch 73/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.0636
Epoch 74/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.0632
Epoch 75/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.0627
Epoch 76/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.0622
Epoch 77/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.0617
Epoch 78/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0462 - val_loss: 0.0612
Epoch 79/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.0608
Epoch 80/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.0603
Epoch 81/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.0598
Epoch 82/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0594
Epoch 83/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.0589
Epoch 84/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.0585
Epoch 85/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0580
Epoch 86/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0576
Epoch 87/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0571
Epoch 88/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0567
Epoch 89/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0562
Epoch 90/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.0558
Execution time:  31.344316482543945
DNN:
Mean Absolute Error: 0.0449
Root Mean Square Error: 0.0534
Mean Square Error: 0.0028

Train RMSE: 0.053
Train MSE: 0.003
Train MAE: 0.045
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_95&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_285 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_286 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_95 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_287 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 6ms/step - loss: 0.0869 - val_loss: 0.1066
Epoch 2/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 0.1064
Epoch 3/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0865 - val_loss: 0.1061
Epoch 4/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 0.1059
Epoch 5/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 0.1057
Epoch 6/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 0.1054
Epoch 7/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 0.1052
Epoch 8/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 0.1049
Epoch 9/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0851 - val_loss: 0.1047
Epoch 10/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.1044
Epoch 11/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.1042
Epoch 12/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0841 - val_loss: 0.1039
Epoch 13/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 0.1036
Epoch 14/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 0.1033
Epoch 15/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 0.1031
Epoch 16/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 0.1028
Epoch 17/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.1025
Epoch 18/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 0.1022
Epoch 19/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 0.1019
Epoch 20/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0823 - val_loss: 0.1016
Epoch 21/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 0.1013
Epoch 22/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0816 - val_loss: 0.1010
Epoch 23/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.1007
Epoch 24/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 0.1004
Epoch 25/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0808 - val_loss: 0.1001
Epoch 26/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0806 - val_loss: 0.0998
Epoch 27/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0800 - val_loss: 0.0995
Epoch 28/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0799 - val_loss: 0.0991
Epoch 29/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0988
Epoch 30/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0985
Epoch 31/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 0.0982
Epoch 32/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.0979
Epoch 33/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0976
Epoch 34/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.0972
Epoch 35/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 0.0969
Epoch 36/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 0.0966
Epoch 37/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 0.0963
Epoch 38/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 0.0960
Epoch 39/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 0.0956
Epoch 40/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0764 - val_loss: 0.0953
Epoch 41/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 0.0950
Epoch 42/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 0.0947
Epoch 43/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 0.0944
Epoch 44/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.0940
Epoch 45/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 0.0937
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.0934
Epoch 47/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.0931
Epoch 48/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.0928
Epoch 49/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 0.0924
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 0.0921
Epoch 51/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0918
Epoch 52/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0915
Execution time:  8.825892448425293
DNN:
Mean Absolute Error: 0.0742
Root Mean Square Error: 0.0819
Mean Square Error: 0.0067

Train RMSE: 0.082
Train MSE: 0.007
Train MAE: 0.074
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_96&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_288 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_289 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_96 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_290 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
115/115 [==============================] - 0s 4ms/step - loss: 0.1477 - val_loss: 0.0349
Epoch 2/80
115/115 [==============================] - 0s 3ms/step - loss: 0.1003 - val_loss: 0.0322
Epoch 3/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.0288
Epoch 4/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 0.0225
Epoch 5/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.0104
Epoch 6/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.0061
Epoch 7/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.0052
Epoch 8/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0063
Epoch 9/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.0081
Epoch 10/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0111
Epoch 11/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0155
Epoch 12/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0158
Epoch 13/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0102
Epoch 14/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0105
Epoch 15/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0104
Epoch 16/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0105
Epoch 17/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0117
Epoch 18/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0142
Epoch 19/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0115
Epoch 20/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0138
Epoch 21/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0113
Epoch 22/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0130
Epoch 23/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0111
Epoch 24/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0128
Epoch 25/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0112
Epoch 26/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0122
Epoch 27/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0110
Epoch 28/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0112
Epoch 29/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0114
Epoch 30/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0113
Epoch 31/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0110
Epoch 32/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0111
Epoch 33/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0109
Epoch 34/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0111
Epoch 35/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0111
Epoch 36/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0106
Epoch 37/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0115
Epoch 38/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0105
Epoch 39/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0115
Epoch 40/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0104
Epoch 41/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0116
Epoch 42/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0103
Epoch 43/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0111
Epoch 44/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0103
Epoch 45/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0108
Epoch 46/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0105
Epoch 47/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0105
Epoch 48/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0104
Epoch 49/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0108
Epoch 50/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0106
Epoch 51/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0106
Epoch 52/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0107
Epoch 53/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0106
Epoch 54/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0105
Epoch 55/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0104
Epoch 56/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0101
Epoch 57/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0100
Epoch 58/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0098
Epoch 59/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0097
Epoch 60/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0101
Epoch 61/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0098
Epoch 62/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0101
Epoch 63/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0099
Epoch 64/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0101
Epoch 65/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0097
Epoch 66/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0099
Epoch 67/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0100
Epoch 68/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0096
Epoch 69/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0106
Epoch 70/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0097
Epoch 71/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0105
Epoch 72/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0098
Epoch 73/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0099
Epoch 74/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0102
Epoch 75/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0102
Epoch 76/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0096
Epoch 77/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0105
Epoch 78/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0097
Epoch 79/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0099
Epoch 80/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0100
Execution time:  26.248983144760132
DNN:
Mean Absolute Error: 0.0212
Root Mean Square Error: 0.0333
Mean Square Error: 0.0011

Train RMSE: 0.033
Train MSE: 0.001
Train MAE: 0.021
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_97&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_291 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_292 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_97 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_293 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 1s 4ms/step - loss: 0.0973 - val_loss: 0.0191
Epoch 2/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.0167
Epoch 3/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.0144
Epoch 4/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0135
Epoch 5/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.0107
Epoch 6/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.0091
Epoch 7/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0064
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0054
Epoch 9/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.0050
Epoch 10/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 0.0058
Epoch 11/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0067
Epoch 12/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0078
Epoch 13/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0085
Epoch 14/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0091
Epoch 15/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0101
Epoch 16/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0118
Epoch 17/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0127
Epoch 18/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0130
Epoch 19/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0142
Epoch 20/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0138
Epoch 21/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0141
Epoch 22/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0158
Epoch 23/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0151
Epoch 24/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0150
Epoch 25/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0154
Epoch 26/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0160
Epoch 27/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0151
Epoch 28/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0154
Epoch 29/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0151
Epoch 30/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0149
Epoch 31/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0148
Epoch 32/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0150
Epoch 33/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0139
Epoch 34/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0139
Epoch 35/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0140
Epoch 36/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0143
Epoch 37/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0164
Epoch 38/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0185
Epoch 39/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0197
Epoch 40/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0194
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0208
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0201
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0194
Epoch 44/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0179
Epoch 45/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0184
Epoch 46/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0175
Epoch 47/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0166
Epoch 48/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0162
Epoch 49/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0170
Epoch 50/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0169
Epoch 51/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0154
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0150
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0143
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0128
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0122
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0139
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0126
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0124
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0123
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0123
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0124
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0125
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0124
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0140
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0121
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0132
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0129
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0128
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0123
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0133
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0143
Epoch 72/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0120
Epoch 73/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0134
Epoch 74/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0123
Epoch 75/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0123
Epoch 76/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0132
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0123
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0132
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0126
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0132
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0139
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0126
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0135
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0129
Epoch 85/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0139
Epoch 86/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0125
Epoch 87/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0123
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0135
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0124
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0145
Execution time:  32.98895621299744
DNN:
Mean Absolute Error: 0.0167
Root Mean Square Error: 0.0272
Mean Square Error: 0.0007

Train RMSE: 0.027
Train MSE: 0.001
Train MAE: 0.017
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_98&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_294 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_295 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_98 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_296 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 5ms/step - loss: 0.4063 - val_loss: 0.2297
Epoch 2/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1793 - val_loss: 0.0595
Epoch 3/52
63/63 [==============================] - 0s 3ms/step - loss: 0.1049 - val_loss: 0.0205
Epoch 4/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0934 - val_loss: 0.0204
Epoch 5/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0854 - val_loss: 0.0149
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0767 - val_loss: 0.0070
Epoch 7/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0684 - val_loss: 0.0062
Epoch 8/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.0083
Epoch 9/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.0091
Epoch 10/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.0085
Epoch 11/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.0079
Epoch 12/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.0084
Epoch 13/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0100
Epoch 14/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0109
Epoch 15/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0120
Epoch 16/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0135
Epoch 17/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0139
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0134
Epoch 19/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0127
Epoch 20/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0127
Epoch 21/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0124
Epoch 22/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0123
Epoch 23/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0126
Epoch 24/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0131
Epoch 25/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0130
Epoch 26/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0130
Epoch 27/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0130
Epoch 28/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0131
Epoch 29/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 30/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 31/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0121
Epoch 32/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0124
Epoch 33/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0123
Epoch 34/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0128
Epoch 35/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0129
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0135
Epoch 37/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0134
Epoch 38/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0129
Epoch 39/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0129
Epoch 40/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0125
Epoch 41/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0126
Epoch 42/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0121
Epoch 43/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0121
Epoch 44/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0118
Epoch 45/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0113
Epoch 46/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0108
Epoch 47/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0106
Epoch 48/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0105
Epoch 49/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0103
Epoch 50/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0104
Epoch 51/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0100
Epoch 52/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0106
Execution time:  9.37382435798645
DNN:
Mean Absolute Error: 0.0190
Root Mean Square Error: 0.0316
Mean Square Error: 0.0010

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_99&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_297 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_298 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_99 (Dropout)         (None, 36, 16)            0         
_________________________________________________________________
dense_299 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
103/103 [==============================] - 0s 5ms/step - loss: 0.1290 - val_loss: 0.0330
Epoch 2/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 0.0371
Epoch 3/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.0358
Epoch 4/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 0.0344
Epoch 5/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0693 - val_loss: 0.0308
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0631 - val_loss: 0.0280
Epoch 7/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.0229
Epoch 8/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.0202
Epoch 9/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.0215
Epoch 10/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.0224
Epoch 11/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.0198
Epoch 12/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0180
Epoch 13/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0180
Epoch 14/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0170
Epoch 15/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0164
Epoch 16/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0158
Epoch 17/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0156
Epoch 18/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0155
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0154
Epoch 20/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0152
Epoch 21/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0150
Epoch 22/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0149
Epoch 23/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0150
Epoch 24/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0150
Epoch 25/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0150
Epoch 26/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0151
Epoch 27/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 28/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0151
Epoch 29/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0153
Epoch 30/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0150
Epoch 31/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0148
Epoch 32/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0149
Epoch 33/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0149
Epoch 34/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0148
Epoch 35/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0148
Epoch 36/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0147
Epoch 37/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0148
Epoch 38/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0147
Epoch 39/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0147
Epoch 40/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0147
Epoch 41/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0146
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0146
Epoch 43/80
103/103 [==============================] - ETA: 0s - loss: 0.026 - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0147
Epoch 44/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0146
Epoch 45/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0146
Epoch 46/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0146
Epoch 47/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0146
Epoch 48/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0146
Epoch 49/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0145
Epoch 50/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0146
Epoch 51/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0146
Epoch 52/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0146
Epoch 53/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0145
Epoch 54/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0146
Epoch 55/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0146
Epoch 56/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0146
Epoch 57/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0145
Epoch 58/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0146
Epoch 59/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0145
Epoch 60/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0146
Epoch 61/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0145
Epoch 62/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0146
Epoch 63/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0145
Epoch 64/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0146
Epoch 65/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0145
Epoch 66/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0146
Epoch 67/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0145
Epoch 68/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0145
Epoch 69/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0146
Epoch 70/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0146
Epoch 71/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0146
Epoch 72/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0145
Epoch 73/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0146
Epoch 74/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0145
Epoch 75/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0146
Epoch 76/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0145
Epoch 77/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0146
Epoch 78/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0145
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0146
Epoch 80/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0145
Execution time:  24.636602640151978
DNN:
Mean Absolute Error: 0.0196
Root Mean Square Error: 0.0306
Mean Square Error: 0.0009

Train RMSE: 0.031
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_100&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_300 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_301 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_100 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_302 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
  1/127 [..............................] - ETA: 0s - loss: 0.4089WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0106s). Check your callbacks.
127/127 [==============================] - 1s 4ms/step - loss: 0.0867 - val_loss: 0.0394
Epoch 2/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.0387
Epoch 3/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.0363
Epoch 4/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.0318
Epoch 5/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0268
Epoch 6/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.0250
Epoch 7/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.0230
Epoch 8/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.0193
Epoch 9/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.0187
Epoch 10/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0191
Epoch 11/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0181
Epoch 12/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0192
Epoch 13/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0194
Epoch 14/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0242
Epoch 15/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0173
Epoch 16/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0162
Epoch 17/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0167
Epoch 18/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0160
Epoch 19/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0148
Epoch 20/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0155
Epoch 21/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0151
Epoch 22/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0134
Epoch 23/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0130
Epoch 24/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0129
Epoch 25/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0129
Epoch 26/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0130
Epoch 27/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0128
Epoch 28/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0124
Epoch 29/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0127
Epoch 30/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0129
Epoch 31/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0129
Epoch 32/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0129
Epoch 33/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0129
Epoch 34/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0131
Epoch 35/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0131
Epoch 36/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0133
Epoch 37/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0133
Epoch 38/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0134
Epoch 39/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0132
Epoch 40/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0131
Epoch 41/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0133
Epoch 42/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0133
Epoch 43/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0133
Epoch 44/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0128
Epoch 45/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0128
Epoch 46/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0131
Epoch 47/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0131
Epoch 48/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0140
Epoch 49/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0140
Epoch 50/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0140
Epoch 51/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0137
Epoch 52/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0138
Epoch 53/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0141
Epoch 54/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0142
Epoch 55/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0144
Epoch 56/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0143
Epoch 57/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 0.0143
Epoch 58/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0146
Epoch 59/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0146
Epoch 60/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0197 - val_loss: 0.0147
Epoch 61/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0144
Epoch 62/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0146
Epoch 63/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0144
Epoch 64/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0143
Epoch 65/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0145
Epoch 66/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0145
Epoch 67/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0143
Epoch 68/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0143
Epoch 69/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0144
Epoch 70/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0144
Epoch 71/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0144
Epoch 72/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0144
Epoch 73/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0146
Epoch 74/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0145
Epoch 75/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0145
Epoch 76/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0145
Epoch 77/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0147
Epoch 78/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0145
Epoch 79/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0145
Epoch 80/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0146
Epoch 81/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0145
Epoch 82/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0145
Epoch 83/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0146
Epoch 84/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0145
Epoch 85/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0145
Epoch 86/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0147
Epoch 87/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0145
Epoch 88/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0145
Epoch 89/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0145
Epoch 90/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0147
Execution time:  32.414623975753784
DNN:
Mean Absolute Error: 0.0175
Root Mean Square Error: 0.0285
Mean Square Error: 0.0008

Train RMSE: 0.029
Train MSE: 0.001
Train MAE: 0.017
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_101&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_303 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_304 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_101 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_305 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 6ms/step - loss: 0.4200 - val_loss: 0.2630
Epoch 2/52
56/56 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 0.0851
Epoch 3/52
56/56 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.0239
Epoch 4/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 0.0166
Epoch 5/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0843 - val_loss: 0.0150
Epoch 6/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0146
Epoch 7/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0154
Epoch 8/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.0156
Epoch 9/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.0151
Epoch 10/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.0147
Epoch 11/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.0147
Epoch 12/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.0147
Epoch 13/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.0153
Epoch 14/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.0160
Epoch 15/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.0166
Epoch 16/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0171
Epoch 17/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0178
Epoch 18/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0182
Epoch 19/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0184
Epoch 20/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0184
Epoch 21/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0178
Epoch 22/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0175
Epoch 23/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0172
Epoch 24/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0172
Epoch 25/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0172
Epoch 26/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0170
Epoch 27/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0170
Epoch 28/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0170
Epoch 29/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0169
Epoch 30/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0168
Epoch 31/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0168
Epoch 32/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0168
Epoch 33/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0169
Epoch 34/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0169
Epoch 35/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0168
Epoch 36/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0168
Epoch 37/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0169
Epoch 38/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0169
Epoch 39/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0168
Epoch 40/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0168
Epoch 41/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0166
Epoch 42/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0164
Epoch 43/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0163
Epoch 44/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0162
Epoch 45/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0161
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0159
Epoch 47/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0158
Epoch 48/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0156
Epoch 49/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0154
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0153
Epoch 51/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0151
Epoch 52/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0149
Execution time:  9.131410360336304
DNN:
Mean Absolute Error: 0.0193
Root Mean Square Error: 0.0324
Mean Square Error: 0.0010

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_102&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_306 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_307 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_102 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_308 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
115/115 [==============================] - 0s 4ms/step - loss: 0.0484 - val_loss: 0.0313
Epoch 2/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.0271
Epoch 3/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.0260
Epoch 4/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.0252
Epoch 5/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.0240
Epoch 6/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0228
Epoch 7/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0220
Epoch 8/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0208
Epoch 9/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0201
Epoch 10/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0195
Epoch 11/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0192
Epoch 12/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0183
Epoch 13/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0178
Epoch 14/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0171
Epoch 15/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0165
Epoch 16/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0160
Epoch 17/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0156
Epoch 18/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0153
Epoch 19/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0149
Epoch 20/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0147
Epoch 21/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0144
Epoch 22/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0143
Epoch 23/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0139
Epoch 24/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0136
Epoch 25/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0133
Epoch 26/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0132
Epoch 27/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0128
Epoch 28/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0128
Epoch 29/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0123
Epoch 30/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0121
Epoch 31/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0119
Epoch 32/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0123
Epoch 33/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0114
Epoch 34/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0116
Epoch 35/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0114
Epoch 36/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0113
Epoch 37/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0114
Epoch 38/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0112
Epoch 39/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0115
Epoch 40/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0111
Epoch 41/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0109
Epoch 42/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0109
Epoch 43/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0109
Epoch 44/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0108
Epoch 45/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0105
Epoch 46/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0108
Epoch 47/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0106
Epoch 48/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0107
Epoch 49/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0107
Epoch 50/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0108
Epoch 51/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0107
Epoch 52/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0108
Epoch 53/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0107
Epoch 54/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0109
Epoch 55/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0108
Epoch 56/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0108
Epoch 57/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0108
Epoch 58/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0109
Epoch 59/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0110
Epoch 60/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0113
Epoch 61/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0112
Epoch 62/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0113
Epoch 63/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0113
Epoch 64/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0114
Epoch 65/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0113
Epoch 66/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0119
Epoch 67/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0115
Epoch 68/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0119
Epoch 69/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0116
Epoch 70/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0117
Epoch 71/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0115
Epoch 72/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0118
Epoch 73/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0118
Epoch 74/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0117
Epoch 75/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0117
Epoch 76/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0120
Epoch 77/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0119
Epoch 78/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0116
Epoch 79/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0119
Epoch 80/80
115/115 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0117
Execution time:  25.96048378944397
DNN:
Mean Absolute Error: 0.0154
Root Mean Square Error: 0.0276
Mean Square Error: 0.0008

Train RMSE: 0.028
Train MSE: 0.001
Train MAE: 0.015
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_103&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_309 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_310 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_103 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_311 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
143/143 [==============================] - 1s 5ms/step - loss: 0.0835 - val_loss: 0.0922
Epoch 2/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0388
Epoch 3/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0217
Epoch 4/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0204
Epoch 5/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0203
Epoch 6/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0203
Epoch 7/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0185
Epoch 8/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0157
Epoch 9/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0205
Epoch 10/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0196
Epoch 11/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0193
Epoch 12/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0191
Epoch 13/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0190
Epoch 14/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0187
Epoch 15/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0193
Epoch 16/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0173
Epoch 17/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0184
Epoch 18/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0183
Epoch 19/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0183
Epoch 20/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0181
Epoch 21/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0180
Epoch 22/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0211
Epoch 23/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0180
Epoch 24/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0182
Epoch 25/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0179
Epoch 26/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0180
Epoch 27/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0176
Epoch 28/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0180
Epoch 29/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0176
Epoch 30/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0177
Epoch 31/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0175
Epoch 32/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0178
Epoch 33/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0177
Epoch 34/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0177
Epoch 35/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0176
Epoch 36/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0175
Epoch 37/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0175
Epoch 38/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0174
Epoch 39/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0173
Epoch 40/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0172
Epoch 41/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0173
Epoch 42/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0172
Epoch 43/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0170
Epoch 44/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0171
Epoch 45/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0171
Epoch 46/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0170
Epoch 47/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0170
Epoch 48/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0171
Epoch 49/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0171
Epoch 50/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0171
Epoch 51/90
143/143 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0182
Epoch 52/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0143
Epoch 53/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0173
Epoch 54/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 55/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0172
Epoch 56/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0173
Epoch 57/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0169
Epoch 58/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0168
Epoch 59/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0148
Epoch 60/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0168
Epoch 61/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0147
Epoch 62/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0163
Epoch 63/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0145
Epoch 64/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0157
Epoch 65/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0146
Epoch 66/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0146
Epoch 67/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0138
Epoch 68/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0137
Epoch 69/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0135
Epoch 70/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0132
Epoch 71/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0128
Epoch 72/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0128
Epoch 73/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0124
Epoch 74/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0123
Epoch 75/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0119
Epoch 76/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0117
Epoch 77/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0115
Epoch 78/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0114
Epoch 79/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0113
Epoch 80/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0113
Epoch 81/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0113
Epoch 82/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0112
Epoch 83/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0111
Epoch 84/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0111
Epoch 85/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0111
Epoch 86/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0110
Epoch 87/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0111
Epoch 88/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0110
Epoch 89/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0109
Epoch 90/90
143/143 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0108
Execution time:  33.86900043487549
DNN:
Mean Absolute Error: 0.0244
Root Mean Square Error: 0.0419
Mean Square Error: 0.0018

Train RMSE: 0.042
Train MSE: 0.002
Train MAE: 0.024
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_104&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_312 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_313 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_104 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_314 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
63/63 [==============================] - 0s 6ms/step - loss: 0.0593 - val_loss: 0.0412
Epoch 2/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.0414
Epoch 3/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.0404
Epoch 4/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.0410
Epoch 5/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.0393
Epoch 6/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0382
Epoch 7/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.0373
Epoch 8/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.0364
Epoch 9/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.0362
Epoch 10/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.0354
Epoch 11/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0348
Epoch 12/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.0342
Epoch 13/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0333
Epoch 14/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0329
Epoch 15/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0320
Epoch 16/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0315
Epoch 17/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0309
Epoch 18/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0302
Epoch 19/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0294
Epoch 20/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0284
Epoch 21/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0279
Epoch 22/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0273
Epoch 23/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0309 - val_loss: 0.0264
Epoch 24/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0259
Epoch 25/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0252
Epoch 26/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0245
Epoch 27/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0240
Epoch 28/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0232
Epoch 29/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0227
Epoch 30/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0220
Epoch 31/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0215
Epoch 32/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0209
Epoch 33/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0202
Epoch 34/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0198
Epoch 35/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0192
Epoch 36/52
63/63 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0190
Epoch 37/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0185
Epoch 38/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0181
Epoch 39/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0177
Epoch 40/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0172
Epoch 41/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0168
Epoch 42/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0165
Epoch 43/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0161
Epoch 44/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0157
Epoch 45/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0154
Epoch 46/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0150
Epoch 47/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0146
Epoch 48/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0144
Epoch 49/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0139
Epoch 50/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0138
Epoch 51/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0131
Epoch 52/52
63/63 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0131
Execution time:  9.73934292793274
DNN:
Mean Absolute Error: 0.0203
Root Mean Square Error: 0.0327
Mean Square Error: 0.0011

Train RMSE: 0.033
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  6h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_105&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_315 (Dense)            (None, 36, 87)            174       
_________________________________________________________________
dense_316 (Dense)            (None, 36, 16)            1408      
_________________________________________________________________
dropout_105 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_317 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
  1/103 [..............................] - ETA: 0s - loss: 0.0732WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0090s). Check your callbacks.
103/103 [==============================] - 0s 5ms/step - loss: 0.0501 - val_loss: 0.0287
Epoch 2/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.0273
Epoch 3/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.0257
Epoch 4/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.0249
Epoch 5/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.0241
Epoch 6/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0237
Epoch 7/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.0230
Epoch 8/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0225
Epoch 9/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0221
Epoch 10/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0217
Epoch 11/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0215
Epoch 12/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0212
Epoch 13/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0208
Epoch 14/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0205
Epoch 15/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0199
Epoch 16/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0196
Epoch 17/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0192
Epoch 18/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0189
Epoch 19/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0185
Epoch 20/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0184
Epoch 21/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0180
Epoch 22/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0178
Epoch 23/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0176
Epoch 24/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0173
Epoch 25/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0171
Epoch 26/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0168
Epoch 27/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0165
Epoch 28/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0163
Epoch 29/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0161
Epoch 30/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0159
Epoch 31/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0157
Epoch 32/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0155
Epoch 33/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0154
Epoch 34/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0152
Epoch 35/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 36/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0150
Epoch 37/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0149
Epoch 38/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 39/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0148
Epoch 40/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0147
Epoch 41/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0145
Epoch 42/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0145
Epoch 43/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0144
Epoch 44/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0144
Epoch 45/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0144
Epoch 46/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0143
Epoch 47/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0143
Epoch 48/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0142
Epoch 49/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0141
Epoch 50/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0141
Epoch 51/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0140
Epoch 52/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0140
Epoch 53/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0139
Epoch 54/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0138
Epoch 55/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0138
Epoch 56/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0138
Epoch 57/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0138
Epoch 58/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0137
Epoch 59/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0137
Epoch 60/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0137
Epoch 61/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0137
Epoch 62/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0137
Epoch 63/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0137
Epoch 64/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0136
Epoch 65/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0136
Epoch 66/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0136
Epoch 67/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0136
Epoch 68/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0136
Epoch 69/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0136
Epoch 70/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0137
Epoch 71/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0137
Epoch 72/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0138
Epoch 73/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0138
Epoch 74/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0138
Epoch 75/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0138
Epoch 76/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0138
Epoch 77/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0139
Epoch 78/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0138
Epoch 79/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0139
Epoch 80/80
103/103 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0138
Execution time:  24.764704942703247
DNN:
Mean Absolute Error: 0.0143
Root Mean Square Error: 0.0296
Mean Square Error: 0.0009

Train RMSE: 0.030
Train MSE: 0.001
Train MAE: 0.014
###########################

MODEL:  DNN
sequence:  6h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_106&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_318 (Dense)            (None, 36, 80)            160       
_________________________________________________________________
dense_319 (Dense)            (None, 36, 16)            1296      
_________________________________________________________________
dropout_106 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_320 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
127/127 [==============================] - 1s 4ms/step - loss: 0.0397 - val_loss: 0.0201
Epoch 2/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0199
Epoch 3/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.0193
Epoch 4/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.0196
Epoch 5/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0194
Epoch 6/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0192
Epoch 7/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0190
Epoch 8/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0189
Epoch 9/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0189
Epoch 10/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0188
Epoch 11/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0188
Epoch 12/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0185
Epoch 13/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0184
Epoch 14/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0184
Epoch 15/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0182
Epoch 16/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0180
Epoch 17/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0181
Epoch 18/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0179
Epoch 19/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0178
Epoch 20/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0176
Epoch 21/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0177
Epoch 22/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0175
Epoch 23/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0176
Epoch 24/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0174
Epoch 25/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0174
Epoch 26/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0173
Epoch 27/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0172
Epoch 28/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0172
Epoch 29/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0171
Epoch 30/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0169
Epoch 31/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0168
Epoch 32/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0168
Epoch 33/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0166
Epoch 34/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0164
Epoch 35/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0163
Epoch 36/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0161
Epoch 37/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0160
Epoch 38/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0158
Epoch 39/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0158
Epoch 40/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0154
Epoch 41/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0153
Epoch 42/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0151
Epoch 43/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0150
Epoch 44/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0147
Epoch 45/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0146
Epoch 46/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0146
Epoch 47/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0144
Epoch 48/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0143
Epoch 49/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0142
Epoch 50/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0143
Epoch 51/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0141
Epoch 52/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0140
Epoch 53/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0139
Epoch 54/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0139
Epoch 55/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0137
Epoch 56/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0137
Epoch 57/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0137
Epoch 58/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0136
Epoch 59/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0135
Epoch 60/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0135
Epoch 61/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0134
Epoch 62/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0134
Epoch 63/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0133
Epoch 64/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0133
Epoch 65/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0133
Epoch 66/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0132
Epoch 67/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0132
Epoch 68/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0131
Epoch 69/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0131
Epoch 70/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0131
Epoch 71/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0131
Epoch 72/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0197 - val_loss: 0.0130
Epoch 73/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0131
Epoch 74/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0131
Epoch 75/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0130
Epoch 76/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0129
Epoch 77/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0133
Epoch 78/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0129
Epoch 79/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0132
Epoch 80/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0131
Epoch 81/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0131
Epoch 82/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0131
Epoch 83/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0130
Epoch 84/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0129
Epoch 85/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0131
Epoch 86/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0129
Epoch 87/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0130
Epoch 88/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0130
Epoch 89/90
127/127 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0131
Epoch 90/90
127/127 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0129
Execution time:  31.256271839141846
DNN:
Mean Absolute Error: 0.0109
Root Mean Square Error: 0.0232
Mean Square Error: 0.0005

Train RMSE: 0.023
Train MSE: 0.001
Train MAE: 0.011
###########################

MODEL:  DNN
sequence:  6h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_107&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_321 (Dense)            (None, 36, 12)            24        
_________________________________________________________________
dense_322 (Dense)            (None, 36, 16)            208       
_________________________________________________________________
dropout_107 (Dropout)        (None, 36, 16)            0         
_________________________________________________________________
dense_323 (Dense)            (None, 36, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
56/56 [==============================] - 0s 6ms/step - loss: 0.0647 - val_loss: 0.0504
Epoch 2/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0398
Epoch 3/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.0380
Epoch 4/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.0366
Epoch 5/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.0358
Epoch 6/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.0352
Epoch 7/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.0344
Epoch 8/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0336
Epoch 9/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.0328
Epoch 10/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0321
Epoch 11/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0317
Epoch 12/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0314
Epoch 13/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0309
Epoch 14/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0303
Epoch 15/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0300
Epoch 16/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0297
Epoch 17/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0291
Epoch 18/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0285
Epoch 19/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0281
Epoch 20/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0279
Epoch 21/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0274
Epoch 22/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0272
Epoch 23/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0268
Epoch 24/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0265
Epoch 25/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0258
Epoch 26/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0254
Epoch 27/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0250
Epoch 28/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0245
Epoch 29/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0242
Epoch 30/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0238
Epoch 31/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0234
Epoch 32/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0231
Epoch 33/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0228
Epoch 34/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0225
Epoch 35/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0222
Epoch 36/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0219
Epoch 37/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0217
Epoch 38/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0214
Epoch 39/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0211
Epoch 40/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0209
Epoch 41/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0207
Epoch 42/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0204
Epoch 43/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0202
Epoch 44/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0200
Epoch 45/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0198
Epoch 46/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0196
Epoch 47/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194
Epoch 48/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0193
Epoch 49/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191
Epoch 50/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0189
Epoch 51/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0188
Epoch 52/52
56/56 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0187
Execution time:  9.233843326568604
DNN:
Mean Absolute Error: 0.0232
Root Mean Square Error: 0.0372
Mean Square Error: 0.0014

Train RMSE: 0.037
Train MSE: 0.001
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_108&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_324 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_325 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_108 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_326 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
114/114 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.0273
Epoch 2/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0860 - val_loss: 0.0096
Epoch 3/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0105
Epoch 4/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.0135
Epoch 5/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.0171
Epoch 6/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.0166
Epoch 7/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0199
Epoch 8/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0150
Epoch 9/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.0084
Epoch 10/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0094
Epoch 11/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0100
Epoch 12/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0098
Epoch 13/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0102
Epoch 14/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0102
Epoch 15/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0107
Epoch 16/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0105
Epoch 17/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0115
Epoch 18/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0108
Epoch 19/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0119
Epoch 20/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0106
Epoch 21/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0130
Epoch 22/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0109
Epoch 23/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0122
Epoch 24/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0109
Epoch 25/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0132
Epoch 26/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0119
Epoch 27/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0124
Epoch 28/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0117
Epoch 29/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0136
Epoch 30/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0117
Epoch 31/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0135
Epoch 32/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0118
Epoch 33/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0136
Epoch 34/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0118
Epoch 35/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0135
Epoch 36/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0114
Epoch 37/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0136
Epoch 38/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0122
Epoch 39/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 40/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0123
Epoch 41/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0132
Epoch 42/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0122
Epoch 43/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 44/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0124
Epoch 45/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 46/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0125
Epoch 47/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0124
Epoch 48/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 49/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0124
Epoch 50/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0123
Epoch 51/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0125
Epoch 52/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0123
Epoch 53/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0127
Epoch 54/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0121
Epoch 55/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0122
Epoch 56/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0121
Epoch 57/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0122
Epoch 58/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0121
Epoch 59/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0117
Epoch 60/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0119
Epoch 61/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0116
Epoch 62/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0118
Epoch 63/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0117
Epoch 64/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0120
Epoch 65/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0117
Epoch 66/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0117
Epoch 67/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0116
Epoch 68/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0112
Epoch 69/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0112
Epoch 70/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0113
Epoch 71/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0108
Epoch 72/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0111
Epoch 73/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0109
Epoch 74/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0107
Epoch 75/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0108
Epoch 76/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0107
Epoch 77/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0105
Epoch 78/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0108
Epoch 79/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0112
Epoch 80/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0106
Execution time:  32.62877941131592
DNN:
Mean Absolute Error: 0.0257
Root Mean Square Error: 0.0446
Mean Square Error: 0.0020

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_109&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_327 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_328 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_109 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_329 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
142/142 [==============================] - 1s 4ms/step - loss: 0.1127 - val_loss: 0.0126
Epoch 2/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.0073
Epoch 3/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.0189
Epoch 4/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.0170
Epoch 5/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.0108
Epoch 6/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0080
Epoch 7/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0068
Epoch 8/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.0079
Epoch 9/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0053
Epoch 10/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0129
Epoch 11/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0120
Epoch 12/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0132
Epoch 13/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0157
Epoch 14/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0190
Epoch 15/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0197
Epoch 16/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0216
Epoch 17/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0216
Epoch 18/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0194
Epoch 19/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0190
Epoch 20/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0190
Epoch 21/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 22/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0161
Epoch 23/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0138
Epoch 24/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0108
Epoch 25/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0105
Epoch 26/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0106
Epoch 27/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0103
Epoch 28/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0105
Epoch 29/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0103
Epoch 30/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0114
Epoch 31/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0104
Epoch 32/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0108
Epoch 33/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0107
Epoch 34/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0109
Epoch 35/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0114
Epoch 36/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0109
Epoch 37/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0111
Epoch 38/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0110
Epoch 39/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0109
Epoch 40/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0111
Epoch 41/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0112
Epoch 42/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0113
Epoch 43/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0111
Epoch 44/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0111
Epoch 45/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0113
Epoch 46/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0112
Epoch 47/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0112
Epoch 48/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0110
Epoch 49/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0104
Epoch 50/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0109
Epoch 51/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0102
Epoch 52/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0111
Epoch 53/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0110
Epoch 54/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0112
Epoch 55/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0111
Epoch 56/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0112
Epoch 57/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0112
Epoch 58/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0111
Epoch 59/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0111
Epoch 60/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0113
Epoch 61/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0111
Epoch 62/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0111
Epoch 63/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0111
Epoch 64/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0111
Epoch 65/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0109
Epoch 66/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0111
Epoch 67/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0110
Epoch 68/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0108
Epoch 69/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0110
Epoch 70/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0109
Epoch 71/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0108
Epoch 72/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0107
Epoch 73/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0107
Epoch 74/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0106
Epoch 75/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0104
Epoch 76/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0104
Epoch 77/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0103
Epoch 78/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0103
Epoch 79/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0103
Epoch 80/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0101
Epoch 81/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0100
Epoch 82/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0104
Epoch 83/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0100
Epoch 84/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0101
Epoch 85/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0099
Epoch 86/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0099
Epoch 87/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0098
Epoch 88/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0098
Epoch 89/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0103
Epoch 90/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0103
Execution time:  38.99303841590881
DNN:
Mean Absolute Error: 0.0247
Root Mean Square Error: 0.0426
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_110&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_330 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_331 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_110 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_332 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
62/62 [==============================] - 0s 6ms/step - loss: 0.2038 - val_loss: 0.0484
Epoch 2/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 0.0214
Epoch 3/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1021 - val_loss: 0.0163
Epoch 4/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0892 - val_loss: 0.0214
Epoch 5/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.0155
Epoch 6/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0690 - val_loss: 0.0076
Epoch 7/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0075
Epoch 8/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.0068
Epoch 9/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.0066
Epoch 10/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.0070
Epoch 11/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.0071
Epoch 12/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0079
Epoch 13/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0083
Epoch 14/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0080
Epoch 15/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0084
Epoch 16/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0095
Epoch 17/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0098
Epoch 18/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0125
Epoch 19/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0128
Epoch 20/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0102
Epoch 21/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0147
Epoch 22/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0126
Epoch 23/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0155
Epoch 24/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0138
Epoch 25/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0125
Epoch 26/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0119
Epoch 27/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0147
Epoch 28/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0136
Epoch 29/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0134
Epoch 30/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0124
Epoch 31/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0141
Epoch 32/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0132
Epoch 33/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0137
Epoch 34/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0142
Epoch 35/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0150
Epoch 36/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0146
Epoch 37/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0129
Epoch 38/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 39/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0117
Epoch 40/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0130
Epoch 41/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0144
Epoch 42/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0147
Epoch 43/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0131
Epoch 44/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0118
Epoch 45/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0111
Epoch 46/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0108
Epoch 47/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0122
Epoch 48/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0135
Epoch 49/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0140
Epoch 50/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0139
Epoch 51/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0114
Epoch 52/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0105
Execution time:  12.056646823883057
DNN:
Mean Absolute Error: 0.0231
Root Mean Square Error: 0.0385
Mean Square Error: 0.0015

Train RMSE: 0.039
Train MSE: 0.001
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_111&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_333 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_334 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_111 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_335 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
102/102 [==============================] - 1s 6ms/step - loss: 0.1279 - val_loss: 0.0304
Epoch 2/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0825 - val_loss: 0.0221
Epoch 3/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0182
Epoch 4/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0151
Epoch 5/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0495 - val_loss: 0.0153
Epoch 6/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.0155
Epoch 7/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0375 - val_loss: 0.0160
Epoch 8/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0154
Epoch 9/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0327 - val_loss: 0.0169
Epoch 10/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 0.0180
Epoch 11/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 0.0172
Epoch 12/80
102/102 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0164
Epoch 13/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0309 - val_loss: 0.0162
Epoch 14/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0305 - val_loss: 0.0160
Epoch 15/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0312 - val_loss: 0.0160
Epoch 16/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0297 - val_loss: 0.0175
Epoch 17/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0179
Epoch 18/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0170
Epoch 19/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0179
Epoch 20/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0173
Epoch 21/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0169
Epoch 22/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0164
Epoch 23/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0172
Epoch 24/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0177
Epoch 25/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0173
Epoch 26/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0172
Epoch 27/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0175
Epoch 28/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0174
Epoch 29/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0173
Epoch 30/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0174
Epoch 31/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0174
Epoch 32/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0174
Epoch 33/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 34/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0174
Epoch 35/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0175
Epoch 36/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0174
Epoch 37/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0173
Epoch 38/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 39/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0173
Epoch 40/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0173
Epoch 41/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0173
Epoch 42/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0172
Epoch 43/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 44/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 45/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0172
Epoch 46/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 47/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0170
Epoch 48/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0170
Epoch 49/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0171
Epoch 50/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0171
Epoch 51/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0170
Epoch 52/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0171
Epoch 53/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0171
Epoch 54/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0171
Epoch 55/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0170
Epoch 56/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0169
Epoch 57/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0169
Epoch 58/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0168
Epoch 59/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0169
Epoch 60/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0168
Epoch 61/80
102/102 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0167
Epoch 62/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0167
Epoch 63/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0167
Epoch 64/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0167
Epoch 65/80
102/102 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0167
Epoch 66/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0166
Epoch 67/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0167
Epoch 68/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0166
Epoch 69/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0167
Epoch 70/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0164
Epoch 71/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0167
Epoch 72/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0163
Epoch 73/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0166
Epoch 74/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0165
Epoch 75/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0165
Epoch 76/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0164
Epoch 77/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0166
Epoch 78/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0164
Epoch 79/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0164
Epoch 80/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0165
Execution time:  31.652584075927734
DNN:
Mean Absolute Error: 0.0241
Root Mean Square Error: 0.0407
Mean Square Error: 0.0017

Train RMSE: 0.041
Train MSE: 0.002
Train MAE: 0.024
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_112&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_336 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_337 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_112 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_338 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
126/126 [==============================] - 1s 5ms/step - loss: 0.0997 - val_loss: 0.0307
Epoch 2/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.0290
Epoch 3/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.0328
Epoch 4/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0508 - val_loss: 0.0240
Epoch 5/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.0219
Epoch 6/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.0172
Epoch 7/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.0176
Epoch 8/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.0159
Epoch 9/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.0164
Epoch 10/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0161
Epoch 11/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0156
Epoch 12/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0156
Epoch 13/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0157
Epoch 14/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0156
Epoch 15/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0154
Epoch 16/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0163
Epoch 17/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0158
Epoch 18/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0157
Epoch 19/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0165
Epoch 20/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0152
Epoch 21/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0163
Epoch 22/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0163
Epoch 23/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0159
Epoch 24/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0159
Epoch 25/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0159
Epoch 26/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0160
Epoch 27/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0160
Epoch 28/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0161
Epoch 29/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0161
Epoch 30/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0161
Epoch 31/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0160
Epoch 32/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0160
Epoch 33/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0160
Epoch 34/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0160
Epoch 35/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0160
Epoch 36/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0160
Epoch 37/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0160
Epoch 38/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0160
Epoch 39/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0161
Epoch 40/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0160
Epoch 41/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0159
Epoch 42/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0162
Epoch 43/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0159
Epoch 44/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0161
Epoch 45/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0160
Epoch 46/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0162
Epoch 47/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0160
Epoch 48/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0161
Epoch 49/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0161
Epoch 50/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0161
Epoch 51/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0162
Epoch 52/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0160
Epoch 53/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0162
Epoch 54/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 55/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0162
Epoch 56/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 57/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 58/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0162
Epoch 59/90
126/126 [==============================] - ETA: 0s - loss: 0.031 - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 60/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 61/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0160
Epoch 62/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0162
Epoch 63/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0160
Epoch 64/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 65/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0159
Epoch 66/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0160
Epoch 67/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0159
Epoch 68/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0160
Epoch 69/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0159
Epoch 70/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0160
Epoch 71/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0159
Epoch 72/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0159
Epoch 73/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0159
Epoch 74/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0161
Epoch 75/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0160
Epoch 76/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0159
Epoch 77/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0159
Epoch 78/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0159
Epoch 79/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0159
Epoch 80/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0159
Epoch 81/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0160
Epoch 82/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0160
Epoch 83/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 84/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 85/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0160
Epoch 86/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0160
Epoch 87/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0160
Epoch 88/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0160
Epoch 89/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0160
Epoch 90/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0160
Execution time:  37.655999183654785
DNN:
Mean Absolute Error: 0.0252
Root Mean Square Error: 0.0428
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_113&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_339 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_340 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_113 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_341 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
55/55 [==============================] - 0s 7ms/step - loss: 0.1432 - val_loss: 0.0198
Epoch 2/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 0.0214
Epoch 3/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 0.0148
Epoch 4/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 0.0147
Epoch 5/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.0152
Epoch 6/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.0177
Epoch 7/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.0248
Epoch 8/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.0158
Epoch 9/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.0153
Epoch 10/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.0150
Epoch 11/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0149
Epoch 12/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0150
Epoch 13/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0148
Epoch 14/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0153
Epoch 15/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0154
Epoch 16/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0300 - val_loss: 0.0151
Epoch 17/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0156
Epoch 18/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0157
Epoch 19/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0164
Epoch 20/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0155
Epoch 21/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0159
Epoch 22/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0170
Epoch 23/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0158
Epoch 24/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0159
Epoch 25/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0158
Epoch 26/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0159
Epoch 27/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0156
Epoch 28/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0158
Epoch 29/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0156
Epoch 30/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0159
Epoch 31/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0157
Epoch 32/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0157
Epoch 33/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0156
Epoch 34/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0167
Epoch 35/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0167
Epoch 36/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0159
Epoch 37/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0157
Epoch 38/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0157
Epoch 39/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0156
Epoch 40/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0157
Epoch 41/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0158
Epoch 42/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 43/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 44/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0157
Epoch 45/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0158
Epoch 46/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0159
Epoch 47/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0159
Epoch 48/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0162
Epoch 49/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0250 - val_loss: 0.0160
Epoch 50/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0251 - val_loss: 0.0161
Epoch 51/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0161
Epoch 52/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0250 - val_loss: 0.0166
Execution time:  10.367257595062256
DNN:
Mean Absolute Error: 0.0182
Root Mean Square Error: 0.0291
Mean Square Error: 0.0008

Train RMSE: 0.029
Train MSE: 0.001
Train MAE: 0.018
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_114&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_342 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_343 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_114 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_344 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
114/114 [==============================] - 1s 5ms/step - loss: 0.0434 - val_loss: 0.0238
Epoch 2/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.0227
Epoch 3/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.0214
Epoch 4/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.0211
Epoch 5/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0212
Epoch 6/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0204
Epoch 7/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0244
Epoch 8/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0184
Epoch 9/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0182
Epoch 10/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0181
Epoch 11/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0179
Epoch 12/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0175
Epoch 13/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0170
Epoch 14/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0167
Epoch 15/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0166
Epoch 16/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0162
Epoch 17/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0161
Epoch 18/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0159
Epoch 19/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0156
Epoch 20/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0158
Epoch 21/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0150
Epoch 22/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0158
Epoch 23/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0159
Epoch 24/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0153
Epoch 25/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0154
Epoch 26/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154
Epoch 27/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0154
Epoch 28/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0154
Epoch 29/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0152
Epoch 30/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0153
Epoch 31/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0151
Epoch 32/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0153
Epoch 33/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0152
Epoch 34/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0152
Epoch 35/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0151
Epoch 36/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0150
Epoch 37/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0152
Epoch 38/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0151
Epoch 39/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0151
Epoch 40/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0151
Epoch 41/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0150
Epoch 42/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0151
Epoch 43/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0150
Epoch 44/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0150
Epoch 45/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0150
Epoch 46/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0150
Epoch 47/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0150
Epoch 48/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0149
Epoch 49/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0149
Epoch 50/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0147
Epoch 51/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0146
Epoch 52/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0143
Epoch 53/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0142
Epoch 54/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0139
Epoch 55/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0139
Epoch 56/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0140
Epoch 57/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0138
Epoch 58/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0137
Epoch 59/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0132
Epoch 60/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0129
Epoch 61/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 62/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0129
Epoch 63/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0120
Epoch 64/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0122
Epoch 65/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0129
Epoch 66/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0124
Epoch 67/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0120
Epoch 68/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0124
Epoch 69/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0118
Epoch 70/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0125
Epoch 71/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0122
Epoch 72/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0124
Epoch 73/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0123
Epoch 74/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0119
Epoch 75/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0128
Epoch 76/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0125
Epoch 77/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0120
Epoch 78/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0116
Epoch 79/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0114
Epoch 80/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0116
Execution time:  32.72496724128723
DNN:
Mean Absolute Error: 0.0210
Root Mean Square Error: 0.0372
Mean Square Error: 0.0014

Train RMSE: 0.037
Train MSE: 0.001
Train MAE: 0.021
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_115&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_345 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_346 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_115 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_347 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
142/142 [==============================] - 1s 4ms/step - loss: 0.0386 - val_loss: 0.0265
Epoch 2/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.0250
Epoch 3/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0241
Epoch 4/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0235
Epoch 5/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0223
Epoch 6/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0218
Epoch 7/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0314
Epoch 8/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0205
Epoch 9/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0189
Epoch 10/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0189
Epoch 11/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0185
Epoch 12/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0173
Epoch 13/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0180
Epoch 14/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0168
Epoch 15/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0168
Epoch 16/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0159
Epoch 17/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0159
Epoch 18/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0151
Epoch 19/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0151
Epoch 20/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0152
Epoch 21/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0152
Epoch 22/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0150
Epoch 23/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0143
Epoch 24/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0142
Epoch 25/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0138
Epoch 26/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0137
Epoch 27/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0139
Epoch 28/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0137
Epoch 29/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0135
Epoch 30/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0141
Epoch 31/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0142
Epoch 32/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0142
Epoch 33/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0142
Epoch 34/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0142
Epoch 35/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0142
Epoch 36/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0141
Epoch 37/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0139
Epoch 38/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0139
Epoch 39/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0138
Epoch 40/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0138
Epoch 41/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0138
Epoch 42/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 43/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 44/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0130
Epoch 45/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0142
Epoch 46/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 47/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0129
Epoch 48/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0127
Epoch 49/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0130
Epoch 50/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0130
Epoch 51/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0130
Epoch 52/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0129
Epoch 53/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0129
Epoch 54/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0129
Epoch 55/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0127
Epoch 56/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0125
Epoch 57/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0127
Epoch 58/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0126
Epoch 59/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0126
Epoch 60/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0126
Epoch 61/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0125
Epoch 62/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0125
Epoch 63/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0123
Epoch 64/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0124
Epoch 65/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0124
Epoch 66/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0125
Epoch 67/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0122
Epoch 68/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0120
Epoch 69/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0118
Epoch 70/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0117
Epoch 71/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0117
Epoch 72/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0118
Epoch 73/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0114
Epoch 74/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0120
Epoch 75/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0118
Epoch 76/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0121
Epoch 77/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0117
Epoch 78/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0119
Epoch 79/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0126
Epoch 80/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0117
Epoch 81/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0115
Epoch 82/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0127
Epoch 83/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0122
Epoch 84/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0128
Epoch 85/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0123
Epoch 86/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0132
Epoch 87/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0131
Epoch 88/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0124
Epoch 89/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0126
Epoch 90/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0124
Execution time:  39.5747172832489
DNN:
Mean Absolute Error: 0.0211
Root Mean Square Error: 0.0361
Mean Square Error: 0.0013

Train RMSE: 0.036
Train MSE: 0.001
Train MAE: 0.021
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_116&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_348 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_349 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_116 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_350 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
62/62 [==============================] - 0s 6ms/step - loss: 0.0759 - val_loss: 0.0721
Epoch 2/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.0470
Epoch 3/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.0381
Epoch 4/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.0364
Epoch 5/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.0337
Epoch 6/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0325
Epoch 7/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0316
Epoch 8/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0307
Epoch 9/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0298
Epoch 10/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0281
Epoch 11/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0270
Epoch 12/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0263
Epoch 13/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0253
Epoch 14/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0246
Epoch 15/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0240
Epoch 16/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0232
Epoch 17/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0228
Epoch 18/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0219
Epoch 19/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0218
Epoch 20/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0207
Epoch 21/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0207
Epoch 22/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0196
Epoch 23/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0195
Epoch 24/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0182
Epoch 25/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0187
Epoch 26/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0176
Epoch 27/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0180
Epoch 28/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0174
Epoch 29/52
62/62 [==============================] - ETA: 0s - loss: 0.027 - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0169
Epoch 30/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0168
Epoch 31/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0163
Epoch 32/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0163
Epoch 33/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0159
Epoch 34/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0158
Epoch 35/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0156
Epoch 36/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0155
Epoch 37/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0151
Epoch 38/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0152
Epoch 39/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0147
Epoch 40/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0148
Epoch 41/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0147
Epoch 42/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0143
Epoch 43/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0145
Epoch 44/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0141
Epoch 45/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0151
Epoch 46/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0146
Epoch 47/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0146
Epoch 48/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0144
Epoch 49/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0143
Epoch 50/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0141
Epoch 51/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0141
Epoch 52/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0140
Execution time:  12.363379716873169
DNN:
Mean Absolute Error: 0.0238
Root Mean Square Error: 0.0404
Mean Square Error: 0.0016

Train RMSE: 0.040
Train MSE: 0.002
Train MAE: 0.024
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_117&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_351 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_352 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_117 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_353 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
102/102 [==============================] - 1s 5ms/step - loss: 0.0432 - val_loss: 0.0190
Epoch 2/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0431 - val_loss: 0.0195
Epoch 3/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0414 - val_loss: 0.0196
Epoch 4/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0198
Epoch 5/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.0198
Epoch 6/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.0198
Epoch 7/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.0198
Epoch 8/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0196
Epoch 9/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0196
Epoch 10/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0195
Epoch 11/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0337 - val_loss: 0.0194
Epoch 12/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0331 - val_loss: 0.0193
Epoch 13/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 0.0193
Epoch 14/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0321 - val_loss: 0.0193
Epoch 15/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0316 - val_loss: 0.0191
Epoch 16/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0312 - val_loss: 0.0190
Epoch 17/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0309 - val_loss: 0.0189
Epoch 18/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0305 - val_loss: 0.0188
Epoch 19/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0303 - val_loss: 0.0186
Epoch 20/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0301 - val_loss: 0.0186
Epoch 21/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0184
Epoch 22/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0296 - val_loss: 0.0183
Epoch 23/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0182
Epoch 24/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0181
Epoch 25/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0180
Epoch 26/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0178
Epoch 27/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0178
Epoch 28/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0177
Epoch 29/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0176
Epoch 30/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0175
Epoch 31/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0174
Epoch 32/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0173
Epoch 33/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0169
Epoch 34/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0171
Epoch 35/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0170
Epoch 36/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0170
Epoch 37/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0169
Epoch 38/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0169
Epoch 39/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0169
Epoch 40/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0168
Epoch 41/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0168
Epoch 42/80
102/102 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0168
Epoch 43/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0167
Epoch 44/80
102/102 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0167
Epoch 45/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0166
Epoch 46/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0166
Epoch 47/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0166
Epoch 48/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0166
Epoch 49/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0167
Epoch 50/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0167
Epoch 51/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0166
Epoch 52/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0166
Epoch 53/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0166
Epoch 54/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0166
Epoch 55/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0166
Epoch 56/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0166
Epoch 57/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0166
Epoch 58/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0165
Epoch 59/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0166
Epoch 60/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0167
Epoch 61/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0166
Epoch 62/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0167
Epoch 63/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0169
Epoch 64/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0170
Epoch 65/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0168
Epoch 66/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0168
Epoch 67/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0164
Epoch 68/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0159
Epoch 69/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0159
Epoch 70/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0157
Epoch 71/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0157
Epoch 72/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0156
Epoch 73/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0156
Epoch 74/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0156
Epoch 75/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0155
Epoch 76/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0157
Epoch 77/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0157
Epoch 78/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0157
Epoch 79/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0155
Epoch 80/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0250 - val_loss: 0.0156
Execution time:  31.187089443206787
DNN:
Mean Absolute Error: 0.0199
Root Mean Square Error: 0.0356
Mean Square Error: 0.0013

Train RMSE: 0.036
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_118&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_354 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_355 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_118 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_356 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
126/126 [==============================] - 1s 4ms/step - loss: 0.0383 - val_loss: 0.0205
Epoch 2/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.0192
Epoch 3/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.0185
Epoch 4/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.0182
Epoch 5/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.0179
Epoch 6/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0177
Epoch 7/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0175
Epoch 8/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0174
Epoch 9/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0172
Epoch 10/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0170
Epoch 11/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0170
Epoch 12/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0170
Epoch 13/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0171
Epoch 14/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0171
Epoch 15/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0172
Epoch 16/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0171
Epoch 17/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0170
Epoch 18/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0171
Epoch 19/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0172
Epoch 20/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0171
Epoch 21/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0172
Epoch 22/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0173
Epoch 23/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0172
Epoch 24/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0172
Epoch 25/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0172
Epoch 26/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0172
Epoch 27/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0172
Epoch 28/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0172
Epoch 29/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0173
Epoch 30/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0172
Epoch 31/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0172
Epoch 32/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0172
Epoch 33/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0171
Epoch 34/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0171
Epoch 35/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0172
Epoch 36/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0172
Epoch 37/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0171
Epoch 38/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0171
Epoch 39/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0171
Epoch 40/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0170
Epoch 41/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0170
Epoch 42/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0169
Epoch 43/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0169
Epoch 44/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0169
Epoch 45/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0168
Epoch 46/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0168
Epoch 47/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0167
Epoch 48/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0167
Epoch 49/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0167
Epoch 50/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0166
Epoch 51/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0166
Epoch 52/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0165
Epoch 53/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0165
Epoch 54/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0165
Epoch 55/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0164
Epoch 56/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0163
Epoch 57/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0159
Epoch 58/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0163
Epoch 59/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0162
Epoch 60/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0161
Epoch 61/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0161
Epoch 62/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0160
Epoch 63/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0159
Epoch 64/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0159
Epoch 65/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0158
Epoch 66/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0158
Epoch 67/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0158
Epoch 68/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0158
Epoch 69/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0158
Epoch 70/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0157
Epoch 71/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0157
Epoch 72/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0158
Epoch 73/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0157
Epoch 74/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0158
Epoch 75/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0158
Epoch 76/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0157
Epoch 77/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0157
Epoch 78/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 79/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0158
Epoch 80/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0158
Epoch 81/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0158
Epoch 82/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0158
Epoch 83/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0158
Epoch 84/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0159
Epoch 85/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0159
Epoch 86/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0160
Epoch 87/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0160
Epoch 88/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0160
Epoch 89/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0160
Epoch 90/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0162
Execution time:  37.21115493774414
DNN:
Mean Absolute Error: 0.0211
Root Mean Square Error: 0.0355
Mean Square Error: 0.0013

Train RMSE: 0.035
Train MSE: 0.001
Train MAE: 0.021
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_119&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_357 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_358 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_119 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_359 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
55/55 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 0.0344
Epoch 2/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.0357
Epoch 3/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.0345
Epoch 4/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.0334
Epoch 5/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0274
Epoch 6/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.0304
Epoch 7/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0301
Epoch 8/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0295
Epoch 9/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0290
Epoch 10/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0283
Epoch 11/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0278
Epoch 12/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0271
Epoch 13/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0267
Epoch 14/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0262
Epoch 15/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0258
Epoch 16/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0254
Epoch 17/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0327 - val_loss: 0.0250
Epoch 18/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0324 - val_loss: 0.0245
Epoch 19/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0321 - val_loss: 0.0238
Epoch 20/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0317 - val_loss: 0.0235
Epoch 21/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0314 - val_loss: 0.0230
Epoch 22/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0312 - val_loss: 0.0228
Epoch 23/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0309 - val_loss: 0.0224
Epoch 24/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.0223
Epoch 25/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0303 - val_loss: 0.0218
Epoch 26/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0218
Epoch 27/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0213
Epoch 28/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0214
Epoch 29/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0207
Epoch 30/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0208
Epoch 31/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0205
Epoch 32/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0204
Epoch 33/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0201
Epoch 34/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0198
Epoch 35/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0196
Epoch 36/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0193
Epoch 37/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0193
Epoch 38/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0190
Epoch 39/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0188
Epoch 40/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0188
Epoch 41/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0184
Epoch 42/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0185
Epoch 43/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0183
Epoch 44/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0183
Epoch 45/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0181
Epoch 46/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0180
Epoch 47/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0178
Epoch 48/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0177
Epoch 49/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0177
Epoch 50/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0175
Epoch 51/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0175
Epoch 52/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0174
Execution time:  11.021506786346436
DNN:
Mean Absolute Error: 0.0217
Root Mean Square Error: 0.0372
Mean Square Error: 0.0014

Train RMSE: 0.037
Train MSE: 0.001
Train MAE: 0.022
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_120&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_360 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_361 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_120 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_362 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
114/114 [==============================] - 1s 5ms/step - loss: 0.4316 - val_loss: 0.3928
Epoch 2/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4299 - val_loss: 0.3911
Epoch 3/80
114/114 [==============================] - 0s 4ms/step - loss: 0.4281 - val_loss: 0.3892
Epoch 4/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4260 - val_loss: 0.3872
Epoch 5/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4239 - val_loss: 0.3852
Epoch 6/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4218 - val_loss: 0.3832
Epoch 7/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4198 - val_loss: 0.3812
Epoch 8/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4177 - val_loss: 0.3792
Epoch 9/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4156 - val_loss: 0.3771
Epoch 10/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4133 - val_loss: 0.3749
Epoch 11/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4110 - val_loss: 0.3727
Epoch 12/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4087 - val_loss: 0.3704
Epoch 13/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4066 - val_loss: 0.3692
Epoch 14/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4053 - val_loss: 0.3681
Epoch 15/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4040 - val_loss: 0.3669
Epoch 16/80
114/114 [==============================] - 0s 4ms/step - loss: 0.4027 - val_loss: 0.3657
Epoch 17/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4015 - val_loss: 0.3645
Epoch 18/80
114/114 [==============================] - 0s 3ms/step - loss: 0.4002 - val_loss: 0.3633
Epoch 19/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3989 - val_loss: 0.3621
Epoch 20/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3976 - val_loss: 0.3608
Epoch 21/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3963 - val_loss: 0.3595
Epoch 22/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3949 - val_loss: 0.3585
Epoch 23/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3940 - val_loss: 0.3580
Epoch 24/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3934 - val_loss: 0.3576
Epoch 25/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3929 - val_loss: 0.3571
Epoch 26/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3924 - val_loss: 0.3567
Epoch 27/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3918 - val_loss: 0.3562
Epoch 28/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3913 - val_loss: 0.3557
Epoch 29/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3908 - val_loss: 0.3552
Epoch 30/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3903 - val_loss: 0.3547
Epoch 31/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3898 - val_loss: 0.3542
Epoch 32/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3893 - val_loss: 0.3537
Epoch 33/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3888 - val_loss: 0.3532
Epoch 34/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3882 - val_loss: 0.3526
Epoch 35/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3877 - val_loss: 0.3521
Epoch 36/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3872 - val_loss: 0.3516
Epoch 37/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3866 - val_loss: 0.3510
Epoch 38/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3861 - val_loss: 0.3505
Epoch 39/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3855 - val_loss: 0.3499
Epoch 40/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3850 - val_loss: 0.3493
Epoch 41/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3844 - val_loss: 0.3488
Epoch 42/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3838 - val_loss: 0.3482
Epoch 43/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3832 - val_loss: 0.3476
Epoch 44/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3826 - val_loss: 0.3470
Epoch 45/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3820 - val_loss: 0.3464
Epoch 46/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3814 - val_loss: 0.3458
Epoch 47/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3808 - val_loss: 0.3452
Epoch 48/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3802 - val_loss: 0.3446
Epoch 49/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3796 - val_loss: 0.3440
Epoch 50/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3790 - val_loss: 0.3433
Epoch 51/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3783 - val_loss: 0.3427
Epoch 52/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3777 - val_loss: 0.3421
Epoch 53/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3771 - val_loss: 0.3414
Epoch 54/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3764 - val_loss: 0.3408
Epoch 55/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3758 - val_loss: 0.3401
Epoch 56/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3751 - val_loss: 0.3394
Epoch 57/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3744 - val_loss: 0.3388
Epoch 58/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3738 - val_loss: 0.3381
Epoch 59/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3731 - val_loss: 0.3374
Epoch 60/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3724 - val_loss: 0.3367
Epoch 61/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3717 - val_loss: 0.3360
Epoch 62/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3710 - val_loss: 0.3354
Epoch 63/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3703 - val_loss: 0.3347
Epoch 64/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3696 - val_loss: 0.3339
Epoch 65/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3689 - val_loss: 0.3332
Epoch 66/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3682 - val_loss: 0.3325
Epoch 67/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3675 - val_loss: 0.3318
Epoch 68/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 0.3311
Epoch 69/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3660 - val_loss: 0.3303
Epoch 70/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3653 - val_loss: 0.3296
Epoch 71/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3646 - val_loss: 0.3289
Epoch 72/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3638 - val_loss: 0.3281
Epoch 73/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3631 - val_loss: 0.3274
Epoch 74/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3623 - val_loss: 0.3266
Epoch 75/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3616 - val_loss: 0.3259
Epoch 76/80
114/114 [==============================] - 0s 4ms/step - loss: 0.3608 - val_loss: 0.3251
Epoch 77/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3600 - val_loss: 0.3243
Epoch 78/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3593 - val_loss: 0.3236
Epoch 79/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3585 - val_loss: 0.3228
Epoch 80/80
114/114 [==============================] - 0s 3ms/step - loss: 0.3577 - val_loss: 0.3220
Execution time:  33.14318227767944
DNN:
Mean Absolute Error: 0.3544
Root Mean Square Error: 0.3568
Mean Square Error: 0.1273

Train RMSE: 0.357
Train MSE: 0.127
Train MAE: 0.354
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_121&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_363 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_364 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_121 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_365 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
142/142 [==============================] - 1s 4ms/step - loss: 0.4083 - val_loss: 0.3713
Epoch 2/90
142/142 [==============================] - 0s 3ms/step - loss: 0.4061 - val_loss: 0.3690
Epoch 3/90
142/142 [==============================] - 0s 3ms/step - loss: 0.4036 - val_loss: 0.3665
Epoch 4/90
142/142 [==============================] - 0s 3ms/step - loss: 0.4008 - val_loss: 0.3637
Epoch 5/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3979 - val_loss: 0.3608
Epoch 6/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3947 - val_loss: 0.3576
Epoch 7/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3913 - val_loss: 0.3543
Epoch 8/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3879 - val_loss: 0.3511
Epoch 9/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3844 - val_loss: 0.3477
Epoch 10/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3808 - val_loss: 0.3442
Epoch 11/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3772 - val_loss: 0.3408
Epoch 12/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3737 - val_loss: 0.3373
Epoch 13/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3700 - val_loss: 0.3337
Epoch 14/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3662 - val_loss: 0.3300
Epoch 15/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3623 - val_loss: 0.3263
Epoch 16/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3583 - val_loss: 0.3224
Epoch 17/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3542 - val_loss: 0.3183
Epoch 18/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3499 - val_loss: 0.3142
Epoch 19/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3456 - val_loss: 0.3100
Epoch 20/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3412 - val_loss: 0.3057
Epoch 21/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3367 - val_loss: 0.3012
Epoch 22/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3320 - val_loss: 0.2967
Epoch 23/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3272 - val_loss: 0.2922
Epoch 24/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3225 - val_loss: 0.2875
Epoch 25/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3174 - val_loss: 0.2827
Epoch 26/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 0.2778
Epoch 27/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3073 - val_loss: 0.2728
Epoch 28/90
142/142 [==============================] - 0s 3ms/step - loss: 0.3020 - val_loss: 0.2677
Epoch 29/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2967 - val_loss: 0.2625
Epoch 30/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2912 - val_loss: 0.2573
Epoch 31/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2857 - val_loss: 0.2519
Epoch 32/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2800 - val_loss: 0.2464
Epoch 33/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2743 - val_loss: 0.2408
Epoch 34/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2683 - val_loss: 0.2350
Epoch 35/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2623 - val_loss: 0.2291
Epoch 36/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2563 - val_loss: 0.2231
Epoch 37/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2498 - val_loss: 0.2170
Epoch 38/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2434 - val_loss: 0.2108
Epoch 39/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2369 - val_loss: 0.2045
Epoch 40/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2304 - val_loss: 0.1988
Epoch 41/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2243 - val_loss: 0.1929
Epoch 42/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2183 - val_loss: 0.1870
Epoch 43/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2117 - val_loss: 0.1809
Epoch 44/90
142/142 [==============================] - 0s 3ms/step - loss: 0.2055 - val_loss: 0.1747
Epoch 45/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 0.1684
Epoch 46/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1925 - val_loss: 0.1620
Epoch 47/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1857 - val_loss: 0.1554
Epoch 48/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 0.1487
Epoch 49/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1722 - val_loss: 0.1419
Epoch 50/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1653 - val_loss: 0.1350
Epoch 51/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 0.1279
Epoch 52/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1506 - val_loss: 0.1207
Epoch 53/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1430 - val_loss: 0.1133
Epoch 54/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1355 - val_loss: 0.1058
Epoch 55/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1279 - val_loss: 0.0981
Epoch 56/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.0903
Epoch 57/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 0.0824
Epoch 58/90
142/142 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 0.0744
Epoch 59/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.0663
Epoch 60/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 0.0583
Epoch 61/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 0.0505
Epoch 62/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0430
Epoch 63/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.0364
Epoch 64/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.0310
Epoch 65/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0289
Epoch 66/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.0274
Epoch 67/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.0262
Epoch 68/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.0251
Epoch 69/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0594 - val_loss: 0.0246
Epoch 70/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.0241
Epoch 71/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0237
Epoch 72/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0233
Epoch 73/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.0230
Epoch 74/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0227
Epoch 75/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0225
Epoch 76/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.0223
Epoch 77/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0221
Epoch 78/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0219
Epoch 79/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0217
Epoch 80/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0594 - val_loss: 0.0216
Epoch 81/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.0214
Epoch 82/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0213
Epoch 83/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0212
Epoch 84/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.0210
Epoch 85/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0210
Epoch 86/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0210
Epoch 87/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.0210
Epoch 88/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0210
Epoch 89/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.0210
Epoch 90/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.0210
Execution time:  39.133760929107666
DNN:
Mean Absolute Error: 0.0416
Root Mean Square Error: 0.0493
Mean Square Error: 0.0024

Train RMSE: 0.049
Train MSE: 0.002
Train MAE: 0.042
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_122&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_366 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_367 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_122 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_368 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
62/62 [==============================] - 0s 6ms/step - loss: 0.3775 - val_loss: 0.3432
Epoch 2/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3766 - val_loss: 0.3423
Epoch 3/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3754 - val_loss: 0.3414
Epoch 4/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3749 - val_loss: 0.3404
Epoch 5/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3734 - val_loss: 0.3394
Epoch 6/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3727 - val_loss: 0.3383
Epoch 7/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3713 - val_loss: 0.3372
Epoch 8/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3705 - val_loss: 0.3361
Epoch 9/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3687 - val_loss: 0.3349
Epoch 10/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3680 - val_loss: 0.3337
Epoch 11/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 0.3324
Epoch 12/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3651 - val_loss: 0.3311
Epoch 13/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3636 - val_loss: 0.3298
Epoch 14/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3625 - val_loss: 0.3284
Epoch 15/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3613 - val_loss: 0.3270
Epoch 16/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3596 - val_loss: 0.3256
Epoch 17/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3583 - val_loss: 0.3241
Epoch 18/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3567 - val_loss: 0.3226
Epoch 19/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3555 - val_loss: 0.3211
Epoch 20/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3538 - val_loss: 0.3196
Epoch 21/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 0.3180
Epoch 22/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3505 - val_loss: 0.3164
Epoch 23/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3489 - val_loss: 0.3148
Epoch 24/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3472 - val_loss: 0.3131
Epoch 25/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3456 - val_loss: 0.3114
Epoch 26/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3434 - val_loss: 0.3097
Epoch 27/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3424 - val_loss: 0.3079
Epoch 28/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3400 - val_loss: 0.3062
Epoch 29/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3394 - val_loss: 0.3044
Epoch 30/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3366 - val_loss: 0.3026
Epoch 31/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3340 - val_loss: 0.3007
Epoch 32/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3327 - val_loss: 0.2988
Epoch 33/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3310 - val_loss: 0.2969
Epoch 34/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3287 - val_loss: 0.2950
Epoch 35/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3273 - val_loss: 0.2931
Epoch 36/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3254 - val_loss: 0.2911
Epoch 37/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3230 - val_loss: 0.2891
Epoch 38/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3210 - val_loss: 0.2871
Epoch 39/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3190 - val_loss: 0.2851
Epoch 40/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3172 - val_loss: 0.2830
Epoch 41/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 0.2809
Epoch 42/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 0.2789
Epoch 43/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3113 - val_loss: 0.2771
Epoch 44/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3090 - val_loss: 0.2752
Epoch 45/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3069 - val_loss: 0.2733
Epoch 46/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3050 - val_loss: 0.2714
Epoch 47/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3031 - val_loss: 0.2695
Epoch 48/52
62/62 [==============================] - 0s 3ms/step - loss: 0.3008 - val_loss: 0.2676
Epoch 49/52
62/62 [==============================] - 0s 3ms/step - loss: 0.2993 - val_loss: 0.2656
Epoch 50/52
62/62 [==============================] - 0s 3ms/step - loss: 0.2976 - val_loss: 0.2637
Epoch 51/52
62/62 [==============================] - 0s 3ms/step - loss: 0.2950 - val_loss: 0.2617
Epoch 52/52
62/62 [==============================] - 0s 3ms/step - loss: 0.2934 - val_loss: 0.2598
Execution time:  12.370065927505493
DNN:
Mean Absolute Error: 0.2871
Root Mean Square Error: 0.2892
Mean Square Error: 0.0836

Train RMSE: 0.289
Train MSE: 0.084
Train MAE: 0.287
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_123&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_369 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_370 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_123 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_371 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
102/102 [==============================] - 1s 5ms/step - loss: 0.4704 - val_loss: 0.4441
Epoch 2/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4674 - val_loss: 0.4410
Epoch 3/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4642 - val_loss: 0.4377
Epoch 4/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4607 - val_loss: 0.4342
Epoch 5/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4571 - val_loss: 0.4304
Epoch 6/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4531 - val_loss: 0.4265
Epoch 7/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4490 - val_loss: 0.4224
Epoch 8/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4449 - val_loss: 0.4185
Epoch 9/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4409 - val_loss: 0.4146
Epoch 10/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4366 - val_loss: 0.4105
Epoch 11/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4323 - val_loss: 0.4063
Epoch 12/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4280 - val_loss: 0.4020
Epoch 13/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4235 - val_loss: 0.3976
Epoch 14/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4190 - val_loss: 0.3932
Epoch 15/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4149 - val_loss: 0.3894
Epoch 16/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4109 - val_loss: 0.3857
Epoch 17/80
102/102 [==============================] - 0s 4ms/step - loss: 0.4070 - val_loss: 0.3820
Epoch 18/80
102/102 [==============================] - 0s 3ms/step - loss: 0.4031 - val_loss: 0.3781
Epoch 19/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3990 - val_loss: 0.3742
Epoch 20/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3949 - val_loss: 0.3702
Epoch 21/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3909 - val_loss: 0.3661
Epoch 22/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3866 - val_loss: 0.3619
Epoch 23/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3822 - val_loss: 0.3576
Epoch 24/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3778 - val_loss: 0.3533
Epoch 25/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3735 - val_loss: 0.3493
Epoch 26/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3694 - val_loss: 0.3452
Epoch 27/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3649 - val_loss: 0.3411
Epoch 28/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3607 - val_loss: 0.3369
Epoch 29/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3564 - val_loss: 0.3327
Epoch 30/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3520 - val_loss: 0.3284
Epoch 31/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3476 - val_loss: 0.3245
Epoch 32/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3438 - val_loss: 0.3211
Epoch 33/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3401 - val_loss: 0.3177
Epoch 34/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3367 - val_loss: 0.3143
Epoch 35/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 0.3109
Epoch 36/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3296 - val_loss: 0.3079
Epoch 37/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3266 - val_loss: 0.3055
Epoch 38/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3239 - val_loss: 0.3030
Epoch 39/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3214 - val_loss: 0.3006
Epoch 40/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3188 - val_loss: 0.2982
Epoch 41/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3162 - val_loss: 0.2957
Epoch 42/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3138 - val_loss: 0.2934
Epoch 43/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3112 - val_loss: 0.2909
Epoch 44/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3086 - val_loss: 0.2885
Epoch 45/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3060 - val_loss: 0.2860
Epoch 46/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3033 - val_loss: 0.2834
Epoch 47/80
102/102 [==============================] - 0s 4ms/step - loss: 0.3008 - val_loss: 0.2808
Epoch 48/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2980 - val_loss: 0.2781
Epoch 49/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2953 - val_loss: 0.2754
Epoch 50/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2925 - val_loss: 0.2726
Epoch 51/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2896 - val_loss: 0.2698
Epoch 52/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2865 - val_loss: 0.2669
Epoch 53/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2837 - val_loss: 0.2639
Epoch 54/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2807 - val_loss: 0.2609
Epoch 55/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2778 - val_loss: 0.2579
Epoch 56/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2745 - val_loss: 0.2547
Epoch 57/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2711 - val_loss: 0.2516
Epoch 58/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2680 - val_loss: 0.2483
Epoch 59/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2645 - val_loss: 0.2450
Epoch 60/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2612 - val_loss: 0.2416
Epoch 61/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2579 - val_loss: 0.2381
Epoch 62/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2543 - val_loss: 0.2346
Epoch 63/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2508 - val_loss: 0.2310
Epoch 64/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2469 - val_loss: 0.2273
Epoch 65/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2430 - val_loss: 0.2236
Epoch 66/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2395 - val_loss: 0.2198
Epoch 67/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2355 - val_loss: 0.2159
Epoch 68/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2314 - val_loss: 0.2120
Epoch 69/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2275 - val_loss: 0.2079
Epoch 70/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2236 - val_loss: 0.2038
Epoch 71/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2193 - val_loss: 0.1996
Epoch 72/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2150 - val_loss: 0.1954
Epoch 73/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2110 - val_loss: 0.1910
Epoch 74/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2065 - val_loss: 0.1866
Epoch 75/80
102/102 [==============================] - 0s 4ms/step - loss: 0.2019 - val_loss: 0.1821
Epoch 76/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1976 - val_loss: 0.1775
Epoch 77/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1928 - val_loss: 0.1728
Epoch 78/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1880 - val_loss: 0.1681
Epoch 79/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1834 - val_loss: 0.1633
Epoch 80/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1789 - val_loss: 0.1584
Execution time:  31.474199771881104
DNN:
Mean Absolute Error: 0.1714
Root Mean Square Error: 0.1742
Mean Square Error: 0.0303

Train RMSE: 0.174
Train MSE: 0.030
Train MAE: 0.171
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_124&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_372 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_373 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_124 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_374 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
126/126 [==============================] - 1s 4ms/step - loss: 0.4022 - val_loss: 0.3795
Epoch 2/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3992 - val_loss: 0.3762
Epoch 3/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3956 - val_loss: 0.3726
Epoch 4/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3918 - val_loss: 0.3687
Epoch 5/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3877 - val_loss: 0.3645
Epoch 6/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3831 - val_loss: 0.3600
Epoch 7/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3785 - val_loss: 0.3553
Epoch 8/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3734 - val_loss: 0.3502
Epoch 9/90
126/126 [==============================] - 0s 4ms/step - loss: 0.3681 - val_loss: 0.3450
Epoch 10/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3627 - val_loss: 0.3395
Epoch 11/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3570 - val_loss: 0.3339
Epoch 12/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3512 - val_loss: 0.3281
Epoch 13/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3451 - val_loss: 0.3220
Epoch 14/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3386 - val_loss: 0.3157
Epoch 15/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3321 - val_loss: 0.3091
Epoch 16/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3251 - val_loss: 0.3023
Epoch 17/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3181 - val_loss: 0.2953
Epoch 18/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3107 - val_loss: 0.2880
Epoch 19/90
126/126 [==============================] - 0s 3ms/step - loss: 0.3032 - val_loss: 0.2804
Epoch 20/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2952 - val_loss: 0.2725
Epoch 21/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2869 - val_loss: 0.2643
Epoch 22/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2784 - val_loss: 0.2557
Epoch 23/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 0.2468
Epoch 24/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2604 - val_loss: 0.2376
Epoch 25/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2506 - val_loss: 0.2280
Epoch 26/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 0.2179
Epoch 27/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 0.2074
Epoch 28/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2193 - val_loss: 0.1965
Epoch 29/90
126/126 [==============================] - 0s 3ms/step - loss: 0.2078 - val_loss: 0.1851
Epoch 30/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1958 - val_loss: 0.1731
Epoch 31/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 0.1607
Epoch 32/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1707 - val_loss: 0.1484
Epoch 33/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 0.1358
Epoch 34/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1450 - val_loss: 0.1227
Epoch 35/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1312 - val_loss: 0.1091
Epoch 36/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1172 - val_loss: 0.0948
Epoch 37/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.0800
Epoch 38/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 0.0650
Epoch 39/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 0.0503
Epoch 40/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.0409
Epoch 41/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0348
Epoch 42/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.0308
Epoch 43/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.0283
Epoch 44/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.0266
Epoch 45/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0537 - val_loss: 0.0253
Epoch 46/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.0242
Epoch 47/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0536 - val_loss: 0.0233
Epoch 48/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.0226
Epoch 49/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.0219
Epoch 50/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.0214
Epoch 51/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0529 - val_loss: 0.0208
Epoch 52/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.0203
Epoch 53/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.0199
Epoch 54/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.0195
Epoch 55/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.0191
Epoch 56/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.0188
Epoch 57/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.0185
Epoch 58/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.0182
Epoch 59/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.0179
Epoch 60/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.0177
Epoch 61/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.0174
Epoch 62/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.0172
Epoch 63/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.0170
Epoch 64/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.0168
Epoch 65/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0166
Epoch 66/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0164
Epoch 67/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0162
Epoch 68/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.0161
Epoch 69/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.0159
Epoch 70/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0158
Epoch 71/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.0157
Epoch 72/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.0155
Epoch 73/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.0155
Epoch 74/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.0154
Epoch 75/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.0154
Epoch 76/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0154
Epoch 77/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0153
Epoch 78/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0153
Epoch 79/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0153
Epoch 80/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0152
Epoch 81/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0152
Epoch 82/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.0152
Epoch 83/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.0152
Epoch 84/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.0152
Epoch 85/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0151
Epoch 86/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.0151
Epoch 87/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0151
Epoch 88/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.0151
Epoch 89/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.0150
Epoch 90/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0150
Execution time:  37.762080907821655
DNN:
Mean Absolute Error: 0.0201
Root Mean Square Error: 0.0313
Mean Square Error: 0.0010

Train RMSE: 0.031
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_125&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_375 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_376 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_125 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_377 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
55/55 [==============================] - 0s 7ms/step - loss: 0.4327 - val_loss: 0.4094
Epoch 2/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4318 - val_loss: 0.4088
Epoch 3/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4317 - val_loss: 0.4083
Epoch 4/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4309 - val_loss: 0.4077
Epoch 5/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4303 - val_loss: 0.4071
Epoch 6/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4297 - val_loss: 0.4065
Epoch 7/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4286 - val_loss: 0.4059
Epoch 8/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4284 - val_loss: 0.4052
Epoch 9/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4275 - val_loss: 0.4045
Epoch 10/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4266 - val_loss: 0.4038
Epoch 11/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4261 - val_loss: 0.4031
Epoch 12/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4254 - val_loss: 0.4023
Epoch 13/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4246 - val_loss: 0.4016
Epoch 14/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4238 - val_loss: 0.4008
Epoch 15/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4226 - val_loss: 0.4000
Epoch 16/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4222 - val_loss: 0.3992
Epoch 17/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4215 - val_loss: 0.3983
Epoch 18/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4205 - val_loss: 0.3975
Epoch 19/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4194 - val_loss: 0.3966
Epoch 20/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4186 - val_loss: 0.3957
Epoch 21/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4176 - val_loss: 0.3948
Epoch 22/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4165 - val_loss: 0.3939
Epoch 23/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4158 - val_loss: 0.3930
Epoch 24/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4150 - val_loss: 0.3921
Epoch 25/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4140 - val_loss: 0.3911
Epoch 26/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4131 - val_loss: 0.3901
Epoch 27/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4122 - val_loss: 0.3891
Epoch 28/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4110 - val_loss: 0.3881
Epoch 29/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4101 - val_loss: 0.3871
Epoch 30/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4091 - val_loss: 0.3861
Epoch 31/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4079 - val_loss: 0.3850
Epoch 32/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4068 - val_loss: 0.3840
Epoch 33/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4058 - val_loss: 0.3829
Epoch 34/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4047 - val_loss: 0.3818
Epoch 35/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4036 - val_loss: 0.3808
Epoch 36/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4026 - val_loss: 0.3796
Epoch 37/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4015 - val_loss: 0.3785
Epoch 38/52
55/55 [==============================] - 0s 3ms/step - loss: 0.4003 - val_loss: 0.3774
Epoch 39/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3990 - val_loss: 0.3763
Epoch 40/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3980 - val_loss: 0.3751
Epoch 41/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3965 - val_loss: 0.3739
Epoch 42/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 0.3728
Epoch 43/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3940 - val_loss: 0.3716
Epoch 44/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3934 - val_loss: 0.3704
Epoch 45/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3919 - val_loss: 0.3691
Epoch 46/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3907 - val_loss: 0.3679
Epoch 47/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3897 - val_loss: 0.3667
Epoch 48/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3881 - val_loss: 0.3654
Epoch 49/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3869 - val_loss: 0.3642
Epoch 50/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 0.3629
Epoch 51/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3844 - val_loss: 0.3616
Epoch 52/52
55/55 [==============================] - 0s 3ms/step - loss: 0.3833 - val_loss: 0.3603
Execution time:  10.224256038665771
DNN:
Mean Absolute Error: 0.3785
Root Mean Square Error: 0.3808
Mean Square Error: 0.1450

Train RMSE: 0.381
Train MSE: 0.145
Train MAE: 0.378
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_126&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_378 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_379 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_126 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_380 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
114/114 [==============================] - 1s 5ms/step - loss: 0.0891 - val_loss: 0.1219
Epoch 2/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 0.1211
Epoch 3/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 0.1202
Epoch 4/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 0.1194
Epoch 5/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 0.1185
Epoch 6/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0850 - val_loss: 0.1175
Epoch 7/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0841 - val_loss: 0.1165
Epoch 8/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0831 - val_loss: 0.1155
Epoch 9/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0822 - val_loss: 0.1144
Epoch 10/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0811 - val_loss: 0.1134
Epoch 11/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0802 - val_loss: 0.1124
Epoch 12/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0793 - val_loss: 0.1115
Epoch 13/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 0.1105
Epoch 14/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0776 - val_loss: 0.1095
Epoch 15/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0767 - val_loss: 0.1085
Epoch 16/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0759 - val_loss: 0.1075
Epoch 17/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.1065
Epoch 18/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.1054
Epoch 19/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0732 - val_loss: 0.1044
Epoch 20/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0724 - val_loss: 0.1034
Epoch 21/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.1023
Epoch 22/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0706 - val_loss: 0.1013
Epoch 23/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.1003
Epoch 24/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.0992
Epoch 25/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0680 - val_loss: 0.0982
Epoch 26/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0672 - val_loss: 0.0972
Epoch 27/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0664 - val_loss: 0.0962
Epoch 28/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.0952
Epoch 29/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.0942
Epoch 30/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.0932
Epoch 31/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.0922
Epoch 32/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.0913
Epoch 33/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.0904
Epoch 34/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.0894
Epoch 35/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0887
Epoch 36/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.0879
Epoch 37/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.0872
Epoch 38/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0865
Epoch 39/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.0858
Epoch 40/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0851
Epoch 41/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.0845
Epoch 42/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.0838
Epoch 43/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.0832
Epoch 44/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.0826
Epoch 45/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0563 - val_loss: 0.0820
Epoch 46/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0560 - val_loss: 0.0814
Epoch 47/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0555 - val_loss: 0.0807
Epoch 48/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0552 - val_loss: 0.0802
Epoch 49/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0548 - val_loss: 0.0796
Epoch 50/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0545 - val_loss: 0.0790
Epoch 51/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0541 - val_loss: 0.0784
Epoch 52/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.0778
Epoch 53/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.0773
Epoch 54/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.0767
Epoch 55/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.0762
Epoch 56/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0524 - val_loss: 0.0756
Epoch 57/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.0751
Epoch 58/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0520 - val_loss: 0.0746
Epoch 59/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0741
Epoch 60/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.0735
Epoch 61/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.0730
Epoch 62/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.0725
Epoch 63/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.0720
Epoch 64/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0504 - val_loss: 0.0716
Epoch 65/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0711
Epoch 66/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.0706
Epoch 67/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0701
Epoch 68/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.0697
Epoch 69/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.0692
Epoch 70/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.0688
Epoch 71/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.0684
Epoch 72/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.0679
Epoch 73/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.0675
Epoch 74/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.0671
Epoch 75/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.0667
Epoch 76/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.0663
Epoch 77/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.0659
Epoch 78/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.0655
Epoch 79/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.0651
Epoch 80/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.0648
Execution time:  32.76419496536255
DNN:
Mean Absolute Error: 0.0454
Root Mean Square Error: 0.0539
Mean Square Error: 0.0029

Train RMSE: 0.054
Train MSE: 0.003
Train MAE: 0.045
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_127&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_381 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_382 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_127 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_383 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
142/142 [==============================] - 1s 6ms/step - loss: 0.0777 - val_loss: 0.1106
Epoch 2/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 0.1097
Epoch 3/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 0.1088
Epoch 4/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.1077
Epoch 5/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 0.1067
Epoch 6/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0732 - val_loss: 0.1055
Epoch 7/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.1043
Epoch 8/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.1031
Epoch 9/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.1019
Epoch 10/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.1006
Epoch 11/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.0993
Epoch 12/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.0980
Epoch 13/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.0966
Epoch 14/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0645 - val_loss: 0.0953
Epoch 15/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.0939
Epoch 16/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.0925
Epoch 17/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.0911
Epoch 18/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0898
Epoch 19/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0884
Epoch 20/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.0870
Epoch 21/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.0857
Epoch 22/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.0843
Epoch 23/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.0830
Epoch 24/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.0818
Epoch 25/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.0805
Epoch 26/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.0793
Epoch 27/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.0781
Epoch 28/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.0769
Epoch 29/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0757
Epoch 30/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.0746
Epoch 31/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.0734
Epoch 32/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.0724
Epoch 33/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.0713
Epoch 34/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.0702
Epoch 35/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.0691
Epoch 36/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.0681
Epoch 37/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.0671
Epoch 38/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0661
Epoch 39/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.0651
Epoch 40/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0641
Epoch 41/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.0631
Epoch 42/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.0621
Epoch 43/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0612
Epoch 44/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.0603
Epoch 45/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0593
Epoch 46/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.0584
Epoch 47/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0575
Epoch 48/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.0566
Epoch 49/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.0557
Epoch 50/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.0549
Epoch 51/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.0540
Epoch 52/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0532
Epoch 53/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.0524
Epoch 54/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.0516
Epoch 55/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0508
Epoch 56/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0500
Epoch 57/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0493
Epoch 58/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0485
Epoch 59/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0478
Epoch 60/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0471
Epoch 61/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0464
Epoch 62/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0458
Epoch 63/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0451
Epoch 64/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0445
Epoch 65/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0439
Epoch 66/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0433
Epoch 67/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0428
Epoch 68/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0423
Epoch 69/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0418
Epoch 70/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0413
Epoch 71/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0408
Epoch 72/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0404
Epoch 73/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0399
Epoch 74/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0395
Epoch 75/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0391
Epoch 76/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0388
Epoch 77/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0384
Epoch 78/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0381
Epoch 79/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0377
Epoch 80/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0374
Epoch 81/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0371
Epoch 82/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0368
Epoch 83/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0366
Epoch 84/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0363
Epoch 85/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0360
Epoch 86/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0358
Epoch 87/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0356
Epoch 88/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0354
Epoch 89/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0352
Epoch 90/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0350
Execution time:  39.81482410430908
DNN:
Mean Absolute Error: 0.0325
Root Mean Square Error: 0.0471
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.032
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_128&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_384 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_385 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_128 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_386 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
62/62 [==============================] - 0s 6ms/step - loss: 0.1020 - val_loss: 0.1350
Epoch 2/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1019 - val_loss: 0.1349
Epoch 3/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 0.1347
Epoch 4/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1017 - val_loss: 0.1346
Epoch 5/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1016 - val_loss: 0.1345
Epoch 6/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 0.1344
Epoch 7/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 0.1342
Epoch 8/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 0.1341
Epoch 9/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1011 - val_loss: 0.1340
Epoch 10/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1009 - val_loss: 0.1338
Epoch 11/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1337
Epoch 12/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1007 - val_loss: 0.1335
Epoch 13/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1005 - val_loss: 0.1333
Epoch 14/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1003 - val_loss: 0.1332
Epoch 15/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 0.1330
Epoch 16/52
62/62 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 0.1328
Epoch 17/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 0.1327
Epoch 18/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 0.1325
Epoch 19/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0995 - val_loss: 0.1323
Epoch 20/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0993 - val_loss: 0.1321
Epoch 21/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 0.1320
Epoch 22/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 0.1318
Epoch 23/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 0.1316
Epoch 24/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 0.1314
Epoch 25/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0984 - val_loss: 0.1312
Epoch 26/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 0.1310
Epoch 27/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 0.1308
Epoch 28/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 0.1306
Epoch 29/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 0.1304
Epoch 30/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 0.1302
Epoch 31/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0972 - val_loss: 0.1299
Epoch 32/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 0.1297
Epoch 33/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.1295
Epoch 34/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0965 - val_loss: 0.1293
Epoch 35/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 0.1291
Epoch 36/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 0.1288
Epoch 37/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.1286
Epoch 38/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 0.1283
Epoch 39/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 0.1280
Epoch 40/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 0.1277
Epoch 41/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 0.1274
Epoch 42/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0945 - val_loss: 0.1271
Epoch 43/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0942 - val_loss: 0.1268
Epoch 44/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0939 - val_loss: 0.1265
Epoch 45/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0936 - val_loss: 0.1262
Epoch 46/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0933 - val_loss: 0.1259
Epoch 47/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 0.1256
Epoch 48/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0927 - val_loss: 0.1253
Epoch 49/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.1250
Epoch 50/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 0.1246
Epoch 51/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.1243
Epoch 52/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 0.1240
Execution time:  12.301923751831055
DNN:
Mean Absolute Error: 0.0938
Root Mean Square Error: 0.1003
Mean Square Error: 0.0101

Train RMSE: 0.100
Train MSE: 0.010
Train MAE: 0.094
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_129&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_387 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_388 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_129 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_389 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
102/102 [==============================] - 1s 5ms/step - loss: 0.1077 - val_loss: 0.1270
Epoch 2/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1072 - val_loss: 0.1265
Epoch 3/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1065 - val_loss: 0.1259
Epoch 4/80
102/102 [==============================] - 0s 3ms/step - loss: 0.1059 - val_loss: 0.1252
Epoch 5/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1053 - val_loss: 0.1246
Epoch 6/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1045 - val_loss: 0.1239
Epoch 7/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1038 - val_loss: 0.1232
Epoch 8/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1030 - val_loss: 0.1224
Epoch 9/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1022 - val_loss: 0.1217
Epoch 10/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1015 - val_loss: 0.1209
Epoch 11/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1006 - val_loss: 0.1200
Epoch 12/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0998 - val_loss: 0.1192
Epoch 13/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0989 - val_loss: 0.1183
Epoch 14/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0980 - val_loss: 0.1175
Epoch 15/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 0.1166
Epoch 16/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0961 - val_loss: 0.1157
Epoch 17/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0953 - val_loss: 0.1150
Epoch 18/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0945 - val_loss: 0.1141
Epoch 19/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0934 - val_loss: 0.1130
Epoch 20/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0923 - val_loss: 0.1120
Epoch 21/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0912 - val_loss: 0.1109
Epoch 22/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0902 - val_loss: 0.1098
Epoch 23/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0891 - val_loss: 0.1088
Epoch 24/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0879 - val_loss: 0.1077
Epoch 25/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0868 - val_loss: 0.1066
Epoch 26/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0857 - val_loss: 0.1055
Epoch 27/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0846 - val_loss: 0.1045
Epoch 28/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0835 - val_loss: 0.1034
Epoch 29/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0824 - val_loss: 0.1023
Epoch 30/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0814 - val_loss: 0.1012
Epoch 31/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0803 - val_loss: 0.1001
Epoch 32/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 0.0991
Epoch 33/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0782 - val_loss: 0.0980
Epoch 34/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0771 - val_loss: 0.0969
Epoch 35/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0761 - val_loss: 0.0958
Epoch 36/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0750 - val_loss: 0.0948
Epoch 37/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0740 - val_loss: 0.0937
Epoch 38/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0927
Epoch 39/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0917
Epoch 40/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0908
Epoch 41/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.0900
Epoch 42/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0695 - val_loss: 0.0893
Epoch 43/80
102/102 [==============================] - 0s 3ms/step - loss: 0.0687 - val_loss: 0.0885
Epoch 44/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0680 - val_loss: 0.0878
Epoch 45/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.0870
Epoch 46/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0667 - val_loss: 0.0863
Epoch 47/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0661 - val_loss: 0.0856
Epoch 48/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0654 - val_loss: 0.0848
Epoch 49/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0648 - val_loss: 0.0841
Epoch 50/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0642 - val_loss: 0.0834
Epoch 51/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0635 - val_loss: 0.0827
Epoch 52/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0629 - val_loss: 0.0821
Epoch 53/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0624 - val_loss: 0.0814
Epoch 54/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0619 - val_loss: 0.0807
Epoch 55/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.0801
Epoch 56/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0609 - val_loss: 0.0794
Epoch 57/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0788
Epoch 58/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0782
Epoch 59/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0776
Epoch 60/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0770
Epoch 61/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0764
Epoch 62/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0758
Epoch 63/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0752
Epoch 64/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0572 - val_loss: 0.0747
Epoch 65/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0568 - val_loss: 0.0741
Epoch 66/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.0736
Epoch 67/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0560 - val_loss: 0.0731
Epoch 68/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0556 - val_loss: 0.0725
Epoch 69/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0553 - val_loss: 0.0720
Epoch 70/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0549 - val_loss: 0.0715
Epoch 71/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0545 - val_loss: 0.0710
Epoch 72/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0542 - val_loss: 0.0705
Epoch 73/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0539 - val_loss: 0.0701
Epoch 74/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0536 - val_loss: 0.0696
Epoch 75/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0532 - val_loss: 0.0691
Epoch 76/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0530 - val_loss: 0.0687
Epoch 77/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0527 - val_loss: 0.0682
Epoch 78/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0524 - val_loss: 0.0678
Epoch 79/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0521 - val_loss: 0.0673
Epoch 80/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0519 - val_loss: 0.0669
Execution time:  31.24636197090149
DNN:
Mean Absolute Error: 0.0527
Root Mean Square Error: 0.0608
Mean Square Error: 0.0037

Train RMSE: 0.061
Train MSE: 0.004
Train MAE: 0.053
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_130&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_390 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_391 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_130 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_392 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
126/126 [==============================] - 1s 4ms/step - loss: 0.1154 - val_loss: 0.1342
Epoch 2/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1147 - val_loss: 0.1334
Epoch 3/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1139 - val_loss: 0.1326
Epoch 4/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 0.1317
Epoch 5/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 0.1308
Epoch 6/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1110 - val_loss: 0.1298
Epoch 7/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 0.1288
Epoch 8/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 0.1277
Epoch 9/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1077 - val_loss: 0.1266
Epoch 10/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1066 - val_loss: 0.1255
Epoch 11/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 0.1244
Epoch 12/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1043 - val_loss: 0.1232
Epoch 13/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1031 - val_loss: 0.1222
Epoch 14/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1021 - val_loss: 0.1214
Epoch 15/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 0.1206
Epoch 16/90
126/126 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.1198
Epoch 17/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0995 - val_loss: 0.1189
Epoch 18/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 0.1181
Epoch 19/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 0.1172
Epoch 20/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.1164
Epoch 21/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.1155
Epoch 22/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.1146
Epoch 23/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 0.1137
Epoch 24/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 0.1127
Epoch 25/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0922 - val_loss: 0.1118
Epoch 26/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 0.1108
Epoch 27/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 0.1099
Epoch 28/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0891 - val_loss: 0.1086
Epoch 29/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0877 - val_loss: 0.1072
Epoch 30/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 0.1059
Epoch 31/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.1046
Epoch 32/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0836 - val_loss: 0.1032
Epoch 33/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 0.1019
Epoch 34/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0809 - val_loss: 0.1005
Epoch 35/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0992
Epoch 36/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0781 - val_loss: 0.0977
Epoch 37/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0765 - val_loss: 0.0959
Epoch 38/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 0.0942
Epoch 39/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0924
Epoch 40/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0907
Epoch 41/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.0890
Epoch 42/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0680 - val_loss: 0.0873
Epoch 43/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0664 - val_loss: 0.0856
Epoch 44/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.0840
Epoch 45/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.0823
Epoch 46/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.0807
Epoch 47/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0792
Epoch 48/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.0777
Epoch 49/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0577 - val_loss: 0.0762
Epoch 50/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.0748
Epoch 51/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.0733
Epoch 52/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0541 - val_loss: 0.0720
Epoch 53/90
126/126 [==============================] - ETA: 0s - loss: 0.051 - 0s 3ms/step - loss: 0.0531 - val_loss: 0.0707
Epoch 54/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.0695
Epoch 55/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.0683
Epoch 56/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0671
Epoch 57/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.0660
Epoch 58/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.0649
Epoch 59/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.0639
Epoch 60/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.0629
Epoch 61/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.0619
Epoch 62/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.0610
Epoch 63/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.0601
Epoch 64/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.0593
Epoch 65/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0585
Epoch 66/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0577
Epoch 67/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.0569
Epoch 68/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.0561
Epoch 69/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.0554
Epoch 70/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.0546
Epoch 71/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.0539
Epoch 72/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.0532
Epoch 73/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0525
Epoch 74/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.0518
Epoch 75/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0512
Epoch 76/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.0505
Epoch 77/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.0499
Epoch 78/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0493
Epoch 79/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.0487
Epoch 80/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.0481
Epoch 81/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.0475
Epoch 82/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.0469
Epoch 83/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.0464
Epoch 84/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0459
Epoch 85/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.0453
Epoch 86/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0449
Epoch 87/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0444
Epoch 88/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0439
Epoch 89/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0435
Epoch 90/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0431
Execution time:  37.128817558288574
DNN:
Mean Absolute Error: 0.0373
Root Mean Square Error: 0.0466
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.037
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_131&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_393 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_394 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_131 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_395 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
55/55 [==============================] - 0s 6ms/step - loss: 0.0897 - val_loss: 0.1101
Epoch 2/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0895 - val_loss: 0.1100
Epoch 3/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 0.1099
Epoch 4/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 0.1098
Epoch 5/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0892 - val_loss: 0.1097
Epoch 6/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0892 - val_loss: 0.1096
Epoch 7/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0891 - val_loss: 0.1095
Epoch 8/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0889 - val_loss: 0.1094
Epoch 9/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 0.1092
Epoch 10/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 0.1091
Epoch 11/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0886 - val_loss: 0.1090
Epoch 12/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0884 - val_loss: 0.1089
Epoch 13/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 0.1087
Epoch 14/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 0.1086
Epoch 15/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0880 - val_loss: 0.1084
Epoch 16/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 0.1083
Epoch 17/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0877 - val_loss: 0.1082
Epoch 18/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 0.1080
Epoch 19/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 0.1079
Epoch 20/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 0.1077
Epoch 21/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 0.1076
Epoch 22/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 0.1074
Epoch 23/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 0.1073
Epoch 24/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 0.1071
Epoch 25/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0865 - val_loss: 0.1069
Epoch 26/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 0.1068
Epoch 27/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 0.1066
Epoch 28/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0860 - val_loss: 0.1064
Epoch 29/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 0.1063
Epoch 30/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0857 - val_loss: 0.1061
Epoch 31/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 0.1059
Epoch 32/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0853 - val_loss: 0.1058
Epoch 33/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 0.1056
Epoch 34/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.1054
Epoch 35/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 0.1052
Epoch 36/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.1051
Epoch 37/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0845 - val_loss: 0.1049
Epoch 38/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0843 - val_loss: 0.1047
Epoch 39/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0842 - val_loss: 0.1045
Epoch 40/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 0.1043
Epoch 41/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.1042
Epoch 42/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0836 - val_loss: 0.1040
Epoch 43/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0835 - val_loss: 0.1038
Epoch 44/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0833 - val_loss: 0.1036
Epoch 45/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0831 - val_loss: 0.1034
Epoch 46/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0829 - val_loss: 0.1032
Epoch 47/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0827 - val_loss: 0.1030
Epoch 48/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 0.1028
Epoch 49/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0823 - val_loss: 0.1026
Epoch 50/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 0.1025
Epoch 51/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 0.1023
Epoch 52/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 0.1021
Execution time:  10.273429155349731
DNN:
Mean Absolute Error: 0.0849
Root Mean Square Error: 0.0920
Mean Square Error: 0.0085

Train RMSE: 0.092
Train MSE: 0.008
Train MAE: 0.085
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_132&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_396 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_397 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_132 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_398 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
114/114 [==============================] - 1s 5ms/step - loss: 0.1883 - val_loss: 0.0435
Epoch 2/80
114/114 [==============================] - 0s 3ms/step - loss: 0.1075 - val_loss: 0.0260
Epoch 3/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 0.0095
Epoch 4/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.0064
Epoch 5/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.0061
Epoch 6/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.0056
Epoch 7/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0059
Epoch 8/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.0061
Epoch 9/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.0072
Epoch 10/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0077
Epoch 11/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0142
Epoch 12/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0319 - val_loss: 0.0129
Epoch 13/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0309 - val_loss: 0.0118
Epoch 14/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0120
Epoch 15/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0100
Epoch 16/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0098
Epoch 17/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0093
Epoch 18/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0097
Epoch 19/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0096
Epoch 20/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0118
Epoch 21/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0130
Epoch 22/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0132
Epoch 23/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0121
Epoch 24/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 25/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0114
Epoch 26/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0147
Epoch 27/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0123
Epoch 28/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0110
Epoch 29/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0117
Epoch 30/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0121
Epoch 31/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0113
Epoch 32/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0121
Epoch 33/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0113
Epoch 34/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0122
Epoch 35/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0106
Epoch 36/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0104
Epoch 37/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0111
Epoch 38/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0111
Epoch 39/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0107
Epoch 40/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0104
Epoch 41/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0109
Epoch 42/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0104
Epoch 43/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0104
Epoch 44/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0103
Epoch 45/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0105
Epoch 46/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0104
Epoch 47/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0109
Epoch 48/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0103
Epoch 49/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0108
Epoch 50/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0104
Epoch 51/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0110
Epoch 52/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0103
Epoch 53/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0107
Epoch 54/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0100
Epoch 55/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0107
Epoch 56/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0108
Epoch 57/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0110
Epoch 58/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0098
Epoch 59/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0108
Epoch 60/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0100
Epoch 61/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0110
Epoch 62/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0102
Epoch 63/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0105
Epoch 64/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0102
Epoch 65/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0105
Epoch 66/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0103
Epoch 67/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0105
Epoch 68/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0102
Epoch 69/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0105
Epoch 70/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0097
Epoch 71/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0104
Epoch 72/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0250 - val_loss: 0.0098
Epoch 73/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0105
Epoch 74/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0248 - val_loss: 0.0097
Epoch 75/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0105
Epoch 76/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.0100
Epoch 77/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0107
Epoch 78/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0245 - val_loss: 0.0100
Epoch 79/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0108
Epoch 80/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0096
Execution time:  33.429774045944214
DNN:
Mean Absolute Error: 0.0234
Root Mean Square Error: 0.0383
Mean Square Error: 0.0015

Train RMSE: 0.038
Train MSE: 0.001
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_133&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_399 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_400 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_133 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_401 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
142/142 [==============================] - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0263
Epoch 2/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.0186
Epoch 3/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0181
Epoch 4/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.0149
Epoch 5/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.0106
Epoch 6/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.0124
Epoch 7/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.0092
Epoch 8/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0077
Epoch 9/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.0064
Epoch 10/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.0073
Epoch 11/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0084
Epoch 12/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0084
Epoch 13/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0086
Epoch 14/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0090
Epoch 15/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0085
Epoch 16/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0102
Epoch 17/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0107
Epoch 18/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0120
Epoch 19/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0111
Epoch 20/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0121
Epoch 21/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0127
Epoch 22/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0137
Epoch 23/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0143
Epoch 24/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0156
Epoch 25/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 26/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 27/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0180
Epoch 28/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 29/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0187
Epoch 30/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0187
Epoch 31/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0198
Epoch 32/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0181
Epoch 33/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0179
Epoch 34/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0180
Epoch 35/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0177
Epoch 36/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0162
Epoch 37/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0128
Epoch 38/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0114
Epoch 39/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0126
Epoch 40/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0103
Epoch 41/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0147
Epoch 42/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0112
Epoch 43/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0120
Epoch 44/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0111
Epoch 45/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0120
Epoch 46/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0112
Epoch 47/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0118
Epoch 48/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0118
Epoch 49/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0118
Epoch 50/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0111
Epoch 51/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0115
Epoch 52/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0105
Epoch 53/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0114
Epoch 54/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0107
Epoch 55/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0115
Epoch 56/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0104
Epoch 57/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0119
Epoch 58/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0101
Epoch 59/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0131
Epoch 60/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0102
Epoch 61/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0123
Epoch 62/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0104
Epoch 63/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 64/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0103
Epoch 65/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0118
Epoch 66/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0102
Epoch 67/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0114
Epoch 68/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0100
Epoch 69/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0115
Epoch 70/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0101
Epoch 71/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0122
Epoch 72/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0102
Epoch 73/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0125
Epoch 74/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0101
Epoch 75/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0125
Epoch 76/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0100
Epoch 77/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0124
Epoch 78/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0098
Epoch 79/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0124
Epoch 80/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0104
Epoch 81/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0119
Epoch 82/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0108
Epoch 83/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0120
Epoch 84/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0115
Epoch 85/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0120
Epoch 86/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0120
Epoch 87/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0120
Epoch 88/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0121
Epoch 89/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0118
Epoch 90/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0121
Execution time:  39.64670276641846
DNN:
Mean Absolute Error: 0.0193
Root Mean Square Error: 0.0348
Mean Square Error: 0.0012

Train RMSE: 0.035
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_134&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_402 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_403 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_134 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_404 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
62/62 [==============================] - 0s 6ms/step - loss: 0.1889 - val_loss: 0.0122
Epoch 2/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0988 - val_loss: 0.0120
Epoch 3/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0906 - val_loss: 0.0092
Epoch 4/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 0.0129
Epoch 5/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.0153
Epoch 6/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0765 - val_loss: 0.0145
Epoch 7/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0116
Epoch 8/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.0100
Epoch 9/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.0084
Epoch 10/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.0064
Epoch 11/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.0057
Epoch 12/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.0061
Epoch 13/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.0069
Epoch 14/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0075
Epoch 15/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.0083
Epoch 16/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0394 - val_loss: 0.0091
Epoch 17/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 0.0098
Epoch 18/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0093
Epoch 19/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0333 - val_loss: 0.0090
Epoch 20/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.0085
Epoch 21/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.0081
Epoch 22/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0081
Epoch 23/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0082
Epoch 24/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0084
Epoch 25/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0090
Epoch 26/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0091
Epoch 27/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0097
Epoch 28/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0102
Epoch 29/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0116
Epoch 30/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0116
Epoch 31/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0128
Epoch 32/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0133
Epoch 33/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0138
Epoch 34/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0124
Epoch 35/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0125
Epoch 36/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0147
Epoch 37/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 38/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0147
Epoch 39/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0136
Epoch 40/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0142
Epoch 41/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0133
Epoch 42/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0135
Epoch 43/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0136
Epoch 44/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0129
Epoch 45/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0132
Epoch 46/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0132
Epoch 47/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0127
Epoch 48/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0126
Epoch 49/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0131
Epoch 50/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0125
Epoch 51/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0123
Epoch 52/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0129
Execution time:  12.446700811386108
DNN:
Mean Absolute Error: 0.0228
Root Mean Square Error: 0.0380
Mean Square Error: 0.0014

Train RMSE: 0.038
Train MSE: 0.001
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_135&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_405 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_406 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_135 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_407 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
102/102 [==============================] - 1s 5ms/step - loss: 0.1519 - val_loss: 0.0446
Epoch 2/80
102/102 [==============================] - 0s 4ms/step - loss: 0.1040 - val_loss: 0.0384
Epoch 3/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0928 - val_loss: 0.0337
Epoch 4/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0833 - val_loss: 0.0295
Epoch 5/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0737 - val_loss: 0.0203
Epoch 6/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0648 - val_loss: 0.0172
Epoch 7/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0569 - val_loss: 0.0150
Epoch 8/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0496 - val_loss: 0.0148
Epoch 9/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.0165
Epoch 10/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.0178
Epoch 11/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0359 - val_loss: 0.0171
Epoch 12/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0334 - val_loss: 0.0172
Epoch 13/80
102/102 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0161
Epoch 14/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.0156
Epoch 15/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0301 - val_loss: 0.0158
Epoch 16/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0158
Epoch 17/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.0158
Epoch 18/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0170
Epoch 19/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0174
Epoch 20/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 21/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0173
Epoch 22/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0176
Epoch 23/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0185
Epoch 24/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0176
Epoch 25/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0174
Epoch 26/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0166
Epoch 27/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0161
Epoch 28/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0168
Epoch 29/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0170
Epoch 30/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0163
Epoch 31/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0172
Epoch 32/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0166
Epoch 33/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0164
Epoch 34/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0165
Epoch 35/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0168
Epoch 36/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0162
Epoch 37/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0170
Epoch 38/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0161
Epoch 39/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0168
Epoch 40/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0159
Epoch 41/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0168
Epoch 42/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0158
Epoch 43/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0167
Epoch 44/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0161
Epoch 45/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0165
Epoch 46/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0160
Epoch 47/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0166
Epoch 48/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0160
Epoch 49/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0169
Epoch 50/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0160
Epoch 51/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0167
Epoch 52/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0160
Epoch 53/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0169
Epoch 54/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0160
Epoch 55/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0167
Epoch 56/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0160
Epoch 57/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0168
Epoch 58/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0161
Epoch 59/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0166
Epoch 60/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0160
Epoch 61/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0163
Epoch 62/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0161
Epoch 63/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0160
Epoch 64/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0161
Epoch 65/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0162
Epoch 66/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0161
Epoch 67/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0161
Epoch 68/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0163
Epoch 69/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0161
Epoch 70/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0162
Epoch 71/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0161
Epoch 72/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0163
Epoch 73/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0161
Epoch 74/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0165
Epoch 75/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0161
Epoch 76/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0166
Epoch 77/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0161
Epoch 78/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0163
Epoch 79/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0161
Epoch 80/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0166
Execution time:  31.21041512489319
DNN:
Mean Absolute Error: 0.0197
Root Mean Square Error: 0.0320
Mean Square Error: 0.0010

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_136&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_408 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_409 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_136 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_410 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
  1/126 [..............................] - ETA: 0s - loss: 0.4152WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.
126/126 [==============================] - 1s 4ms/step - loss: 0.1270 - val_loss: 0.0342
Epoch 2/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0599 - val_loss: 0.0311
Epoch 3/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.0308
Epoch 4/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.0290
Epoch 5/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.0278
Epoch 6/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.0287
Epoch 7/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.0269
Epoch 8/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0301
Epoch 9/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.0270
Epoch 10/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.0245
Epoch 11/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.0280
Epoch 12/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0236
Epoch 13/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.0250
Epoch 14/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.0231
Epoch 15/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0225
Epoch 16/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.0216
Epoch 17/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0228
Epoch 18/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0207
Epoch 19/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0191
Epoch 20/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0176
Epoch 21/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0171
Epoch 22/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0169
Epoch 23/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0166
Epoch 24/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0309 - val_loss: 0.0166
Epoch 25/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0161
Epoch 26/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0164
Epoch 27/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0164
Epoch 28/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0160
Epoch 29/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0157
Epoch 30/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0159
Epoch 31/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0158
Epoch 32/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0159
Epoch 33/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0157
Epoch 34/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0161
Epoch 35/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0159
Epoch 36/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0160
Epoch 37/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0158
Epoch 38/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0161
Epoch 39/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 40/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0160
Epoch 41/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0160
Epoch 42/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0160
Epoch 43/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 44/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 45/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 46/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 47/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 48/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 49/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0159
Epoch 50/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 51/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 52/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 53/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 54/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0160
Epoch 55/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 56/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 57/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0160
Epoch 58/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 59/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0160
Epoch 60/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0160
Epoch 61/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 62/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0160
Epoch 63/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 64/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 65/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 66/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 67/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0160
Epoch 68/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0160
Epoch 69/90
126/126 [==============================] - 1s 4ms/step - loss: 0.0283 - val_loss: 0.0160
Epoch 70/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0160
Epoch 71/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0160
Epoch 72/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0160
Epoch 73/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0160
Epoch 74/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0160
Epoch 75/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0160
Epoch 76/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0160
Epoch 77/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0160
Epoch 78/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0160
Epoch 79/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0160
Epoch 80/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160
Epoch 81/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160
Epoch 82/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160
Epoch 83/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0160
Epoch 84/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0160
Epoch 85/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0160
Epoch 86/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0161
Epoch 87/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0160
Epoch 88/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0161
Epoch 89/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0161
Epoch 90/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0161
Execution time:  38.35908555984497
DNN:
Mean Absolute Error: 0.0245
Root Mean Square Error: 0.0401
Mean Square Error: 0.0016

Train RMSE: 0.040
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_137&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_411 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_412 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_137 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_413 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
55/55 [==============================] - 0s 7ms/step - loss: 0.3019 - val_loss: 0.1354
Epoch 2/52
55/55 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 0.0388
Epoch 3/52
55/55 [==============================] - 0s 4ms/step - loss: 0.1139 - val_loss: 0.0320
Epoch 4/52
55/55 [==============================] - 0s 4ms/step - loss: 0.1026 - val_loss: 0.0243
Epoch 5/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0937 - val_loss: 0.0186
Epoch 6/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0856 - val_loss: 0.0169
Epoch 7/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0186
Epoch 8/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0190
Epoch 9/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0686 - val_loss: 0.0171
Epoch 10/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0633 - val_loss: 0.0158
Epoch 11/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0153
Epoch 12/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.0148
Epoch 13/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.0148
Epoch 14/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.0153
Epoch 15/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.0164
Epoch 16/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.0176
Epoch 17/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0169
Epoch 18/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0169
Epoch 19/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0172
Epoch 20/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0174
Epoch 21/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0173
Epoch 22/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.0172
Epoch 23/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0168
Epoch 24/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0166
Epoch 25/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0166
Epoch 26/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0169
Epoch 27/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0167
Epoch 28/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0169
Epoch 29/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0171
Epoch 30/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0173
Epoch 31/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0173
Epoch 32/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0175
Epoch 33/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0175
Epoch 34/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0176
Epoch 35/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0176
Epoch 36/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0175
Epoch 37/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0175
Epoch 38/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0175
Epoch 39/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0174
Epoch 40/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0174
Epoch 41/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0173
Epoch 42/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0173
Epoch 43/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0173
Epoch 44/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0173
Epoch 45/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0172
Epoch 46/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0172
Epoch 47/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0172
Epoch 48/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0172
Epoch 49/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0172
Epoch 50/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0172
Epoch 51/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0171
Epoch 52/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0171
Execution time:  10.70771050453186
DNN:
Mean Absolute Error: 0.0188
Root Mean Square Error: 0.0315
Mean Square Error: 0.0010

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.019
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_138&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_414 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_415 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_138 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_416 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
114/114 [==============================] - 1s 5ms/step - loss: 0.0536 - val_loss: 0.0400
Epoch 2/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.0354
Epoch 3/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0403 - val_loss: 0.0322
Epoch 4/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.0298
Epoch 5/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.0283
Epoch 6/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.0255
Epoch 7/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0214
Epoch 8/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0216
Epoch 9/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0210
Epoch 10/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0208
Epoch 11/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0198
Epoch 12/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0192
Epoch 13/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0315 - val_loss: 0.0176
Epoch 14/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0183
Epoch 15/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.0177
Epoch 16/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0302 - val_loss: 0.0172
Epoch 17/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0169
Epoch 18/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0164
Epoch 19/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0157
Epoch 20/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0153
Epoch 21/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0149
Epoch 22/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0146
Epoch 23/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0145
Epoch 24/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0141
Epoch 25/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 26/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0135
Epoch 27/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0133
Epoch 28/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0131
Epoch 29/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0134
Epoch 30/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0128
Epoch 31/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0127
Epoch 32/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0126
Epoch 33/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0126
Epoch 34/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0126
Epoch 35/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0125
Epoch 36/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0127
Epoch 37/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0127
Epoch 38/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0125
Epoch 39/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0124
Epoch 40/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0124
Epoch 41/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0123
Epoch 42/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0127
Epoch 43/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0120
Epoch 44/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0123
Epoch 45/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0127
Epoch 46/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0128
Epoch 47/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0128
Epoch 48/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0129
Epoch 49/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 50/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0124
Epoch 51/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0125
Epoch 52/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 53/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 54/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0128
Epoch 55/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0127
Epoch 56/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0127
Epoch 57/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0129
Epoch 58/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0129
Epoch 59/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0128
Epoch 60/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0126
Epoch 61/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0124
Epoch 62/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0125
Epoch 63/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0126
Epoch 64/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0127
Epoch 65/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0127
Epoch 66/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0127
Epoch 67/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0126
Epoch 68/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0126
Epoch 69/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0126
Epoch 70/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0127
Epoch 71/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0125
Epoch 72/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0127
Epoch 73/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0127
Epoch 74/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0124
Epoch 75/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0126
Epoch 76/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0127
Epoch 77/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0127
Epoch 78/80
114/114 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0127
Epoch 79/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0122
Epoch 80/80
114/114 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0127
Execution time:  33.901615619659424
DNN:
Mean Absolute Error: 0.0225
Root Mean Square Error: 0.0390
Mean Square Error: 0.0015

Train RMSE: 0.039
Train MSE: 0.002
Train MAE: 0.022
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_139&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_417 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_418 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_139 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_419 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
142/142 [==============================] - 1s 4ms/step - loss: 0.0466 - val_loss: 0.0264
Epoch 2/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0262
Epoch 3/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0233
Epoch 4/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0228
Epoch 5/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0219
Epoch 6/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0220
Epoch 7/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0228
Epoch 8/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0216
Epoch 9/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0205
Epoch 10/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0197
Epoch 11/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0197
Epoch 12/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0178
Epoch 13/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0192
Epoch 14/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0181
Epoch 15/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0192
Epoch 16/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0169
Epoch 17/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0159
Epoch 18/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0164
Epoch 19/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0174
Epoch 20/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0155
Epoch 21/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0156
Epoch 22/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0148
Epoch 23/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0145
Epoch 24/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0141
Epoch 25/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0139
Epoch 26/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0142
Epoch 27/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0140
Epoch 28/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0144
Epoch 29/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0135
Epoch 30/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0144
Epoch 31/90
142/142 [==============================] - ETA: 0s - loss: 0.029 - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0142
Epoch 32/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0142
Epoch 33/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0141
Epoch 34/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0138
Epoch 35/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0140
Epoch 36/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0131
Epoch 37/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0136
Epoch 38/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0134
Epoch 39/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0134
Epoch 40/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0124
Epoch 41/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0132
Epoch 42/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0130
Epoch 43/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0131
Epoch 44/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0123
Epoch 45/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0128
Epoch 46/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0127
Epoch 47/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0125
Epoch 48/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0125
Epoch 49/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0123
Epoch 50/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0122
Epoch 51/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0122
Epoch 52/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0123
Epoch 53/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0116
Epoch 54/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0115
Epoch 55/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0117
Epoch 56/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0116
Epoch 57/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0115
Epoch 58/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0114
Epoch 59/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0114
Epoch 60/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0114
Epoch 61/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0114
Epoch 62/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0115
Epoch 63/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0114
Epoch 64/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0114
Epoch 65/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0113
Epoch 66/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0114
Epoch 67/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0112
Epoch 68/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0113
Epoch 69/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0112
Epoch 70/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0113
Epoch 71/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0112
Epoch 72/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0110
Epoch 73/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0109
Epoch 74/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0107
Epoch 75/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0107
Epoch 76/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0106
Epoch 77/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0105
Epoch 78/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0103
Epoch 79/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0106
Epoch 80/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0106
Epoch 81/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0106
Epoch 82/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0105
Epoch 83/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0108
Epoch 84/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0104
Epoch 85/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0107
Epoch 86/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0106
Epoch 87/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0104
Epoch 88/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.0105
Epoch 89/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0117
Epoch 90/90
142/142 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0106
Execution time:  39.81014561653137
DNN:
Mean Absolute Error: 0.0227
Root Mean Square Error: 0.0394
Mean Square Error: 0.0016

Train RMSE: 0.039
Train MSE: 0.002
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_140&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_420 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_421 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_140 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_422 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
62/62 [==============================] - 0s 6ms/step - loss: 0.0749 - val_loss: 0.0770
Epoch 2/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.0512
Epoch 3/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.0466
Epoch 4/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0444
Epoch 5/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0422
Epoch 6/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.0395
Epoch 7/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.0376
Epoch 8/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0365
Epoch 9/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0355
Epoch 10/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0348
Epoch 11/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0342
Epoch 12/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0330
Epoch 13/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0322
Epoch 14/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0315
Epoch 15/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0308
Epoch 16/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0302
Epoch 17/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0298
Epoch 18/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0292
Epoch 19/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0288
Epoch 20/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0282
Epoch 21/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0279
Epoch 22/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0274
Epoch 23/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0266
Epoch 24/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0257
Epoch 25/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0252
Epoch 26/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0242
Epoch 27/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0230
Epoch 28/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0221
Epoch 29/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0218
Epoch 30/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0215
Epoch 31/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0207
Epoch 32/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0207
Epoch 33/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0207
Epoch 34/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0204
Epoch 35/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0197
Epoch 36/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0201
Epoch 37/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0193
Epoch 38/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0192
Epoch 39/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196
Epoch 40/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0189
Epoch 41/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189
Epoch 42/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0191
Epoch 43/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0190
Epoch 44/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0189
Epoch 45/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0188
Epoch 46/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0188
Epoch 47/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0186
Epoch 48/52
62/62 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0186
Epoch 49/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0184
Epoch 50/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0183
Epoch 51/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0182
Epoch 52/52
62/62 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0181
Execution time:  12.661014795303345
DNN:
Mean Absolute Error: 0.0252
Root Mean Square Error: 0.0414
Mean Square Error: 0.0017

Train RMSE: 0.041
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  12h
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_141&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_423 (Dense)            (None, 72, 87)            174       
_________________________________________________________________
dense_424 (Dense)            (None, 72, 16)            1408      
_________________________________________________________________
dropout_141 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_425 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
102/102 [==============================] - 1s 5ms/step - loss: 0.0501 - val_loss: 0.0378
Epoch 2/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0447 - val_loss: 0.0355
Epoch 3/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0430 - val_loss: 0.0345
Epoch 4/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0414 - val_loss: 0.0312
Epoch 5/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.0290
Epoch 6/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.0282
Epoch 7/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.0270
Epoch 8/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.0254
Epoch 9/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.0243
Epoch 10/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0233
Epoch 11/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0227
Epoch 12/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0216
Epoch 13/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0332 - val_loss: 0.0209
Epoch 14/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 0.0207
Epoch 15/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.0201
Epoch 16/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0317 - val_loss: 0.0199
Epoch 17/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0197
Epoch 18/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.0194
Epoch 19/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0304 - val_loss: 0.0193
Epoch 20/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.0188
Epoch 21/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0297 - val_loss: 0.0188
Epoch 22/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0185
Epoch 23/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0184
Epoch 24/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0183
Epoch 25/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0181
Epoch 26/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0179
Epoch 27/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0177
Epoch 28/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0177
Epoch 29/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0177
Epoch 30/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0175
Epoch 31/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0176
Epoch 32/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0176
Epoch 33/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0175
Epoch 34/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0175
Epoch 35/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0175
Epoch 36/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0174
Epoch 37/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0174
Epoch 38/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0174
Epoch 39/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0173
Epoch 40/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0174
Epoch 41/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0173
Epoch 42/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0173
Epoch 43/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0173
Epoch 44/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0172
Epoch 45/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0172
Epoch 46/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0172
Epoch 47/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0172
Epoch 48/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0172
Epoch 49/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0172
Epoch 50/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0172
Epoch 51/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0171
Epoch 52/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0171
Epoch 53/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.0171
Epoch 54/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0170
Epoch 55/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0171
Epoch 56/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0170
Epoch 57/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0170
Epoch 58/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0171
Epoch 59/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0172
Epoch 60/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0170
Epoch 61/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0170
Epoch 62/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0169
Epoch 63/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0169
Epoch 64/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0169
Epoch 65/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0168
Epoch 66/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0169
Epoch 67/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0169
Epoch 68/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0168
Epoch 69/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0169
Epoch 70/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0167
Epoch 71/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0166
Epoch 72/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0166
Epoch 73/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0164
Epoch 74/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0163
Epoch 75/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0162
Epoch 76/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0163
Epoch 77/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0163
Epoch 78/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0160
Epoch 79/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 80/80
102/102 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0159
Execution time:  31.364622831344604
DNN:
Mean Absolute Error: 0.0203
Root Mean Square Error: 0.0372
Mean Square Error: 0.0014

Train RMSE: 0.037
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  12h
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_142&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_426 (Dense)            (None, 72, 80)            160       
_________________________________________________________________
dense_427 (Dense)            (None, 72, 16)            1296      
_________________________________________________________________
dropout_142 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_428 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
126/126 [==============================] - 1s 5ms/step - loss: 0.0555 - val_loss: 0.0207
Epoch 2/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0192
Epoch 3/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0192
Epoch 4/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0189
Epoch 5/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0190
Epoch 6/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0190
Epoch 7/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0187
Epoch 8/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0189
Epoch 9/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0187
Epoch 10/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0186
Epoch 11/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0186
Epoch 12/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0184
Epoch 13/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0184
Epoch 14/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0183
Epoch 15/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0184
Epoch 16/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0183
Epoch 17/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0183
Epoch 18/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0182
Epoch 19/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0181
Epoch 20/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0309 - val_loss: 0.0181
Epoch 21/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0180
Epoch 22/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0180
Epoch 23/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0180
Epoch 24/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0179
Epoch 25/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0179
Epoch 26/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0178
Epoch 27/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0178
Epoch 28/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0178
Epoch 29/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0178
Epoch 30/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0178
Epoch 31/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0178
Epoch 32/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0178
Epoch 33/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0179
Epoch 34/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0177
Epoch 35/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0177
Epoch 36/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0177
Epoch 37/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0177
Epoch 38/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0176
Epoch 39/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0177
Epoch 40/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0175
Epoch 41/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0176
Epoch 42/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0174
Epoch 43/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0175
Epoch 44/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0174
Epoch 45/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0174
Epoch 46/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0174
Epoch 47/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0173
Epoch 48/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0173
Epoch 49/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0173
Epoch 50/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0171
Epoch 51/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0171
Epoch 52/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0171
Epoch 53/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0171
Epoch 54/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0170
Epoch 55/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0169
Epoch 56/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0169
Epoch 57/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0168
Epoch 58/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0168
Epoch 59/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0167
Epoch 60/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0168
Epoch 61/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0166
Epoch 62/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0167
Epoch 63/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0166
Epoch 64/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0166
Epoch 65/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0166
Epoch 66/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0166
Epoch 67/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0166
Epoch 68/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0166
Epoch 69/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0166
Epoch 70/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0166
Epoch 71/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0166
Epoch 72/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0165
Epoch 73/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0165
Epoch 74/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0166
Epoch 75/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0161
Epoch 76/90
126/126 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0163
Epoch 77/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0159
Epoch 78/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0160
Epoch 79/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0159
Epoch 80/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0159
Epoch 81/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0159
Epoch 82/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0159
Epoch 83/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0159
Epoch 84/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0158
Epoch 85/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0158
Epoch 86/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0158
Epoch 87/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0158
Epoch 88/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0159
Epoch 89/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0158
Epoch 90/90
126/126 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0158
Execution time:  37.747394323349
DNN:
Mean Absolute Error: 0.0175
Root Mean Square Error: 0.0321
Mean Square Error: 0.0010

Train RMSE: 0.032
Train MSE: 0.001
Train MAE: 0.018
###########################

MODEL:  DNN
sequence:  12h
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_143&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_429 (Dense)            (None, 72, 12)            24        
_________________________________________________________________
dense_430 (Dense)            (None, 72, 16)            208       
_________________________________________________________________
dropout_143 (Dropout)        (None, 72, 16)            0         
_________________________________________________________________
dense_431 (Dense)            (None, 72, 1)             17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
 1/55 [..............................] - ETA: 0s - loss: 0.0661WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.
55/55 [==============================] - 0s 7ms/step - loss: 0.0646 - val_loss: 0.0543
Epoch 2/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.0464
Epoch 3/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.0441
Epoch 4/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.0418
Epoch 5/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.0404
Epoch 6/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.0383
Epoch 7/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.0367
Epoch 8/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.0354
Epoch 9/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.0347
Epoch 10/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.0339
Epoch 11/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.0336
Epoch 12/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0331
Epoch 13/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0325
Epoch 14/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0319
Epoch 15/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0302
Epoch 16/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0286
Epoch 17/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0279
Epoch 18/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0274
Epoch 19/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0268
Epoch 20/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0264
Epoch 21/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0262
Epoch 22/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0257
Epoch 23/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0255
Epoch 24/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0252
Epoch 25/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0250
Epoch 26/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0309 - val_loss: 0.0247
Epoch 27/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0243
Epoch 28/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0240
Epoch 29/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0237
Epoch 30/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0237
Epoch 31/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0232
Epoch 32/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0231
Epoch 33/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0229
Epoch 34/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0227
Epoch 35/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0223
Epoch 36/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0220
Epoch 37/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0219
Epoch 38/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0217
Epoch 39/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0215
Epoch 40/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0213
Epoch 41/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0211
Epoch 42/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0210
Epoch 43/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0209
Epoch 44/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0206
Epoch 45/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0206
Epoch 46/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0203
Epoch 47/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0204
Epoch 48/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0201
Epoch 49/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0202
Epoch 50/52
55/55 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0199
Epoch 51/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0200
Epoch 52/52
55/55 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0197
Execution time:  10.4955313205719
DNN:
Mean Absolute Error: 0.0245
Root Mean Square Error: 0.0402
Mean Square Error: 0.0016

Train RMSE: 0.040
Train MSE: 0.002
Train MAE: 0.024
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_144&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_432 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_433 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_144 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_434 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
112/112 [==============================] - 1s 8ms/step - loss: 0.1217 - val_loss: 0.0079
Epoch 2/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0728 - val_loss: 0.0096
Epoch 3/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0630 - val_loss: 0.0080
Epoch 4/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0540 - val_loss: 0.0091
Epoch 5/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0459 - val_loss: 0.0114
Epoch 6/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0394 - val_loss: 0.0127
Epoch 7/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0352 - val_loss: 0.0126
Epoch 8/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.0205
Epoch 9/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0327 - val_loss: 0.0232
Epoch 10/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0314 - val_loss: 0.0154
Epoch 11/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0099
Epoch 12/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0289 - val_loss: 0.0098
Epoch 13/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0098
Epoch 14/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0102
Epoch 15/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0107
Epoch 16/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0107
Epoch 17/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0120
Epoch 18/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0126
Epoch 19/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0130
Epoch 20/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0131
Epoch 21/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0129
Epoch 22/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0134
Epoch 23/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0131
Epoch 24/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0129
Epoch 25/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0136
Epoch 26/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 27/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 28/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0167
Epoch 29/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0126
Epoch 30/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 31/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0137
Epoch 32/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0140
Epoch 33/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0130
Epoch 34/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0142
Epoch 35/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0137
Epoch 36/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0140
Epoch 37/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0132
Epoch 38/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0141
Epoch 39/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0138
Epoch 40/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0137
Epoch 41/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0139
Epoch 42/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0136
Epoch 43/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 44/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0139
Epoch 45/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 46/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 47/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 48/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 49/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 50/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 51/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 52/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 53/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 54/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 55/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 56/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 57/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 58/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 59/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 60/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 61/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 62/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 63/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 64/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 65/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 66/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0139
Epoch 67/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 68/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 69/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0138
Epoch 70/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 71/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 72/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 73/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 74/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0139
Epoch 75/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 76/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0136
Epoch 77/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0139
Epoch 78/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0135
Epoch 79/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0137
Epoch 80/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0138
Execution time:  60.13489866256714
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0447
Mean Square Error: 0.0020

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_145&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_435 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_436 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_145 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_437 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
139/139 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0058
Epoch 2/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0467 - val_loss: 0.0071
Epoch 3/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0436 - val_loss: 0.0105
Epoch 4/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0417 - val_loss: 0.0116
Epoch 5/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0400 - val_loss: 0.0110
Epoch 6/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0388 - val_loss: 0.0117
Epoch 7/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0372 - val_loss: 0.0128
Epoch 8/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0359 - val_loss: 0.0144
Epoch 9/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0349 - val_loss: 0.0153
Epoch 10/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0340 - val_loss: 0.0159
Epoch 11/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0332 - val_loss: 0.0166
Epoch 12/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0327 - val_loss: 0.0132
Epoch 13/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0201
Epoch 14/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0321 - val_loss: 0.0210
Epoch 15/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0317 - val_loss: 0.0208
Epoch 16/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0314 - val_loss: 0.0199
Epoch 17/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0310 - val_loss: 0.0196
Epoch 18/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0174
Epoch 19/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0304 - val_loss: 0.0169
Epoch 20/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0301 - val_loss: 0.0171
Epoch 21/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0299 - val_loss: 0.0158
Epoch 22/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0296 - val_loss: 0.0154
Epoch 23/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0152
Epoch 24/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0292 - val_loss: 0.0147
Epoch 25/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0291 - val_loss: 0.0145
Epoch 26/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0289 - val_loss: 0.0142
Epoch 27/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0289 - val_loss: 0.0137
Epoch 28/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0124
Epoch 29/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0122
Epoch 30/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0119
Epoch 31/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0119
Epoch 32/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0118
Epoch 33/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0116
Epoch 34/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0110
Epoch 35/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0283 - val_loss: 0.0108
Epoch 36/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0283 - val_loss: 0.0104
Epoch 37/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0283 - val_loss: 0.0102
Epoch 38/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0104
Epoch 39/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0281 - val_loss: 0.0108
Epoch 40/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0136
Epoch 41/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0110
Epoch 42/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0110
Epoch 43/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0110
Epoch 44/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0110
Epoch 45/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0109
Epoch 46/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0123
Epoch 47/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0108
Epoch 48/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0121
Epoch 49/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0166
Epoch 50/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0114
Epoch 51/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0117
Epoch 52/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0119
Epoch 53/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0114
Epoch 54/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0124
Epoch 55/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0111
Epoch 56/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0124
Epoch 57/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0114
Epoch 58/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0124
Epoch 59/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0114
Epoch 60/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0125
Epoch 61/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0115
Epoch 62/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0123
Epoch 63/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0116
Epoch 64/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0121
Epoch 65/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0117
Epoch 66/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0124
Epoch 67/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0116
Epoch 68/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0123
Epoch 69/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0118
Epoch 70/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0120
Epoch 71/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0119
Epoch 72/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0119
Epoch 73/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0119
Epoch 74/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0121
Epoch 75/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0113
Epoch 76/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0125
Epoch 77/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0113
Epoch 78/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0123
Epoch 79/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0118
Epoch 80/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0119
Epoch 81/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0116
Epoch 82/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0126
Epoch 83/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0112
Epoch 84/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0123
Epoch 85/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0117
Epoch 86/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0120
Epoch 87/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0116
Epoch 88/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0122
Epoch 89/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0114
Epoch 90/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0121
Execution time:  68.77367877960205
DNN:
Mean Absolute Error: 0.0262
Root Mean Square Error: 0.0457
Mean Square Error: 0.0021

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_146&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_438 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_439 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_146 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_440 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
61/61 [==============================] - 1s 8ms/step - loss: 0.2083 - val_loss: 0.0307
Epoch 2/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1147 - val_loss: 0.0149
Epoch 3/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1004 - val_loss: 0.0300
Epoch 4/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0904 - val_loss: 0.0266
Epoch 5/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0812 - val_loss: 0.0234
Epoch 6/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0175
Epoch 7/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0630 - val_loss: 0.0102
Epoch 8/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0525 - val_loss: 0.0059
Epoch 9/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0455 - val_loss: 0.0058
Epoch 10/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0401 - val_loss: 0.0064
Epoch 11/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0358 - val_loss: 0.0069
Epoch 12/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0331 - val_loss: 0.0073
Epoch 13/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0311 - val_loss: 0.0078
Epoch 14/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0292 - val_loss: 0.0095
Epoch 15/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0280 - val_loss: 0.0104
Epoch 16/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0100
Epoch 17/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0126
Epoch 18/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0133
Epoch 19/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0140
Epoch 20/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0145
Epoch 21/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0145
Epoch 22/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0150
Epoch 23/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0148
Epoch 24/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0150
Epoch 25/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0151
Epoch 26/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0153
Epoch 27/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0154
Epoch 28/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0155
Epoch 29/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0156
Epoch 30/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0157
Epoch 31/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0156
Epoch 32/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0158
Epoch 33/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0159
Epoch 34/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0157
Epoch 35/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0159
Epoch 36/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0160
Epoch 37/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0164
Epoch 38/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0265 - val_loss: 0.0157
Epoch 39/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0163
Epoch 40/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0165
Epoch 41/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0157
Epoch 42/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0166
Epoch 43/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0161
Epoch 44/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0161
Epoch 45/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0165
Epoch 46/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0159
Epoch 47/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0161
Epoch 48/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0166
Epoch 49/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0160
Epoch 50/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0161
Epoch 51/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0167
Epoch 52/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0159
Execution time:  17.93683385848999
DNN:
Mean Absolute Error: 0.0249
Root Mean Square Error: 0.0412
Mean Square Error: 0.0017

Train RMSE: 0.041
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_147&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_441 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_442 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_147 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_443 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
100/100 [==============================] - 1s 9ms/step - loss: 0.1031 - val_loss: 0.0328
Epoch 2/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0739 - val_loss: 0.0187
Epoch 3/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0641 - val_loss: 0.0158
Epoch 4/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0559 - val_loss: 0.0156
Epoch 5/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0484 - val_loss: 0.0159
Epoch 6/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0425 - val_loss: 0.0162
Epoch 7/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0380 - val_loss: 0.0167
Epoch 8/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0157
Epoch 9/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0341 - val_loss: 0.0181
Epoch 10/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.0176
Epoch 11/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0316 - val_loss: 0.0168
Epoch 12/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0166
Epoch 13/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0304 - val_loss: 0.0165
Epoch 14/80
100/100 [==============================] - 1s 8ms/step - loss: 0.0298 - val_loss: 0.0163
Epoch 15/80
100/100 [==============================] - 1s 8ms/step - loss: 0.0293 - val_loss: 0.0173
Epoch 16/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0290 - val_loss: 0.0178
Epoch 17/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0177
Epoch 18/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 19/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.0184
Epoch 20/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0286 - val_loss: 0.0182
Epoch 21/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0286 - val_loss: 0.0185
Epoch 22/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0184
Epoch 23/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0187
Epoch 24/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0183
Epoch 25/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0188
Epoch 26/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0184
Epoch 27/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0286 - val_loss: 0.0182
Epoch 28/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0187
Epoch 29/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 30/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 31/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0188
Epoch 32/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0187
Epoch 33/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 34/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 35/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 36/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0188
Epoch 37/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0185
Epoch 38/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0188
Epoch 39/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0186
Epoch 40/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0188
Epoch 41/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0187
Epoch 42/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 43/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 44/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0188
Epoch 45/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0185
Epoch 46/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 47/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0187
Epoch 48/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0188
Epoch 49/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0184
Epoch 50/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0188
Epoch 51/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0186
Epoch 52/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0188
Epoch 53/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0187
Epoch 54/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 55/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0185
Epoch 56/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 57/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0188
Epoch 58/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0186
Epoch 59/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 60/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0185
Epoch 61/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 62/80
100/100 [==============================] - 1s 8ms/step - loss: 0.0283 - val_loss: 0.0187
Epoch 63/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 64/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 65/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 66/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0185
Epoch 67/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 68/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0186
Epoch 69/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 70/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0185
Epoch 71/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 72/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0185
Epoch 73/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 74/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 75/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0187
Epoch 76/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0186
Epoch 77/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 78/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 79/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Epoch 80/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0186
Execution time:  57.299383878707886
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0437
Mean Square Error: 0.0019

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_148&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_444 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_445 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_148 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_446 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
124/124 [==============================] - 1s 8ms/step - loss: 0.1278 - val_loss: 0.0281
Epoch 2/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0519 - val_loss: 0.0226
Epoch 3/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0485 - val_loss: 0.0260
Epoch 4/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0457 - val_loss: 0.0225
Epoch 5/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0434 - val_loss: 0.0214
Epoch 6/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0413 - val_loss: 0.0183
Epoch 7/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0397 - val_loss: 0.0200
Epoch 8/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0380 - val_loss: 0.0164
Epoch 9/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0369 - val_loss: 0.0168
Epoch 10/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0357 - val_loss: 0.0170
Epoch 11/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0349 - val_loss: 0.0166
Epoch 12/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0343 - val_loss: 0.0170
Epoch 13/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0339 - val_loss: 0.0171
Epoch 14/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0334 - val_loss: 0.0176
Epoch 15/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0330 - val_loss: 0.0178
Epoch 16/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0177
Epoch 17/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0320 - val_loss: 0.0175
Epoch 18/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0316 - val_loss: 0.0173
Epoch 19/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0314 - val_loss: 0.0169
Epoch 20/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0310 - val_loss: 0.0169
Epoch 21/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0308 - val_loss: 0.0167
Epoch 22/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0305 - val_loss: 0.0165
Epoch 23/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0161
Epoch 24/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 0.0165
Epoch 25/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0165
Epoch 26/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0299 - val_loss: 0.0164
Epoch 27/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 0.0164
Epoch 28/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.0163
Epoch 29/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0296 - val_loss: 0.0163
Epoch 30/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0163
Epoch 31/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0294 - val_loss: 0.0164
Epoch 32/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0164
Epoch 33/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0292 - val_loss: 0.0162
Epoch 34/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0162
Epoch 35/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0163
Epoch 36/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0290 - val_loss: 0.0164
Epoch 37/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0290 - val_loss: 0.0166
Epoch 38/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0289 - val_loss: 0.0169
Epoch 39/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0168
Epoch 40/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0171
Epoch 41/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0172
Epoch 42/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0174
Epoch 43/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0176
Epoch 44/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0176
Epoch 45/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0177
Epoch 46/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 47/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0177
Epoch 48/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 49/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 50/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 51/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 52/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 53/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 54/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0178
Epoch 55/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0178
Epoch 56/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0176
Epoch 57/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 58/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 59/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0179
Epoch 60/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0179
Epoch 61/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0179
Epoch 62/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0180
Epoch 63/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0180
Epoch 64/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0179
Epoch 65/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 66/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0180
Epoch 67/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 68/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0181
Epoch 69/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 70/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 71/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0289 - val_loss: 0.0180
Epoch 72/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 73/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 74/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 75/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 76/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 77/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 78/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 79/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 80/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 81/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 82/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 83/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 84/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 85/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0289 - val_loss: 0.0180
Epoch 86/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 87/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Epoch 88/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 89/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 90/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0180
Execution time:  64.02716088294983
DNN:
Mean Absolute Error: 0.0261
Root Mean Square Error: 0.0445
Mean Square Error: 0.0020

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_149&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_447 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_448 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_149 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_449 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
54/54 [==============================] - 0s 9ms/step - loss: 0.3771 - val_loss: 0.2602
Epoch 2/52
54/54 [==============================] - 0s 6ms/step - loss: 0.1806 - val_loss: 0.0534
Epoch 3/52
54/54 [==============================] - 0s 5ms/step - loss: 0.1177 - val_loss: 0.0330
Epoch 4/52
54/54 [==============================] - 0s 6ms/step - loss: 0.1059 - val_loss: 0.0418
Epoch 5/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0966 - val_loss: 0.0396
Epoch 6/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0882 - val_loss: 0.0331
Epoch 7/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0797 - val_loss: 0.0264
Epoch 8/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0711 - val_loss: 0.0199
Epoch 9/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0627 - val_loss: 0.0170
Epoch 10/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0542 - val_loss: 0.0159
Epoch 11/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0464 - val_loss: 0.0175
Epoch 12/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0395 - val_loss: 0.0194
Epoch 13/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0345 - val_loss: 0.0190
Epoch 14/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0315 - val_loss: 0.0173
Epoch 15/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0303 - val_loss: 0.0174
Epoch 16/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0286 - val_loss: 0.0181
Epoch 17/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0281 - val_loss: 0.0186
Epoch 18/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0187
Epoch 19/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0185
Epoch 20/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0193
Epoch 21/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0189
Epoch 22/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0193
Epoch 23/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0197
Epoch 24/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0190
Epoch 25/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0192
Epoch 26/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0193
Epoch 27/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0196
Epoch 28/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0198
Epoch 29/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0194
Epoch 30/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 31/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0277 - val_loss: 0.0195
Epoch 32/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0196
Epoch 33/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0188
Epoch 34/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0195
Epoch 35/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.0192
Epoch 36/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0194
Epoch 37/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0196
Epoch 38/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0194
Epoch 39/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0192
Epoch 40/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0192
Epoch 41/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0192
Epoch 42/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0196
Epoch 43/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0197
Epoch 44/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 45/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0191
Epoch 46/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0193
Epoch 47/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0195
Epoch 48/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.0194
Epoch 49/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.0192
Epoch 50/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0272 - val_loss: 0.0191
Epoch 51/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0272 - val_loss: 0.0194
Epoch 52/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.0192
Execution time:  17.335540771484375
DNN:
Mean Absolute Error: 0.0239
Root Mean Square Error: 0.0395
Mean Square Error: 0.0016

Train RMSE: 0.040
Train MSE: 0.002
Train MAE: 0.024
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_150&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_450 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_451 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_150 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_452 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
112/112 [==============================] - 1s 8ms/step - loss: 0.0452 - val_loss: 0.0252
Epoch 2/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0425 - val_loss: 0.0247
Epoch 3/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0401 - val_loss: 0.0234
Epoch 4/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0386 - val_loss: 0.0208
Epoch 5/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0371 - val_loss: 0.0203
Epoch 6/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0193
Epoch 7/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0348 - val_loss: 0.0194
Epoch 8/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0336 - val_loss: 0.0189
Epoch 9/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0327 - val_loss: 0.0183
Epoch 10/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0319 - val_loss: 0.0184
Epoch 11/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0312 - val_loss: 0.0180
Epoch 12/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0306 - val_loss: 0.0178
Epoch 13/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0171
Epoch 14/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0296 - val_loss: 0.0171
Epoch 15/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0292 - val_loss: 0.0166
Epoch 16/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0163
Epoch 17/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0161
Epoch 18/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0159
Epoch 19/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0158
Epoch 20/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0158
Epoch 21/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0157
Epoch 22/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0160
Epoch 23/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0158
Epoch 24/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0158
Epoch 25/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0157
Epoch 26/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0155
Epoch 27/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0155
Epoch 28/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0156
Epoch 29/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0155
Epoch 30/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0158
Epoch 31/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0156
Epoch 32/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0156
Epoch 33/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0156
Epoch 34/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0158
Epoch 35/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0157
Epoch 36/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0156
Epoch 37/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0164
Epoch 38/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.0165
Epoch 39/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0267 - val_loss: 0.0161
Epoch 40/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0161
Epoch 41/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0160
Epoch 42/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0166
Epoch 43/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0266 - val_loss: 0.0171
Epoch 44/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0266 - val_loss: 0.0172
Epoch 45/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0265 - val_loss: 0.0173
Epoch 46/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0177
Epoch 47/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0265 - val_loss: 0.0176
Epoch 48/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0176
Epoch 49/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 50/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 51/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 52/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0177
Epoch 53/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 54/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 55/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 56/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 57/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 58/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 59/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 60/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 61/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 62/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 63/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 64/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 65/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 66/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 67/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0178
Epoch 68/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 69/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 70/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 71/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 72/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 73/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 74/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 75/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 76/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 77/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 78/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 79/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0264 - val_loss: 0.0172
Epoch 80/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0172
Execution time:  60.753777265548706
DNN:
Mean Absolute Error: 0.0258
Root Mean Square Error: 0.0435
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_151&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_453 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_454 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_151 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_455 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
139/139 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0255
Epoch 2/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0362 - val_loss: 0.0218
Epoch 3/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0353 - val_loss: 0.0210
Epoch 4/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0344 - val_loss: 0.0207
Epoch 5/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0336 - val_loss: 0.0203
Epoch 6/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0331 - val_loss: 0.0186
Epoch 7/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0329 - val_loss: 0.0184
Epoch 8/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.0182
Epoch 9/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0318 - val_loss: 0.0180
Epoch 10/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0314 - val_loss: 0.0176
Epoch 11/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0311 - val_loss: 0.0174
Epoch 12/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0174
Epoch 13/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0305 - val_loss: 0.0164
Epoch 14/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0302 - val_loss: 0.0167
Epoch 15/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0299 - val_loss: 0.0165
Epoch 16/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0297 - val_loss: 0.0156
Epoch 17/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0157
Epoch 18/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0158
Epoch 19/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0291 - val_loss: 0.0158
Epoch 20/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0290 - val_loss: 0.0151
Epoch 21/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0289 - val_loss: 0.0168
Epoch 22/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0150
Epoch 23/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0150
Epoch 24/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0144
Epoch 25/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0144
Epoch 26/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0146
Epoch 27/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0140
Epoch 28/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0139
Epoch 29/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0283 - val_loss: 0.0141
Epoch 30/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0142
Epoch 31/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0136
Epoch 32/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0136
Epoch 33/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0281 - val_loss: 0.0137
Epoch 34/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0281 - val_loss: 0.0138
Epoch 35/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0138
Epoch 36/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0139
Epoch 37/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0139
Epoch 38/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0139
Epoch 39/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 40/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 41/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0141
Epoch 42/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 43/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0141
Epoch 44/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 45/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 46/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 47/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 48/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 49/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 50/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 51/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0138
Epoch 52/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0136
Epoch 53/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0136
Epoch 54/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0137
Epoch 55/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0136
Epoch 56/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0135
Epoch 57/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0136
Epoch 58/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0139
Epoch 59/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0133
Epoch 60/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0138
Epoch 61/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0138
Epoch 62/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0140
Epoch 63/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0138
Epoch 64/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0139
Epoch 65/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0140
Epoch 66/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0140
Epoch 67/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0140
Epoch 68/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0271 - val_loss: 0.0141
Epoch 69/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0271 - val_loss: 0.0141
Epoch 70/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0141
Epoch 71/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0143
Epoch 72/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0142
Epoch 73/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 74/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0144
Epoch 75/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 76/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 77/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0146
Epoch 78/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0146
Epoch 79/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0147
Epoch 80/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0146
Epoch 81/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0144
Epoch 82/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 83/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0144
Epoch 84/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 85/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 86/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0147
Epoch 87/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0266 - val_loss: 0.0147
Epoch 88/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0266 - val_loss: 0.0147
Epoch 89/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0147
Epoch 90/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0146
Execution time:  68.42319536209106
DNN:
Mean Absolute Error: 0.0254
Root Mean Square Error: 0.0437
Mean Square Error: 0.0019

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_152&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_456 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_457 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_152 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_458 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
61/61 [==============================] - 0s 8ms/step - loss: 0.0794 - val_loss: 0.0870
Epoch 2/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0472 - val_loss: 0.0492
Epoch 3/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0424 - val_loss: 0.0424
Epoch 4/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0412 - val_loss: 0.0413
Epoch 5/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0400
Epoch 6/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0384 - val_loss: 0.0360
Epoch 7/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0366 - val_loss: 0.0334
Epoch 8/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0352 - val_loss: 0.0324
Epoch 9/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0343 - val_loss: 0.0311
Epoch 10/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0334 - val_loss: 0.0273
Epoch 11/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0328 - val_loss: 0.0261
Epoch 12/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0320 - val_loss: 0.0256
Epoch 13/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0313 - val_loss: 0.0226
Epoch 14/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0308 - val_loss: 0.0246
Epoch 15/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0303 - val_loss: 0.0239
Epoch 16/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0300 - val_loss: 0.0233
Epoch 17/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0296 - val_loss: 0.0230
Epoch 18/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0293 - val_loss: 0.0224
Epoch 19/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0290 - val_loss: 0.0221
Epoch 20/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0287 - val_loss: 0.0219
Epoch 21/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0284 - val_loss: 0.0214
Epoch 22/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0209
Epoch 23/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0280 - val_loss: 0.0206
Epoch 24/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0201
Epoch 25/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0198
Epoch 26/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0195
Epoch 27/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0193
Epoch 28/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0192
Epoch 29/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0191
Epoch 30/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0190
Epoch 31/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0189
Epoch 32/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0189
Epoch 33/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0187
Epoch 34/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0187
Epoch 35/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0186
Epoch 36/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0184
Epoch 37/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0183
Epoch 38/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0182
Epoch 39/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0181
Epoch 40/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0180
Epoch 41/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0265 - val_loss: 0.0179
Epoch 42/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0178
Epoch 43/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0176
Epoch 44/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0176
Epoch 45/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0264 - val_loss: 0.0175
Epoch 46/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0174
Epoch 47/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0173
Epoch 48/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0175
Epoch 49/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0175
Epoch 50/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0178
Epoch 51/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0262 - val_loss: 0.0178
Epoch 52/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0262 - val_loss: 0.0179
Execution time:  17.819113731384277
DNN:
Mean Absolute Error: 0.0255
Root Mean Square Error: 0.0429
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_153&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_459 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_460 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_153 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_461 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
100/100 [==============================] - 1s 9ms/step - loss: 0.0467 - val_loss: 0.0198
Epoch 2/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0211
Epoch 3/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0211
Epoch 4/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.0215
Epoch 5/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0216
Epoch 6/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0378 - val_loss: 0.0216
Epoch 7/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.0216
Epoch 8/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0210
Epoch 9/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0207
Epoch 10/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0206
Epoch 11/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0168
Epoch 12/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0325 - val_loss: 0.0201
Epoch 13/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.0198
Epoch 14/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0198
Epoch 15/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0306 - val_loss: 0.0196
Epoch 16/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0302 - val_loss: 0.0196
Epoch 17/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0194
Epoch 18/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0296 - val_loss: 0.0193
Epoch 19/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0193
Epoch 20/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0291 - val_loss: 0.0191
Epoch 21/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0191
Epoch 22/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0286 - val_loss: 0.0189
Epoch 23/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0189
Epoch 24/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0181
Epoch 25/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0191
Epoch 26/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0187
Epoch 27/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0189
Epoch 28/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 29/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0189
Epoch 30/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0192
Epoch 31/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0187
Epoch 32/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0186
Epoch 33/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0186
Epoch 34/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0187
Epoch 35/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0184
Epoch 36/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0186
Epoch 37/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0186
Epoch 38/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0187
Epoch 39/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0186
Epoch 40/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0185
Epoch 41/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0184
Epoch 42/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0184
Epoch 43/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0187
Epoch 44/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0191
Epoch 45/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0186
Epoch 46/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0185
Epoch 47/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0184
Epoch 48/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 49/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0188
Epoch 50/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0188
Epoch 51/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0189
Epoch 52/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 53/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 54/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0186
Epoch 55/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 56/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0188
Epoch 57/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 58/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0188
Epoch 59/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 60/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 61/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 62/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 63/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0187
Epoch 64/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0186
Epoch 65/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0186
Epoch 66/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0186
Epoch 67/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0185
Epoch 68/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0185
Epoch 69/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0185
Epoch 70/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0185
Epoch 71/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0185
Epoch 72/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0184
Epoch 73/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0185
Epoch 74/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0184
Epoch 75/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0184
Epoch 76/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0184
Epoch 77/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0183
Epoch 78/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0183
Epoch 79/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0182
Epoch 80/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0182
Execution time:  57.21766257286072
DNN:
Mean Absolute Error: 0.0250
Root Mean Square Error: 0.0427
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_154&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_462 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_463 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_154 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_464 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
124/124 [==============================] - 1s 8ms/step - loss: 0.0438 - val_loss: 0.0181
Epoch 2/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0363 - val_loss: 0.0180
Epoch 3/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0359 - val_loss: 0.0179
Epoch 4/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0352 - val_loss: 0.0178
Epoch 5/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0346 - val_loss: 0.0179
Epoch 6/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0341 - val_loss: 0.0177
Epoch 7/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0176
Epoch 8/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0332 - val_loss: 0.0177
Epoch 9/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0329 - val_loss: 0.0174
Epoch 10/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0178
Epoch 11/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0321 - val_loss: 0.0175
Epoch 12/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0319 - val_loss: 0.0176
Epoch 13/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0316 - val_loss: 0.0176
Epoch 14/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0313 - val_loss: 0.0177
Epoch 15/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0310 - val_loss: 0.0175
Epoch 16/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0309 - val_loss: 0.0176
Epoch 17/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.0170
Epoch 18/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0308 - val_loss: 0.0173
Epoch 19/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0304 - val_loss: 0.0177
Epoch 20/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0301 - val_loss: 0.0177
Epoch 21/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0299 - val_loss: 0.0177
Epoch 22/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0298 - val_loss: 0.0177
Epoch 23/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.0178
Epoch 24/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0296 - val_loss: 0.0179
Epoch 25/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0295 - val_loss: 0.0179
Epoch 26/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0179
Epoch 27/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0293 - val_loss: 0.0179
Epoch 28/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0180
Epoch 29/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0291 - val_loss: 0.0180
Epoch 30/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0291 - val_loss: 0.0180
Epoch 31/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0290 - val_loss: 0.0181
Epoch 32/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0289 - val_loss: 0.0182
Epoch 33/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 34/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0182
Epoch 35/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0182
Epoch 36/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0183
Epoch 37/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0183
Epoch 38/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0183
Epoch 39/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0184
Epoch 40/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0183
Epoch 41/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0183
Epoch 42/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0184
Epoch 43/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0184
Epoch 44/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0184
Epoch 45/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0184
Epoch 46/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0184
Epoch 47/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0184
Epoch 48/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0184
Epoch 49/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0283 - val_loss: 0.0184
Epoch 50/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0184
Epoch 51/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0184
Epoch 52/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0184
Epoch 53/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0184
Epoch 54/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0184
Epoch 55/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0184
Epoch 56/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0281 - val_loss: 0.0184
Epoch 57/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0281 - val_loss: 0.0184
Epoch 58/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0184
Epoch 59/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0281 - val_loss: 0.0185
Epoch 60/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0184
Epoch 61/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0185
Epoch 62/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0185
Epoch 63/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0184
Epoch 64/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0185
Epoch 65/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0184
Epoch 66/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0185
Epoch 67/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0185
Epoch 68/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0185
Epoch 69/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0185
Epoch 70/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0185
Epoch 71/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0185
Epoch 72/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0185
Epoch 73/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0186
Epoch 74/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0186
Epoch 75/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0185
Epoch 76/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0185
Epoch 77/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0185
Epoch 78/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0185
Epoch 79/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0185
Epoch 80/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0185
Epoch 81/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0184
Epoch 82/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0184
Epoch 83/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0185
Epoch 84/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0184
Epoch 85/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0184
Epoch 86/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0184
Epoch 87/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0184
Epoch 88/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0184
Epoch 89/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0184
Epoch 90/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0184
Execution time:  64.38503503799438
DNN:
Mean Absolute Error: 0.0252
Root Mean Square Error: 0.0431
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_155&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_465 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_466 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_155 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_467 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
54/54 [==============================] - 0s 8ms/step - loss: 0.0471 - val_loss: 0.0361
Epoch 2/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0441 - val_loss: 0.0371
Epoch 3/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0426 - val_loss: 0.0347
Epoch 4/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0414 - val_loss: 0.0343
Epoch 5/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0401 - val_loss: 0.0331
Epoch 6/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0392 - val_loss: 0.0319
Epoch 7/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0385 - val_loss: 0.0311
Epoch 8/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0378 - val_loss: 0.0305
Epoch 9/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0373 - val_loss: 0.0301
Epoch 10/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0363 - val_loss: 0.0242
Epoch 11/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0358 - val_loss: 0.0262
Epoch 12/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0346 - val_loss: 0.0260
Epoch 13/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.0257
Epoch 14/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0336 - val_loss: 0.0254
Epoch 15/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0332 - val_loss: 0.0248
Epoch 16/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0328 - val_loss: 0.0244
Epoch 17/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0324 - val_loss: 0.0240
Epoch 18/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0320 - val_loss: 0.0237
Epoch 19/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0317 - val_loss: 0.0233
Epoch 20/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.0230
Epoch 21/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0310 - val_loss: 0.0226
Epoch 22/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0308 - val_loss: 0.0225
Epoch 23/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0305 - val_loss: 0.0221
Epoch 24/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0303 - val_loss: 0.0217
Epoch 25/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0301 - val_loss: 0.0217
Epoch 26/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0298 - val_loss: 0.0213
Epoch 27/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0296 - val_loss: 0.0209
Epoch 28/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0294 - val_loss: 0.0208
Epoch 29/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0292 - val_loss: 0.0205
Epoch 30/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0291 - val_loss: 0.0201
Epoch 31/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0289 - val_loss: 0.0201
Epoch 32/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0287 - val_loss: 0.0199
Epoch 33/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0286 - val_loss: 0.0195
Epoch 34/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0285 - val_loss: 0.0193
Epoch 35/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.0192
Epoch 36/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0190
Epoch 37/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 38/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0280 - val_loss: 0.0188
Epoch 39/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0280 - val_loss: 0.0187
Epoch 40/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0279 - val_loss: 0.0186
Epoch 41/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0186
Epoch 42/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0185
Epoch 43/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0183
Epoch 44/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0182
Epoch 45/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0182
Epoch 46/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0181
Epoch 47/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0181
Epoch 48/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0180
Epoch 49/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0179
Epoch 50/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0179
Epoch 51/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0272 - val_loss: 0.0179
Epoch 52/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.0178
Execution time:  17.002779245376587
DNN:
Mean Absolute Error: 0.0226
Root Mean Square Error: 0.0394
Mean Square Error: 0.0016

Train RMSE: 0.039
Train MSE: 0.002
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_156&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_468 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_469 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_156 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_470 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
112/112 [==============================] - 1s 8ms/step - loss: 0.2757 - val_loss: 0.2496
Epoch 2/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2725 - val_loss: 0.2463
Epoch 3/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2688 - val_loss: 0.2426
Epoch 4/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2649 - val_loss: 0.2386
Epoch 5/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2604 - val_loss: 0.2343
Epoch 6/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2561 - val_loss: 0.2298
Epoch 7/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2514 - val_loss: 0.2251
Epoch 8/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2464 - val_loss: 0.2201
Epoch 9/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2414 - val_loss: 0.2149
Epoch 10/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2360 - val_loss: 0.2095
Epoch 11/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2299 - val_loss: 0.2039
Epoch 12/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2243 - val_loss: 0.1980
Epoch 13/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2181 - val_loss: 0.1920
Epoch 14/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2119 - val_loss: 0.1858
Epoch 15/80
112/112 [==============================] - 1s 6ms/step - loss: 0.2056 - val_loss: 0.1795
Epoch 16/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1990 - val_loss: 0.1730
Epoch 17/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1924 - val_loss: 0.1664
Epoch 18/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1860 - val_loss: 0.1598
Epoch 19/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1794 - val_loss: 0.1534
Epoch 20/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1730 - val_loss: 0.1472
Epoch 21/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1668 - val_loss: 0.1410
Epoch 22/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1609 - val_loss: 0.1349
Epoch 23/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1555 - val_loss: 0.1291
Epoch 24/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1505 - val_loss: 0.1234
Epoch 25/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1457 - val_loss: 0.1180
Epoch 26/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1415 - val_loss: 0.1128
Epoch 27/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 0.1078
Epoch 28/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1342 - val_loss: 0.1032
Epoch 29/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1316 - val_loss: 0.0990
Epoch 30/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0952
Epoch 31/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0915
Epoch 32/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0881
Epoch 33/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0850
Epoch 34/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0820
Epoch 35/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0792
Epoch 36/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0765
Epoch 37/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1181 - val_loss: 0.0741
Epoch 38/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0717
Epoch 39/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0695
Epoch 40/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0675
Epoch 41/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1145 - val_loss: 0.0655
Epoch 42/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1138 - val_loss: 0.0636
Epoch 43/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1132 - val_loss: 0.0619
Epoch 44/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0602
Epoch 45/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0585
Epoch 46/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0570
Epoch 47/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0555
Epoch 48/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0540
Epoch 49/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0526
Epoch 50/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1092 - val_loss: 0.0513
Epoch 51/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0500
Epoch 52/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0488
Epoch 53/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0476
Epoch 54/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0465
Epoch 55/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0454
Epoch 56/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0444
Epoch 57/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0434
Epoch 58/80
112/112 [==============================] - ETA: 0s - loss: 0.106 - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0424
Epoch 59/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0414
Epoch 60/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0405
Epoch 61/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0396
Epoch 62/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.0387
Epoch 63/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1045 - val_loss: 0.0379
Epoch 64/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1040 - val_loss: 0.0370
Epoch 65/80
112/112 [==============================] - 1s 7ms/step - loss: 0.1040 - val_loss: 0.0362
Epoch 66/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0354
Epoch 67/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0347
Epoch 68/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0339
Epoch 69/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0332
Epoch 70/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0326
Epoch 71/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1021 - val_loss: 0.0320
Epoch 72/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0313
Epoch 73/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0308
Epoch 74/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0302
Epoch 75/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.0296
Epoch 76/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0291
Epoch 77/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0286
Epoch 78/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0282
Epoch 79/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1004 - val_loss: 0.0277
Epoch 80/80
112/112 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0273
Execution time:  59.93169403076172
DNN:
Mean Absolute Error: 0.0426
Root Mean Square Error: 0.0469
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.043
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_157&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_471 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_472 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_157 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_473 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
139/139 [==============================] - 1s 6ms/step - loss: 0.4103 - val_loss: 0.3733
Epoch 2/90
139/139 [==============================] - 1s 6ms/step - loss: 0.4069 - val_loss: 0.3698
Epoch 3/90
139/139 [==============================] - 1s 6ms/step - loss: 0.4031 - val_loss: 0.3663
Epoch 4/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3996 - val_loss: 0.3630
Epoch 5/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3960 - val_loss: 0.3594
Epoch 6/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3922 - val_loss: 0.3556
Epoch 7/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3881 - val_loss: 0.3515
Epoch 8/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3838 - val_loss: 0.3472
Epoch 9/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3792 - val_loss: 0.3427
Epoch 10/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3745 - val_loss: 0.3387
Epoch 11/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3708 - val_loss: 0.3361
Epoch 12/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3678 - val_loss: 0.3334
Epoch 13/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3649 - val_loss: 0.3306
Epoch 14/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3619 - val_loss: 0.3277
Epoch 15/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3588 - val_loss: 0.3247
Epoch 16/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3557 - val_loss: 0.3216
Epoch 17/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3523 - val_loss: 0.3183
Epoch 18/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3488 - val_loss: 0.3149
Epoch 19/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3453 - val_loss: 0.3114
Epoch 20/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3416 - val_loss: 0.3078
Epoch 21/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3378 - val_loss: 0.3043
Epoch 22/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3314 - val_loss: 0.2951
Epoch 23/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3215 - val_loss: 0.2854
Epoch 24/90
139/139 [==============================] - 1s 6ms/step - loss: 0.3114 - val_loss: 0.2756
Epoch 25/90
139/139 [==============================] - 1s 5ms/step - loss: 0.3011 - val_loss: 0.2655
Epoch 26/90
139/139 [==============================] - 1s 5ms/step - loss: 0.2906 - val_loss: 0.2551
Epoch 27/90
139/139 [==============================] - 1s 5ms/step - loss: 0.2798 - val_loss: 0.2444
Epoch 28/90
139/139 [==============================] - 1s 5ms/step - loss: 0.2687 - val_loss: 0.2334
Epoch 29/90
139/139 [==============================] - 1s 5ms/step - loss: 0.2572 - val_loss: 0.2220
Epoch 30/90
139/139 [==============================] - 1s 5ms/step - loss: 0.2445 - val_loss: 0.2073
Epoch 31/90
139/139 [==============================] - 1s 5ms/step - loss: 0.2280 - val_loss: 0.1905
Epoch 32/90
139/139 [==============================] - 1s 5ms/step - loss: 0.2107 - val_loss: 0.1736
Epoch 33/90
139/139 [==============================] - 1s 5ms/step - loss: 0.1932 - val_loss: 0.1568
Epoch 34/90
139/139 [==============================] - 1s 5ms/step - loss: 0.1758 - val_loss: 0.1401
Epoch 35/90
139/139 [==============================] - 1s 5ms/step - loss: 0.1587 - val_loss: 0.1234
Epoch 36/90
139/139 [==============================] - 1s 5ms/step - loss: 0.1413 - val_loss: 0.1062
Epoch 37/90
139/139 [==============================] - 1s 5ms/step - loss: 0.1237 - val_loss: 0.0887
Epoch 38/90
139/139 [==============================] - 1s 5ms/step - loss: 0.1056 - val_loss: 0.0707
Epoch 39/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0877 - val_loss: 0.0530
Epoch 40/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0374
Epoch 41/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0629 - val_loss: 0.0297
Epoch 42/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0607 - val_loss: 0.0252
Epoch 43/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0599 - val_loss: 0.0225
Epoch 44/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0593 - val_loss: 0.0210
Epoch 45/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0592 - val_loss: 0.0199
Epoch 46/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0591 - val_loss: 0.0191
Epoch 47/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0590 - val_loss: 0.0186
Epoch 48/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0589 - val_loss: 0.0183
Epoch 49/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0590 - val_loss: 0.0180
Epoch 50/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.0178
Epoch 51/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.0176
Epoch 52/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.0175
Epoch 53/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0173
Epoch 54/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0590 - val_loss: 0.0173
Epoch 55/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0590 - val_loss: 0.0172
Epoch 56/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0172
Epoch 57/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.0172
Epoch 58/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.0172
Epoch 59/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0172
Epoch 60/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.0171
Epoch 61/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.0171
Epoch 62/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0171
Epoch 63/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0171
Epoch 64/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0586 - val_loss: 0.0171
Epoch 65/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.0171
Epoch 66/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0171
Epoch 67/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0588 - val_loss: 0.0171
Epoch 68/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0587 - val_loss: 0.0171
Epoch 69/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0171
Epoch 70/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 71/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.0170
Epoch 72/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0586 - val_loss: 0.0170
Epoch 73/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0585 - val_loss: 0.0170
Epoch 74/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 75/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 76/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 77/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 78/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.0170
Epoch 79/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 80/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 81/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0586 - val_loss: 0.0170
Epoch 82/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0170
Epoch 83/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0586 - val_loss: 0.0169
Epoch 84/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0587 - val_loss: 0.0169
Epoch 85/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0584 - val_loss: 0.0169
Epoch 86/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.0169
Epoch 87/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0586 - val_loss: 0.0169
Epoch 88/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0585 - val_loss: 0.0169
Epoch 89/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0586 - val_loss: 0.0169
Epoch 90/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0585 - val_loss: 0.0169
Execution time:  67.70560669898987
DNN:
Mean Absolute Error: 0.0383
Root Mean Square Error: 0.0472
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.038
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_158&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_474 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_475 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_158 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_476 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
61/61 [==============================] - 0s 8ms/step - loss: 0.5223 - val_loss: 0.4778
Epoch 2/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5214 - val_loss: 0.4770
Epoch 3/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5208 - val_loss: 0.4761
Epoch 4/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5198 - val_loss: 0.4752
Epoch 5/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5189 - val_loss: 0.4743
Epoch 6/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5178 - val_loss: 0.4733
Epoch 7/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5167 - val_loss: 0.4723
Epoch 8/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5158 - val_loss: 0.4713
Epoch 9/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5146 - val_loss: 0.4702
Epoch 10/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5137 - val_loss: 0.4691
Epoch 11/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5124 - val_loss: 0.4680
Epoch 12/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5116 - val_loss: 0.4669
Epoch 13/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5102 - val_loss: 0.4657
Epoch 14/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5090 - val_loss: 0.4645
Epoch 15/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5078 - val_loss: 0.4632
Epoch 16/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5064 - val_loss: 0.4620
Epoch 17/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5049 - val_loss: 0.4607
Epoch 18/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5036 - val_loss: 0.4594
Epoch 19/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5023 - val_loss: 0.4580
Epoch 20/52
61/61 [==============================] - 0s 5ms/step - loss: 0.5009 - val_loss: 0.4567
Epoch 21/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4997 - val_loss: 0.4553
Epoch 22/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4982 - val_loss: 0.4539
Epoch 23/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4965 - val_loss: 0.4524
Epoch 24/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4951 - val_loss: 0.4510
Epoch 25/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.4495
Epoch 26/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4920 - val_loss: 0.4480
Epoch 27/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4905 - val_loss: 0.4464
Epoch 28/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4891 - val_loss: 0.4449
Epoch 29/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4873 - val_loss: 0.4433
Epoch 30/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4856 - val_loss: 0.4417
Epoch 31/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4841 - val_loss: 0.4401
Epoch 32/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4823 - val_loss: 0.4385
Epoch 33/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4807 - val_loss: 0.4368
Epoch 34/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4790 - val_loss: 0.4351
Epoch 35/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4771 - val_loss: 0.4334
Epoch 36/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4756 - val_loss: 0.4317
Epoch 37/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4736 - val_loss: 0.4299
Epoch 38/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4717 - val_loss: 0.4282
Epoch 39/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4699 - val_loss: 0.4264
Epoch 40/52
61/61 [==============================] - 0s 6ms/step - loss: 0.4684 - val_loss: 0.4246
Epoch 41/52
61/61 [==============================] - 0s 6ms/step - loss: 0.4662 - val_loss: 0.4228
Epoch 42/52
61/61 [==============================] - 0s 6ms/step - loss: 0.4645 - val_loss: 0.4209
Epoch 43/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4626 - val_loss: 0.4191
Epoch 44/52
61/61 [==============================] - 0s 6ms/step - loss: 0.4606 - val_loss: 0.4172
Epoch 45/52
61/61 [==============================] - 0s 6ms/step - loss: 0.4588 - val_loss: 0.4153
Epoch 46/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4567 - val_loss: 0.4134
Epoch 47/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4549 - val_loss: 0.4115
Epoch 48/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4526 - val_loss: 0.4095
Epoch 49/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4507 - val_loss: 0.4075
Epoch 50/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4488 - val_loss: 0.4055
Epoch 51/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4467 - val_loss: 0.4035
Epoch 52/52
61/61 [==============================] - 0s 5ms/step - loss: 0.4445 - val_loss: 0.4015
Execution time:  17.95234441757202
DNN:
Mean Absolute Error: 0.4408
Root Mean Square Error: 0.4437
Mean Square Error: 0.1968

Train RMSE: 0.444
Train MSE: 0.197
Train MAE: 0.441
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_159&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_477 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_478 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_159 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_479 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
100/100 [==============================] - 1s 8ms/step - loss: 0.3960 - val_loss: 0.3748
Epoch 2/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3937 - val_loss: 0.3725
Epoch 3/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3913 - val_loss: 0.3699
Epoch 4/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3886 - val_loss: 0.3672
Epoch 5/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3858 - val_loss: 0.3644
Epoch 6/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3827 - val_loss: 0.3613
Epoch 7/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3796 - val_loss: 0.3581
Epoch 8/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3762 - val_loss: 0.3548
Epoch 9/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3729 - val_loss: 0.3514
Epoch 10/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3692 - val_loss: 0.3479
Epoch 11/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3655 - val_loss: 0.3442
Epoch 12/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3617 - val_loss: 0.3405
Epoch 13/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3578 - val_loss: 0.3366
Epoch 14/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3539 - val_loss: 0.3327
Epoch 15/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3498 - val_loss: 0.3287
Epoch 16/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3457 - val_loss: 0.3246
Epoch 17/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3414 - val_loss: 0.3204
Epoch 18/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3371 - val_loss: 0.3160
Epoch 19/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3326 - val_loss: 0.3116
Epoch 20/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3280 - val_loss: 0.3071
Epoch 21/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3233 - val_loss: 0.3024
Epoch 22/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3187 - val_loss: 0.2980
Epoch 23/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3140 - val_loss: 0.2936
Epoch 24/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3096 - val_loss: 0.2891
Epoch 25/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3050 - val_loss: 0.2846
Epoch 26/80
100/100 [==============================] - 1s 7ms/step - loss: 0.3003 - val_loss: 0.2800
Epoch 27/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2956 - val_loss: 0.2755
Epoch 28/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2910 - val_loss: 0.2711
Epoch 29/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2865 - val_loss: 0.2667
Epoch 30/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2821 - val_loss: 0.2623
Epoch 31/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2775 - val_loss: 0.2579
Epoch 32/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2731 - val_loss: 0.2535
Epoch 33/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2688 - val_loss: 0.2491
Epoch 34/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2641 - val_loss: 0.2446
Epoch 35/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2597 - val_loss: 0.2401
Epoch 36/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2550 - val_loss: 0.2355
Epoch 37/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2504 - val_loss: 0.2309
Epoch 38/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2458 - val_loss: 0.2263
Epoch 39/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2411 - val_loss: 0.2216
Epoch 40/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2362 - val_loss: 0.2168
Epoch 41/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2314 - val_loss: 0.2120
Epoch 42/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2267 - val_loss: 0.2072
Epoch 43/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.2023
Epoch 44/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2168 - val_loss: 0.1973
Epoch 45/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2115 - val_loss: 0.1918
Epoch 46/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2064 - val_loss: 0.1862
Epoch 47/80
100/100 [==============================] - 1s 7ms/step - loss: 0.2004 - val_loss: 0.1803
Epoch 48/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1947 - val_loss: 0.1743
Epoch 49/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1884 - val_loss: 0.1683
Epoch 50/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1829 - val_loss: 0.1622
Epoch 51/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1768 - val_loss: 0.1562
Epoch 52/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1709 - val_loss: 0.1501
Epoch 53/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1655 - val_loss: 0.1442
Epoch 54/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1603 - val_loss: 0.1388
Epoch 55/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1563 - val_loss: 0.1337
Epoch 56/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1526 - val_loss: 0.1292
Epoch 57/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1495 - val_loss: 0.1250
Epoch 58/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1472 - val_loss: 0.1212
Epoch 59/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1448 - val_loss: 0.1177
Epoch 60/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1428 - val_loss: 0.1145
Epoch 61/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1414 - val_loss: 0.1114
Epoch 62/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1398 - val_loss: 0.1086
Epoch 63/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1382 - val_loss: 0.1058
Epoch 64/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1368 - val_loss: 0.1033
Epoch 65/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1357 - val_loss: 0.1008
Epoch 66/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1344 - val_loss: 0.0985
Epoch 67/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1333 - val_loss: 0.0963
Epoch 68/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.0943
Epoch 69/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1312 - val_loss: 0.0922
Epoch 70/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1307 - val_loss: 0.0903
Epoch 71/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1294 - val_loss: 0.0883
Epoch 72/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1290 - val_loss: 0.0865
Epoch 73/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1280 - val_loss: 0.0847
Epoch 74/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1275 - val_loss: 0.0829
Epoch 75/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1263 - val_loss: 0.0812
Epoch 76/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1259 - val_loss: 0.0795
Epoch 77/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1252 - val_loss: 0.0778
Epoch 78/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1245 - val_loss: 0.0762
Epoch 79/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1241 - val_loss: 0.0746
Epoch 80/80
100/100 [==============================] - 1s 7ms/step - loss: 0.1232 - val_loss: 0.0731
Execution time:  56.73365259170532
DNN:
Mean Absolute Error: 0.0849
Root Mean Square Error: 0.0892
Mean Square Error: 0.0080

Train RMSE: 0.089
Train MSE: 0.008
Train MAE: 0.085
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_160&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_480 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_481 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_160 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_482 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
124/124 [==============================] - 1s 7ms/step - loss: 0.3835 - val_loss: 0.3623
Epoch 2/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3798 - val_loss: 0.3585
Epoch 3/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3758 - val_loss: 0.3544
Epoch 4/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3714 - val_loss: 0.3500
Epoch 5/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3670 - val_loss: 0.3455
Epoch 6/90
124/124 [==============================] - 1s 5ms/step - loss: 0.3621 - val_loss: 0.3407
Epoch 7/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3570 - val_loss: 0.3356
Epoch 8/90
124/124 [==============================] - 1s 5ms/step - loss: 0.3518 - val_loss: 0.3303
Epoch 9/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3462 - val_loss: 0.3248
Epoch 10/90
124/124 [==============================] - 1s 5ms/step - loss: 0.3403 - val_loss: 0.3190
Epoch 11/90
124/124 [==============================] - 1s 5ms/step - loss: 0.3344 - val_loss: 0.3130
Epoch 12/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3281 - val_loss: 0.3068
Epoch 13/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3216 - val_loss: 0.3004
Epoch 14/90
124/124 [==============================] - 1s 5ms/step - loss: 0.3150 - val_loss: 0.2938
Epoch 15/90
124/124 [==============================] - 1s 5ms/step - loss: 0.3081 - val_loss: 0.2870
Epoch 16/90
124/124 [==============================] - 1s 6ms/step - loss: 0.3010 - val_loss: 0.2799
Epoch 17/90
124/124 [==============================] - 1s 6ms/step - loss: 0.2937 - val_loss: 0.2731
Epoch 18/90
124/124 [==============================] - 1s 6ms/step - loss: 0.2870 - val_loss: 0.2665
Epoch 19/90
124/124 [==============================] - 1s 5ms/step - loss: 0.2799 - val_loss: 0.2599
Epoch 20/90
124/124 [==============================] - 1s 5ms/step - loss: 0.2730 - val_loss: 0.2531
Epoch 21/90
124/124 [==============================] - 1s 5ms/step - loss: 0.2660 - val_loss: 0.2461
Epoch 22/90
124/124 [==============================] - 1s 5ms/step - loss: 0.2587 - val_loss: 0.2389
Epoch 23/90
124/124 [==============================] - 1s 5ms/step - loss: 0.2512 - val_loss: 0.2315
Epoch 24/90
124/124 [==============================] - 1s 5ms/step - loss: 0.2436 - val_loss: 0.2240
Epoch 25/90
124/124 [==============================] - 1s 6ms/step - loss: 0.2358 - val_loss: 0.2162
Epoch 26/90
124/124 [==============================] - 1s 6ms/step - loss: 0.2278 - val_loss: 0.2083
Epoch 27/90
124/124 [==============================] - 1s 6ms/step - loss: 0.2196 - val_loss: 0.2001
Epoch 28/90
124/124 [==============================] - 1s 6ms/step - loss: 0.2111 - val_loss: 0.1918
Epoch 29/90
124/124 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.1832
Epoch 30/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1939 - val_loss: 0.1752
Epoch 31/90
124/124 [==============================] - 1s 6ms/step - loss: 0.1858 - val_loss: 0.1673
Epoch 32/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1776 - val_loss: 0.1598
Epoch 33/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1700 - val_loss: 0.1525
Epoch 34/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1626 - val_loss: 0.1452
Epoch 35/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1549 - val_loss: 0.1377
Epoch 36/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1473 - val_loss: 0.1301
Epoch 37/90
124/124 [==============================] - 1s 6ms/step - loss: 0.1397 - val_loss: 0.1224
Epoch 38/90
124/124 [==============================] - 1s 6ms/step - loss: 0.1318 - val_loss: 0.1146
Epoch 39/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1239 - val_loss: 0.1067
Epoch 40/90
124/124 [==============================] - 1s 5ms/step - loss: 0.1160 - val_loss: 0.0987
Epoch 41/90
124/124 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0906
Epoch 42/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0998 - val_loss: 0.0824
Epoch 43/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0917 - val_loss: 0.0743
Epoch 44/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0842 - val_loss: 0.0664
Epoch 45/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0771 - val_loss: 0.0587
Epoch 46/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0704 - val_loss: 0.0515
Epoch 47/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0648 - val_loss: 0.0451
Epoch 48/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0604 - val_loss: 0.0414
Epoch 49/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0588 - val_loss: 0.0386
Epoch 50/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0578 - val_loss: 0.0365
Epoch 51/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0570 - val_loss: 0.0349
Epoch 52/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0564 - val_loss: 0.0336
Epoch 53/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0560 - val_loss: 0.0325
Epoch 54/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0558 - val_loss: 0.0317
Epoch 55/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0553 - val_loss: 0.0310
Epoch 56/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0552 - val_loss: 0.0303
Epoch 57/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0550 - val_loss: 0.0297
Epoch 58/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0548 - val_loss: 0.0291
Epoch 59/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0547 - val_loss: 0.0286
Epoch 60/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0546 - val_loss: 0.0281
Epoch 61/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0544 - val_loss: 0.0276
Epoch 62/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0541 - val_loss: 0.0271
Epoch 63/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0543 - val_loss: 0.0266
Epoch 64/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0541 - val_loss: 0.0262
Epoch 65/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0540 - val_loss: 0.0257
Epoch 66/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0539 - val_loss: 0.0253
Epoch 67/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0539 - val_loss: 0.0250
Epoch 68/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0538 - val_loss: 0.0246
Epoch 69/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0537 - val_loss: 0.0243
Epoch 70/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0536 - val_loss: 0.0240
Epoch 71/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0536 - val_loss: 0.0237
Epoch 72/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0536 - val_loss: 0.0234
Epoch 73/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0535 - val_loss: 0.0231
Epoch 74/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0535 - val_loss: 0.0228
Epoch 75/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0534 - val_loss: 0.0225
Epoch 76/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0535 - val_loss: 0.0222
Epoch 77/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0532 - val_loss: 0.0220
Epoch 78/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0533 - val_loss: 0.0217
Epoch 79/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0533 - val_loss: 0.0215
Epoch 80/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.0212
Epoch 81/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.0210
Epoch 82/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.0208
Epoch 83/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.0207
Epoch 84/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0531 - val_loss: 0.0205
Epoch 85/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0532 - val_loss: 0.0204
Epoch 86/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0531 - val_loss: 0.0202
Epoch 87/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0530 - val_loss: 0.0201
Epoch 88/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0531 - val_loss: 0.0199
Epoch 89/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0529 - val_loss: 0.0198
Epoch 90/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0529 - val_loss: 0.0196
Execution time:  64.18820762634277
DNN:
Mean Absolute Error: 0.0250
Root Mean Square Error: 0.0333
Mean Square Error: 0.0011

Train RMSE: 0.033
Train MSE: 0.001
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_161&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_483 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_484 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_161 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_485 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
54/54 [==============================] - 0s 9ms/step - loss: 0.3045 - val_loss: 0.2873
Epoch 2/52
54/54 [==============================] - 0s 6ms/step - loss: 0.3040 - val_loss: 0.2864
Epoch 3/52
54/54 [==============================] - 0s 6ms/step - loss: 0.3031 - val_loss: 0.2855
Epoch 4/52
54/54 [==============================] - 0s 6ms/step - loss: 0.3020 - val_loss: 0.2845
Epoch 5/52
54/54 [==============================] - 0s 6ms/step - loss: 0.3011 - val_loss: 0.2835
Epoch 6/52
54/54 [==============================] - 0s 6ms/step - loss: 0.3003 - val_loss: 0.2825
Epoch 7/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2990 - val_loss: 0.2814
Epoch 8/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2980 - val_loss: 0.2803
Epoch 9/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2969 - val_loss: 0.2791
Epoch 10/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2954 - val_loss: 0.2780
Epoch 11/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2944 - val_loss: 0.2767
Epoch 12/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2934 - val_loss: 0.2755
Epoch 13/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2918 - val_loss: 0.2742
Epoch 14/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2909 - val_loss: 0.2729
Epoch 15/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2893 - val_loss: 0.2715
Epoch 16/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2877 - val_loss: 0.2702
Epoch 17/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2865 - val_loss: 0.2687
Epoch 18/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2853 - val_loss: 0.2673
Epoch 19/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2841 - val_loss: 0.2658
Epoch 20/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2826 - val_loss: 0.2644
Epoch 21/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2809 - val_loss: 0.2628
Epoch 22/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2793 - val_loss: 0.2613
Epoch 23/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2777 - val_loss: 0.2597
Epoch 24/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2761 - val_loss: 0.2581
Epoch 25/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2742 - val_loss: 0.2565
Epoch 26/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2731 - val_loss: 0.2549
Epoch 27/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2715 - val_loss: 0.2532
Epoch 28/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2699 - val_loss: 0.2515
Epoch 29/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2681 - val_loss: 0.2498
Epoch 30/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2662 - val_loss: 0.2481
Epoch 31/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2649 - val_loss: 0.2463
Epoch 32/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2632 - val_loss: 0.2445
Epoch 33/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2615 - val_loss: 0.2427
Epoch 34/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2596 - val_loss: 0.2409
Epoch 35/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2576 - val_loss: 0.2391
Epoch 36/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2557 - val_loss: 0.2372
Epoch 37/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2544 - val_loss: 0.2353
Epoch 38/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2522 - val_loss: 0.2334
Epoch 39/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2508 - val_loss: 0.2315
Epoch 40/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2488 - val_loss: 0.2295
Epoch 41/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2474 - val_loss: 0.2276
Epoch 42/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2448 - val_loss: 0.2256
Epoch 43/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2430 - val_loss: 0.2236
Epoch 44/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2413 - val_loss: 0.2216
Epoch 45/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2394 - val_loss: 0.2196
Epoch 46/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2377 - val_loss: 0.2176
Epoch 47/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2355 - val_loss: 0.2156
Epoch 48/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2341 - val_loss: 0.2135
Epoch 49/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2322 - val_loss: 0.2114
Epoch 50/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2297 - val_loss: 0.2094
Epoch 51/52
54/54 [==============================] - 0s 5ms/step - loss: 0.2281 - val_loss: 0.2073
Epoch 52/52
54/54 [==============================] - 0s 6ms/step - loss: 0.2262 - val_loss: 0.2052
Execution time:  16.786931037902832
DNN:
Mean Absolute Error: 0.2174
Root Mean Square Error: 0.2192
Mean Square Error: 0.0481

Train RMSE: 0.219
Train MSE: 0.048
Train MAE: 0.217
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_162&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_486 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_487 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_162 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_488 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
112/112 [==============================] - 1s 8ms/step - loss: 0.0962 - val_loss: 0.1283
Epoch 2/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0957 - val_loss: 0.1278
Epoch 3/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0951 - val_loss: 0.1272
Epoch 4/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0945 - val_loss: 0.1266
Epoch 5/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0939 - val_loss: 0.1260
Epoch 6/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0933 - val_loss: 0.1253
Epoch 7/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0927 - val_loss: 0.1247
Epoch 8/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0920 - val_loss: 0.1240
Epoch 9/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0913 - val_loss: 0.1233
Epoch 10/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0906 - val_loss: 0.1225
Epoch 11/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0899 - val_loss: 0.1217
Epoch 12/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0891 - val_loss: 0.1210
Epoch 13/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0884 - val_loss: 0.1201
Epoch 14/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0876 - val_loss: 0.1193
Epoch 15/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0868 - val_loss: 0.1185
Epoch 16/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0860 - val_loss: 0.1176
Epoch 17/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0851 - val_loss: 0.1167
Epoch 18/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0843 - val_loss: 0.1158
Epoch 19/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0834 - val_loss: 0.1149
Epoch 20/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0825 - val_loss: 0.1140
Epoch 21/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0817 - val_loss: 0.1130
Epoch 22/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0808 - val_loss: 0.1121
Epoch 23/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0799 - val_loss: 0.1111
Epoch 24/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0790 - val_loss: 0.1102
Epoch 25/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0781 - val_loss: 0.1092
Epoch 26/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0772 - val_loss: 0.1082
Epoch 27/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0764 - val_loss: 0.1072
Epoch 28/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0754 - val_loss: 0.1062
Epoch 29/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0745 - val_loss: 0.1052
Epoch 30/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0736 - val_loss: 0.1042
Epoch 31/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0727 - val_loss: 0.1032
Epoch 32/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0718 - val_loss: 0.1021
Epoch 33/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0709 - val_loss: 0.1011
Epoch 34/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0701 - val_loss: 0.1002
Epoch 35/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0692 - val_loss: 0.0993
Epoch 36/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0685 - val_loss: 0.0984
Epoch 37/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0678 - val_loss: 0.0976
Epoch 38/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0670 - val_loss: 0.0966
Epoch 39/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0662 - val_loss: 0.0956
Epoch 40/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0654 - val_loss: 0.0947
Epoch 41/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0646 - val_loss: 0.0937
Epoch 42/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0638 - val_loss: 0.0928
Epoch 43/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0631 - val_loss: 0.0919
Epoch 44/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0623 - val_loss: 0.0909
Epoch 45/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0616 - val_loss: 0.0900
Epoch 46/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0609 - val_loss: 0.0891
Epoch 47/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0603 - val_loss: 0.0883
Epoch 48/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0597 - val_loss: 0.0874
Epoch 49/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0590 - val_loss: 0.0865
Epoch 50/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0584 - val_loss: 0.0857
Epoch 51/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0578 - val_loss: 0.0849
Epoch 52/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0572 - val_loss: 0.0841
Epoch 53/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0566 - val_loss: 0.0833
Epoch 54/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0561 - val_loss: 0.0825
Epoch 55/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0556 - val_loss: 0.0817
Epoch 56/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0550 - val_loss: 0.0810
Epoch 57/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0545 - val_loss: 0.0802
Epoch 58/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0540 - val_loss: 0.0795
Epoch 59/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0536 - val_loss: 0.0788
Epoch 60/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0530 - val_loss: 0.0781
Epoch 61/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0526 - val_loss: 0.0774
Epoch 62/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0522 - val_loss: 0.0767
Epoch 63/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0517 - val_loss: 0.0760
Epoch 64/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0753
Epoch 65/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.0747
Epoch 66/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0504 - val_loss: 0.0740
Epoch 67/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0501 - val_loss: 0.0734
Epoch 68/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0497 - val_loss: 0.0727
Epoch 69/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0493 - val_loss: 0.0721
Epoch 70/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0489 - val_loss: 0.0715
Epoch 71/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0486 - val_loss: 0.0709
Epoch 72/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0483 - val_loss: 0.0703
Epoch 73/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0479 - val_loss: 0.0697
Epoch 74/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0476 - val_loss: 0.0691
Epoch 75/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0472 - val_loss: 0.0685
Epoch 76/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0470 - val_loss: 0.0679
Epoch 77/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0466 - val_loss: 0.0673
Epoch 78/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0462 - val_loss: 0.0668
Epoch 79/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0460 - val_loss: 0.0662
Epoch 80/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0457 - val_loss: 0.0657
Execution time:  60.93491840362549
DNN:
Mean Absolute Error: 0.0458
Root Mean Square Error: 0.0542
Mean Square Error: 0.0029

Train RMSE: 0.054
Train MSE: 0.003
Train MAE: 0.046
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_163&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_489 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_490 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_163 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_491 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
139/139 [==============================] - 1s 7ms/step - loss: 0.0945 - val_loss: 0.1267
Epoch 2/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0940 - val_loss: 0.1263
Epoch 3/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0935 - val_loss: 0.1257
Epoch 4/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0930 - val_loss: 0.1252
Epoch 5/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0924 - val_loss: 0.1246
Epoch 6/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0919 - val_loss: 0.1240
Epoch 7/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0913 - val_loss: 0.1234
Epoch 8/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0907 - val_loss: 0.1228
Epoch 9/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0901 - val_loss: 0.1222
Epoch 10/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0895 - val_loss: 0.1215
Epoch 11/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0888 - val_loss: 0.1208
Epoch 12/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0882 - val_loss: 0.1201
Epoch 13/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0875 - val_loss: 0.1194
Epoch 14/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0868 - val_loss: 0.1186
Epoch 15/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0861 - val_loss: 0.1179
Epoch 16/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0853 - val_loss: 0.1171
Epoch 17/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0846 - val_loss: 0.1163
Epoch 18/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0838 - val_loss: 0.1155
Epoch 19/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0831 - val_loss: 0.1147
Epoch 20/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0823 - val_loss: 0.1139
Epoch 21/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0816 - val_loss: 0.1131
Epoch 22/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0808 - val_loss: 0.1123
Epoch 23/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0800 - val_loss: 0.1114
Epoch 24/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0792 - val_loss: 0.1106
Epoch 25/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0784 - val_loss: 0.1097
Epoch 26/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0776 - val_loss: 0.1088
Epoch 27/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0768 - val_loss: 0.1080
Epoch 28/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0760 - val_loss: 0.1071
Epoch 29/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0752 - val_loss: 0.1062
Epoch 30/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0744 - val_loss: 0.1053
Epoch 31/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0736 - val_loss: 0.1044
Epoch 32/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0728 - val_loss: 0.1034
Epoch 33/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0719 - val_loss: 0.1025
Epoch 34/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0711 - val_loss: 0.1016
Epoch 35/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0703 - val_loss: 0.1006
Epoch 36/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0695 - val_loss: 0.0996
Epoch 37/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0686 - val_loss: 0.0987
Epoch 38/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0678 - val_loss: 0.0977
Epoch 39/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0670 - val_loss: 0.0968
Epoch 40/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0661 - val_loss: 0.0958
Epoch 41/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0653 - val_loss: 0.0948
Epoch 42/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0645 - val_loss: 0.0939
Epoch 43/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0637 - val_loss: 0.0929
Epoch 44/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0629 - val_loss: 0.0920
Epoch 45/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0622 - val_loss: 0.0910
Epoch 46/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0614 - val_loss: 0.0901
Epoch 47/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0606 - val_loss: 0.0891
Epoch 48/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0599 - val_loss: 0.0882
Epoch 49/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0591 - val_loss: 0.0872
Epoch 50/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0583 - val_loss: 0.0863
Epoch 51/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0576 - val_loss: 0.0854
Epoch 52/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0569 - val_loss: 0.0844
Epoch 53/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0562 - val_loss: 0.0835
Epoch 54/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0555 - val_loss: 0.0826
Epoch 55/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0549 - val_loss: 0.0818
Epoch 56/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0543 - val_loss: 0.0810
Epoch 57/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0537 - val_loss: 0.0801
Epoch 58/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0531 - val_loss: 0.0793
Epoch 59/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0525 - val_loss: 0.0786
Epoch 60/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0520 - val_loss: 0.0778
Epoch 61/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0515 - val_loss: 0.0770
Epoch 62/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0510 - val_loss: 0.0763
Epoch 63/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0505 - val_loss: 0.0755
Epoch 64/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0500 - val_loss: 0.0748
Epoch 65/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0495 - val_loss: 0.0741
Epoch 66/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0490 - val_loss: 0.0734
Epoch 67/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0486 - val_loss: 0.0726
Epoch 68/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0482 - val_loss: 0.0720
Epoch 69/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0478 - val_loss: 0.0713
Epoch 70/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0473 - val_loss: 0.0706
Epoch 71/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0469 - val_loss: 0.0699
Epoch 72/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0465 - val_loss: 0.0693
Epoch 73/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0462 - val_loss: 0.0687
Epoch 74/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0458 - val_loss: 0.0680
Epoch 75/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0454 - val_loss: 0.0674
Epoch 76/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0451 - val_loss: 0.0667
Epoch 77/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0447 - val_loss: 0.0661
Epoch 78/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0444 - val_loss: 0.0655
Epoch 79/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0440 - val_loss: 0.0649
Epoch 80/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0437 - val_loss: 0.0643
Epoch 81/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0434 - val_loss: 0.0636
Epoch 82/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0430 - val_loss: 0.0630
Epoch 83/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0427 - val_loss: 0.0624
Epoch 84/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0424 - val_loss: 0.0618
Epoch 85/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0421 - val_loss: 0.0612
Epoch 86/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0417 - val_loss: 0.0606
Epoch 87/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0414 - val_loss: 0.0600
Epoch 88/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0411 - val_loss: 0.0594
Epoch 89/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0408 - val_loss: 0.0588
Epoch 90/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0406 - val_loss: 0.0582
Execution time:  68.71140575408936
DNN:
Mean Absolute Error: 0.0418
Root Mean Square Error: 0.0504
Mean Square Error: 0.0025

Train RMSE: 0.050
Train MSE: 0.003
Train MAE: 0.042
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_164&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_492 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_493 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_164 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_494 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
61/61 [==============================] - 0s 8ms/step - loss: 0.1072 - val_loss: 0.1381
Epoch 2/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1071 - val_loss: 0.1379
Epoch 3/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1067 - val_loss: 0.1377
Epoch 4/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1065 - val_loss: 0.1374
Epoch 5/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1064 - val_loss: 0.1372
Epoch 6/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1062 - val_loss: 0.1369
Epoch 7/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1058 - val_loss: 0.1367
Epoch 8/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1055 - val_loss: 0.1364
Epoch 9/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1052 - val_loss: 0.1361
Epoch 10/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1050 - val_loss: 0.1358
Epoch 11/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1047 - val_loss: 0.1355
Epoch 12/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1045 - val_loss: 0.1352
Epoch 13/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1042 - val_loss: 0.1349
Epoch 14/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1039 - val_loss: 0.1346
Epoch 15/52
61/61 [==============================] - 0s 6ms/step - loss: 0.1036 - val_loss: 0.1343
Epoch 16/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1033 - val_loss: 0.1339
Epoch 17/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1030 - val_loss: 0.1336
Epoch 18/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1027 - val_loss: 0.1333
Epoch 19/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1023 - val_loss: 0.1329
Epoch 20/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1020 - val_loss: 0.1326
Epoch 21/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1017 - val_loss: 0.1323
Epoch 22/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1014 - val_loss: 0.1319
Epoch 23/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1010 - val_loss: 0.1315
Epoch 24/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1007 - val_loss: 0.1312
Epoch 25/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1004 - val_loss: 0.1308
Epoch 26/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1000 - val_loss: 0.1305
Epoch 27/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0997 - val_loss: 0.1301
Epoch 28/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0993 - val_loss: 0.1297
Epoch 29/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0989 - val_loss: 0.1293
Epoch 30/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0986 - val_loss: 0.1290
Epoch 31/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0983 - val_loss: 0.1286
Epoch 32/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0979 - val_loss: 0.1282
Epoch 33/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0975 - val_loss: 0.1278
Epoch 34/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0971 - val_loss: 0.1274
Epoch 35/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0968 - val_loss: 0.1270
Epoch 36/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0964 - val_loss: 0.1266
Epoch 37/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0960 - val_loss: 0.1262
Epoch 38/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0956 - val_loss: 0.1258
Epoch 39/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0953 - val_loss: 0.1254
Epoch 40/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0950 - val_loss: 0.1250
Epoch 41/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0946 - val_loss: 0.1246
Epoch 42/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0941 - val_loss: 0.1242
Epoch 43/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0938 - val_loss: 0.1238
Epoch 44/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0935 - val_loss: 0.1234
Epoch 45/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0931 - val_loss: 0.1230
Epoch 46/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0927 - val_loss: 0.1226
Epoch 47/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0924 - val_loss: 0.1222
Epoch 48/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0920 - val_loss: 0.1218
Epoch 49/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0916 - val_loss: 0.1213
Epoch 50/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0913 - val_loss: 0.1209
Epoch 51/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0908 - val_loss: 0.1205
Epoch 52/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0904 - val_loss: 0.1201
Execution time:  17.919365882873535
DNN:
Mean Absolute Error: 0.0903
Root Mean Square Error: 0.0970
Mean Square Error: 0.0094

Train RMSE: 0.097
Train MSE: 0.009
Train MAE: 0.090
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_165&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_495 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_496 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_165 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_497 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
100/100 [==============================] - 1s 8ms/step - loss: 0.0977 - val_loss: 0.1167
Epoch 2/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0970 - val_loss: 0.1160
Epoch 3/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.1152
Epoch 4/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0954 - val_loss: 0.1144
Epoch 5/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0946 - val_loss: 0.1136
Epoch 6/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0937 - val_loss: 0.1127
Epoch 7/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0928 - val_loss: 0.1118
Epoch 8/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0919 - val_loss: 0.1109
Epoch 9/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0909 - val_loss: 0.1100
Epoch 10/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0899 - val_loss: 0.1090
Epoch 11/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0889 - val_loss: 0.1080
Epoch 12/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0879 - val_loss: 0.1070
Epoch 13/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0870 - val_loss: 0.1062
Epoch 14/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0861 - val_loss: 0.1053
Epoch 15/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0852 - val_loss: 0.1045
Epoch 16/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0844 - val_loss: 0.1036
Epoch 17/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0834 - val_loss: 0.1027
Epoch 18/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0826 - val_loss: 0.1018
Epoch 19/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0816 - val_loss: 0.1009
Epoch 20/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0807 - val_loss: 0.1000
Epoch 21/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0798 - val_loss: 0.0990
Epoch 22/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0789 - val_loss: 0.0981
Epoch 23/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0780 - val_loss: 0.0972
Epoch 24/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0771 - val_loss: 0.0965
Epoch 25/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0764 - val_loss: 0.0957
Epoch 26/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0756 - val_loss: 0.0949
Epoch 27/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0749 - val_loss: 0.0942
Epoch 28/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0742 - val_loss: 0.0934
Epoch 29/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0735 - val_loss: 0.0927
Epoch 30/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.0919
Epoch 31/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.0912
Epoch 32/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0713 - val_loss: 0.0904
Epoch 33/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0706 - val_loss: 0.0896
Epoch 34/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0698 - val_loss: 0.0889
Epoch 35/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0691 - val_loss: 0.0882
Epoch 36/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0685 - val_loss: 0.0874
Epoch 37/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0678 - val_loss: 0.0867
Epoch 38/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0671 - val_loss: 0.0860
Epoch 39/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0665 - val_loss: 0.0853
Epoch 40/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0659 - val_loss: 0.0845
Epoch 41/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0652 - val_loss: 0.0837
Epoch 42/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0645 - val_loss: 0.0829
Epoch 43/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0638 - val_loss: 0.0821
Epoch 44/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0631 - val_loss: 0.0813
Epoch 45/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0624 - val_loss: 0.0805
Epoch 46/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0618 - val_loss: 0.0798
Epoch 47/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0611 - val_loss: 0.0790
Epoch 48/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0605 - val_loss: 0.0784
Epoch 49/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0600 - val_loss: 0.0777
Epoch 50/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0594 - val_loss: 0.0770
Epoch 51/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0589 - val_loss: 0.0764
Epoch 52/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0585 - val_loss: 0.0758
Epoch 53/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0580 - val_loss: 0.0751
Epoch 54/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0575 - val_loss: 0.0745
Epoch 55/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.0739
Epoch 56/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0566 - val_loss: 0.0733
Epoch 57/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0561 - val_loss: 0.0728
Epoch 58/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0558 - val_loss: 0.0722
Epoch 59/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0553 - val_loss: 0.0716
Epoch 60/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.0711
Epoch 61/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0545 - val_loss: 0.0706
Epoch 62/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.0700
Epoch 63/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.0695
Epoch 64/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0534 - val_loss: 0.0690
Epoch 65/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0685
Epoch 66/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0680
Epoch 67/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0675
Epoch 68/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0670
Epoch 69/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0665
Epoch 70/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0660
Epoch 71/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0656
Epoch 72/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0508 - val_loss: 0.0651
Epoch 73/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0504 - val_loss: 0.0646
Epoch 74/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.0642
Epoch 75/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0499 - val_loss: 0.0637
Epoch 76/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.0633
Epoch 77/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0628
Epoch 78/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0491 - val_loss: 0.0624
Epoch 79/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.0620
Epoch 80/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.0615
Execution time:  56.42977023124695
DNN:
Mean Absolute Error: 0.0493
Root Mean Square Error: 0.0574
Mean Square Error: 0.0033

Train RMSE: 0.057
Train MSE: 0.003
Train MAE: 0.049
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_166&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_498 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_499 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_166 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_500 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
124/124 [==============================] - 1s 7ms/step - loss: 0.0986 - val_loss: 0.1176
Epoch 2/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0979 - val_loss: 0.1169
Epoch 3/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0972 - val_loss: 0.1162
Epoch 4/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.1155
Epoch 5/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0957 - val_loss: 0.1148
Epoch 6/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0949 - val_loss: 0.1140
Epoch 7/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0941 - val_loss: 0.1132
Epoch 8/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0933 - val_loss: 0.1125
Epoch 9/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0925 - val_loss: 0.1116
Epoch 10/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0916 - val_loss: 0.1108
Epoch 11/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0908 - val_loss: 0.1099
Epoch 12/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0899 - val_loss: 0.1090
Epoch 13/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0890 - val_loss: 0.1081
Epoch 14/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0880 - val_loss: 0.1072
Epoch 15/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0870 - val_loss: 0.1062
Epoch 16/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0861 - val_loss: 0.1054
Epoch 17/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0852 - val_loss: 0.1045
Epoch 18/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0843 - val_loss: 0.1037
Epoch 19/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0834 - val_loss: 0.1028
Epoch 20/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0826 - val_loss: 0.1019
Epoch 21/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0816 - val_loss: 0.1010
Epoch 22/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0807 - val_loss: 0.1001
Epoch 23/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0798 - val_loss: 0.0992
Epoch 24/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0789 - val_loss: 0.0983
Epoch 25/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0780 - val_loss: 0.0973
Epoch 26/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0770 - val_loss: 0.0964
Epoch 27/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0761 - val_loss: 0.0955
Epoch 28/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0752 - val_loss: 0.0945
Epoch 29/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0743 - val_loss: 0.0936
Epoch 30/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0733 - val_loss: 0.0926
Epoch 31/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0724 - val_loss: 0.0916
Epoch 32/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0714 - val_loss: 0.0907
Epoch 33/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0705 - val_loss: 0.0897
Epoch 34/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0695 - val_loss: 0.0887
Epoch 35/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0686 - val_loss: 0.0877
Epoch 36/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0676 - val_loss: 0.0867
Epoch 37/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0666 - val_loss: 0.0857
Epoch 38/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0657 - val_loss: 0.0847
Epoch 39/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0648 - val_loss: 0.0837
Epoch 40/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0639 - val_loss: 0.0827
Epoch 41/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0630 - val_loss: 0.0817
Epoch 42/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0621 - val_loss: 0.0808
Epoch 43/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0613 - val_loss: 0.0798
Epoch 44/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0604 - val_loss: 0.0788
Epoch 45/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0595 - val_loss: 0.0779
Epoch 46/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0587 - val_loss: 0.0769
Epoch 47/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0579 - val_loss: 0.0759
Epoch 48/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0571 - val_loss: 0.0750
Epoch 49/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0562 - val_loss: 0.0741
Epoch 50/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0555 - val_loss: 0.0732
Epoch 51/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0548 - val_loss: 0.0723
Epoch 52/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0541 - val_loss: 0.0714
Epoch 53/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0534 - val_loss: 0.0706
Epoch 54/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0528 - val_loss: 0.0697
Epoch 55/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0522 - val_loss: 0.0689
Epoch 56/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0517 - val_loss: 0.0682
Epoch 57/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0511 - val_loss: 0.0674
Epoch 58/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0506 - val_loss: 0.0666
Epoch 59/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0500 - val_loss: 0.0659
Epoch 60/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0495 - val_loss: 0.0652
Epoch 61/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0490 - val_loss: 0.0644
Epoch 62/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0485 - val_loss: 0.0637
Epoch 63/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0480 - val_loss: 0.0630
Epoch 64/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0476 - val_loss: 0.0624
Epoch 65/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0471 - val_loss: 0.0617
Epoch 66/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0467 - val_loss: 0.0611
Epoch 67/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0463 - val_loss: 0.0604
Epoch 68/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0459 - val_loss: 0.0598
Epoch 69/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0455 - val_loss: 0.0592
Epoch 70/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0451 - val_loss: 0.0586
Epoch 71/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0448 - val_loss: 0.0580
Epoch 72/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0444 - val_loss: 0.0574
Epoch 73/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0441 - val_loss: 0.0568
Epoch 74/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0437 - val_loss: 0.0562
Epoch 75/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0434 - val_loss: 0.0556
Epoch 76/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0431 - val_loss: 0.0550
Epoch 77/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0428 - val_loss: 0.0545
Epoch 78/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0424 - val_loss: 0.0539
Epoch 79/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0422 - val_loss: 0.0533
Epoch 80/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0418 - val_loss: 0.0528
Epoch 81/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0416 - val_loss: 0.0522
Epoch 82/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0413 - val_loss: 0.0517
Epoch 83/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0410 - val_loss: 0.0512
Epoch 84/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0407 - val_loss: 0.0506
Epoch 85/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0404 - val_loss: 0.0501
Epoch 86/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0401 - val_loss: 0.0496
Epoch 87/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0399 - val_loss: 0.0491
Epoch 88/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0397 - val_loss: 0.0486
Epoch 89/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0394 - val_loss: 0.0481
Epoch 90/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0392 - val_loss: 0.0476
Execution time:  64.22369146347046
DNN:
Mean Absolute Error: 0.0404
Root Mean Square Error: 0.0494
Mean Square Error: 0.0024

Train RMSE: 0.049
Train MSE: 0.002
Train MAE: 0.040
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_167&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_501 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_502 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_167 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_503 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
54/54 [==============================] - 0s 9ms/step - loss: 0.0789 - val_loss: 0.0986
Epoch 2/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0788 - val_loss: 0.0985
Epoch 3/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0787 - val_loss: 0.0983
Epoch 4/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0786 - val_loss: 0.0982
Epoch 5/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0785 - val_loss: 0.0981
Epoch 6/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0784 - val_loss: 0.0980
Epoch 7/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0783 - val_loss: 0.0978
Epoch 8/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0781 - val_loss: 0.0977
Epoch 9/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0780 - val_loss: 0.0976
Epoch 10/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.0974
Epoch 11/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0777 - val_loss: 0.0973
Epoch 12/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0776 - val_loss: 0.0971
Epoch 13/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0774 - val_loss: 0.0970
Epoch 14/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0773 - val_loss: 0.0968
Epoch 15/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.0967
Epoch 16/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0770 - val_loss: 0.0965
Epoch 17/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0769 - val_loss: 0.0964
Epoch 18/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0768 - val_loss: 0.0962
Epoch 19/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0766 - val_loss: 0.0961
Epoch 20/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0765 - val_loss: 0.0959
Epoch 21/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0763 - val_loss: 0.0958
Epoch 22/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0762 - val_loss: 0.0956
Epoch 23/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0760 - val_loss: 0.0954
Epoch 24/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0759 - val_loss: 0.0953
Epoch 25/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0756 - val_loss: 0.0951
Epoch 26/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0756 - val_loss: 0.0949
Epoch 27/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0754 - val_loss: 0.0948
Epoch 28/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0752 - val_loss: 0.0946
Epoch 29/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0751 - val_loss: 0.0944
Epoch 30/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0749 - val_loss: 0.0942
Epoch 31/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0747 - val_loss: 0.0941
Epoch 32/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0746 - val_loss: 0.0939
Epoch 33/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0744 - val_loss: 0.0937
Epoch 34/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0743 - val_loss: 0.0935
Epoch 35/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0741 - val_loss: 0.0934
Epoch 36/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0739 - val_loss: 0.0932
Epoch 37/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0737 - val_loss: 0.0930
Epoch 38/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0928
Epoch 39/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0927
Epoch 40/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0925
Epoch 41/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0923
Epoch 42/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0921
Epoch 43/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0919
Epoch 44/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0917
Epoch 45/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0916
Epoch 46/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0914
Epoch 47/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0912
Epoch 48/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0910
Epoch 49/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0908
Epoch 50/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0906
Epoch 51/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0905
Epoch 52/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0712 - val_loss: 0.0903
Execution time:  17.163371324539185
DNN:
Mean Absolute Error: 0.0735
Root Mean Square Error: 0.0812
Mean Square Error: 0.0066

Train RMSE: 0.081
Train MSE: 0.007
Train MAE: 0.074
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_168&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_504 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_505 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_168 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_506 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
112/112 [==============================] - 1s 9ms/step - loss: 0.1612 - val_loss: 0.0158
Epoch 2/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0997 - val_loss: 0.0225
Epoch 3/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0848 - val_loss: 0.0130
Epoch 4/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0716 - val_loss: 0.0087
Epoch 5/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0636 - val_loss: 0.0097
Epoch 6/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0579 - val_loss: 0.0071
Epoch 7/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0517 - val_loss: 0.0060
Epoch 8/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0472 - val_loss: 0.0065
Epoch 9/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0437 - val_loss: 0.0072
Epoch 10/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0407 - val_loss: 0.0080
Epoch 11/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.0090
Epoch 12/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0358 - val_loss: 0.0118
Epoch 13/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0342 - val_loss: 0.0136
Epoch 14/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0329 - val_loss: 0.0139
Epoch 15/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0320 - val_loss: 0.0113
Epoch 16/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0306 - val_loss: 0.0098
Epoch 17/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0103
Epoch 18/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0291 - val_loss: 0.0099
Epoch 19/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0097
Epoch 20/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0097
Epoch 21/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0101
Epoch 22/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0109
Epoch 23/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0112
Epoch 24/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0119
Epoch 25/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0122
Epoch 26/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0129
Epoch 27/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0133
Epoch 28/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0139
Epoch 29/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0137
Epoch 30/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0124
Epoch 31/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0133
Epoch 32/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0139
Epoch 33/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0140
Epoch 34/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0141
Epoch 35/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0127
Epoch 36/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0141
Epoch 37/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0128
Epoch 38/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0132
Epoch 39/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0123
Epoch 40/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0139
Epoch 41/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0127
Epoch 42/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0129
Epoch 43/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0130
Epoch 44/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0136
Epoch 45/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0126
Epoch 46/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0133
Epoch 47/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0136
Epoch 48/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0118
Epoch 49/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0130
Epoch 50/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0131
Epoch 51/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0121
Epoch 52/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0125
Epoch 53/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0128
Epoch 54/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0122
Epoch 55/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0121
Epoch 56/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0126
Epoch 57/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0122
Epoch 58/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0123
Epoch 59/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0121
Epoch 60/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0125
Epoch 61/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0125
Epoch 62/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0108
Epoch 63/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0124
Epoch 64/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0118
Epoch 65/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0113
Epoch 66/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0119
Epoch 67/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0116
Epoch 68/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0106
Epoch 69/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0118
Epoch 70/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0112
Epoch 71/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0103
Epoch 72/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0114
Epoch 73/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0112
Epoch 74/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0104
Epoch 75/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0112
Epoch 76/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0108
Epoch 77/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0106
Epoch 78/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0115
Epoch 79/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.0101
Epoch 80/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0115
Execution time:  60.94595503807068
DNN:
Mean Absolute Error: 0.0242
Root Mean Square Error: 0.0416
Mean Square Error: 0.0017

Train RMSE: 0.042
Train MSE: 0.002
Train MAE: 0.024
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_169&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_507 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_508 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_169 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_509 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
139/139 [==============================] - 1s 7ms/step - loss: 0.0973 - val_loss: 0.0099
Epoch 2/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.0084
Epoch 3/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0511 - val_loss: 0.0066
Epoch 4/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0489 - val_loss: 0.0058
Epoch 5/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0470 - val_loss: 0.0065
Epoch 6/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0453 - val_loss: 0.0069
Epoch 7/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0437 - val_loss: 0.0078
Epoch 8/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0421 - val_loss: 0.0083
Epoch 9/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0410 - val_loss: 0.0096
Epoch 10/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0397 - val_loss: 0.0098
Epoch 11/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0387 - val_loss: 0.0097
Epoch 12/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0377 - val_loss: 0.0099
Epoch 13/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0369 - val_loss: 0.0120
Epoch 14/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0363 - val_loss: 0.0133
Epoch 15/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0356 - val_loss: 0.0130
Epoch 16/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0349 - val_loss: 0.0143
Epoch 17/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0343 - val_loss: 0.0154
Epoch 18/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0339 - val_loss: 0.0166
Epoch 19/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0334 - val_loss: 0.0170
Epoch 20/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0330 - val_loss: 0.0174
Epoch 21/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0326 - val_loss: 0.0172
Epoch 22/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0321 - val_loss: 0.0172
Epoch 23/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0317 - val_loss: 0.0167
Epoch 24/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0313 - val_loss: 0.0157
Epoch 25/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0310 - val_loss: 0.0164
Epoch 26/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0306 - val_loss: 0.0160
Epoch 27/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0304 - val_loss: 0.0162
Epoch 28/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0159
Epoch 29/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 0.0156
Epoch 30/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0297 - val_loss: 0.0154
Epoch 31/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0158
Epoch 32/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0291 - val_loss: 0.0154
Epoch 33/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0289 - val_loss: 0.0150
Epoch 34/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0149
Epoch 35/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0147
Epoch 36/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0134
Epoch 37/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0133
Epoch 38/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0128
Epoch 39/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0130
Epoch 40/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0128
Epoch 41/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0124
Epoch 42/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0123
Epoch 43/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0122
Epoch 44/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0113
Epoch 45/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0109
Epoch 46/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0110
Epoch 47/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0105
Epoch 48/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0102
Epoch 49/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0105
Epoch 50/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0108
Epoch 51/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0112
Epoch 52/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0116
Epoch 53/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0280 - val_loss: 0.0107
Epoch 54/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0108
Epoch 55/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0108
Epoch 56/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0118
Epoch 57/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0113
Epoch 58/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0109
Epoch 59/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0120
Epoch 60/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0126
Epoch 61/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0112
Epoch 62/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0121
Epoch 63/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0125
Epoch 64/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0118
Epoch 65/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0123
Epoch 66/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0119
Epoch 67/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0121
Epoch 68/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0118
Epoch 69/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0120
Epoch 70/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0118
Epoch 71/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0119
Epoch 72/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0117
Epoch 73/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0118
Epoch 74/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0119
Epoch 75/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0118
Epoch 76/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0115
Epoch 77/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0114
Epoch 78/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0116
Epoch 79/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0117
Epoch 80/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0114
Epoch 81/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0112
Epoch 82/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0113
Epoch 83/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0114
Epoch 84/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0113
Epoch 85/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0113
Epoch 86/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0108
Epoch 87/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0107
Epoch 88/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0108
Epoch 89/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0107
Epoch 90/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0106
Execution time:  69.50578308105469
DNN:
Mean Absolute Error: 0.0258
Root Mean Square Error: 0.0455
Mean Square Error: 0.0021

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_170&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_510 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_511 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_170 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_512 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
61/61 [==============================] - 1s 8ms/step - loss: 0.2658 - val_loss: 0.0907
Epoch 2/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1204 - val_loss: 0.0144
Epoch 3/52
61/61 [==============================] - 0s 5ms/step - loss: 0.1073 - val_loss: 0.0065
Epoch 4/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0978 - val_loss: 0.0064
Epoch 5/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0893 - val_loss: 0.0068
Epoch 6/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0824 - val_loss: 0.0093
Epoch 7/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0761 - val_loss: 0.0084
Epoch 8/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0704 - val_loss: 0.0070
Epoch 9/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0648 - val_loss: 0.0057
Epoch 10/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0586 - val_loss: 0.0071
Epoch 11/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0523 - val_loss: 0.0084
Epoch 12/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0477 - val_loss: 0.0079
Epoch 13/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0444 - val_loss: 0.0077
Epoch 14/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0417 - val_loss: 0.0079
Epoch 15/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0392 - val_loss: 0.0087
Epoch 16/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0368 - val_loss: 0.0093
Epoch 17/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0347 - val_loss: 0.0101
Epoch 18/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0329 - val_loss: 0.0107
Epoch 19/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0313 - val_loss: 0.0106
Epoch 20/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0302 - val_loss: 0.0102
Epoch 21/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0294 - val_loss: 0.0102
Epoch 22/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0287 - val_loss: 0.0099
Epoch 23/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0097
Epoch 24/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0279 - val_loss: 0.0096
Epoch 25/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0095
Epoch 26/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0096
Epoch 27/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0100
Epoch 28/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0108
Epoch 29/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0107
Epoch 30/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0120
Epoch 31/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0132
Epoch 32/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0126
Epoch 33/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0145
Epoch 34/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0153
Epoch 35/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0151
Epoch 36/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0156
Epoch 37/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0157
Epoch 38/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0161
Epoch 39/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0265 - val_loss: 0.0158
Epoch 40/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0265 - val_loss: 0.0150
Epoch 41/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0158
Epoch 42/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0153
Epoch 43/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0162
Epoch 44/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0164
Epoch 45/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0155
Epoch 46/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0174
Epoch 47/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0262 - val_loss: 0.0158
Epoch 48/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0169
Epoch 49/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0161
Epoch 50/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0171
Epoch 51/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0166
Epoch 52/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0164
Execution time:  17.956063508987427
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0420
Mean Square Error: 0.0018

Train RMSE: 0.042
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_171&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_513 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_514 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_171 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_515 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
100/100 [==============================] - 1s 12ms/step - loss: 0.1146 - val_loss: 0.0327
Epoch 2/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0890 - val_loss: 0.0416
Epoch 3/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0827 - val_loss: 0.0386
Epoch 4/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0773 - val_loss: 0.0370
Epoch 5/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0713 - val_loss: 0.0342
Epoch 6/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.0324
Epoch 7/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0604 - val_loss: 0.0298
Epoch 8/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.0278
Epoch 9/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0499 - val_loss: 0.0266
Epoch 10/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0460 - val_loss: 0.0246
Epoch 11/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0238
Epoch 12/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.0224
Epoch 13/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0380 - val_loss: 0.0210
Epoch 14/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0195
Epoch 15/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0185
Epoch 16/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0180
Epoch 17/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0181
Epoch 18/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.0171
Epoch 19/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0169
Epoch 20/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0313 - val_loss: 0.0166
Epoch 21/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0302 - val_loss: 0.0164
Epoch 22/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0300 - val_loss: 0.0162
Epoch 23/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0163
Epoch 24/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0164
Epoch 25/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0286 - val_loss: 0.0182
Epoch 26/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0187
Epoch 27/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0186
Epoch 28/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0189
Epoch 29/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0189
Epoch 30/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0190
Epoch 31/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 32/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 33/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0191
Epoch 34/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 35/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0190
Epoch 36/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 37/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0190
Epoch 38/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 39/80
100/100 [==============================] - 1s 8ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 40/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 41/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 42/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 43/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 44/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 45/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 46/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0191
Epoch 47/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0191
Epoch 48/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0191
Epoch 49/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0190
Epoch 50/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0191
Epoch 51/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0189
Epoch 52/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0191
Epoch 53/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0191
Epoch 54/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0188
Epoch 55/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0162
Epoch 56/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0186
Epoch 57/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0190
Epoch 58/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0189
Epoch 59/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0188
Epoch 60/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0188
Epoch 61/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0188
Epoch 62/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0188
Epoch 63/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0187
Epoch 64/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0187
Epoch 65/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0186
Epoch 66/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0186
Epoch 67/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0185
Epoch 68/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0187
Epoch 69/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0185
Epoch 70/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0186
Epoch 71/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0186
Epoch 72/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0187
Epoch 73/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0182
Epoch 74/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0187
Epoch 75/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0187
Epoch 76/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0183
Epoch 77/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0186
Epoch 78/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0186
Epoch 79/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0182
Epoch 80/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0188
Execution time:  57.25297522544861
DNN:
Mean Absolute Error: 0.0257
Root Mean Square Error: 0.0429
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_172&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_516 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_517 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_172 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_518 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
124/124 [==============================] - 1s 8ms/step - loss: 0.1098 - val_loss: 0.0359
Epoch 2/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0523 - val_loss: 0.0351
Epoch 3/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0505 - val_loss: 0.0387
Epoch 4/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0491 - val_loss: 0.0374
Epoch 5/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0479 - val_loss: 0.0354
Epoch 6/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0467 - val_loss: 0.0327
Epoch 7/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0454 - val_loss: 0.0321
Epoch 8/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0442 - val_loss: 0.0298
Epoch 9/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0430 - val_loss: 0.0298
Epoch 10/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0417 - val_loss: 0.0283
Epoch 11/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0406 - val_loss: 0.0273
Epoch 12/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0399 - val_loss: 0.0269
Epoch 13/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0390 - val_loss: 0.0254
Epoch 14/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0381 - val_loss: 0.0243
Epoch 15/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0373 - val_loss: 0.0235
Epoch 16/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0368 - val_loss: 0.0218
Epoch 17/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0360 - val_loss: 0.0213
Epoch 18/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0354 - val_loss: 0.0201
Epoch 19/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0350 - val_loss: 0.0197
Epoch 20/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0345 - val_loss: 0.0203
Epoch 21/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0343 - val_loss: 0.0194
Epoch 22/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0339 - val_loss: 0.0180
Epoch 23/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0333 - val_loss: 0.0177
Epoch 24/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0329 - val_loss: 0.0179
Epoch 25/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0327 - val_loss: 0.0174
Epoch 26/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0317 - val_loss: 0.0170
Epoch 27/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0315 - val_loss: 0.0170
Epoch 28/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0314 - val_loss: 0.0172
Epoch 29/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0312 - val_loss: 0.0170
Epoch 30/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0310 - val_loss: 0.0170
Epoch 31/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0307 - val_loss: 0.0169
Epoch 32/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0316 - val_loss: 0.0172
Epoch 33/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0312 - val_loss: 0.0171
Epoch 34/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0309 - val_loss: 0.0169
Epoch 35/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0168
Epoch 36/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0305 - val_loss: 0.0168
Epoch 37/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0304 - val_loss: 0.0169
Epoch 38/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0166
Epoch 39/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.0166
Epoch 40/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0169
Epoch 41/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0168
Epoch 42/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0306 - val_loss: 0.0169
Epoch 43/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.0169
Epoch 44/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0168
Epoch 45/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0170
Epoch 46/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 0.0167
Epoch 47/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0300 - val_loss: 0.0167
Epoch 48/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0300 - val_loss: 0.0165
Epoch 49/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 0.0165
Epoch 50/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.0165
Epoch 51/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.0165
Epoch 52/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0297 - val_loss: 0.0165
Epoch 53/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0297 - val_loss: 0.0165
Epoch 54/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0296 - val_loss: 0.0165
Epoch 55/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0295 - val_loss: 0.0165
Epoch 56/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0164
Epoch 57/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0295 - val_loss: 0.0163
Epoch 58/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0293 - val_loss: 0.0163
Epoch 59/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0290 - val_loss: 0.0163
Epoch 60/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0290 - val_loss: 0.0169
Epoch 61/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0181
Epoch 62/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0164
Epoch 63/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0179
Epoch 64/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0165
Epoch 65/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0179
Epoch 66/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0164
Epoch 67/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0179
Epoch 68/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0164
Epoch 69/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 70/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0174
Epoch 71/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0181
Epoch 72/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0170
Epoch 73/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0178
Epoch 74/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0172
Epoch 75/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0177
Epoch 76/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0177
Epoch 77/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0180
Epoch 78/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0179
Epoch 79/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0176
Epoch 80/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0177
Epoch 81/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0177
Epoch 82/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0178
Epoch 83/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0177
Epoch 84/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0178
Epoch 85/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0178
Epoch 86/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0178
Epoch 87/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0178
Epoch 88/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0177
Epoch 89/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0177
Epoch 90/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0177
Execution time:  65.03902173042297
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0446
Mean Square Error: 0.0020

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_173&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_519 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_520 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_173 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_521 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
54/54 [==============================] - 0s 9ms/step - loss: 0.1762 - val_loss: 0.0299
Epoch 2/52
54/54 [==============================] - 0s 5ms/step - loss: 0.1153 - val_loss: 0.0256
Epoch 3/52
54/54 [==============================] - 0s 6ms/step - loss: 0.1065 - val_loss: 0.0297
Epoch 4/52
54/54 [==============================] - 0s 5ms/step - loss: 0.1004 - val_loss: 0.0350
Epoch 5/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0958 - val_loss: 0.0326
Epoch 6/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0908 - val_loss: 0.0312
Epoch 7/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.0284
Epoch 8/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0811 - val_loss: 0.0256
Epoch 9/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0759 - val_loss: 0.0221
Epoch 10/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0704 - val_loss: 0.0200
Epoch 11/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0656 - val_loss: 0.0181
Epoch 12/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0609 - val_loss: 0.0165
Epoch 13/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0565 - val_loss: 0.0160
Epoch 14/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0523 - val_loss: 0.0156
Epoch 15/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0484 - val_loss: 0.0155
Epoch 16/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0447 - val_loss: 0.0156
Epoch 17/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0415 - val_loss: 0.0160
Epoch 18/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0385 - val_loss: 0.0166
Epoch 19/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0357 - val_loss: 0.0160
Epoch 20/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0337 - val_loss: 0.0158
Epoch 21/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0324 - val_loss: 0.0160
Epoch 22/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0312 - val_loss: 0.0157
Epoch 23/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0304 - val_loss: 0.0155
Epoch 24/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.0156
Epoch 25/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0295 - val_loss: 0.0158
Epoch 26/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0288 - val_loss: 0.0158
Epoch 27/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.0159
Epoch 28/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0279 - val_loss: 0.0169
Epoch 29/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0181
Epoch 30/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0185
Epoch 31/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0188
Epoch 32/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0188
Epoch 33/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0192
Epoch 34/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0192
Epoch 35/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 36/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 37/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0194
Epoch 38/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0194
Epoch 39/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0194
Epoch 40/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0193
Epoch 41/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0193
Epoch 42/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0194
Epoch 43/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0194
Epoch 44/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0193
Epoch 45/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0194
Epoch 46/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0192
Epoch 47/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 48/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0195
Epoch 49/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0268 - val_loss: 0.0192
Epoch 50/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0191
Epoch 51/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 52/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0270 - val_loss: 0.0193
Execution time:  17.174506664276123
DNN:
Mean Absolute Error: 0.0231
Root Mean Square Error: 0.0375
Mean Square Error: 0.0014

Train RMSE: 0.038
Train MSE: 0.001
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_174&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_522 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_523 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_174 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_524 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
112/112 [==============================] - 1s 8ms/step - loss: 0.0608 - val_loss: 0.0495
Epoch 2/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0477 - val_loss: 0.0439
Epoch 3/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0430 - val_loss: 0.0401
Epoch 4/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0404 - val_loss: 0.0368
Epoch 5/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0384 - val_loss: 0.0338
Epoch 6/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0368 - val_loss: 0.0318
Epoch 7/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0352 - val_loss: 0.0296
Epoch 8/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0339 - val_loss: 0.0277
Epoch 9/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0328 - val_loss: 0.0261
Epoch 10/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0318 - val_loss: 0.0237
Epoch 11/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0310 - val_loss: 0.0226
Epoch 12/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.0215
Epoch 13/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0296 - val_loss: 0.0194
Epoch 14/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0292 - val_loss: 0.0183
Epoch 15/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0178
Epoch 16/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0165
Epoch 17/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0168
Epoch 18/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0169
Epoch 19/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0164
Epoch 20/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0189
Epoch 21/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0186
Epoch 22/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0183
Epoch 23/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0181
Epoch 24/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0265 - val_loss: 0.0190
Epoch 25/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0195
Epoch 26/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 27/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0188
Epoch 28/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 29/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 30/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 31/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 32/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 33/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 34/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 35/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 36/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 37/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 38/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 39/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 40/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 41/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 42/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 43/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 44/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 45/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 46/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 47/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 48/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 49/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 50/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 51/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 52/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 53/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 54/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 55/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 56/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 57/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 58/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 59/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 60/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 61/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 62/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 63/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 64/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 65/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 66/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 67/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 68/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 69/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 70/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 71/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 72/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 73/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Epoch 74/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0188
Epoch 75/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 76/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 77/80
112/112 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0188
Epoch 78/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 79/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 80/80
112/112 [==============================] - 1s 6ms/step - loss: 0.0263 - val_loss: 0.0185
Execution time:  60.222731590270996
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0435
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_175&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_525 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_526 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_175 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_527 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0475 - val_loss: 0.0222
Epoch 2/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0339 - val_loss: 0.0206
Epoch 3/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0337 - val_loss: 0.0203
Epoch 4/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0333 - val_loss: 0.0202
Epoch 5/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0330 - val_loss: 0.0199
Epoch 6/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0327 - val_loss: 0.0195
Epoch 7/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.0191
Epoch 8/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0320 - val_loss: 0.0193
Epoch 9/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0317 - val_loss: 0.0192
Epoch 10/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0315 - val_loss: 0.0193
Epoch 11/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0313 - val_loss: 0.0178
Epoch 12/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0311 - val_loss: 0.0176
Epoch 13/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0179
Epoch 14/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0305 - val_loss: 0.0176
Epoch 15/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0304 - val_loss: 0.0175
Epoch 16/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.0175
Epoch 17/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0300 - val_loss: 0.0176
Epoch 18/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0297 - val_loss: 0.0174
Epoch 19/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0297 - val_loss: 0.0162
Epoch 20/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0296 - val_loss: 0.0168
Epoch 21/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0298 - val_loss: 0.0170
Epoch 22/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0161
Epoch 23/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0291 - val_loss: 0.0161
Epoch 24/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0290 - val_loss: 0.0150
Epoch 25/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0290 - val_loss: 0.0149
Epoch 26/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0153
Epoch 27/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0153
Epoch 28/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0152
Epoch 29/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.0142
Epoch 30/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0141
Epoch 31/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0283 - val_loss: 0.0142
Epoch 32/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0138
Epoch 33/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0144
Epoch 34/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0283 - val_loss: 0.0136
Epoch 35/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.0140
Epoch 36/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0138
Epoch 37/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0142
Epoch 38/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0277 - val_loss: 0.0142
Epoch 39/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0142
Epoch 40/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0138
Epoch 41/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0278 - val_loss: 0.0132
Epoch 42/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0129
Epoch 43/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0134
Epoch 44/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0135
Epoch 45/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0135
Epoch 46/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0135
Epoch 47/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0271 - val_loss: 0.0134
Epoch 48/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0271 - val_loss: 0.0133
Epoch 49/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0271 - val_loss: 0.0132
Epoch 50/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0132
Epoch 51/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0132
Epoch 52/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0270 - val_loss: 0.0130
Epoch 53/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0129
Epoch 54/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0130
Epoch 55/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0129
Epoch 56/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0128
Epoch 57/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0269 - val_loss: 0.0128
Epoch 58/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0128
Epoch 59/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0127
Epoch 60/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0127
Epoch 61/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0127
Epoch 62/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0126
Epoch 63/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0268 - val_loss: 0.0127
Epoch 64/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0126
Epoch 65/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0125
Epoch 66/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0267 - val_loss: 0.0126
Epoch 67/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0267 - val_loss: 0.0125
Epoch 68/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0267 - val_loss: 0.0126
Epoch 69/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0125
Epoch 70/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.0127
Epoch 71/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0127
Epoch 72/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0126
Epoch 73/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0126
Epoch 74/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0126
Epoch 75/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0126
Epoch 76/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0126
Epoch 77/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0127
Epoch 78/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0126
Epoch 79/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0123
Epoch 80/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0123
Epoch 81/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0123
Epoch 82/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0122
Epoch 83/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0265 - val_loss: 0.0123
Epoch 84/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0122
Epoch 85/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0122
Epoch 86/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0121
Epoch 87/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.0121
Epoch 88/90
139/139 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0120
Epoch 89/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0263 - val_loss: 0.0119
Epoch 90/90
139/139 [==============================] - 1s 5ms/step - loss: 0.0263 - val_loss: 0.0119
Execution time:  68.08814096450806
DNN:
Mean Absolute Error: 0.0246
Root Mean Square Error: 0.0427
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_176&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_528 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_529 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_176 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_530 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
61/61 [==============================] - 0s 8ms/step - loss: 0.0631 - val_loss: 0.0585
Epoch 2/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0455 - val_loss: 0.0478
Epoch 3/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0432 - val_loss: 0.0440
Epoch 4/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0412 - val_loss: 0.0414
Epoch 5/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0396 - val_loss: 0.0392
Epoch 6/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0384 - val_loss: 0.0375
Epoch 7/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0374 - val_loss: 0.0362
Epoch 8/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0365 - val_loss: 0.0356
Epoch 9/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0358 - val_loss: 0.0345
Epoch 10/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0352 - val_loss: 0.0338
Epoch 11/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0346 - val_loss: 0.0330
Epoch 12/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0340 - val_loss: 0.0323
Epoch 13/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0335 - val_loss: 0.0315
Epoch 14/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0331 - val_loss: 0.0306
Epoch 15/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0327 - val_loss: 0.0299
Epoch 16/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0323 - val_loss: 0.0292
Epoch 17/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0320 - val_loss: 0.0287
Epoch 18/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0317 - val_loss: 0.0281
Epoch 19/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.0276
Epoch 20/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0312 - val_loss: 0.0271
Epoch 21/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0309 - val_loss: 0.0267
Epoch 22/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0306 - val_loss: 0.0264
Epoch 23/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0304 - val_loss: 0.0259
Epoch 24/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0302 - val_loss: 0.0256
Epoch 25/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0300 - val_loss: 0.0251
Epoch 26/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0297 - val_loss: 0.0247
Epoch 27/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0295 - val_loss: 0.0244
Epoch 28/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0294 - val_loss: 0.0241
Epoch 29/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0292 - val_loss: 0.0238
Epoch 30/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0290 - val_loss: 0.0232
Epoch 31/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0287 - val_loss: 0.0224
Epoch 32/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0285 - val_loss: 0.0222
Epoch 33/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0284 - val_loss: 0.0219
Epoch 34/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.0217
Epoch 35/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0215
Epoch 36/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0280 - val_loss: 0.0213
Epoch 37/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0280 - val_loss: 0.0212
Epoch 38/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0279 - val_loss: 0.0210
Epoch 39/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0278 - val_loss: 0.0209
Epoch 40/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0277 - val_loss: 0.0206
Epoch 41/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0204
Epoch 42/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0275 - val_loss: 0.0202
Epoch 43/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0201
Epoch 44/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0200
Epoch 45/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0198
Epoch 46/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0197
Epoch 47/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0196
Epoch 48/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0196
Epoch 49/52
61/61 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.0195
Epoch 50/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0194
Epoch 51/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 52/52
61/61 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0192
Execution time:  18.359009265899658
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0430
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  1d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_177&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_531 (Dense)            (None, 144, 87)           174       
_________________________________________________________________
dense_532 (Dense)            (None, 144, 16)           1408      
_________________________________________________________________
dropout_177 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_533 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
100/100 [==============================] - 1s 8ms/step - loss: 0.0473 - val_loss: 0.0298
Epoch 2/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0429 - val_loss: 0.0270
Epoch 3/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0260
Epoch 4/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0252
Epoch 5/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0398 - val_loss: 0.0244
Epoch 6/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0236
Epoch 7/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.0230
Epoch 8/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0222
Epoch 9/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.0219
Epoch 10/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0214
Epoch 11/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0211
Epoch 12/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0209
Epoch 13/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0341 - val_loss: 0.0203
Epoch 14/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.0201
Epoch 15/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0200
Epoch 16/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0197
Epoch 17/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.0197
Epoch 18/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0320 - val_loss: 0.0195
Epoch 19/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0314 - val_loss: 0.0194
Epoch 20/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0192
Epoch 21/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0191
Epoch 22/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0304 - val_loss: 0.0190
Epoch 23/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.0188
Epoch 24/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0187
Epoch 25/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0296 - val_loss: 0.0187
Epoch 26/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.0188
Epoch 27/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0292 - val_loss: 0.0186
Epoch 28/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0290 - val_loss: 0.0186
Epoch 29/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0185
Epoch 30/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.0184
Epoch 31/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0183
Epoch 32/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0183
Epoch 33/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0184
Epoch 34/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0182
Epoch 35/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0182
Epoch 36/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0181
Epoch 37/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0181
Epoch 38/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0179
Epoch 39/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0179
Epoch 40/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0179
Epoch 41/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0176
Epoch 42/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0177
Epoch 43/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0178
Epoch 44/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0176
Epoch 45/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0174
Epoch 46/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0175
Epoch 47/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0175
Epoch 48/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0175
Epoch 49/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0175
Epoch 50/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0175
Epoch 51/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0174
Epoch 52/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0175
Epoch 53/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0175
Epoch 54/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0174
Epoch 55/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0173
Epoch 56/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0173
Epoch 57/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0174
Epoch 58/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0176
Epoch 59/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0173
Epoch 60/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0173
Epoch 61/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0173
Epoch 62/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0173
Epoch 63/80
100/100 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0173
Epoch 64/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.0172
Epoch 65/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.0171
Epoch 66/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.0171
Epoch 67/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0170
Epoch 68/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.0170
Epoch 69/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.0170
Epoch 70/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0171
Epoch 71/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0172
Epoch 72/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0172
Epoch 73/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0266 - val_loss: 0.0171
Epoch 74/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0171
Epoch 75/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0170
Epoch 76/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0266 - val_loss: 0.0170
Epoch 77/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0266 - val_loss: 0.0171
Epoch 78/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0266 - val_loss: 0.0172
Epoch 79/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0266 - val_loss: 0.0172
Epoch 80/80
100/100 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0172
Execution time:  56.93525433540344
DNN:
Mean Absolute Error: 0.0233
Root Mean Square Error: 0.0407
Mean Square Error: 0.0017

Train RMSE: 0.041
Train MSE: 0.002
Train MAE: 0.023
###########################

MODEL:  DNN
sequence:  1d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_178&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_534 (Dense)            (None, 144, 80)           160       
_________________________________________________________________
dense_535 (Dense)            (None, 144, 16)           1296      
_________________________________________________________________
dropout_178 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_536 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
124/124 [==============================] - 1s 8ms/step - loss: 0.0557 - val_loss: 0.0199
Epoch 2/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0343 - val_loss: 0.0187
Epoch 3/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0341 - val_loss: 0.0197
Epoch 4/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0336 - val_loss: 0.0193
Epoch 5/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0335 - val_loss: 0.0195
Epoch 6/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0332 - val_loss: 0.0193
Epoch 7/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0330 - val_loss: 0.0192
Epoch 8/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0328 - val_loss: 0.0189
Epoch 9/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0327 - val_loss: 0.0188
Epoch 10/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0325 - val_loss: 0.0188
Epoch 11/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.0188
Epoch 12/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0320 - val_loss: 0.0189
Epoch 13/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0318 - val_loss: 0.0188
Epoch 14/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0316 - val_loss: 0.0186
Epoch 15/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0315 - val_loss: 0.0187
Epoch 16/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0313 - val_loss: 0.0187
Epoch 17/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0311 - val_loss: 0.0186
Epoch 18/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0309 - val_loss: 0.0186
Epoch 19/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0307 - val_loss: 0.0186
Epoch 20/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0307 - val_loss: 0.0187
Epoch 21/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0305 - val_loss: 0.0186
Epoch 22/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0187
Epoch 23/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0188
Epoch 24/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0188
Epoch 25/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0300 - val_loss: 0.0187
Epoch 26/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.0187
Epoch 27/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0297 - val_loss: 0.0188
Epoch 28/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0296 - val_loss: 0.0188
Epoch 29/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0295 - val_loss: 0.0188
Epoch 30/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0188
Epoch 31/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0293 - val_loss: 0.0189
Epoch 32/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0190
Epoch 33/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0291 - val_loss: 0.0190
Epoch 34/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0290 - val_loss: 0.0189
Epoch 35/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0289 - val_loss: 0.0189
Epoch 36/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0189
Epoch 37/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0183
Epoch 38/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0190
Epoch 39/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0190
Epoch 40/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0190
Epoch 41/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0190
Epoch 42/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0191
Epoch 43/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0191
Epoch 44/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0190
Epoch 45/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0190
Epoch 46/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0190
Epoch 47/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0191
Epoch 48/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0192
Epoch 49/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0190
Epoch 50/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0191
Epoch 51/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0177
Epoch 52/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0192
Epoch 53/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0191
Epoch 54/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0190
Epoch 55/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0191
Epoch 56/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0190
Epoch 57/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0190
Epoch 58/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0190
Epoch 59/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0190
Epoch 60/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0190
Epoch 61/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0190
Epoch 62/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0190
Epoch 63/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0276 - val_loss: 0.0190
Epoch 64/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0198
Epoch 65/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0189
Epoch 66/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0275 - val_loss: 0.0189
Epoch 67/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0173
Epoch 68/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0191
Epoch 69/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0189
Epoch 70/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0189
Epoch 71/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0189
Epoch 72/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 73/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 74/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 75/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 76/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 77/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 78/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 79/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 80/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0274 - val_loss: 0.0188
Epoch 81/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0189
Epoch 82/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0188
Epoch 83/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0188
Epoch 84/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0189
Epoch 85/90
124/124 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0189
Epoch 86/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0189
Epoch 87/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0188
Epoch 88/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0273 - val_loss: 0.0189
Epoch 89/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0188
Epoch 90/90
124/124 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.0188
Execution time:  64.6579475402832
DNN:
Mean Absolute Error: 0.0255
Root Mean Square Error: 0.0430
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  1d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_179&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_537 (Dense)            (None, 144, 12)           24        
_________________________________________________________________
dense_538 (Dense)            (None, 144, 16)           208       
_________________________________________________________________
dropout_179 (Dropout)        (None, 144, 16)           0         
_________________________________________________________________
dense_539 (Dense)            (None, 144, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
54/54 [==============================] - 0s 9ms/step - loss: 0.0776 - val_loss: 0.0737
Epoch 2/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0468 - val_loss: 0.0455
Epoch 3/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0413 - val_loss: 0.0383
Epoch 4/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0404 - val_loss: 0.0366
Epoch 5/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0395 - val_loss: 0.0352
Epoch 6/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0386 - val_loss: 0.0343
Epoch 7/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0379 - val_loss: 0.0334
Epoch 8/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0372 - val_loss: 0.0327
Epoch 9/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0366 - val_loss: 0.0319
Epoch 10/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0361 - val_loss: 0.0315
Epoch 11/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0355 - val_loss: 0.0308
Epoch 12/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0350 - val_loss: 0.0304
Epoch 13/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0344 - val_loss: 0.0299
Epoch 14/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.0295
Epoch 15/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0336 - val_loss: 0.0290
Epoch 16/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0333 - val_loss: 0.0285
Epoch 17/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0328 - val_loss: 0.0279
Epoch 18/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0274
Epoch 19/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0322 - val_loss: 0.0270
Epoch 20/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0319 - val_loss: 0.0268
Epoch 21/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0316 - val_loss: 0.0265
Epoch 22/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0312 - val_loss: 0.0262
Epoch 23/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0310 - val_loss: 0.0262
Epoch 24/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0308 - val_loss: 0.0261
Epoch 25/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0305 - val_loss: 0.0259
Epoch 26/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0303 - val_loss: 0.0258
Epoch 27/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0300 - val_loss: 0.0256
Epoch 28/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0298 - val_loss: 0.0255
Epoch 29/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0296 - val_loss: 0.0251
Epoch 30/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0294 - val_loss: 0.0247
Epoch 31/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0293 - val_loss: 0.0244
Epoch 32/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0291 - val_loss: 0.0240
Epoch 33/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0290 - val_loss: 0.0236
Epoch 34/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0289 - val_loss: 0.0233
Epoch 35/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0287 - val_loss: 0.0230
Epoch 36/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0286 - val_loss: 0.0228
Epoch 37/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0285 - val_loss: 0.0226
Epoch 38/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0284 - val_loss: 0.0224
Epoch 39/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0283 - val_loss: 0.0221
Epoch 40/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0219
Epoch 41/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0282 - val_loss: 0.0218
Epoch 42/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0281 - val_loss: 0.0215
Epoch 43/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0280 - val_loss: 0.0214
Epoch 44/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0279 - val_loss: 0.0212
Epoch 45/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0279 - val_loss: 0.0210
Epoch 46/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0278 - val_loss: 0.0209
Epoch 47/52
54/54 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0208
Epoch 48/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0277 - val_loss: 0.0207
Epoch 49/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0277 - val_loss: 0.0206
Epoch 50/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0205
Epoch 51/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0205
Epoch 52/52
54/54 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0204
Execution time:  17.237717866897583
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0427
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_180&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_540 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_541 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_180 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_542 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
104/104 [==============================] - 2s 17ms/step - loss: 0.1940 - val_loss: 0.0587
Epoch 2/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1232 - val_loss: 0.0281
Epoch 3/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0963 - val_loss: 0.0069
Epoch 4/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0682 - val_loss: 0.0060
Epoch 5/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0467 - val_loss: 0.0113
Epoch 6/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0345 - val_loss: 0.0114
Epoch 7/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0285 - val_loss: 0.0098
Epoch 8/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0267 - val_loss: 0.0111
Epoch 9/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0266 - val_loss: 0.0136
Epoch 10/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0263 - val_loss: 0.0134
Epoch 11/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0263 - val_loss: 0.0136
Epoch 12/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0193
Epoch 13/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0127
Epoch 14/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0263 - val_loss: 0.0139
Epoch 15/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0262 - val_loss: 0.0139
Epoch 16/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0139
Epoch 17/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0140
Epoch 18/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0140
Epoch 19/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0141
Epoch 20/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0141
Epoch 21/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0141
Epoch 22/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0141
Epoch 23/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0139
Epoch 24/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 25/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 26/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 27/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 28/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0141
Epoch 29/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 30/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0141
Epoch 31/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 32/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 33/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 34/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 35/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 36/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 37/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 38/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 39/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 40/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 41/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 42/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 43/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 44/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 45/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 46/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 47/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 48/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 49/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 50/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 51/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 52/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 53/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 54/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 55/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 56/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 57/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 58/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 59/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 60/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 61/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 62/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 63/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 64/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 65/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 66/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 67/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 68/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 69/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 70/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 71/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 72/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 73/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 74/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 75/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 76/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 77/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 78/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 79/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Epoch 80/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - val_loss: 0.0142
Execution time:  129.49278450012207
DNN:
Mean Absolute Error: 0.0253
Root Mean Square Error: 0.0445
Mean Square Error: 0.0020

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_181&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_543 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_544 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_181 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_545 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
129/129 [==============================] - 2s 13ms/step - loss: 0.1104 - val_loss: 0.0205
Epoch 2/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0562 - val_loss: 0.0148
Epoch 3/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0530 - val_loss: 0.0099
Epoch 4/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0502 - val_loss: 0.0070
Epoch 5/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0473 - val_loss: 0.0073
Epoch 6/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0444 - val_loss: 0.0067
Epoch 7/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0400 - val_loss: 0.0072
Epoch 8/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0386 - val_loss: 0.0118
Epoch 9/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0375 - val_loss: 0.0141
Epoch 10/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0362 - val_loss: 0.0162
Epoch 11/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0352 - val_loss: 0.0175
Epoch 12/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0348 - val_loss: 0.0199
Epoch 13/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0336 - val_loss: 0.0183
Epoch 14/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0328 - val_loss: 0.0177
Epoch 15/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0320 - val_loss: 0.0176
Epoch 16/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0314 - val_loss: 0.0158
Epoch 17/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0307 - val_loss: 0.0157
Epoch 18/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0300 - val_loss: 0.0153
Epoch 19/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0295 - val_loss: 0.0154
Epoch 20/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0292 - val_loss: 0.0150
Epoch 21/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0288 - val_loss: 0.0162
Epoch 22/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0286 - val_loss: 0.0128
Epoch 23/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.0122
Epoch 24/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0282 - val_loss: 0.0124
Epoch 25/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0280 - val_loss: 0.0110
Epoch 26/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0279 - val_loss: 0.0108
Epoch 27/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0277 - val_loss: 0.0109
Epoch 28/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0275 - val_loss: 0.0108
Epoch 29/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0274 - val_loss: 0.0105
Epoch 30/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0273 - val_loss: 0.0105
Epoch 31/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0272 - val_loss: 0.0107
Epoch 32/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0271 - val_loss: 0.0107
Epoch 33/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0270 - val_loss: 0.0109
Epoch 34/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0107
Epoch 35/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0269 - val_loss: 0.0106
Epoch 36/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0269 - val_loss: 0.0104
Epoch 37/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0268 - val_loss: 0.0103
Epoch 38/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0268 - val_loss: 0.0103
Epoch 39/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0267 - val_loss: 0.0106
Epoch 40/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0266 - val_loss: 0.0118
Epoch 41/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0266 - val_loss: 0.0119
Epoch 42/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0265 - val_loss: 0.0122
Epoch 43/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0260 - val_loss: 0.0194
Epoch 44/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0110
Epoch 45/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0265 - val_loss: 0.0124
Epoch 46/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0124
Epoch 47/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0124
Epoch 48/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0125
Epoch 49/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0125
Epoch 50/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0264 - val_loss: 0.0125
Epoch 51/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0122
Epoch 52/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 53/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 54/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 55/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 56/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 57/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 58/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 59/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 60/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 61/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0124
Epoch 62/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0124
Epoch 63/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0125
Epoch 64/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0125
Epoch 65/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0122
Epoch 66/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 67/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0122
Epoch 68/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 69/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 70/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 71/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 72/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 73/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 74/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 75/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 76/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 77/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0122
Epoch 78/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 79/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0122
Epoch 80/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 81/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 82/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 83/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 84/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0126
Epoch 85/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 86/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 87/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 88/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0264 - val_loss: 0.0127
Epoch 89/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0123
Epoch 90/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0127
Execution time:  140.69767260551453
DNN:
Mean Absolute Error: 0.0255
Root Mean Square Error: 0.0454
Mean Square Error: 0.0021

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_182&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_546 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_547 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_182 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_548 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
57/57 [==============================] - 1s 17ms/step - loss: 0.1662 - val_loss: 0.0131
Epoch 2/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1020 - val_loss: 0.0078
Epoch 3/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0833 - val_loss: 0.0090
Epoch 4/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0729 - val_loss: 0.0082
Epoch 5/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0668 - val_loss: 0.0081
Epoch 6/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0621 - val_loss: 0.0074
Epoch 7/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0577 - val_loss: 0.0068
Epoch 8/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0535 - val_loss: 0.0069
Epoch 9/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0495 - val_loss: 0.0070
Epoch 10/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0459 - val_loss: 0.0071
Epoch 11/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0426 - val_loss: 0.0071
Epoch 12/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0398 - val_loss: 0.0073
Epoch 13/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0374 - val_loss: 0.0075
Epoch 14/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0352 - val_loss: 0.0077
Epoch 15/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0333 - val_loss: 0.0079
Epoch 16/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0319 - val_loss: 0.0081
Epoch 17/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0307 - val_loss: 0.0082
Epoch 18/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0295 - val_loss: 0.0085
Epoch 19/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0288 - val_loss: 0.0085
Epoch 20/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0283 - val_loss: 0.0091
Epoch 21/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0276 - val_loss: 0.0097
Epoch 22/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0273 - val_loss: 0.0105
Epoch 23/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0271 - val_loss: 0.0116
Epoch 24/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0269 - val_loss: 0.0120
Epoch 25/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0267 - val_loss: 0.0127
Epoch 26/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0265 - val_loss: 0.0133
Epoch 27/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0264 - val_loss: 0.0137
Epoch 28/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0263 - val_loss: 0.0144
Epoch 29/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0262 - val_loss: 0.0149
Epoch 30/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0261 - val_loss: 0.0151
Epoch 31/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0260 - val_loss: 0.0153
Epoch 32/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0259 - val_loss: 0.0156
Epoch 33/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0259 - val_loss: 0.0159
Epoch 34/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0258 - val_loss: 0.0161
Epoch 35/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0258 - val_loss: 0.0163
Epoch 36/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0258 - val_loss: 0.0166
Epoch 37/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0257 - val_loss: 0.0171
Epoch 38/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0256 - val_loss: 0.0172
Epoch 39/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0256 - val_loss: 0.0172
Epoch 40/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0255 - val_loss: 0.0173
Epoch 41/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0255 - val_loss: 0.0174
Epoch 42/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0255 - val_loss: 0.0174
Epoch 43/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0255 - val_loss: 0.0174
Epoch 44/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0175
Epoch 45/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0174
Epoch 46/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0174
Epoch 47/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0175
Epoch 48/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0254 - val_loss: 0.0174
Epoch 49/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0176
Epoch 50/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0253 - val_loss: 0.0173
Epoch 51/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0176
Epoch 52/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0176
Execution time:  44.26756954193115
DNN:
Mean Absolute Error: 0.0250
Root Mean Square Error: 0.0427
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_183&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_549 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_550 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_183 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_551 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
92/92 [==============================] - 2s 19ms/step - loss: 0.1448 - val_loss: 0.0446
Epoch 2/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0882 - val_loss: 0.0149
Epoch 3/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0745 - val_loss: 0.0124
Epoch 4/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0649 - val_loss: 0.0119
Epoch 5/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0556 - val_loss: 0.0119
Epoch 6/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0478 - val_loss: 0.0122
Epoch 7/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0415 - val_loss: 0.0116
Epoch 8/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0371 - val_loss: 0.0122
Epoch 9/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0354 - val_loss: 0.0131
Epoch 10/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0353 - val_loss: 0.0127
Epoch 11/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0342 - val_loss: 0.0116
Epoch 12/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0322 - val_loss: 0.0109
Epoch 13/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0301 - val_loss: 0.0109
Epoch 14/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0294 - val_loss: 0.0112
Epoch 15/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0291 - val_loss: 0.0118
Epoch 16/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0288 - val_loss: 0.0126
Epoch 17/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0286 - val_loss: 0.0128
Epoch 18/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0284 - val_loss: 0.0130
Epoch 19/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0284 - val_loss: 0.0132
Epoch 20/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0283 - val_loss: 0.0133
Epoch 21/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0282 - val_loss: 0.0134
Epoch 22/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0282 - val_loss: 0.0134
Epoch 23/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0281 - val_loss: 0.0136
Epoch 24/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0281 - val_loss: 0.0135
Epoch 25/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 26/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 27/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 28/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 29/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 30/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 31/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 32/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 33/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 34/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0280 - val_loss: 0.0137
Epoch 35/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0280 - val_loss: 0.0137
Epoch 36/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0137
Epoch 37/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0137
Epoch 38/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0136
Epoch 39/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0136
Epoch 40/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 41/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 42/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 43/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 44/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0280 - val_loss: 0.0137
Epoch 45/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 46/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 47/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 48/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 49/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 50/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 51/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 52/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 53/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 54/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 55/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 56/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 57/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 58/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 59/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 60/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 61/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 62/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 63/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 64/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 65/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 66/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 67/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 68/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 69/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 70/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 71/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 72/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 73/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 74/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 75/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 76/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 77/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 78/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 79/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Epoch 80/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0137
Execution time:  121.11843204498291
DNN:
Mean Absolute Error: 0.0252
Root Mean Square Error: 0.0436
Mean Square Error: 0.0019

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_184&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_552 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_553 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_184 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_554 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
114/114 [==============================] - 2s 14ms/step - loss: 0.1489 - val_loss: 0.0193
Epoch 2/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0543 - val_loss: 0.0196
Epoch 3/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0160
Epoch 4/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0498 - val_loss: 0.0168
Epoch 5/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0475 - val_loss: 0.0148
Epoch 6/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0453 - val_loss: 0.0137
Epoch 7/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0435 - val_loss: 0.0127
Epoch 8/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0417 - val_loss: 0.0119
Epoch 9/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0404 - val_loss: 0.0124
Epoch 10/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0393 - val_loss: 0.0134
Epoch 11/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0386 - val_loss: 0.0142
Epoch 12/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0378 - val_loss: 0.0144
Epoch 13/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0370 - val_loss: 0.0113
Epoch 14/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0351 - val_loss: 0.0126
Epoch 15/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0355 - val_loss: 0.0112
Epoch 16/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0343 - val_loss: 0.0119
Epoch 17/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0338 - val_loss: 0.0119
Epoch 18/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0333 - val_loss: 0.0115
Epoch 19/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0325 - val_loss: 0.0114
Epoch 20/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0319 - val_loss: 0.0112
Epoch 21/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0314 - val_loss: 0.0111
Epoch 22/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0309 - val_loss: 0.0111
Epoch 23/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0306 - val_loss: 0.0109
Epoch 24/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0302 - val_loss: 0.0109
Epoch 25/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0300 - val_loss: 0.0109
Epoch 26/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0298 - val_loss: 0.0110
Epoch 27/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0295 - val_loss: 0.0109
Epoch 28/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0294 - val_loss: 0.0109
Epoch 29/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0293 - val_loss: 0.0109
Epoch 30/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0292 - val_loss: 0.0108
Epoch 31/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0288 - val_loss: 0.0109
Epoch 32/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0291 - val_loss: 0.0109
Epoch 33/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0290 - val_loss: 0.0109
Epoch 34/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0289 - val_loss: 0.0110
Epoch 35/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0288 - val_loss: 0.0112
Epoch 36/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0288 - val_loss: 0.0114
Epoch 37/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0287 - val_loss: 0.0116
Epoch 38/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0287 - val_loss: 0.0117
Epoch 39/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0287 - val_loss: 0.0119
Epoch 40/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0286 - val_loss: 0.0120
Epoch 41/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0286 - val_loss: 0.0121
Epoch 42/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0286 - val_loss: 0.0122
Epoch 43/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0285 - val_loss: 0.0122
Epoch 44/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0175
Epoch 45/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0299 - val_loss: 0.0122
Epoch 46/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0286 - val_loss: 0.0111
Epoch 47/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0282 - val_loss: 0.0118
Epoch 48/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0286 - val_loss: 0.0125
Epoch 49/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0124
Epoch 50/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0283 - val_loss: 0.0125
Epoch 51/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0283 - val_loss: 0.0125
Epoch 52/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 53/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0125
Epoch 54/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 55/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 56/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 57/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 58/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 59/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 60/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 61/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 62/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 63/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 64/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 65/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 66/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 67/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 68/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 69/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 70/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 71/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 72/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 73/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 74/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 75/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 76/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 77/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 78/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 79/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 80/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0125
Epoch 81/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 82/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.0127
Epoch 83/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0127
Epoch 84/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0284 - val_loss: 0.0128
Epoch 85/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0283 - val_loss: 0.0127
Epoch 86/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0126
Epoch 87/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.0127
Epoch 88/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0127
Epoch 89/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.0128
Epoch 90/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0127
Execution time:  129.84763097763062
DNN:
Mean Absolute Error: 0.0253
Root Mean Square Error: 0.0445
Mean Square Error: 0.0020

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_185&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_555 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_556 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_185 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_557 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
50/50 [==============================] - 1s 24ms/step - loss: 0.2455 - val_loss: 0.1232
Epoch 2/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1460 - val_loss: 0.0556
Epoch 3/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1189 - val_loss: 0.0169
Epoch 4/52
50/50 [==============================] - 1s 14ms/step - loss: 0.0968 - val_loss: 0.0226
Epoch 5/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0876 - val_loss: 0.0177
Epoch 6/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0776 - val_loss: 0.0114
Epoch 7/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0699 - val_loss: 0.0113
Epoch 8/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0643 - val_loss: 0.0113
Epoch 9/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0590 - val_loss: 0.0112
Epoch 10/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0540 - val_loss: 0.0111
Epoch 11/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0505 - val_loss: 0.0109
Epoch 12/52
50/50 [==============================] - 1s 14ms/step - loss: 0.0457 - val_loss: 0.0112
Epoch 13/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0422 - val_loss: 0.0109
Epoch 14/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0395 - val_loss: 0.0109
Epoch 15/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0372 - val_loss: 0.0112
Epoch 16/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0352 - val_loss: 0.0116
Epoch 17/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0336 - val_loss: 0.0119
Epoch 18/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0322 - val_loss: 0.0121
Epoch 19/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0310 - val_loss: 0.0135
Epoch 20/52
50/50 [==============================] - 1s 14ms/step - loss: 0.0297 - val_loss: 0.0136
Epoch 21/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0290 - val_loss: 0.0137
Epoch 22/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0283 - val_loss: 0.0136
Epoch 23/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0281 - val_loss: 0.0137
Epoch 24/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0280 - val_loss: 0.0138
Epoch 25/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0280 - val_loss: 0.0140
Epoch 26/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0279 - val_loss: 0.0141
Epoch 27/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0278 - val_loss: 0.0142
Epoch 28/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0143
Epoch 29/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0144
Epoch 30/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0276 - val_loss: 0.0146
Epoch 31/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0275 - val_loss: 0.0147
Epoch 32/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0275 - val_loss: 0.0147
Epoch 33/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0274 - val_loss: 0.0147
Epoch 34/52
50/50 [==============================] - 1s 14ms/step - loss: 0.0274 - val_loss: 0.0148
Epoch 35/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0273 - val_loss: 0.0150
Epoch 36/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0273 - val_loss: 0.0150
Epoch 37/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0273 - val_loss: 0.0150
Epoch 38/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0273 - val_loss: 0.0151
Epoch 39/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0273 - val_loss: 0.0142
Epoch 40/52
50/50 [==============================] - 1s 17ms/step - loss: 0.0274 - val_loss: 0.0149
Epoch 41/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 42/52
50/50 [==============================] - 1s 17ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 43/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 44/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0152
Epoch 45/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 46/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 47/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0272 - val_loss: 0.0150
Epoch 48/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0152
Epoch 49/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 50/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 51/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0152
Epoch 52/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0271 - val_loss: 0.0150
Execution time:  41.80355381965637
DNN:
Mean Absolute Error: 0.0248
Root Mean Square Error: 0.0420
Mean Square Error: 0.0018

Train RMSE: 0.042
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_186&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_558 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_559 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_186 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_560 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
104/104 [==============================] - 2s 17ms/step - loss: 0.0571 - val_loss: 0.0343
Epoch 2/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0414 - val_loss: 0.0257
Epoch 3/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0391 - val_loss: 0.0246
Epoch 4/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0379 - val_loss: 0.0231
Epoch 5/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0362 - val_loss: 0.0218
Epoch 6/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0347 - val_loss: 0.0211
Epoch 7/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0334 - val_loss: 0.0209
Epoch 8/80
104/104 [==============================] - 2s 17ms/step - loss: 0.0323 - val_loss: 0.0201
Epoch 9/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0313 - val_loss: 0.0195
Epoch 10/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0304 - val_loss: 0.0190
Epoch 11/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0296 - val_loss: 0.0183
Epoch 12/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0289 - val_loss: 0.0179
Epoch 13/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0284 - val_loss: 0.0175
Epoch 14/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0279 - val_loss: 0.0172
Epoch 15/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0273 - val_loss: 0.0229
Epoch 16/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0268 - val_loss: 0.0182
Epoch 17/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0268 - val_loss: 0.0166
Epoch 18/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0267 - val_loss: 0.0163
Epoch 19/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0265 - val_loss: 0.0164
Epoch 20/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0263 - val_loss: 0.0165
Epoch 21/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0166
Epoch 22/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0260 - val_loss: 0.0166
Epoch 23/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0165
Epoch 24/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0165
Epoch 25/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0257 - val_loss: 0.0164
Epoch 26/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0256 - val_loss: 0.0163
Epoch 27/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0256 - val_loss: 0.0163
Epoch 28/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0255 - val_loss: 0.0165
Epoch 29/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0255 - val_loss: 0.0173
Epoch 30/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0256 - val_loss: 0.0182
Epoch 31/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0173
Epoch 32/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0172
Epoch 33/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0172- l
Epoch 34/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0173
Epoch 35/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0177
Epoch 36/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0177
Epoch 37/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0171
Epoch 38/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0251 - val_loss: 0.0181
Epoch 39/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0168
Epoch 40/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0180
Epoch 41/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0179
Epoch 42/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 43/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 44/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 45/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 46/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 47/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 48/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 49/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 50/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 51/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 52/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 53/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 54/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 55/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 56/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 57/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 58/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 59/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 60/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 61/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 62/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 63/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 64/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 65/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 66/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 67/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 68/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 69/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 70/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 71/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 72/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Epoch 73/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 74/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 75/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 76/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 77/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 78/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 79/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 80/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0183
Execution time:  130.35565376281738
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0431
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_187&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_561 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_562 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_187 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_563 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0476 - val_loss: 0.0262
Epoch 2/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0330 - val_loss: 0.0303
Epoch 3/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0338 - val_loss: 0.0239
Epoch 4/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0323 - val_loss: 0.0284
Epoch 5/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0325 - val_loss: 0.0218
Epoch 6/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0316 - val_loss: 0.0195
Epoch 7/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0312 - val_loss: 0.0196
Epoch 8/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0307 - val_loss: 0.0193
Epoch 9/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0304 - val_loss: 0.0172
Epoch 10/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0300 - val_loss: 0.0173
Epoch 11/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0296 - val_loss: 0.0272
Epoch 12/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0305 - val_loss: 0.0173
Epoch 13/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0291 - val_loss: 0.0163
Epoch 14/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0289 - val_loss: 0.0163
Epoch 15/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0286 - val_loss: 0.0152
Epoch 16/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0284 - val_loss: 0.0152
Epoch 17/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0282 - val_loss: 0.0141
Epoch 18/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0283 - val_loss: 0.0143
Epoch 19/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0281 - val_loss: 0.0143
Epoch 20/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0277 - val_loss: 0.0138
Epoch 21/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0131
Epoch 22/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0274 - val_loss: 0.0137
Epoch 23/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0272 - val_loss: 0.0138
Epoch 24/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0270 - val_loss: 0.0141
Epoch 25/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0143
Epoch 26/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0268 - val_loss: 0.0145
Epoch 27/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0267 - val_loss: 0.0143
Epoch 28/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0266 - val_loss: 0.0143
Epoch 29/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0265 - val_loss: 0.0141
Epoch 30/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0265 - val_loss: 0.0140
Epoch 31/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0261 - val_loss: 0.0167
Epoch 32/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0265 - val_loss: 0.0143
Epoch 33/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0138
Epoch 34/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0140
Epoch 35/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0141
Epoch 36/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0262 - val_loss: 0.0145
Epoch 37/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0262 - val_loss: 0.0146
Epoch 38/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0147
Epoch 39/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0148
Epoch 40/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0148
Epoch 41/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0260 - val_loss: 0.0148
Epoch 42/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0260 - val_loss: 0.0149
Epoch 43/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0260 - val_loss: 0.0151
Epoch 44/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 45/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 46/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0259 - val_loss: 0.0153
Epoch 47/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0258 - val_loss: 0.0154
Epoch 48/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0258 - val_loss: 0.0154
Epoch 49/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0258 - val_loss: 0.0155
Epoch 50/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0258 - val_loss: 0.0156
Epoch 51/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0257 - val_loss: 0.0156
Epoch 52/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0257 - val_loss: 0.0157
Epoch 53/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 54/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 55/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0257 - val_loss: 0.0159
Epoch 56/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0256 - val_loss: 0.0158
Epoch 57/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0256 - val_loss: 0.0160
Epoch 58/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0256 - val_loss: 0.0159
Epoch 59/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0256 - val_loss: 0.0160
Epoch 60/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0256 - val_loss: 0.0161
Epoch 61/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0160
Epoch 62/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0162
Epoch 63/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0162
Epoch 64/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0163
Epoch 65/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0163
Epoch 66/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0163
Epoch 67/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0163
Epoch 68/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0254 - val_loss: 0.0166A: 0s - loss:
Epoch 69/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0163
Epoch 70/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0166
Epoch 71/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0253 - val_loss: 0.0164
Epoch 72/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0167
Epoch 73/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0253 - val_loss: 0.0166
Epoch 74/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0253 - val_loss: 0.0168
Epoch 75/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0253 - val_loss: 0.0169
Epoch 76/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0253 - val_loss: 0.0170
Epoch 77/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0253 - val_loss: 0.0171
Epoch 78/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0253 - val_loss: 0.0172
Epoch 79/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0172
Epoch 80/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0172
Epoch 81/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0173
Epoch 82/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0252 - val_loss: 0.0175
Epoch 83/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0252 - val_loss: 0.0174
Epoch 84/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0252 - val_loss: 0.0176
Epoch 85/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0251 - val_loss: 0.0176
Epoch 86/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0251 - val_loss: 0.0177
Epoch 87/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0177
Epoch 88/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0177
Epoch 89/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0178
Epoch 90/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0177
Execution time:  140.09447407722473
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0434
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_188&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_564 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_565 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_188 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_566 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
57/57 [==============================] - 1s 17ms/step - loss: 0.1008 - val_loss: 0.0754
Epoch 2/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0518 - val_loss: 0.0480
Epoch 3/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0445 - val_loss: 0.0395
Epoch 4/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0409 - val_loss: 0.0379
Epoch 5/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0386 - val_loss: 0.0356
Epoch 6/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0369 - val_loss: 0.0327
Epoch 7/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0356 - val_loss: 0.0311
Epoch 8/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0345 - val_loss: 0.0298
Epoch 9/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0335 - val_loss: 0.0290
Epoch 10/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0327 - val_loss: 0.0278
Epoch 11/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0320 - val_loss: 0.0267
Epoch 12/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0314 - val_loss: 0.0258
Epoch 13/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0308 - val_loss: 0.0249
Epoch 14/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0303 - val_loss: 0.0242
Epoch 15/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0298 - val_loss: 0.0235
Epoch 16/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0293 - val_loss: 0.0228
Epoch 17/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0289 - val_loss: 0.0222
Epoch 18/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0285 - val_loss: 0.0218
Epoch 19/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0282 - val_loss: 0.0216
Epoch 20/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0279 - val_loss: 0.0213
Epoch 21/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0276 - val_loss: 0.0207
Epoch 22/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0274 - val_loss: 0.0201
Epoch 23/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0272 - val_loss: 0.0197
Epoch 24/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 25/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0268 - val_loss: 0.0191
Epoch 26/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0267 - val_loss: 0.0189
Epoch 27/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0265 - val_loss: 0.0187
Epoch 28/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0264 - val_loss: 0.0187
Epoch 29/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0263 - val_loss: 0.0186
Epoch 30/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0262 - val_loss: 0.0184
Epoch 31/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0261 - val_loss: 0.0182
Epoch 32/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0260 - val_loss: 0.0180
Epoch 33/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0259 - val_loss: 0.0181
Epoch 34/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0258 - val_loss: 0.0190
Epoch 35/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0255 - val_loss: 0.0184
Epoch 36/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0255 - val_loss: 0.0183
Epoch 37/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0255 - val_loss: 0.0182
Epoch 38/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0182
Epoch 39/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0182
Epoch 40/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0182
Epoch 41/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0253 - val_loss: 0.0189
Epoch 42/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0251 - val_loss: 0.0182
Epoch 43/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0251 - val_loss: 0.0180
Epoch 44/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0251 - val_loss: 0.0181
Epoch 45/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0251 - val_loss: 0.0183
Epoch 46/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 47/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0250 - val_loss: 0.0185
Epoch 48/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0250 - val_loss: 0.0185
Epoch 49/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0250 - val_loss: 0.0186
Epoch 50/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0250 - val_loss: 0.0187
Epoch 51/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0250 - val_loss: 0.0187
Epoch 52/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0249 - val_loss: 0.0188
Execution time:  44.15416979789734
DNN:
Mean Absolute Error: 0.0249
Root Mean Square Error: 0.0423
Mean Square Error: 0.0018

Train RMSE: 0.042
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_189&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_567 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_568 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_189 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_569 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0630 - val_loss: 0.0376
Epoch 2/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0460 - val_loss: 0.0336
Epoch 3/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0432 - val_loss: 0.0309
Epoch 4/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0403 - val_loss: 0.0131
Epoch 5/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0402 - val_loss: 0.0264
Epoch 6/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0367 - val_loss: 0.0240
Epoch 7/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0351 - val_loss: 0.0221
Epoch 8/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0337 - val_loss: 0.0206
Epoch 9/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0326 - val_loss: 0.0192
Epoch 10/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0316 - val_loss: 0.0182
Epoch 11/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0309 - val_loss: 0.0173
Epoch 12/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0302 - val_loss: 0.0166
Epoch 13/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0297 - val_loss: 0.0161
Epoch 14/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0292 - val_loss: 0.0158
Epoch 15/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0288 - val_loss: 0.0155
Epoch 16/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0285 - val_loss: 0.0153
Epoch 17/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0119
Epoch 18/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0286 - val_loss: 0.0151
Epoch 19/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0278 - val_loss: 0.0148
Epoch 20/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0146
Epoch 21/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0276 - val_loss: 0.0144
Epoch 22/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0275 - val_loss: 0.0143
Epoch 23/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0274 - val_loss: 0.0143
Epoch 24/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0274 - val_loss: 0.0144
Epoch 25/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 26/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 27/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0273 - val_loss: 0.0150
Epoch 28/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0269 - val_loss: 0.0147
Epoch 29/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0269 - val_loss: 0.0146
Epoch 30/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0269 - val_loss: 0.0147
Epoch 31/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0268 - val_loss: 0.0146
Epoch 32/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0269 - val_loss: 0.0149
Epoch 33/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 34/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 35/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 36/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 37/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 38/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 39/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 40/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 41/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 42/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 43/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 44/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 45/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 46/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0269 - val_loss: 0.0148
Epoch 47/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 48/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 49/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 50/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 51/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 52/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 53/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 54/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 55/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 56/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 57/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 58/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 59/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 60/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 61/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 62/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 63/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 64/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 65/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0144
Epoch 66/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0148
Epoch 67/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0138
Epoch 68/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0271 - val_loss: 0.0148
Epoch 69/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 70/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 71/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 72/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 73/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 74/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 75/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 76/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 77/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 78/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 79/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 80/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Execution time:  121.10552501678467
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0429
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_190&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_570 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_571 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_190 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_572 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
114/114 [==============================] - 2s 17ms/step - loss: 0.0681 - val_loss: 0.0141
Epoch 2/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0378 - val_loss: 0.0133
Epoch 3/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0349 - val_loss: 0.0132
Epoch 4/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0365 - val_loss: 0.0133
Epoch 5/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0337 - val_loss: 0.0133
Epoch 6/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0333 - val_loss: 0.0135
Epoch 7/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0328 - val_loss: 0.0136
Epoch 8/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0325 - val_loss: 0.0133
Epoch 9/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0322 - val_loss: 0.0135
Epoch 10/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0319 - val_loss: 0.0135
Epoch 11/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0316 - val_loss: 0.0135
Epoch 12/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0313 - val_loss: 0.0136
Epoch 13/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0311 - val_loss: 0.0137
Epoch 14/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0308 - val_loss: 0.0137
Epoch 15/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0305 - val_loss: 0.0138
Epoch 16/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0303 - val_loss: 0.0139
Epoch 17/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0301 - val_loss: 0.0139
Epoch 18/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0299 - val_loss: 0.0140
Epoch 19/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0297 - val_loss: 0.0140
Epoch 20/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0296 - val_loss: 0.0140
Epoch 21/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0294 - val_loss: 0.0141
Epoch 22/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0293 - val_loss: 0.0141
Epoch 23/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0292 - val_loss: 0.0141
Epoch 24/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0290 - val_loss: 0.0142
Epoch 25/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0289 - val_loss: 0.0142
Epoch 26/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0288 - val_loss: 0.0142
Epoch 27/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0287 - val_loss: 0.0143
Epoch 28/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0286 - val_loss: 0.0144
Epoch 29/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0285 - val_loss: 0.0144
Epoch 30/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.0143
Epoch 31/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0144
Epoch 32/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0143
Epoch 33/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0281 - val_loss: 0.0144
Epoch 34/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0281 - val_loss: 0.0144
Epoch 35/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0144
Epoch 36/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0279 - val_loss: 0.0145
Epoch 37/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0279 - val_loss: 0.0145
Epoch 38/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0278 - val_loss: 0.0144
Epoch 39/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0278 - val_loss: 0.0144
Epoch 40/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0277 - val_loss: 0.0144
Epoch 41/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0277 - val_loss: 0.0144
Epoch 42/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0144
Epoch 43/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0144
Epoch 44/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0144
Epoch 45/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0144
Epoch 46/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0144
Epoch 47/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0274 - val_loss: 0.0144
Epoch 48/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0274 - val_loss: 0.0144
Epoch 49/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0274 - val_loss: 0.0144
Epoch 50/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0273 - val_loss: 0.0144
Epoch 51/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0273 - val_loss: 0.0144
Epoch 52/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0273 - val_loss: 0.0144
Epoch 53/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 54/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 55/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 56/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 57/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0271 - val_loss: 0.0144
Epoch 58/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0271 - val_loss: 0.0144
Epoch 59/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0271 - val_loss: 0.0144
Epoch 60/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0271 - val_loss: 0.0144
Epoch 61/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0271 - val_loss: 0.0144
Epoch 62/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0271 - val_loss: 0.0144
Epoch 63/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 64/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 65/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 66/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 67/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 68/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 69/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 70/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 71/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 72/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0270 - val_loss: 0.0144
Epoch 73/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 74/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 75/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 76/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 77/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 78/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 79/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 80/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 81/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 82/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0269 - val_loss: 0.0146
Epoch 83/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0146
Epoch 84/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 85/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0146
Epoch 86/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 87/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 88/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0269 - val_loss: 0.0146
Epoch 89/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0260 - val_loss: 0.0111
Epoch 90/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0154
Execution time:  130.6944079399109
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0425
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_191&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_573 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_574 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_191 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_575 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
50/50 [==============================] - 1s 18ms/step - loss: 0.0968 - val_loss: 0.0642
Epoch 2/52
50/50 [==============================] - 1s 17ms/step - loss: 0.0512 - val_loss: 0.0350
Epoch 3/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0449 - val_loss: 0.0332
Epoch 4/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0412 - val_loss: 0.0317
Epoch 5/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0396 - val_loss: 0.0308
Epoch 6/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0385 - val_loss: 0.0301
Epoch 7/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0375 - val_loss: 0.0290
Epoch 8/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0367 - val_loss: 0.0280
Epoch 9/52
50/50 [==============================] - 1s 14ms/step - loss: 0.0360 - val_loss: 0.0272
Epoch 10/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0353 - val_loss: 0.0261
Epoch 11/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0346 - val_loss: 0.0252
Epoch 12/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0340 - val_loss: 0.0245
Epoch 13/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0335 - val_loss: 0.0236
Epoch 14/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0330 - val_loss: 0.0230
Epoch 15/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0326 - val_loss: 0.0224
Epoch 16/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0321 - val_loss: 0.0218
Epoch 17/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0317 - val_loss: 0.0213
Epoch 18/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0314 - val_loss: 0.0207
Epoch 19/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0310 - val_loss: 0.0202
Epoch 20/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0307 - val_loss: 0.0197
Epoch 21/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0304 - val_loss: 0.0193
Epoch 22/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0301 - val_loss: 0.0188
Epoch 23/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0299 - val_loss: 0.0185
Epoch 24/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0296 - val_loss: 0.0182
Epoch 25/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0295 - val_loss: 0.0181
Epoch 26/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0292 - val_loss: 0.0179
Epoch 27/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0291 - val_loss: 0.0177
Epoch 28/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0289 - val_loss: 0.0174
Epoch 29/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0287 - val_loss: 0.0171
Epoch 30/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0284 - val_loss: 0.0140
Epoch 31/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0291 - val_loss: 0.0165
Epoch 32/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0284 - val_loss: 0.0160
Epoch 33/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0283 - val_loss: 0.0158
Epoch 34/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0282 - val_loss: 0.0156
Epoch 35/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0281 - val_loss: 0.0155
Epoch 36/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0280 - val_loss: 0.0153
Epoch 37/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0279 - val_loss: 0.0152
Epoch 38/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0278 - val_loss: 0.0152
Epoch 39/52
50/50 [==============================] - 1s 14ms/step - loss: 0.0278 - val_loss: 0.0152
Epoch 40/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0277 - val_loss: 0.0151
Epoch 41/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0277 - val_loss: 0.0151
Epoch 42/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0276 - val_loss: 0.0151
Epoch 43/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0276 - val_loss: 0.0151
Epoch 44/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0276 - val_loss: 0.0151
Epoch 45/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0275 - val_loss: 0.0151
Epoch 46/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0275 - val_loss: 0.0151
Epoch 47/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0274 - val_loss: 0.0150
Epoch 48/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0274 - val_loss: 0.0151
Epoch 49/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0274 - val_loss: 0.0151
Epoch 50/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0273 - val_loss: 0.0150
Epoch 51/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0273 - val_loss: 0.0151
Epoch 52/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0272 - val_loss: 0.0150
Execution time:  41.05609941482544
DNN:
Mean Absolute Error: 0.0246
Root Mean Square Error: 0.0416
Mean Square Error: 0.0017

Train RMSE: 0.042
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_192&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_576 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_577 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_192 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_578 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
104/104 [==============================] - 2s 17ms/step - loss: 0.3433 - val_loss: 0.3136
Epoch 2/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3415 - val_loss: 0.3117
Epoch 3/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3394 - val_loss: 0.3097
Epoch 4/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3374 - val_loss: 0.3075
Epoch 5/80
104/104 [==============================] - 2s 16ms/step - loss: 0.3351 - val_loss: 0.3052
Epoch 6/80
104/104 [==============================] - 2s 16ms/step - loss: 0.3325 - val_loss: 0.3027
Epoch 7/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3300 - val_loss: 0.3002
Epoch 8/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3273 - val_loss: 0.2975
Epoch 9/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3246 - val_loss: 0.2947
Epoch 10/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3216 - val_loss: 0.2918
Epoch 11/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3186 - val_loss: 0.2887
Epoch 12/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3154 - val_loss: 0.2856
Epoch 13/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3122 - val_loss: 0.2824
Epoch 14/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3088 - val_loss: 0.2790
Epoch 15/80
104/104 [==============================] - 2s 15ms/step - loss: 0.3054 - val_loss: 0.2756
Epoch 16/80
104/104 [==============================] - 2s 16ms/step - loss: 0.3019 - val_loss: 0.2720
Epoch 17/80
104/104 [==============================] - 2s 16ms/step - loss: 0.2981 - val_loss: 0.2684
Epoch 18/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2944 - val_loss: 0.2647
Epoch 19/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2905 - val_loss: 0.2599
Epoch 20/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2839 - val_loss: 0.2523
Epoch 21/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2761 - val_loss: 0.2447
Epoch 22/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2683 - val_loss: 0.2369
Epoch 23/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2604 - val_loss: 0.2291
Epoch 24/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2523 - val_loss: 0.2210
Epoch 25/80
104/104 [==============================] - 2s 16ms/step - loss: 0.2441 - val_loss: 0.2129
Epoch 26/80
104/104 [==============================] - 2s 16ms/step - loss: 0.2360 - val_loss: 0.2048
Epoch 27/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2277 - val_loss: 0.1964
Epoch 28/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2192 - val_loss: 0.1878
Epoch 29/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2105 - val_loss: 0.1791
Epoch 30/80
104/104 [==============================] - 2s 15ms/step - loss: 0.2016 - val_loss: 0.1701
Epoch 31/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1926 - val_loss: 0.1609
Epoch 32/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1835 - val_loss: 0.1516
Epoch 33/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1746 - val_loss: 0.1423
Epoch 34/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1664 - val_loss: 0.1334
Epoch 35/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1595 - val_loss: 0.1255
Epoch 36/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1541 - val_loss: 0.1184
Epoch 37/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1497 - val_loss: 0.1124
Epoch 38/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1466 - val_loss: 0.1069
Epoch 39/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1432 - val_loss: 0.1004
Epoch 40/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1397 - val_loss: 0.0932
Epoch 41/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1360 - val_loss: 0.0866
Epoch 42/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1326 - val_loss: 0.0803
Epoch 43/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1293 - val_loss: 0.0742
Epoch 44/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1262 - val_loss: 0.0685
Epoch 45/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1235 - val_loss: 0.0629
Epoch 46/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1210 - val_loss: 0.0577
Epoch 47/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1189 - val_loss: 0.0529
Epoch 48/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1170 - val_loss: 0.0486
Epoch 49/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1156 - val_loss: 0.0452
Epoch 50/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1147 - val_loss: 0.0423
Epoch 51/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1139 - val_loss: 0.0399
Epoch 52/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1134 - val_loss: 0.0380
Epoch 53/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1129 - val_loss: 0.0364
Epoch 54/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1126 - val_loss: 0.0350
Epoch 55/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1122 - val_loss: 0.0338
Epoch 56/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1119 - val_loss: 0.0328
Epoch 57/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1117 - val_loss: 0.0319
Epoch 58/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1115 - val_loss: 0.0311
Epoch 59/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1115 - val_loss: 0.0303
Epoch 60/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1112 - val_loss: 0.0296
Epoch 61/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1111 - val_loss: 0.0290
Epoch 62/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1111 - val_loss: 0.0285
Epoch 63/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1109 - val_loss: 0.0279
Epoch 64/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1107 - val_loss: 0.0274
Epoch 65/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1106 - val_loss: 0.0270
Epoch 66/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1104 - val_loss: 0.0266
Epoch 67/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1104 - val_loss: 0.0263
Epoch 68/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1105 - val_loss: 0.0260
Epoch 69/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1102 - val_loss: 0.0257
Epoch 70/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1102 - val_loss: 0.0254
Epoch 71/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1100 - val_loss: 0.0251
Epoch 72/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1100 - val_loss: 0.0249
Epoch 73/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1099 - val_loss: 0.0246
Epoch 74/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1099 - val_loss: 0.0244
Epoch 75/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1098 - val_loss: 0.0241
Epoch 76/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1096 - val_loss: 0.0239
Epoch 77/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1095 - val_loss: 0.0237
Epoch 78/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1095 - val_loss: 0.0236
Epoch 79/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1092 - val_loss: 0.0234
Epoch 80/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1093 - val_loss: 0.0232
Execution time:  130.25696182250977
DNN:
Mean Absolute Error: 0.0462
Root Mean Square Error: 0.0528
Mean Square Error: 0.0028

Train RMSE: 0.053
Train MSE: 0.003
Train MAE: 0.046
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_193&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_579 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_580 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_193 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_581 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
129/129 [==============================] - 2s 13ms/step - loss: 0.4363 - val_loss: 0.4001
Epoch 2/90
129/129 [==============================] - 1s 12ms/step - loss: 0.4325 - val_loss: 0.3961
Epoch 3/90
129/129 [==============================] - 1s 12ms/step - loss: 0.4283 - val_loss: 0.3918
Epoch 4/90
129/129 [==============================] - 2s 12ms/step - loss: 0.4237 - val_loss: 0.3872
Epoch 5/90
129/129 [==============================] - 2s 12ms/step - loss: 0.4187 - val_loss: 0.3822
Epoch 6/90
129/129 [==============================] - 1s 12ms/step - loss: 0.4134 - val_loss: 0.3769
Epoch 7/90
129/129 [==============================] - 2s 12ms/step - loss: 0.4078 - val_loss: 0.3713
Epoch 8/90
129/129 [==============================] - 1s 12ms/step - loss: 0.4019 - val_loss: 0.3654
Epoch 9/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3957 - val_loss: 0.3593
Epoch 10/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3893 - val_loss: 0.3529
Epoch 11/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3826 - val_loss: 0.3463
Epoch 12/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3757 - val_loss: 0.3395
Epoch 13/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3685 - val_loss: 0.3324
Epoch 14/90
129/129 [==============================] - 2s 12ms/step - loss: 0.3611 - val_loss: 0.3250
Epoch 15/90
129/129 [==============================] - 2s 12ms/step - loss: 0.3534 - val_loss: 0.3175
Epoch 16/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3455 - val_loss: 0.3096
Epoch 17/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3373 - val_loss: 0.3015
Epoch 18/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3289 - val_loss: 0.2932
Epoch 19/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3202 - val_loss: 0.2846
Epoch 20/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3112 - val_loss: 0.2761
Epoch 21/90
129/129 [==============================] - 1s 12ms/step - loss: 0.3022 - val_loss: 0.2656
Epoch 22/90
129/129 [==============================] - 1s 12ms/step - loss: 0.2884 - val_loss: 0.2494
Epoch 23/90
129/129 [==============================] - 2s 12ms/step - loss: 0.2716 - val_loss: 0.2327
Epoch 24/90
129/129 [==============================] - 2s 12ms/step - loss: 0.2541 - val_loss: 0.2145
Epoch 25/90
129/129 [==============================] - 2s 13ms/step - loss: 0.2327 - val_loss: 0.1894
Epoch 26/90
129/129 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.1629
Epoch 27/90
129/129 [==============================] - 1s 12ms/step - loss: 0.1788 - val_loss: 0.1361
Epoch 28/90
129/129 [==============================] - 2s 12ms/step - loss: 0.1513 - val_loss: 0.1093
Epoch 29/90
129/129 [==============================] - 1s 12ms/step - loss: 0.1236 - val_loss: 0.0815
Epoch 30/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0953 - val_loss: 0.0539
Epoch 31/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0709 - val_loss: 0.0312
Epoch 32/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0591 - val_loss: 0.0199
Epoch 33/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0557 - val_loss: 0.0138
Epoch 34/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0545 - val_loss: 0.0106
Epoch 35/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0537 - val_loss: 0.0090
Epoch 36/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0534 - val_loss: 0.0080
Epoch 37/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0532 - val_loss: 0.0075
Epoch 38/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0530 - val_loss: 0.0072
Epoch 39/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0528 - val_loss: 0.0071
Epoch 40/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0069
Epoch 41/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0068
Epoch 42/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0525 - val_loss: 0.0067
Epoch 43/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0525 - val_loss: 0.0067
Epoch 44/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0067
Epoch 45/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0523 - val_loss: 0.0068
Epoch 46/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0522 - val_loss: 0.0069
Epoch 47/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0522 - val_loss: 0.0070
Epoch 48/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0071
Epoch 49/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0521 - val_loss: 0.0072
Epoch 50/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0073
Epoch 51/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0520 - val_loss: 0.0074
Epoch 52/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0075
Epoch 53/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0519 - val_loss: 0.0076
Epoch 54/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0077
Epoch 55/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0519 - val_loss: 0.0078
Epoch 56/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0519 - val_loss: 0.0078
Epoch 57/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0518 - val_loss: 0.0079
Epoch 58/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0518 - val_loss: 0.0080
Epoch 59/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0518 - val_loss: 0.0080
Epoch 60/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0081
Epoch 61/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0517 - val_loss: 0.0082
Epoch 62/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0517 - val_loss: 0.0082
Epoch 63/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0517 - val_loss: 0.0083
Epoch 64/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0517 - val_loss: 0.0084
Epoch 65/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0517 - val_loss: 0.0084
Epoch 66/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0517 - val_loss: 0.0085
Epoch 67/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0085
Epoch 68/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0517 - val_loss: 0.0086
Epoch 69/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0517 - val_loss: 0.0087
Epoch 70/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0517 - val_loss: 0.0087
Epoch 71/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0088
Epoch 72/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0088
Epoch 73/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0088
Epoch 74/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0089
Epoch 75/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0089
Epoch 76/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0090
Epoch 77/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0516 - val_loss: 0.0090
Epoch 78/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0091
Epoch 79/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0515 - val_loss: 0.0091
Epoch 80/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0092
Epoch 81/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0516 - val_loss: 0.0092
Epoch 82/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0093
Epoch 83/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0093
Epoch 84/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0093
Epoch 85/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0094
Epoch 86/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0094
Epoch 87/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0515 - val_loss: 0.0094
Epoch 88/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0515 - val_loss: 0.0095
Epoch 89/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0095
Epoch 90/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0095
Execution time:  139.52779269218445
DNN:
Mean Absolute Error: 0.0196
Root Mean Square Error: 0.0345
Mean Square Error: 0.0012

Train RMSE: 0.034
Train MSE: 0.001
Train MAE: 0.020
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_194&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_582 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_583 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_194 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_584 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
57/57 [==============================] - 1s 17ms/step - loss: 0.5384 - val_loss: 0.4988
Epoch 2/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5376 - val_loss: 0.4981
Epoch 3/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5368 - val_loss: 0.4974
Epoch 4/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5361 - val_loss: 0.4966
Epoch 5/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5354 - val_loss: 0.4958
Epoch 6/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5346 - val_loss: 0.4949
Epoch 7/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5337 - val_loss: 0.4941
Epoch 8/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5327 - val_loss: 0.4932
Epoch 9/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5318 - val_loss: 0.4923
Epoch 10/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5309 - val_loss: 0.4913
Epoch 11/52
57/57 [==============================] - 1s 15ms/step - loss: 0.5299 - val_loss: 0.4903
Epoch 12/52
57/57 [==============================] - 1s 16ms/step - loss: 0.5289 - val_loss: 0.4893
Epoch 13/52
57/57 [==============================] - 1s 15ms/step - loss: 0.5278 - val_loss: 0.4883
Epoch 14/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5268 - val_loss: 0.4872
Epoch 15/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5259 - val_loss: 0.4862
Epoch 16/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5247 - val_loss: 0.4851
Epoch 17/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5235 - val_loss: 0.4839
Epoch 18/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5226 - val_loss: 0.4828
Epoch 19/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5215 - val_loss: 0.4816
Epoch 20/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5202 - val_loss: 0.4804
Epoch 21/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5188 - val_loss: 0.4792
Epoch 22/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5177 - val_loss: 0.4780
Epoch 23/52
57/57 [==============================] - 1s 15ms/step - loss: 0.5163 - val_loss: 0.4767
Epoch 24/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5153 - val_loss: 0.4755
Epoch 25/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5139 - val_loss: 0.4742
Epoch 26/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5127 - val_loss: 0.4729
Epoch 27/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5112 - val_loss: 0.4715
Epoch 28/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5099 - val_loss: 0.4702
Epoch 29/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5085 - val_loss: 0.4688
Epoch 30/52
57/57 [==============================] - 1s 15ms/step - loss: 0.5070 - val_loss: 0.4674
Epoch 31/52
57/57 [==============================] - 1s 15ms/step - loss: 0.5058 - val_loss: 0.4660
Epoch 32/52
57/57 [==============================] - 1s 16ms/step - loss: 0.5042 - val_loss: 0.4646
Epoch 33/52
57/57 [==============================] - 1s 15ms/step - loss: 0.5027 - val_loss: 0.4632
Epoch 34/52
57/57 [==============================] - 1s 14ms/step - loss: 0.5011 - val_loss: 0.4617
Epoch 35/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4998 - val_loss: 0.4602
Epoch 36/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4984 - val_loss: 0.4588
Epoch 37/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4968 - val_loss: 0.4572
Epoch 38/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4954 - val_loss: 0.4557
Epoch 39/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4939 - val_loss: 0.4542
Epoch 40/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4923 - val_loss: 0.4526
Epoch 41/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4906 - val_loss: 0.4510
Epoch 42/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4889 - val_loss: 0.4495
Epoch 43/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4874 - val_loss: 0.4478
Epoch 44/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4857 - val_loss: 0.4462
Epoch 45/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4841 - val_loss: 0.4446
Epoch 46/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4826 - val_loss: 0.4429
Epoch 47/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4806 - val_loss: 0.4413
Epoch 48/52
57/57 [==============================] - 1s 14ms/step - loss: 0.4791 - val_loss: 0.4396
Epoch 49/52
57/57 [==============================] - 1s 15ms/step - loss: 0.4772 - val_loss: 0.4379
Epoch 50/52
57/57 [==============================] - 1s 15ms/step - loss: 0.4756 - val_loss: 0.4361
Epoch 51/52
57/57 [==============================] - 1s 16ms/step - loss: 0.4739 - val_loss: 0.4345
Epoch 52/52
57/57 [==============================] - 1s 15ms/step - loss: 0.4723 - val_loss: 0.4329
Execution time:  44.01806664466858
DNN:
Mean Absolute Error: 0.4708
Root Mean Square Error: 0.4737
Mean Square Error: 0.2244

Train RMSE: 0.474
Train MSE: 0.224
Train MAE: 0.471
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_195&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_585 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_586 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_195 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_587 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
92/92 [==============================] - 2s 21ms/step - loss: 0.4453 - val_loss: 0.4163
Epoch 2/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4433 - val_loss: 0.4141
Epoch 3/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4411 - val_loss: 0.4118
Epoch 4/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4386 - val_loss: 0.4095
Epoch 5/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4363 - val_loss: 0.4071
Epoch 6/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4338 - val_loss: 0.4046
Epoch 7/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4312 - val_loss: 0.4021
Epoch 8/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4285 - val_loss: 0.3994
Epoch 9/80
92/92 [==============================] - 2s 17ms/step - loss: 0.4258 - val_loss: 0.3967
Epoch 10/80
92/92 [==============================] - 2s 17ms/step - loss: 0.4230 - val_loss: 0.3939
Epoch 11/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4202 - val_loss: 0.3912
Epoch 12/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4173 - val_loss: 0.3884
Epoch 13/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4145 - val_loss: 0.3855
Epoch 14/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4115 - val_loss: 0.3825
Epoch 15/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4085 - val_loss: 0.3796
Epoch 16/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4054 - val_loss: 0.3765
Epoch 17/80
92/92 [==============================] - 1s 16ms/step - loss: 0.4023 - val_loss: 0.3734
Epoch 18/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3991 - val_loss: 0.3703
Epoch 19/80
92/92 [==============================] - 2s 17ms/step - loss: 0.3959 - val_loss: 0.3671
Epoch 20/80
92/92 [==============================] - 2s 17ms/step - loss: 0.3926 - val_loss: 0.3638
Epoch 21/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3893 - val_loss: 0.3605
Epoch 22/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3859 - val_loss: 0.3572
Epoch 23/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3825 - val_loss: 0.3540
Epoch 24/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3793 - val_loss: 0.3510
Epoch 25/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3761 - val_loss: 0.3478
Epoch 26/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3729 - val_loss: 0.3447
Epoch 27/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3697 - val_loss: 0.3415
Epoch 28/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3664 - val_loss: 0.3383
Epoch 29/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3631 - val_loss: 0.3350
Epoch 30/80
92/92 [==============================] - 2s 17ms/step - loss: 0.3598 - val_loss: 0.3316
Epoch 31/80
92/92 [==============================] - 2s 17ms/step - loss: 0.3562 - val_loss: 0.3282
Epoch 32/80
92/92 [==============================] - 2s 17ms/step - loss: 0.3529 - val_loss: 0.3248
Epoch 33/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3494 - val_loss: 0.3214
Epoch 34/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3458 - val_loss: 0.3180
Epoch 35/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3424 - val_loss: 0.3145
Epoch 36/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3388 - val_loss: 0.3110
Epoch 37/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3351 - val_loss: 0.3074
Epoch 38/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3316 - val_loss: 0.3038
Epoch 39/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3279 - val_loss: 0.3001
Epoch 40/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3242 - val_loss: 0.2965
Epoch 41/80
92/92 [==============================] - 2s 17ms/step - loss: 0.3205 - val_loss: 0.2929
Epoch 42/80
92/92 [==============================] - 2s 17ms/step - loss: 0.3169 - val_loss: 0.2895
Epoch 43/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3135 - val_loss: 0.2866
Epoch 44/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3105 - val_loss: 0.2838
Epoch 45/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3075 - val_loss: 0.2810
Epoch 46/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3047 - val_loss: 0.2782
Epoch 47/80
92/92 [==============================] - 1s 16ms/step - loss: 0.3017 - val_loss: 0.2754
Epoch 48/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2988 - val_loss: 0.2726
Epoch 49/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2958 - val_loss: 0.2697
Epoch 50/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2929 - val_loss: 0.2668
Epoch 51/80
92/92 [==============================] - 2s 17ms/step - loss: 0.2899 - val_loss: 0.2638
Epoch 52/80
92/92 [==============================] - 2s 17ms/step - loss: 0.2869 - val_loss: 0.2608
Epoch 53/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2838 - val_loss: 0.2578
Epoch 54/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2807 - val_loss: 0.2547
Epoch 55/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2775 - val_loss: 0.2515
Epoch 56/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2743 - val_loss: 0.2483
Epoch 57/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2709 - val_loss: 0.2450
Epoch 58/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2677 - val_loss: 0.2417
Epoch 59/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2644 - val_loss: 0.2383
Epoch 60/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2608 - val_loss: 0.2349
Epoch 61/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2574 - val_loss: 0.2314
Epoch 62/80
92/92 [==============================] - 2s 17ms/step - loss: 0.2540 - val_loss: 0.2278
Epoch 63/80
92/92 [==============================] - 2s 17ms/step - loss: 0.2504 - val_loss: 0.2242
Epoch 64/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2467 - val_loss: 0.2205
Epoch 65/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2430 - val_loss: 0.2168
Epoch 66/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2393 - val_loss: 0.2130
Epoch 67/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2353 - val_loss: 0.2091
Epoch 68/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2317 - val_loss: 0.2051
Epoch 69/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2277 - val_loss: 0.2011
Epoch 70/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2236 - val_loss: 0.1970
Epoch 71/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2197 - val_loss: 0.1929
Epoch 72/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2155 - val_loss: 0.1886
Epoch 73/80
92/92 [==============================] - 2s 17ms/step - loss: 0.2115 - val_loss: 0.1843
Epoch 74/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2071 - val_loss: 0.1800
Epoch 75/80
92/92 [==============================] - 1s 16ms/step - loss: 0.2028 - val_loss: 0.1755
Epoch 76/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1984 - val_loss: 0.1710
Epoch 77/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1940 - val_loss: 0.1665
Epoch 78/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1899 - val_loss: 0.1618
Epoch 79/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1853 - val_loss: 0.1572
Epoch 80/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1807 - val_loss: 0.1525
Execution time:  121.32032632827759
DNN:
Mean Absolute Error: 0.1728
Root Mean Square Error: 0.1756
Mean Square Error: 0.0308

Train RMSE: 0.176
Train MSE: 0.031
Train MAE: 0.173
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_196&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_588 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_589 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_196 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_590 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
114/114 [==============================] - 2s 14ms/step - loss: 0.4171 - val_loss: 0.3887
Epoch 2/90
114/114 [==============================] - 1s 13ms/step - loss: 0.4139 - val_loss: 0.3854
Epoch 3/90
114/114 [==============================] - 1s 13ms/step - loss: 0.4104 - val_loss: 0.3818
Epoch 4/90
114/114 [==============================] - 1s 12ms/step - loss: 0.4066 - val_loss: 0.3780
Epoch 5/90
114/114 [==============================] - 1s 12ms/step - loss: 0.4026 - val_loss: 0.3739
Epoch 6/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3983 - val_loss: 0.3696
Epoch 7/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3938 - val_loss: 0.3651
Epoch 8/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3891 - val_loss: 0.3603
Epoch 9/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3841 - val_loss: 0.3554
Epoch 10/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3790 - val_loss: 0.3503
Epoch 11/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3450
Epoch 12/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.3395
Epoch 13/90
114/114 [==============================] - 1s 13ms/step - loss: 0.3624 - val_loss: 0.3337
Epoch 14/90
114/114 [==============================] - 1s 13ms/step - loss: 0.3565 - val_loss: 0.3278
Epoch 15/90
114/114 [==============================] - 1s 13ms/step - loss: 0.3503 - val_loss: 0.3217
Epoch 16/90
114/114 [==============================] - 1s 13ms/step - loss: 0.3441 - val_loss: 0.3155
Epoch 17/90
114/114 [==============================] - 2s 13ms/step - loss: 0.3376 - val_loss: 0.3093
Epoch 18/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3314 - val_loss: 0.3032
Epoch 19/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3251 - val_loss: 0.2970
Epoch 20/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3186 - val_loss: 0.2905
Epoch 21/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3119 - val_loss: 0.2839
Epoch 22/90
114/114 [==============================] - 1s 12ms/step - loss: 0.3050 - val_loss: 0.2771
Epoch 23/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2980 - val_loss: 0.2700
Epoch 24/90
114/114 [==============================] - 1s 13ms/step - loss: 0.2907 - val_loss: 0.2628
Epoch 25/90
114/114 [==============================] - 1s 13ms/step - loss: 0.2832 - val_loss: 0.2554
Epoch 26/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2756 - val_loss: 0.2478
Epoch 27/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2677 - val_loss: 0.2402
Epoch 28/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2602 - val_loss: 0.2333
Epoch 29/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2531 - val_loss: 0.2263
Epoch 30/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2458 - val_loss: 0.2191
Epoch 31/90
114/114 [==============================] - 1s 13ms/step - loss: 0.2383 - val_loss: 0.2117
Epoch 32/90
114/114 [==============================] - 1s 13ms/step - loss: 0.2306 - val_loss: 0.2040
Epoch 33/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2227 - val_loss: 0.1961
Epoch 34/90
114/114 [==============================] - 1s 12ms/step - loss: 0.2147 - val_loss: 0.1881
Epoch 35/90
114/114 [==============================] - 1s 13ms/step - loss: 0.2063 - val_loss: 0.1797
Epoch 36/90
114/114 [==============================] - 2s 13ms/step - loss: 0.1978 - val_loss: 0.1712
Epoch 37/90
114/114 [==============================] - 1s 13ms/step - loss: 0.1890 - val_loss: 0.1624
Epoch 38/90
114/114 [==============================] - 1s 12ms/step - loss: 0.1800 - val_loss: 0.1533
Epoch 39/90
114/114 [==============================] - 1s 13ms/step - loss: 0.1707 - val_loss: 0.1440
Epoch 40/90
114/114 [==============================] - 2s 13ms/step - loss: 0.1612 - val_loss: 0.1344
Epoch 41/90
114/114 [==============================] - 1s 12ms/step - loss: 0.1514 - val_loss: 0.1246
Epoch 42/90
114/114 [==============================] - 1s 12ms/step - loss: 0.1413 - val_loss: 0.1144
Epoch 43/90
114/114 [==============================] - 1s 12ms/step - loss: 0.1310 - val_loss: 0.1040
Epoch 44/90
114/114 [==============================] - 1s 12ms/step - loss: 0.1204 - val_loss: 0.0933
Epoch 45/90
114/114 [==============================] - 1s 12ms/step - loss: 0.1089 - val_loss: 0.0800
Epoch 46/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0948 - val_loss: 0.0648
Epoch 47/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0810 - val_loss: 0.0507
Epoch 48/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0707 - val_loss: 0.0397
Epoch 49/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0648 - val_loss: 0.0343
Epoch 50/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0631 - val_loss: 0.0305
Epoch 51/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0621 - val_loss: 0.0280
Epoch 52/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0616 - val_loss: 0.0261
Epoch 53/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0612 - val_loss: 0.0247
Epoch 54/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0611 - val_loss: 0.0237
Epoch 55/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0609 - val_loss: 0.0228
Epoch 56/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0608 - val_loss: 0.0221
Epoch 57/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0607 - val_loss: 0.0215
Epoch 58/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0606 - val_loss: 0.0211
Epoch 59/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0606 - val_loss: 0.0207
Epoch 60/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0605 - val_loss: 0.0204
Epoch 61/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0605 - val_loss: 0.0202
Epoch 62/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0605 - val_loss: 0.0200
Epoch 63/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0605 - val_loss: 0.0198
Epoch 64/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0604 - val_loss: 0.0196
Epoch 65/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0604 - val_loss: 0.0194
Epoch 66/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0603 - val_loss: 0.0193
Epoch 67/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0604 - val_loss: 0.0191
Epoch 68/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0603 - val_loss: 0.0190
Epoch 69/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0603 - val_loss: 0.0188
Epoch 70/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0604 - val_loss: 0.0187
Epoch 71/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0602 - val_loss: 0.0185
Epoch 72/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0602 - val_loss: 0.0184
Epoch 73/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0602 - val_loss: 0.0183
Epoch 74/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0602 - val_loss: 0.0182
Epoch 75/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0181
Epoch 76/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0603 - val_loss: 0.0180
Epoch 77/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0602 - val_loss: 0.0179
Epoch 78/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0602 - val_loss: 0.0178
Epoch 79/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0178
Epoch 80/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0601 - val_loss: 0.0177
Epoch 81/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0601 - val_loss: 0.0177
Epoch 82/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0176
Epoch 83/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0176
Epoch 84/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0175
Epoch 85/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0175
Epoch 86/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0174
Epoch 87/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0600 - val_loss: 0.0174
Epoch 88/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0600 - val_loss: 0.0173
Epoch 89/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0173
Epoch 90/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0173
Execution time:  130.6467320919037
DNN:
Mean Absolute Error: 0.0330
Root Mean Square Error: 0.0420
Mean Square Error: 0.0018

Train RMSE: 0.042
Train MSE: 0.002
Train MAE: 0.033
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_197&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_591 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_592 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_197 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_593 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
50/50 [==============================] - 1s 18ms/step - loss: 0.3168 - val_loss: 0.2933
Epoch 2/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3161 - val_loss: 0.2926
Epoch 3/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3152 - val_loss: 0.2919
Epoch 4/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3145 - val_loss: 0.2911
Epoch 5/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3136 - val_loss: 0.2903
Epoch 6/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3129 - val_loss: 0.2894
Epoch 7/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3121 - val_loss: 0.2886
Epoch 8/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3112 - val_loss: 0.2877
Epoch 9/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3103 - val_loss: 0.2868
Epoch 10/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3092 - val_loss: 0.2858
Epoch 11/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3083 - val_loss: 0.2848
Epoch 12/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3073 - val_loss: 0.2838
Epoch 13/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3063 - val_loss: 0.2828
Epoch 14/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3053 - val_loss: 0.2818
Epoch 15/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3042 - val_loss: 0.2807
Epoch 16/52
50/50 [==============================] - 1s 14ms/step - loss: 0.3031 - val_loss: 0.2796
Epoch 17/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3020 - val_loss: 0.2785
Epoch 18/52
50/50 [==============================] - 1s 15ms/step - loss: 0.3009 - val_loss: 0.2773
Epoch 19/52
50/50 [==============================] - 1s 16ms/step - loss: 0.2997 - val_loss: 0.2762
Epoch 20/52
50/50 [==============================] - 1s 17ms/step - loss: 0.2987 - val_loss: 0.2750
Epoch 21/52
50/50 [==============================] - 1s 16ms/step - loss: 0.2973 - val_loss: 0.2738
Epoch 22/52
50/50 [==============================] - 1s 16ms/step - loss: 0.2961 - val_loss: 0.2725
Epoch 23/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2949 - val_loss: 0.2713
Epoch 24/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2935 - val_loss: 0.2700
Epoch 25/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2923 - val_loss: 0.2687
Epoch 26/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2910 - val_loss: 0.2674
Epoch 27/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2897 - val_loss: 0.2661
Epoch 28/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2883 - val_loss: 0.2648
Epoch 29/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2870 - val_loss: 0.2634
Epoch 30/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2855 - val_loss: 0.2620
Epoch 31/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2842 - val_loss: 0.2606
Epoch 32/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2829 - val_loss: 0.2592
Epoch 33/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2814 - val_loss: 0.2577
Epoch 34/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2800 - val_loss: 0.2563
Epoch 35/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2784 - val_loss: 0.2548
Epoch 36/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2769 - val_loss: 0.2533
Epoch 37/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2754 - val_loss: 0.2518
Epoch 38/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2739 - val_loss: 0.2503
Epoch 39/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2723 - val_loss: 0.2487
Epoch 40/52
50/50 [==============================] - 1s 16ms/step - loss: 0.2709 - val_loss: 0.2472
Epoch 41/52
50/50 [==============================] - 1s 17ms/step - loss: 0.2692 - val_loss: 0.2457
Epoch 42/52
50/50 [==============================] - 1s 17ms/step - loss: 0.2680 - val_loss: 0.2442
Epoch 43/52
50/50 [==============================] - 1s 16ms/step - loss: 0.2662 - val_loss: 0.2427
Epoch 44/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2649 - val_loss: 0.2412
Epoch 45/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2632 - val_loss: 0.2396
Epoch 46/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2617 - val_loss: 0.2380
Epoch 47/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2601 - val_loss: 0.2364
Epoch 48/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2585 - val_loss: 0.2348
Epoch 49/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2568 - val_loss: 0.2332
Epoch 50/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2554 - val_loss: 0.2316
Epoch 51/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2535 - val_loss: 0.2300
Epoch 52/52
50/50 [==============================] - 1s 15ms/step - loss: 0.2522 - val_loss: 0.2284
Execution time:  41.411032915115356
DNN:
Mean Absolute Error: 0.2482
Root Mean Square Error: 0.2499
Mean Square Error: 0.0624

Train RMSE: 0.250
Train MSE: 0.062
Train MAE: 0.248
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_198&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_594 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_595 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_198 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_596 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
104/104 [==============================] - 2s 17ms/step - loss: 0.1050 - val_loss: 0.1345
Epoch 2/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1043 - val_loss: 0.1337
Epoch 3/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1036 - val_loss: 0.1329
Epoch 4/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1027 - val_loss: 0.1320
Epoch 5/80
104/104 [==============================] - 2s 16ms/step - loss: 0.1019 - val_loss: 0.1311
Epoch 6/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1009 - val_loss: 0.1302
Epoch 7/80
104/104 [==============================] - 2s 15ms/step - loss: 0.1000 - val_loss: 0.1291
Epoch 8/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0990 - val_loss: 0.1281
Epoch 9/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0979 - val_loss: 0.1270
Epoch 10/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0968 - val_loss: 0.1259
Epoch 11/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0957 - val_loss: 0.1247
Epoch 12/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0946 - val_loss: 0.1235
Epoch 13/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0935 - val_loss: 0.1223
Epoch 14/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0923 - val_loss: 0.1211
Epoch 15/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0911 - val_loss: 0.1199
Epoch 16/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0900 - val_loss: 0.1186
Epoch 17/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0888 - val_loss: 0.1174
Epoch 18/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0876 - val_loss: 0.1161
Epoch 19/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0864 - val_loss: 0.1148
Epoch 20/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0852 - val_loss: 0.1135
Epoch 21/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0840 - val_loss: 0.1122
Epoch 22/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0828 - val_loss: 0.1109
Epoch 23/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0815 - val_loss: 0.1096
Epoch 24/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0803 - val_loss: 0.1082
Epoch 25/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0791 - val_loss: 0.1069
Epoch 26/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0779 - val_loss: 0.1056
Epoch 27/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0767 - val_loss: 0.1043
Epoch 28/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0756 - val_loss: 0.1029
Epoch 29/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0744 - val_loss: 0.1016
Epoch 30/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0732 - val_loss: 0.1003
Epoch 31/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0721 - val_loss: 0.0990
Epoch 32/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0710 - val_loss: 0.0977
Epoch 33/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0699 - val_loss: 0.0964
Epoch 34/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0689 - val_loss: 0.0951
Epoch 35/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0679 - val_loss: 0.0939
Epoch 36/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0669 - val_loss: 0.0928
Epoch 37/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0660 - val_loss: 0.0918
Epoch 38/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0652 - val_loss: 0.0908
Epoch 39/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0644 - val_loss: 0.0898
Epoch 40/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0637 - val_loss: 0.0889
Epoch 41/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0630 - val_loss: 0.0880
Epoch 42/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0622 - val_loss: 0.0870
Epoch 43/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0616 - val_loss: 0.0861
Epoch 44/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0609 - val_loss: 0.0852
Epoch 45/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0602 - val_loss: 0.0844
Epoch 46/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0596 - val_loss: 0.0835
Epoch 47/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0590 - val_loss: 0.0826
Epoch 48/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0584 - val_loss: 0.0817
Epoch 49/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0577 - val_loss: 0.0809
Epoch 50/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0572 - val_loss: 0.0801
Epoch 51/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0566 - val_loss: 0.0792
Epoch 52/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0560 - val_loss: 0.0784
Epoch 53/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0556 - val_loss: 0.0776
Epoch 54/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0550 - val_loss: 0.0768
Epoch 55/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0545 - val_loss: 0.0760
Epoch 56/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0540 - val_loss: 0.0753
Epoch 57/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0535 - val_loss: 0.0745
Epoch 58/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0530 - val_loss: 0.0738
Epoch 59/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0526 - val_loss: 0.0730- ETA: 0s -
Epoch 60/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0522 - val_loss: 0.0723
Epoch 61/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0518 - val_loss: 0.0716
Epoch 62/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0513 - val_loss: 0.0709
Epoch 63/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0510 - val_loss: 0.0703
Epoch 64/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0506 - val_loss: 0.0696
Epoch 65/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0503 - val_loss: 0.0690
Epoch 66/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0499 - val_loss: 0.0683
Epoch 67/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0496 - val_loss: 0.0677
Epoch 68/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0493 - val_loss: 0.0671
Epoch 69/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0490 - val_loss: 0.0665
Epoch 70/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0487 - val_loss: 0.0659
Epoch 71/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0484 - val_loss: 0.0654
Epoch 72/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0482 - val_loss: 0.0648
Epoch 73/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0479 - val_loss: 0.0643
Epoch 74/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0477 - val_loss: 0.0638
Epoch 75/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0475 - val_loss: 0.0633
Epoch 76/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0473 - val_loss: 0.0628
Epoch 77/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0471 - val_loss: 0.0623
Epoch 78/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0469 - val_loss: 0.0619
Epoch 79/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0468 - val_loss: 0.0615
Epoch 80/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0466 - val_loss: 0.0611
Execution time:  130.25465488433838
DNN:
Mean Absolute Error: 0.0437
Root Mean Square Error: 0.0524
Mean Square Error: 0.0027

Train RMSE: 0.052
Train MSE: 0.003
Train MAE: 0.044
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_199&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_597 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_598 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_199 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_599 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
129/129 [==============================] - 2s 13ms/step - loss: 0.1007 - val_loss: 0.1305
Epoch 2/90
129/129 [==============================] - 2s 12ms/step - loss: 0.1001 - val_loss: 0.1299
Epoch 3/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0995 - val_loss: 0.1293
Epoch 4/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0989 - val_loss: 0.1286
Epoch 5/90
129/129 [==============================] - 1s 11ms/step - loss: 0.0982 - val_loss: 0.1279
Epoch 6/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0974 - val_loss: 0.1271
Epoch 7/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0967 - val_loss: 0.1263
Epoch 8/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0958 - val_loss: 0.1254
Epoch 9/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0950 - val_loss: 0.1245
Epoch 10/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0941 - val_loss: 0.1236
Epoch 11/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0932 - val_loss: 0.1227
Epoch 12/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0923 - val_loss: 0.1217
Epoch 13/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0913 - val_loss: 0.1207
Epoch 14/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0904 - val_loss: 0.1197
Epoch 15/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0894 - val_loss: 0.1186
Epoch 16/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0884 - val_loss: 0.1175
Epoch 17/90
129/129 [==============================] - 1s 11ms/step - loss: 0.0873 - val_loss: 0.1164
Epoch 18/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0863 - val_loss: 0.1153
Epoch 19/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0852 - val_loss: 0.1142
Epoch 20/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0841 - val_loss: 0.1130
Epoch 21/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0830 - val_loss: 0.1118
Epoch 22/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0818 - val_loss: 0.1106
Epoch 23/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0807 - val_loss: 0.1094
Epoch 24/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0796 - val_loss: 0.1082
Epoch 25/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0785 - val_loss: 0.1070
Epoch 26/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0773 - val_loss: 0.1057
Epoch 27/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0762 - val_loss: 0.1045
Epoch 28/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0751 - val_loss: 0.1032
Epoch 29/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0739 - val_loss: 0.1020
Epoch 30/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0728 - val_loss: 0.1007
Epoch 31/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0716 - val_loss: 0.0994
Epoch 32/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0705 - val_loss: 0.0981
Epoch 33/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0693 - val_loss: 0.0969
Epoch 34/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0682 - val_loss: 0.0956
Epoch 35/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0671 - val_loss: 0.0943
Epoch 36/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0660 - val_loss: 0.0930
Epoch 37/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0649 - val_loss: 0.0917
Epoch 38/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0638 - val_loss: 0.0905
Epoch 39/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0627 - val_loss: 0.0892
Epoch 40/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0616 - val_loss: 0.0879
Epoch 41/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0606 - val_loss: 0.0866
Epoch 42/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0595 - val_loss: 0.0853
Epoch 43/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0585 - val_loss: 0.0841
Epoch 44/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0574 - val_loss: 0.0828
Epoch 45/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0564 - val_loss: 0.0816
Epoch 46/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0555 - val_loss: 0.0803
Epoch 47/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0546 - val_loss: 0.0792
Epoch 48/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0537 - val_loss: 0.0780
Epoch 49/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0528 - val_loss: 0.0768
Epoch 50/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0520 - val_loss: 0.0757
Epoch 51/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0512 - val_loss: 0.0746
Epoch 52/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0504 - val_loss: 0.0735
Epoch 53/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0496 - val_loss: 0.0724
Epoch 54/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0489 - val_loss: 0.0713
Epoch 55/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0482 - val_loss: 0.0702
Epoch 56/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0475 - val_loss: 0.0692
Epoch 57/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0468 - val_loss: 0.0681
Epoch 58/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0462 - val_loss: 0.0671
Epoch 59/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0455 - val_loss: 0.0661
Epoch 60/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0449 - val_loss: 0.0652
Epoch 61/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0443 - val_loss: 0.0642
Epoch 62/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0437 - val_loss: 0.0632
Epoch 63/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0432 - val_loss: 0.0623
Epoch 64/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0426 - val_loss: 0.0614
Epoch 65/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0421 - val_loss: 0.0604
Epoch 66/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0415 - val_loss: 0.0595
Epoch 67/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0410 - val_loss: 0.0586
Epoch 68/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0405 - val_loss: 0.0577
Epoch 69/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0400 - val_loss: 0.0568
Epoch 70/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0395 - val_loss: 0.0559
Epoch 71/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0390 - val_loss: 0.0550
Epoch 72/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0386 - val_loss: 0.0542
Epoch 73/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0382 - val_loss: 0.0533
Epoch 74/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0377 - val_loss: 0.0525
Epoch 75/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0373 - val_loss: 0.0517
Epoch 76/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0369 - val_loss: 0.0509
Epoch 77/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0365 - val_loss: 0.0501
Epoch 78/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0362 - val_loss: 0.0494
Epoch 79/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0359 - val_loss: 0.0486
Epoch 80/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0355 - val_loss: 0.0479
Epoch 81/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0352 - val_loss: 0.0472
Epoch 82/90
129/129 [==============================] - 1s 11ms/step - loss: 0.0349 - val_loss: 0.0465
Epoch 83/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0346 - val_loss: 0.0459
Epoch 84/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0343 - val_loss: 0.0452
Epoch 85/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0341 - val_loss: 0.0446
Epoch 86/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0338 - val_loss: 0.0439
Epoch 87/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0336 - val_loss: 0.0433
Epoch 88/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0333 - val_loss: 0.0427
Epoch 89/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0331 - val_loss: 0.0422
Epoch 90/90
129/129 [==============================] - 1s 12ms/step - loss: 0.0329 - val_loss: 0.0416
Execution time:  139.9935998916626
DNN:
Mean Absolute Error: 0.0340
Root Mean Square Error: 0.0459
Mean Square Error: 0.0021

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.034
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_200&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_600 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_601 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_200 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_602 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
57/57 [==============================] - 1s 23ms/step - loss: 0.1114 - val_loss: 0.1409
Epoch 2/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1113 - val_loss: 0.1408
Epoch 3/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1112 - val_loss: 0.1407
Epoch 4/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1111 - val_loss: 0.1405
Epoch 5/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1109 - val_loss: 0.1404
Epoch 6/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1108 - val_loss: 0.1403
Epoch 7/52
57/57 [==============================] - 1s 16ms/step - loss: 0.1106 - val_loss: 0.1401
Epoch 8/52
57/57 [==============================] - 1s 16ms/step - loss: 0.1105 - val_loss: 0.1400
Epoch 9/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1104 - val_loss: 0.1398
Epoch 10/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1102 - val_loss: 0.1397
Epoch 11/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1101 - val_loss: 0.1395
Epoch 12/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1099 - val_loss: 0.1394
Epoch 13/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1097 - val_loss: 0.1392
Epoch 14/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1096 - val_loss: 0.1390
Epoch 15/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1094 - val_loss: 0.1389
Epoch 16/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1092 - val_loss: 0.1387
Epoch 17/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1090 - val_loss: 0.1385
Epoch 18/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1089 - val_loss: 0.1383
Epoch 19/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1087 - val_loss: 0.1381
Epoch 20/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1085 - val_loss: 0.1379
Epoch 21/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1083 - val_loss: 0.1378
Epoch 22/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1082 - val_loss: 0.1376
Epoch 23/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1079 - val_loss: 0.1374
Epoch 24/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1078 - val_loss: 0.1372
Epoch 25/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1076 - val_loss: 0.1370
Epoch 26/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1074 - val_loss: 0.1368
Epoch 27/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1071 - val_loss: 0.1366
Epoch 28/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1070 - val_loss: 0.1363
Epoch 29/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1067 - val_loss: 0.1361
Epoch 30/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1065 - val_loss: 0.1359
Epoch 31/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1063 - val_loss: 0.1357
Epoch 32/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1061 - val_loss: 0.1355
Epoch 33/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1059 - val_loss: 0.1353
Epoch 34/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1057 - val_loss: 0.1350
Epoch 35/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1055 - val_loss: 0.1348
Epoch 36/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1052 - val_loss: 0.1346
Epoch 37/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1050 - val_loss: 0.1344
Epoch 38/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1048 - val_loss: 0.1341
Epoch 39/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1046 - val_loss: 0.1339
Epoch 40/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1043 - val_loss: 0.1337
Epoch 41/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1041 - val_loss: 0.1334
Epoch 42/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1039 - val_loss: 0.1332
Epoch 43/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1037 - val_loss: 0.1330
Epoch 44/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1035 - val_loss: 0.1327
Epoch 45/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1032 - val_loss: 0.1325
Epoch 46/52
57/57 [==============================] - 1s 16ms/step - loss: 0.1030 - val_loss: 0.1323
Epoch 47/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1027 - val_loss: 0.1320
Epoch 48/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1025 - val_loss: 0.1318
Epoch 49/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1023 - val_loss: 0.1315
Epoch 50/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1020 - val_loss: 0.1313
Epoch 51/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1018 - val_loss: 0.1310
Epoch 52/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1015 - val_loss: 0.1308
Execution time:  44.68583822250366
DNN:
Mean Absolute Error: 0.1013
Root Mean Square Error: 0.1072
Mean Square Error: 0.0115

Train RMSE: 0.107
Train MSE: 0.011
Train MAE: 0.101
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_201&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_603 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_604 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_201 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_605 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
92/92 [==============================] - 2s 18ms/step - loss: 0.1097 - val_loss: 0.1333
Epoch 2/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1094 - val_loss: 0.1329
Epoch 3/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1089 - val_loss: 0.1324
Epoch 4/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1085 - val_loss: 0.1319
Epoch 5/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1080 - val_loss: 0.1315
Epoch 6/80
92/92 [==============================] - 2s 17ms/step - loss: 0.1075 - val_loss: 0.1309
Epoch 7/80
92/92 [==============================] - 2s 17ms/step - loss: 0.1070 - val_loss: 0.1304
Epoch 8/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1065 - val_loss: 0.1299
Epoch 9/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1059 - val_loss: 0.1293
Epoch 10/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1054 - val_loss: 0.1287
Epoch 11/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1048 - val_loss: 0.1282
Epoch 12/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1043 - val_loss: 0.1276
Epoch 13/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1036 - val_loss: 0.1270
Epoch 14/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1031 - val_loss: 0.1263
Epoch 15/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1024 - val_loss: 0.1257
Epoch 16/80
92/92 [==============================] - 1s 16ms/step - loss: 0.1018 - val_loss: 0.1251
Epoch 17/80
92/92 [==============================] - 2s 17ms/step - loss: 0.1012 - val_loss: 0.1244
Epoch 18/80
92/92 [==============================] - 2s 17ms/step - loss: 0.1006 - val_loss: 0.1238
Epoch 19/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0999 - val_loss: 0.1231
Epoch 20/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0993 - val_loss: 0.1224
Epoch 21/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0986 - val_loss: 0.1217
Epoch 22/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0979 - val_loss: 0.1210
Epoch 23/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0972 - val_loss: 0.1203
Epoch 24/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0966 - val_loss: 0.1196
Epoch 25/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0959 - val_loss: 0.1189
Epoch 26/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0952 - val_loss: 0.1182
Epoch 27/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0946 - val_loss: 0.1176
Epoch 28/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0940 - val_loss: 0.1171
Epoch 29/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0935 - val_loss: 0.1166
Epoch 30/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0930 - val_loss: 0.1161
Epoch 31/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0925 - val_loss: 0.1156
Epoch 32/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0921 - val_loss: 0.1151
Epoch 33/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0916 - val_loss: 0.1147
Epoch 34/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0911 - val_loss: 0.1142
Epoch 35/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0907 - val_loss: 0.1137
Epoch 36/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0902 - val_loss: 0.1132
Epoch 37/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0897 - val_loss: 0.1127
Epoch 38/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0893 - val_loss: 0.1122
Epoch 39/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0888 - val_loss: 0.1117
Epoch 40/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0883 - val_loss: 0.1112
Epoch 41/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0878 - val_loss: 0.1106
Epoch 42/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0874 - val_loss: 0.1101
Epoch 43/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0869 - val_loss: 0.1096
Epoch 44/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0864 - val_loss: 0.1091
Epoch 45/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0859 - val_loss: 0.1086
Epoch 46/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0855 - val_loss: 0.1080
Epoch 47/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0850 - val_loss: 0.1075
Epoch 48/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0845 - val_loss: 0.1070
Epoch 49/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0840 - val_loss: 0.1065
Epoch 50/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0836 - val_loss: 0.1060
Epoch 51/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0831 - val_loss: 0.1055
Epoch 52/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0827 - val_loss: 0.1050
Epoch 53/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0822 - val_loss: 0.1045
Epoch 54/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0818 - val_loss: 0.1040
Epoch 55/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0813 - val_loss: 0.1035
Epoch 56/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0808 - val_loss: 0.1029
Epoch 57/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0804 - val_loss: 0.1024
Epoch 58/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0800 - val_loss: 0.1020
Epoch 59/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0795 - val_loss: 0.1015
Epoch 60/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0791 - val_loss: 0.1011
Epoch 61/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0788 - val_loss: 0.1007
Epoch 62/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0784 - val_loss: 0.1003
Epoch 63/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0780 - val_loss: 0.0999
Epoch 64/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0777 - val_loss: 0.0995
Epoch 65/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0773 - val_loss: 0.0991
Epoch 66/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0770 - val_loss: 0.0986
Epoch 67/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0766 - val_loss: 0.0982
Epoch 68/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0762 - val_loss: 0.0978
Epoch 69/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0759 - val_loss: 0.0974
Epoch 70/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0755 - val_loss: 0.0970
Epoch 71/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0752 - val_loss: 0.0966
Epoch 72/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0749 - val_loss: 0.0962
Epoch 73/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0745 - val_loss: 0.0958
Epoch 74/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0742 - val_loss: 0.0954
Epoch 75/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0738 - val_loss: 0.0950
Epoch 76/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0735 - val_loss: 0.0946
Epoch 77/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0732 - val_loss: 0.0942
Epoch 78/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0729 - val_loss: 0.0938
Epoch 79/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0726 - val_loss: 0.0934
Epoch 80/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0722 - val_loss: 0.0930
Execution time:  121.4556794166565
DNN:
Mean Absolute Error: 0.0735
Root Mean Square Error: 0.0807
Mean Square Error: 0.0065

Train RMSE: 0.081
Train MSE: 0.007
Train MAE: 0.073
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_202&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_606 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_607 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_202 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_608 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
114/114 [==============================] - 2s 14ms/step - loss: 0.0892 - val_loss: 0.1123
Epoch 2/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0883 - val_loss: 0.1113
Epoch 3/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0873 - val_loss: 0.1103
Epoch 4/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0863 - val_loss: 0.1092
Epoch 5/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0854 - val_loss: 0.1082
Epoch 6/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0843 - val_loss: 0.1070
Epoch 7/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0832 - val_loss: 0.1059
Epoch 8/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0821 - val_loss: 0.1046
Epoch 9/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0810 - val_loss: 0.1034
Epoch 10/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0798 - val_loss: 0.1021
Epoch 11/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0786 - val_loss: 0.1008
Epoch 12/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0774 - val_loss: 0.0994
Epoch 13/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0762 - val_loss: 0.0981
Epoch 14/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0750 - val_loss: 0.0967
Epoch 15/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0737 - val_loss: 0.0952
Epoch 16/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0724 - val_loss: 0.0938
Epoch 17/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0712 - val_loss: 0.0923
Epoch 18/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0698 - val_loss: 0.0908
Epoch 19/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0685 - val_loss: 0.0893
Epoch 20/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0673 - val_loss: 0.0878
Epoch 21/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0660 - val_loss: 0.0863
Epoch 22/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0647 - val_loss: 0.0848
Epoch 23/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0634 - val_loss: 0.0833
Epoch 24/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0622 - val_loss: 0.0818
Epoch 25/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0609 - val_loss: 0.0802
Epoch 26/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0597 - val_loss: 0.0787
Epoch 27/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0585 - val_loss: 0.0773
Epoch 28/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0574 - val_loss: 0.0758
Epoch 29/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0562 - val_loss: 0.0744
Epoch 30/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0552 - val_loss: 0.0730
Epoch 31/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0542 - val_loss: 0.0717
Epoch 32/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0532 - val_loss: 0.0703
Epoch 33/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0523 - val_loss: 0.0691
Epoch 34/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0514 - val_loss: 0.0679
Epoch 35/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0667
Epoch 36/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0498 - val_loss: 0.0656
Epoch 37/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0491 - val_loss: 0.0645
Epoch 38/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0483 - val_loss: 0.0634
Epoch 39/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0477 - val_loss: 0.0623
Epoch 40/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0470 - val_loss: 0.0613
Epoch 41/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0463 - val_loss: 0.0603
Epoch 42/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0457 - val_loss: 0.0593
Epoch 43/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0452 - val_loss: 0.0585
Epoch 44/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0447 - val_loss: 0.0576
Epoch 45/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0442 - val_loss: 0.0568
Epoch 46/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0437 - val_loss: 0.0560
Epoch 47/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0432 - val_loss: 0.0552
Epoch 48/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0428 - val_loss: 0.0544
Epoch 49/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0423 - val_loss: 0.0536
Epoch 50/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0419 - val_loss: 0.0528
Epoch 51/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0415 - val_loss: 0.0521
Epoch 52/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0411 - val_loss: 0.0513
Epoch 53/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0407 - val_loss: 0.0506
Epoch 54/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0403 - val_loss: 0.0499
Epoch 55/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0399 - val_loss: 0.0491
Epoch 56/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0396 - val_loss: 0.0484
Epoch 57/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0392 - val_loss: 0.0477
Epoch 58/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0389 - val_loss: 0.0471
Epoch 59/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0386 - val_loss: 0.0464
Epoch 60/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0382 - val_loss: 0.0457
Epoch 61/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0379 - val_loss: 0.0451
Epoch 62/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0376 - val_loss: 0.0444
Epoch 63/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0373 - val_loss: 0.0438
Epoch 64/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0370 - val_loss: 0.0432
Epoch 65/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0368 - val_loss: 0.0426
Epoch 66/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0365 - val_loss: 0.0420
Epoch 67/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0363 - val_loss: 0.0414
Epoch 68/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0359 - val_loss: 0.0407
Epoch 69/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0356 - val_loss: 0.0399
Epoch 70/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0353 - val_loss: 0.0392
Epoch 71/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0351 - val_loss: 0.0384
Epoch 72/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0348 - val_loss: 0.0378
Epoch 73/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0345 - val_loss: 0.0371
Epoch 74/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0343 - val_loss: 0.0364
Epoch 75/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0340 - val_loss: 0.0358
Epoch 76/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0338 - val_loss: 0.0352
Epoch 77/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0336 - val_loss: 0.0347
Epoch 78/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0334 - val_loss: 0.0341
Epoch 79/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0332 - val_loss: 0.0336
Epoch 80/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0330 - val_loss: 0.0331
Epoch 81/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0329 - val_loss: 0.0326
Epoch 82/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0328 - val_loss: 0.0322
Epoch 83/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0326 - val_loss: 0.0317
Epoch 84/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0325 - val_loss: 0.0313
Epoch 85/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0324 - val_loss: 0.0309
Epoch 86/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0323 - val_loss: 0.0305
Epoch 87/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0322 - val_loss: 0.0302
Epoch 88/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0321 - val_loss: 0.0298
Epoch 89/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0320 - val_loss: 0.0295
Epoch 90/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0319 - val_loss: 0.0292
Execution time:  130.28317308425903
DNN:
Mean Absolute Error: 0.0327
Root Mean Square Error: 0.0462
Mean Square Error: 0.0021

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.033
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_203&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_609 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_610 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_203 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_611 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
50/50 [==============================] - 1s 18ms/step - loss: 0.1458 - val_loss: 0.1685
Epoch 2/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1457 - val_loss: 0.1684
Epoch 3/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1455 - val_loss: 0.1682
Epoch 4/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1454 - val_loss: 0.1680
Epoch 5/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1452 - val_loss: 0.1679
Epoch 6/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1450 - val_loss: 0.1677
Epoch 7/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1449 - val_loss: 0.1675
Epoch 8/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1447 - val_loss: 0.1673
Epoch 9/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1445 - val_loss: 0.1672
Epoch 10/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1443 - val_loss: 0.1670
Epoch 11/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1441 - val_loss: 0.1668
Epoch 12/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1439 - val_loss: 0.1666
Epoch 13/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1437 - val_loss: 0.1663
Epoch 14/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1434 - val_loss: 0.1661
Epoch 15/52
50/50 [==============================] - 1s 16ms/step - loss: 0.1433 - val_loss: 0.1659
Epoch 16/52
50/50 [==============================] - 1s 16ms/step - loss: 0.1430 - val_loss: 0.1657
Epoch 17/52
50/50 [==============================] - 1s 16ms/step - loss: 0.1428 - val_loss: 0.1655
Epoch 18/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1426 - val_loss: 0.1652
Epoch 19/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1424 - val_loss: 0.1650
Epoch 20/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1421 - val_loss: 0.1648
Epoch 21/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1419 - val_loss: 0.1645
Epoch 22/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1416 - val_loss: 0.1643
Epoch 23/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1414 - val_loss: 0.1640
Epoch 24/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1411 - val_loss: 0.1638
Epoch 25/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1409 - val_loss: 0.1635
Epoch 26/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1406 - val_loss: 0.1633
Epoch 27/52
50/50 [==============================] - 1s 16ms/step - loss: 0.1404 - val_loss: 0.1630
Epoch 28/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1401 - val_loss: 0.1628
Epoch 29/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1399 - val_loss: 0.1625
Epoch 30/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1396 - val_loss: 0.1623
Epoch 31/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1394 - val_loss: 0.1621
Epoch 32/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1392 - val_loss: 0.1618
Epoch 33/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1389 - val_loss: 0.1616
Epoch 34/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1387 - val_loss: 0.1613
Epoch 35/52
50/50 [==============================] - 1s 16ms/step - loss: 0.1384 - val_loss: 0.1611
Epoch 36/52
50/50 [==============================] - 1s 16ms/step - loss: 0.1382 - val_loss: 0.1609
Epoch 37/52
50/50 [==============================] - 1s 17ms/step - loss: 0.1379 - val_loss: 0.1606
Epoch 38/52
50/50 [==============================] - 1s 16ms/step - loss: 0.1377 - val_loss: 0.1604
Epoch 39/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1375 - val_loss: 0.1601
Epoch 40/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1372 - val_loss: 0.1598
Epoch 41/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1369 - val_loss: 0.1596
Epoch 42/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1367 - val_loss: 0.1593
Epoch 43/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1364 - val_loss: 0.1591
Epoch 44/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1362 - val_loss: 0.1588
Epoch 45/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1359 - val_loss: 0.1585
Epoch 46/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1356 - val_loss: 0.1583
Epoch 47/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1354 - val_loss: 0.1580
Epoch 48/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1351 - val_loss: 0.1577
Epoch 49/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1348 - val_loss: 0.1575
Epoch 50/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1346 - val_loss: 0.1572
Epoch 51/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1343 - val_loss: 0.1569
Epoch 52/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1340 - val_loss: 0.1566
Execution time:  41.221580505371094
DNN:
Mean Absolute Error: 0.1354
Root Mean Square Error: 0.1392
Mean Square Error: 0.0194

Train RMSE: 0.139
Train MSE: 0.019
Train MAE: 0.135
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_204&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_612 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_613 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_204 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_614 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
104/104 [==============================] - 2s 18ms/step - loss: 0.1371 - val_loss: 0.0174
Epoch 2/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0998 - val_loss: 0.0098
Epoch 3/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0800 - val_loss: 0.0093
Epoch 4/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0721 - val_loss: 0.0074
Epoch 5/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0650 - val_loss: 0.0095
Epoch 6/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0606 - val_loss: 0.0097
Epoch 7/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0570 - val_loss: 0.0103A:
Epoch 8/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0536 - val_loss: 0.0119
Epoch 9/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0504 - val_loss: 0.0138
Epoch 10/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0473 - val_loss: 0.0153
Epoch 11/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0448 - val_loss: 0.0177
Epoch 12/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0425 - val_loss: 0.0188
Epoch 13/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0403 - val_loss: 0.0202
Epoch 14/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0386 - val_loss: 0.0215
Epoch 15/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0369 - val_loss: 0.0212
Epoch 16/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0360 - val_loss: 0.0197
Epoch 17/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0349 - val_loss: 0.0190
Epoch 18/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0340 - val_loss: 0.0177
Epoch 19/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0331 - val_loss: 0.0169
Epoch 20/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0323 - val_loss: 0.0189
Epoch 21/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0318 - val_loss: 0.0153
Epoch 22/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0302 - val_loss: 0.0121
Epoch 23/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0298 - val_loss: 0.0119
Epoch 24/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0289 - val_loss: 0.0109
Epoch 25/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0281 - val_loss: 0.0095
Epoch 26/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0266 - val_loss: 0.0112
Epoch 27/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0265 - val_loss: 0.0123
Epoch 28/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0262 - val_loss: 0.0146
Epoch 29/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0260 - val_loss: 0.0143
Epoch 30/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0146
Epoch 31/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0148
Epoch 32/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0150
Epoch 33/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 34/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 35/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 36/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0150
Epoch 37/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 38/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 39/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 40/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 41/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 42/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 43/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0151
Epoch 44/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0153
Epoch 45/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 46/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 47/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 48/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 49/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 50/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 51/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 52/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 53/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0150
Epoch 54/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 55/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 56/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 57/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 58/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 59/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 60/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0150
Epoch 61/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 62/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 63/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 64/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 65/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 66/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 67/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0150
Epoch 68/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 69/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 70/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 71/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 72/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 73/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 74/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0150
Epoch 75/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 76/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 77/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 78/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 79/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0259 - val_loss: 0.0152
Epoch 80/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0152
Execution time:  130.40651106834412
DNN:
Mean Absolute Error: 0.0253
Root Mean Square Error: 0.0441
Mean Square Error: 0.0019

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_205&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_615 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_616 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_205 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_617 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
129/129 [==============================] - 2s 16ms/step - loss: 0.1208 - val_loss: 0.0090
Epoch 2/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0559 - val_loss: 0.0073
Epoch 3/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0537 - val_loss: 0.0069
Epoch 4/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0513 - val_loss: 0.0077
Epoch 5/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0491 - val_loss: 0.0075
Epoch 6/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0474 - val_loss: 0.0098
Epoch 7/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0454 - val_loss: 0.0109
Epoch 8/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0439 - val_loss: 0.0119
Epoch 9/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0426 - val_loss: 0.0123
Epoch 10/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0415 - val_loss: 0.0133
Epoch 11/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0403 - val_loss: 0.0137
Epoch 12/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0395 - val_loss: 0.0149
Epoch 13/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0387 - val_loss: 0.0124
Epoch 14/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0375 - val_loss: 0.0134
Epoch 15/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0372 - val_loss: 0.0148
Epoch 16/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0365 - val_loss: 0.0138
Epoch 17/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0360 - val_loss: 0.0154
Epoch 18/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0347 - val_loss: 0.0152
Epoch 19/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0347 - val_loss: 0.0152
Epoch 20/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0156
Epoch 21/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0337 - val_loss: 0.0158
Epoch 22/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0328 - val_loss: 0.0159
Epoch 23/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0325 - val_loss: 0.0161
Epoch 24/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0320 - val_loss: 0.0161
Epoch 25/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0314 - val_loss: 0.0142
Epoch 26/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0311 - val_loss: 0.0157
Epoch 27/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0306 - val_loss: 0.0155
Epoch 28/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0302 - val_loss: 0.0139
Epoch 29/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0298 - val_loss: 0.0138
Epoch 30/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0298 - val_loss: 0.0149
Epoch 31/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0292 - val_loss: 0.0135
Epoch 32/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0290 - val_loss: 0.0114
Epoch 33/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0289 - val_loss: 0.0114
Epoch 34/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0288 - val_loss: 0.0106
Epoch 35/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0283 - val_loss: 0.0111
Epoch 36/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0282 - val_loss: 0.0105
Epoch 37/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0278 - val_loss: 0.0105
Epoch 38/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0276 - val_loss: 0.0105
Epoch 39/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0275 - val_loss: 0.0103
Epoch 40/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0273 - val_loss: 0.0104
Epoch 41/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0272 - val_loss: 0.0103
Epoch 42/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0271 - val_loss: 0.0102
Epoch 43/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0270 - val_loss: 0.0104
Epoch 44/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0269 - val_loss: 0.0103
Epoch 45/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0268 - val_loss: 0.0103
Epoch 46/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0268 - val_loss: 0.0103
Epoch 47/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0267 - val_loss: 0.0109
Epoch 48/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0266 - val_loss: 0.0115
Epoch 49/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0265 - val_loss: 0.0121
Epoch 50/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0264 - val_loss: 0.0125
Epoch 51/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0263 - val_loss: 0.0127
Epoch 52/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0129
Epoch 53/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0263 - val_loss: 0.0131
Epoch 54/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0133
Epoch 55/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0135
Epoch 56/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0132
Epoch 57/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0134
Epoch 58/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 59/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0154
Epoch 60/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0269 - val_loss: 0.0106
Epoch 61/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0264 - val_loss: 0.0138
Epoch 62/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0135
Epoch 63/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0132
Epoch 64/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0134
Epoch 65/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 66/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 67/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 68/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 69/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 70/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0133
Epoch 71/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0135
Epoch 72/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0261 - val_loss: 0.0132
Epoch 73/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0134
Epoch 74/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 75/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 76/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 77/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 78/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0133
Epoch 79/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0135
Epoch 80/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0132
Epoch 81/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0134
Epoch 82/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 83/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 84/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 85/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Epoch 86/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0133
Epoch 87/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0135
Epoch 88/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0132
Epoch 89/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0134
Epoch 90/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0134
Execution time:  141.44747591018677
DNN:
Mean Absolute Error: 0.0255
Root Mean Square Error: 0.0451
Mean Square Error: 0.0020

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_206&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_618 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_619 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_206 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_620 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
57/57 [==============================] - 1s 20ms/step - loss: 0.2130 - val_loss: 0.0335
Epoch 2/52
57/57 [==============================] - 1s 15ms/step - loss: 0.1136 - val_loss: 0.0163
Epoch 3/52
57/57 [==============================] - 1s 14ms/step - loss: 0.1036 - val_loss: 0.0194
Epoch 4/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0979 - val_loss: 0.0220
Epoch 5/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0926 - val_loss: 0.0211
Epoch 6/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0875 - val_loss: 0.0190
Epoch 7/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0824 - val_loss: 0.0163
Epoch 8/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0770 - val_loss: 0.0131
Epoch 9/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0717 - val_loss: 0.0105
Epoch 10/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0665 - val_loss: 0.0085
Epoch 11/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0616 - val_loss: 0.0072
Epoch 12/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0571 - val_loss: 0.0066
Epoch 13/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0527 - val_loss: 0.0071
Epoch 14/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0487 - val_loss: 0.0078
Epoch 15/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0451 - val_loss: 0.0085
Epoch 16/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0417 - val_loss: 0.0094
Epoch 17/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0387 - val_loss: 0.0098
Epoch 18/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0361 - val_loss: 0.0099
Epoch 19/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0338 - val_loss: 0.0092
Epoch 20/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0321 - val_loss: 0.0090
Epoch 21/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0307 - val_loss: 0.0087
Epoch 22/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0296 - val_loss: 0.0088
Epoch 23/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0287 - val_loss: 0.0091
Epoch 24/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0279 - val_loss: 0.0123
Epoch 25/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0267 - val_loss: 0.0139
Epoch 26/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0262 - val_loss: 0.0135
Epoch 27/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0261 - val_loss: 0.0138
Epoch 28/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0259 - val_loss: 0.0139
Epoch 29/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0259 - val_loss: 0.0144
Epoch 30/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0258 - val_loss: 0.0149
Epoch 31/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0257 - val_loss: 0.0155
Epoch 32/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0256 - val_loss: 0.0160
Epoch 33/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0256 - val_loss: 0.0165
Epoch 34/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0255 - val_loss: 0.0170
Epoch 35/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0172
Epoch 36/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0172
Epoch 37/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0254 - val_loss: 0.0174
Epoch 38/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0254 - val_loss: 0.0175
Epoch 39/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0254 - val_loss: 0.0177
Epoch 40/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0254 - val_loss: 0.0180
Epoch 41/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0253 - val_loss: 0.0183
Epoch 42/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0182
Epoch 43/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0182
Epoch 44/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0182
Epoch 45/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0182
Epoch 46/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0181
Epoch 47/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0181
Epoch 48/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0252 - val_loss: 0.0182
Epoch 49/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0182
Epoch 50/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0183
Epoch 51/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0252 - val_loss: 0.0182
Epoch 52/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0251 - val_loss: 0.0178
Execution time:  45.24117851257324
DNN:
Mean Absolute Error: 0.0249
Root Mean Square Error: 0.0427
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_207&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_621 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_622 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_207 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_623 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
92/92 [==============================] - 2s 18ms/step - loss: 0.1506 - val_loss: 0.0282
Epoch 2/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0960 - val_loss: 0.0213
Epoch 3/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0828 - val_loss: 0.0236
Epoch 4/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0749 - val_loss: 0.0200
Epoch 5/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0701 - val_loss: 0.0191
Epoch 6/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0660 - val_loss: 0.0157
Epoch 7/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0608 - val_loss: 0.0192
Epoch 8/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0573 - val_loss: 0.0192
Epoch 9/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0546 - val_loss: 0.0189
Epoch 10/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0520 - val_loss: 0.0186
Epoch 11/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0498 - val_loss: 0.0181
Epoch 12/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0476 - val_loss: 0.0184
Epoch 13/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0458 - val_loss: 0.0172
Epoch 14/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0442 - val_loss: 0.0165
Epoch 15/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0427 - val_loss: 0.0150
Epoch 16/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0412 - val_loss: 0.0138
Epoch 17/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0397 - val_loss: 0.0130
Epoch 18/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0383 - val_loss: 0.0126
Epoch 19/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0370 - val_loss: 0.0123
Epoch 20/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0359 - val_loss: 0.0120
Epoch 21/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0348 - val_loss: 0.0118
Epoch 22/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0338 - val_loss: 0.0116
Epoch 23/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0331 - val_loss: 0.0115
Epoch 24/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0325 - val_loss: 0.0113
Epoch 25/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0315 - val_loss: 0.0110
Epoch 26/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0305 - val_loss: 0.0109
Epoch 27/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0298 - val_loss: 0.0109
Epoch 28/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0292 - val_loss: 0.0115
Epoch 29/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0289 - val_loss: 0.0118
Epoch 30/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0288 - val_loss: 0.0121
Epoch 31/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0287 - val_loss: 0.0123
Epoch 32/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0286 - val_loss: 0.0125
Epoch 33/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0285 - val_loss: 0.0127
Epoch 34/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0283 - val_loss: 0.0133
Epoch 35/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0280 - val_loss: 0.0135
Epoch 36/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0139
Epoch 37/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0278 - val_loss: 0.0139
Epoch 38/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 39/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 40/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 41/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0139
Epoch 42/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 43/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 44/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 45/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 46/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 47/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 48/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 49/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0139
Epoch 50/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0139
Epoch 51/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 52/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 53/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 54/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 55/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 56/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 57/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 58/80
92/92 [==============================] - ETA: 0s - loss: 0.026 - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 59/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 60/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0139
Epoch 61/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0139
Epoch 62/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 63/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 64/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 65/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 66/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 67/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 68/80
92/92 [==============================] - 2s 19ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 69/80
92/92 [==============================] - 2s 19ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 70/80
92/92 [==============================] - 2s 19ms/step - loss: 0.0277 - val_loss: 0.0139
Epoch 71/80
92/92 [==============================] - 2s 19ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 72/80
92/92 [==============================] - 2s 20ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 73/80
92/92 [==============================] - 2s 19ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 74/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0280 - val_loss: 0.0134
Epoch 75/80
92/92 [==============================] - 2s 19ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 76/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 77/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 78/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 79/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 80/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0277 - val_loss: 0.0140 - loss: 0.02 - ETA: 0s - loss: 0.0
Execution time:  124.60525321960449
DNN:
Mean Absolute Error: 0.0252
Root Mean Square Error: 0.0436
Mean Square Error: 0.0019

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_208&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_624 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_625 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_208 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_626 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
114/114 [==============================] - 2s 17ms/step - loss: 0.1516 - val_loss: 0.0361
Epoch 2/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0598 - val_loss: 0.0322
Epoch 3/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0582 - val_loss: 0.0337
Epoch 4/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0572 - val_loss: 0.0308
Epoch 5/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0556 - val_loss: 0.0310
Epoch 6/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0547 - val_loss: 0.0290
Epoch 7/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0532 - val_loss: 0.0273
Epoch 8/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0517 - val_loss: 0.0246
Epoch 9/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0503 - val_loss: 0.0249
Epoch 10/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0495 - val_loss: 0.0220
Epoch 11/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0481 - val_loss: 0.0198
Epoch 12/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0469 - val_loss: 0.0180
Epoch 13/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0455 - val_loss: 0.0156
Epoch 14/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0440 - val_loss: 0.0145
Epoch 15/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0428 - val_loss: 0.0134
Epoch 16/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0415 - val_loss: 0.0127
Epoch 17/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0404 - val_loss: 0.0123
Epoch 18/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0393 - val_loss: 0.0122
Epoch 19/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0382 - val_loss: 0.0120
Epoch 20/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0372 - val_loss: 0.0119
Epoch 21/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0362 - val_loss: 0.0120
Epoch 22/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0354 - val_loss: 0.0118
Epoch 23/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0346 - val_loss: 0.0117
Epoch 24/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0338 - val_loss: 0.0116
Epoch 25/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0331 - val_loss: 0.0114
Epoch 26/90
114/114 [==============================] - 2s 14ms/step - loss: 0.0324 - val_loss: 0.0113
Epoch 27/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0319 - val_loss: 0.0111
Epoch 28/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0314 - val_loss: 0.0110
Epoch 29/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0309 - val_loss: 0.0109
Epoch 30/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0305 - val_loss: 0.0109
Epoch 31/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0302 - val_loss: 0.0109
Epoch 32/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0307 - val_loss: 0.0110
Epoch 33/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0303 - val_loss: 0.0109
Epoch 34/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0301 - val_loss: 0.0109
Epoch 35/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0299 - val_loss: 0.0109
Epoch 36/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0297 - val_loss: 0.0109
Epoch 37/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0295 - val_loss: 0.0109
Epoch 38/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0294 - val_loss: 0.0109
Epoch 39/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0293 - val_loss: 0.0109
Epoch 40/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0292 - val_loss: 0.0109
Epoch 41/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0290 - val_loss: 0.0109
Epoch 42/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0295 - val_loss: 0.0122
Epoch 43/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0286 - val_loss: 0.0112
Epoch 44/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0287 - val_loss: 0.0115
Epoch 45/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0286 - val_loss: 0.0127
Epoch 46/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0282 - val_loss: 0.0127
Epoch 47/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0281 - val_loss: 0.0128
Epoch 48/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 49/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 50/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 51/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 52/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 53/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 54/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 55/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 56/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 57/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 58/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0280 - val_loss: 0.0128
Epoch 59/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 60/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0281 - val_loss: 0.0131
Epoch 61/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 62/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 63/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 64/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0281 - val_loss: 0.0131
Epoch 65/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 66/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 67/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 68/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0281 - val_loss: 0.0131
Epoch 69/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 70/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 71/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 72/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0281 - val_loss: 0.0131
Epoch 73/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 74/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 75/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 76/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0281 - val_loss: 0.0131
Epoch 77/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 78/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 79/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 80/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0281 - val_loss: 0.0131
Epoch 81/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 82/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 83/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 84/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0281 - val_loss: 0.0131
Epoch 85/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 86/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 87/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0129
Epoch 88/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0281 - val_loss: 0.0130
Epoch 89/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 90/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0280 - val_loss: 0.0130
Execution time:  133.23346638679504
DNN:
Mean Absolute Error: 0.0253
Root Mean Square Error: 0.0445
Mean Square Error: 0.0020

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_209&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_627 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_628 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_209 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_629 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
50/50 [==============================] - 1s 20ms/step - loss: 0.1816 - val_loss: 0.0198
Epoch 2/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1212 - val_loss: 0.0174
Epoch 3/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1084 - val_loss: 0.0208
Epoch 4/52
50/50 [==============================] - 1s 15ms/step - loss: 0.1011 - val_loss: 0.0192
Epoch 5/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0953 - val_loss: 0.0168
Epoch 6/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0905 - val_loss: 0.0143
Epoch 7/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0862 - val_loss: 0.0134
Epoch 8/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0819 - val_loss: 0.0129
Epoch 9/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0777 - val_loss: 0.0127
Epoch 10/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0734 - val_loss: 0.0125
Epoch 11/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0691 - val_loss: 0.0124
Epoch 12/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0650 - val_loss: 0.0123
Epoch 13/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0612 - val_loss: 0.0122
Epoch 14/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0575 - val_loss: 0.0120
Epoch 15/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0537 - val_loss: 0.0114
Epoch 16/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0498 - val_loss: 0.0115
Epoch 17/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0469 - val_loss: 0.0119
Epoch 18/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0444 - val_loss: 0.0122
Epoch 19/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0422 - val_loss: 0.0122
Epoch 20/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0403 - val_loss: 0.0121
Epoch 21/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0386 - val_loss: 0.0117
Epoch 22/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0372 - val_loss: 0.0116
Epoch 23/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0358 - val_loss: 0.0114
Epoch 24/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0347 - val_loss: 0.0113
Epoch 25/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0338 - val_loss: 0.0112
Epoch 26/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0330 - val_loss: 0.0112
Epoch 27/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0322 - val_loss: 0.0113
Epoch 28/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0112
Epoch 29/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0311 - val_loss: 0.0112
Epoch 30/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0306 - val_loss: 0.0130
Epoch 31/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0296 - val_loss: 0.0130
Epoch 32/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0293 - val_loss: 0.0128
Epoch 33/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0292 - val_loss: 0.0134
Epoch 34/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0289 - val_loss: 0.0134
Epoch 35/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0285 - val_loss: 0.0134
Epoch 36/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0283 - val_loss: 0.0139
Epoch 37/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0280 - val_loss: 0.0141
Epoch 38/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 39/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0276 - val_loss: 0.0142
Epoch 40/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0274 - val_loss: 0.0142
Epoch 41/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0274 - val_loss: 0.0146
Epoch 42/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0273 - val_loss: 0.0147
Epoch 43/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0149
Epoch 44/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0271 - val_loss: 0.0148
Epoch 45/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0271 - val_loss: 0.0148
Epoch 46/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0270 - val_loss: 0.0148
Epoch 47/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0270 - val_loss: 0.0149
Epoch 48/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0269 - val_loss: 0.0149
Epoch 49/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0270 - val_loss: 0.0150
Epoch 50/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0269 - val_loss: 0.0150
Epoch 51/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0269 - val_loss: 0.0150
Epoch 52/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0269 - val_loss: 0.0151
Execution time:  41.195196866989136
DNN:
Mean Absolute Error: 0.0250
Root Mean Square Error: 0.0426
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_210&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_630 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_631 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_210 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_632 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
104/104 [==============================] - 2s 20ms/step - loss: 0.0496 - val_loss: 0.0320
Epoch 2/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0401 - val_loss: 0.0247
Epoch 3/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0380 - val_loss: 0.0214
Epoch 4/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0368 - val_loss: 0.0209
Epoch 5/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0355 - val_loss: 0.0189
Epoch 6/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0351 - val_loss: 0.0187
Epoch 7/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0346 - val_loss: 0.0185
Epoch 8/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0340 - val_loss: 0.0182
Epoch 9/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0335 - val_loss: 0.0178
Epoch 10/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0330 - val_loss: 0.0176
Epoch 11/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0325 - val_loss: 0.0175
Epoch 12/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0321 - val_loss: 0.0173
Epoch 13/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0316 - val_loss: 0.0172
Epoch 14/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0312 - val_loss: 0.0171
Epoch 15/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0308 - val_loss: 0.0169
Epoch 16/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0304 - val_loss: 0.0167
Epoch 17/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0300 - val_loss: 0.0166
Epoch 18/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0297 - val_loss: 0.0163
Epoch 19/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0293 - val_loss: 0.0162
Epoch 20/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0290 - val_loss: 0.0160
Epoch 21/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0287 - val_loss: 0.0160
Epoch 22/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0285 - val_loss: 0.0158
Epoch 23/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0282 - val_loss: 0.0156
Epoch 24/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0280 - val_loss: 0.0154
Epoch 25/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0278 - val_loss: 0.0154
Epoch 26/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0276 - val_loss: 0.0154
Epoch 27/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0274 - val_loss: 0.0154
Epoch 28/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0272 - val_loss: 0.0151
Epoch 29/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0271 - val_loss: 0.0153
Epoch 30/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0270 - val_loss: 0.0153
Epoch 31/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0268 - val_loss: 0.0153
Epoch 32/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0267 - val_loss: 0.0152
Epoch 33/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0266 - val_loss: 0.0152
Epoch 34/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0265 - val_loss: 0.0152
Epoch 35/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0265 - val_loss: 0.0151
Epoch 36/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0264 - val_loss: 0.0153
Epoch 37/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0263 - val_loss: 0.0152
Epoch 38/80
104/104 [==============================] - ETA: 0s - loss: 0.0263- ETA: 0s - - 2s 15ms/step - loss: 0.0263 - val_loss: 0.0154
Epoch 39/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0262 - val_loss: 0.0154
Epoch 40/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0154
Epoch 41/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0261 - val_loss: 0.0154
Epoch 42/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0260 - val_loss: 0.0156
Epoch 43/80
104/104 [==============================] - 2s 17ms/step - loss: 0.0259 - val_loss: 0.0155
Epoch 44/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0157
Epoch 45/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0258 - val_loss: 0.0156
Epoch 46/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0158
Epoch 47/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0157
Epoch 48/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0257 - val_loss: 0.0159
Epoch 49/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0257 - val_loss: 0.0158
Epoch 50/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0256 - val_loss: 0.0159
Epoch 51/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0256 - val_loss: 0.0160
Epoch 52/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0256 - val_loss: 0.0161
Epoch 53/80
104/104 [==============================] - 2s 17ms/step - loss: 0.0256 - val_loss: 0.0162
Epoch 54/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0255 - val_loss: 0.0163
Epoch 55/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0255 - val_loss: 0.0164
Epoch 56/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0255 - val_loss: 0.0165
Epoch 57/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0254 - val_loss: 0.0166
Epoch 58/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0254 - val_loss: 0.0167
Epoch 59/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0168
Epoch 60/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0167
Epoch 61/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0174
Epoch 62/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0174
Epoch 63/80
104/104 [==============================] - 2s 17ms/step - loss: 0.0252 - val_loss: 0.0173
Epoch 64/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0177
Epoch 65/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0181
Epoch 66/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0184
Epoch 67/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0172
Epoch 68/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0185
Epoch 69/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 70/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 71/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 72/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 73/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 74/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 75/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 76/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 77/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 78/80
104/104 [==============================] - 2s 16ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 79/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Epoch 80/80
104/104 [==============================] - 2s 15ms/step - loss: 0.0249 - val_loss: 0.0185
Execution time:  131.9345142841339
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0431
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_211&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_633 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_634 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_211 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_635 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
129/129 [==============================] - 2s 14ms/step - loss: 0.0414 - val_loss: 0.0225
Epoch 2/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0338 - val_loss: 0.0230
Epoch 3/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0334 - val_loss: 0.0224
Epoch 4/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0331 - val_loss: 0.0216
Epoch 5/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0328 - val_loss: 0.0212
Epoch 6/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0325 - val_loss: 0.0208
Epoch 7/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0319 - val_loss: 0.0209
Epoch 8/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0316 - val_loss: 0.0206
Epoch 9/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0314 - val_loss: 0.0203
Epoch 10/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0311 - val_loss: 0.0197
Epoch 11/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0309 - val_loss: 0.0178
Epoch 12/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0307 - val_loss: 0.0177
Epoch 13/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0304 - val_loss: 0.0176
Epoch 14/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0300 - val_loss: 0.0177
Epoch 15/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0298 - val_loss: 0.0158
Epoch 16/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0296 - val_loss: 0.0160
Epoch 17/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0293 - val_loss: 0.0159
Epoch 18/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0291 - val_loss: 0.0162
Epoch 19/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0289 - val_loss: 0.0161
Epoch 20/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0287 - val_loss: 0.0145
Epoch 21/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0286 - val_loss: 0.0148
Epoch 22/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0284 - val_loss: 0.0147
Epoch 23/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0281 - val_loss: 0.0147
Epoch 24/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0280 - val_loss: 0.0146
Epoch 25/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0279 - val_loss: 0.0148
Epoch 26/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0277 - val_loss: 0.0148
Epoch 27/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0276 - val_loss: 0.0149
Epoch 28/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0274 - val_loss: 0.0150
Epoch 29/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0273 - val_loss: 0.0150
Epoch 30/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0271 - val_loss: 0.0149
Epoch 31/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0273 - val_loss: 0.0147
Epoch 32/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0269 - val_loss: 0.0143
Epoch 33/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0268 - val_loss: 0.0144
Epoch 34/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0267 - val_loss: 0.0146
Epoch 35/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 36/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0266 - val_loss: 0.0148
Epoch 37/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0264 - val_loss: 0.0138
Epoch 38/90
129/129 [==============================] - 2s 15ms/step - loss: 0.0265 - val_loss: 0.0150
Epoch 39/90
129/129 [==============================] - 2s 14ms/step - loss: 0.0264 - val_loss: 0.0157
Epoch 40/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0158
Epoch 41/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0158
Epoch 42/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0261 - val_loss: 0.0159
Epoch 43/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0160
Epoch 44/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0260 - val_loss: 0.0152
Epoch 45/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0262 - val_loss: 0.0148
Epoch 46/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0261 - val_loss: 0.0164
Epoch 47/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0258 - val_loss: 0.0164
Epoch 48/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0258 - val_loss: 0.0163
Epoch 49/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0258 - val_loss: 0.0164
Epoch 50/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0257 - val_loss: 0.0164
Epoch 51/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0257 - val_loss: 0.0164
Epoch 52/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0257 - val_loss: 0.0164
Epoch 53/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0256 - val_loss: 0.0165
Epoch 54/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0256 - val_loss: 0.0164
Epoch 55/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0256 - val_loss: 0.0164
Epoch 56/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0165
Epoch 57/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0167
Epoch 58/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0167
Epoch 59/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0168
Epoch 60/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0255 - val_loss: 0.0169
Epoch 61/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0168
Epoch 62/90
129/129 [==============================] - 2s 14ms/step - loss: 0.0254 - val_loss: 0.0169
Epoch 63/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0254 - val_loss: 0.0170
Epoch 64/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0168
Epoch 65/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.01710s - loss:
Epoch 66/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0169
Epoch 67/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0253 - val_loss: 0.0140
Epoch 68/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0257 - val_loss: 0.0171
Epoch 69/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0254 - val_loss: 0.0170
Epoch 70/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0253 - val_loss: 0.0169
Epoch 71/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0253 - val_loss: 0.0172
Epoch 72/90
129/129 [==============================] - 2s 14ms/step - loss: 0.0253 - val_loss: 0.0169
Epoch 73/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0253 - val_loss: 0.0171
Epoch 74/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0253 - val_loss: 0.0169
Epoch 75/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0253 - val_loss: 0.0170
Epoch 76/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0252 - val_loss: 0.0169
Epoch 77/90
129/129 [==============================] - 2s 14ms/step - loss: 0.0252 - val_loss: 0.0170
Epoch 78/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0170
Epoch 79/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0170
Epoch 80/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0171
Epoch 81/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0252 - val_loss: 0.0172
Epoch 82/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0252 - val_loss: 0.0173
Epoch 83/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0174
Epoch 84/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0252 - val_loss: 0.0174
Epoch 85/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0175
Epoch 86/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0176
Epoch 87/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0177
Epoch 88/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0177
Epoch 89/90
129/129 [==============================] - 2s 12ms/step - loss: 0.0251 - val_loss: 0.0177
Epoch 90/90
129/129 [==============================] - 2s 13ms/step - loss: 0.0251 - val_loss: 0.0178
Execution time:  145.31132221221924
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0434
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_212&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_636 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_637 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_212 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_638 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
57/57 [==============================] - 1s 19ms/step - loss: 0.0635 - val_loss: 0.0500
Epoch 2/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0510 - val_loss: 0.0441
Epoch 3/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0454 - val_loss: 0.0410
Epoch 4/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0424 - val_loss: 0.0404
Epoch 5/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0410 - val_loss: 0.0384
Epoch 6/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0396 - val_loss: 0.0364
Epoch 7/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0382 - val_loss: 0.0353
Epoch 8/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0371 - val_loss: 0.0345
Epoch 9/52
57/57 [==============================] - 1s 19ms/step - loss: 0.0361 - val_loss: 0.0337
Epoch 10/52
57/57 [==============================] - 1s 20ms/step - loss: 0.0353 - val_loss: 0.0327
Epoch 11/52
57/57 [==============================] - 1s 18ms/step - loss: 0.0346 - val_loss: 0.0319
Epoch 12/52
57/57 [==============================] - 1s 17ms/step - loss: 0.0341 - val_loss: 0.0315
Epoch 13/52
57/57 [==============================] - 1s 18ms/step - loss: 0.0335 - val_loss: 0.0308
Epoch 14/52
57/57 [==============================] - 1s 17ms/step - loss: 0.0330 - val_loss: 0.0302
Epoch 15/52
57/57 [==============================] - 1s 19ms/step - loss: 0.0326 - val_loss: 0.0295
Epoch 16/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0321 - val_loss: 0.0287
Epoch 17/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0317 - val_loss: 0.0279
Epoch 18/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0313 - val_loss: 0.0273
Epoch 19/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0310 - val_loss: 0.0267
Epoch 20/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0307 - val_loss: 0.0263
Epoch 21/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0303 - val_loss: 0.0257
Epoch 22/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0300 - val_loss: 0.0251
Epoch 23/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0297 - val_loss: 0.0245
Epoch 24/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0295 - val_loss: 0.0240
Epoch 25/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0293 - val_loss: 0.0236
Epoch 26/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0290 - val_loss: 0.0231
Epoch 27/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0288 - val_loss: 0.0227
Epoch 28/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0285 - val_loss: 0.0223
Epoch 29/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0283 - val_loss: 0.0219
Epoch 30/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0281 - val_loss: 0.0215
Epoch 31/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0279 - val_loss: 0.0211
Epoch 32/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0278 - val_loss: 0.0208
Epoch 33/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0276 - val_loss: 0.0205
Epoch 34/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0274 - val_loss: 0.0202
Epoch 35/52
57/57 [==============================] - 1s 16ms/step - loss: 0.0273 - val_loss: 0.0198
Epoch 36/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0272 - val_loss: 0.0196
Epoch 37/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0270 - val_loss: 0.0193
Epoch 38/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0269 - val_loss: 0.0191
Epoch 39/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0268 - val_loss: 0.0188
Epoch 40/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0267 - val_loss: 0.0186
Epoch 41/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0266 - val_loss: 0.0184
Epoch 42/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0265 - val_loss: 0.0183
Epoch 43/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0264 - val_loss: 0.0181
Epoch 44/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0263 - val_loss: 0.0180
Epoch 45/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0263 - val_loss: 0.0180
Epoch 46/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0262 - val_loss: 0.0179
Epoch 47/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0261 - val_loss: 0.0177
Epoch 48/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0261 - val_loss: 0.0178
Epoch 49/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0260 - val_loss: 0.0177
Epoch 50/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0260 - val_loss: 0.0178
Epoch 51/52
57/57 [==============================] - 1s 14ms/step - loss: 0.0259 - val_loss: 0.0176
Epoch 52/52
57/57 [==============================] - 1s 15ms/step - loss: 0.0259 - val_loss: 0.0176
Execution time:  47.04491567611694
DNN:
Mean Absolute Error: 0.0252
Root Mean Square Error: 0.0430
Mean Square Error: 0.0019

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_213&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_639 (Dense)            (None, 432, 87)           174       
_________________________________________________________________
dense_640 (Dense)            (None, 432, 16)           1408      
_________________________________________________________________
dropout_213 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_641 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0538 - val_loss: 0.0396
Epoch 2/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0440 - val_loss: 0.0348
Epoch 3/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0422 - val_loss: 0.0328
Epoch 4/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0407 - val_loss: 0.0310
Epoch 5/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0394 - val_loss: 0.0287
Epoch 6/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0384 - val_loss: 0.0265
Epoch 7/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0374 - val_loss: 0.0251
Epoch 8/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0365 - val_loss: 0.0234
Epoch 9/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0357 - val_loss: 0.0225
Epoch 10/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0349 - val_loss: 0.0216
Epoch 11/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0342 - val_loss: 0.0190
Epoch 12/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0335 - val_loss: 0.0201
Epoch 13/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0327 - val_loss: 0.0192
Epoch 14/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0322 - val_loss: 0.0181
Epoch 15/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0317 - val_loss: 0.0177
Epoch 16/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0312 - val_loss: 0.0171
Epoch 17/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0308 - val_loss: 0.0165
Epoch 18/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0304 - val_loss: 0.0161
Epoch 19/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0301 - val_loss: 0.0157
Epoch 20/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0300 - val_loss: 0.0166
Epoch 21/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0293 - val_loss: 0.0153
Epoch 22/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0291 - val_loss: 0.0151
Epoch 23/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0289 - val_loss: 0.0149
Epoch 24/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0287 - val_loss: 0.0148
Epoch 25/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0285 - val_loss: 0.0148
Epoch 26/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0283 - val_loss: 0.0148
Epoch 27/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0282 - val_loss: 0.0148
Epoch 28/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0147
Epoch 29/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0279 - val_loss: 0.0148
Epoch 30/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0278 - val_loss: 0.0148
Epoch 31/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0276 - val_loss: 0.0148
Epoch 32/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0275 - val_loss: 0.0147
Epoch 33/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0274 - val_loss: 0.0147
Epoch 34/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0274 - val_loss: 0.0147
Epoch 35/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0273 - val_loss: 0.0146
Epoch 36/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0146
Epoch 37/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0272 - val_loss: 0.0146
Epoch 38/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0271 - val_loss: 0.0146
Epoch 39/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0271 - val_loss: 0.0146
Epoch 40/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0270 - val_loss: 0.0146
Epoch 41/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0270 - val_loss: 0.0146
Epoch 42/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0270 - val_loss: 0.0146
Epoch 43/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0269 - val_loss: 0.0146
Epoch 44/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0269 - val_loss: 0.0145
Epoch 45/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0269 - val_loss: 0.0147
Epoch 46/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0268 - val_loss: 0.0146
Epoch 47/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0268 - val_loss: 0.0147
Epoch 48/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0267 - val_loss: 0.0146
Epoch 49/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0268 - val_loss: 0.0149
Epoch 50/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 51/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 52/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 53/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 54/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 55/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 56/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 57/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 58/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 59/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 60/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 61/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 62/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 63/80
92/92 [==============================] - 2s 18ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 64/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 65/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 66/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 67/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 68/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 69/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 70/80
92/92 [==============================] - 2s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 71/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 72/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 73/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 74/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 75/80
92/92 [==============================] - 2s 17ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 76/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0266 - val_loss: 0.0148
Epoch 77/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0148
Epoch 78/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 79/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.0149
Epoch 80/80
92/92 [==============================] - 1s 16ms/step - loss: 0.0266 - val_loss: 0.0148
Execution time:  122.94246935844421
DNN:
Mean Absolute Error: 0.0251
Root Mean Square Error: 0.0429
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_214&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_642 (Dense)            (None, 432, 80)           160       
_________________________________________________________________
dense_643 (Dense)            (None, 432, 16)           1296      
_________________________________________________________________
dropout_214 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_644 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
114/114 [==============================] - 2s 14ms/step - loss: 0.0369 - val_loss: 0.0127
Epoch 2/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0359 - val_loss: 0.0127
Epoch 3/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0356 - val_loss: 0.0128
Epoch 4/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0353 - val_loss: 0.0128
Epoch 5/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0350 - val_loss: 0.0130
Epoch 6/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0347 - val_loss: 0.0132
Epoch 7/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0343 - val_loss: 0.0126
Epoch 8/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0341 - val_loss: 0.0130
Epoch 9/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0337 - val_loss: 0.0128
Epoch 10/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0335 - val_loss: 0.0132
Epoch 11/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0337 - val_loss: 0.0129
Epoch 12/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0329 - val_loss: 0.0122
Epoch 13/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0329 - val_loss: 0.0129
Epoch 14/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0326 - val_loss: 0.0130
Epoch 15/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0324 - val_loss: 0.0130
Epoch 16/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0322 - val_loss: 0.0129
Epoch 17/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0320 - val_loss: 0.0125
Epoch 18/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0319 - val_loss: 0.0126
Epoch 19/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0316 - val_loss: 0.0127
Epoch 20/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0314 - val_loss: 0.0127
Epoch 21/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0313 - val_loss: 0.0126
Epoch 22/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0310 - val_loss: 0.0127
Epoch 23/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0309 - val_loss: 0.0128
Epoch 24/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0308 - val_loss: 0.0127
Epoch 25/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0305 - val_loss: 0.0121
Epoch 26/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0307 - val_loss: 0.0128
Epoch 27/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0304 - val_loss: 0.0127
Epoch 28/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0305 - val_loss: 0.0127
Epoch 29/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0302 - val_loss: 0.0127
Epoch 30/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0301 - val_loss: 0.0128
Epoch 31/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0301 - val_loss: 0.0131
Epoch 32/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0298 - val_loss: 0.0128
Epoch 33/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0298 - val_loss: 0.0129
Epoch 34/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0296 - val_loss: 0.0129
Epoch 35/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0296 - val_loss: 0.0129
Epoch 36/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0293 - val_loss: 0.0119
Epoch 37/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0295 - val_loss: 0.0129
Epoch 38/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0293 - val_loss: 0.0130
Epoch 39/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0292 - val_loss: 0.0130
Epoch 40/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0292 - val_loss: 0.0136
Epoch 41/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0289 - val_loss: 0.0132
Epoch 42/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0289 - val_loss: 0.0133
Epoch 43/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0289 - val_loss: 0.0137
Epoch 44/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0287 - val_loss: 0.0133
Epoch 45/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0286 - val_loss: 0.0133
Epoch 46/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0287 - val_loss: 0.0137
Epoch 47/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0285 - val_loss: 0.0135
Epoch 48/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0285 - val_loss: 0.0135
Epoch 49/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.0135
Epoch 50/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.0135
Epoch 51/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0136
Epoch 52/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0283 - val_loss: 0.0135
Epoch 53/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0136
Epoch 54/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0282 - val_loss: 0.0136
Epoch 55/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0136
Epoch 56/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0281 - val_loss: 0.0137
Epoch 57/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0281 - val_loss: 0.0137
Epoch 58/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0281 - val_loss: 0.0137
Epoch 59/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - val_loss: 0.0139
Epoch 60/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0279 - val_loss: 0.0139
Epoch 61/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0279 - val_loss: 0.0139
Epoch 62/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0278 - val_loss: 0.0139
Epoch 63/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0278 - val_loss: 0.0139
Epoch 64/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0278 - val_loss: 0.0139
Epoch 65/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0278 - val_loss: 0.0140
Epoch 66/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 67/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 68/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 69/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0277 - val_loss: 0.0140
Epoch 70/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 71/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 72/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 73/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 74/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 75/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 76/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0140
Epoch 77/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0276 - val_loss: 0.0141
Epoch 78/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0140
Epoch 79/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0141
Epoch 80/90
114/114 [==============================] - 2s 13ms/step - loss: 0.0275 - val_loss: 0.0140
Epoch 81/90
114/114 [==============================] - 1s 13ms/step - loss: 0.0275 - val_loss: 0.0141
Epoch 82/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0140
Epoch 83/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0141
Epoch 84/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0274 - val_loss: 0.0140
Epoch 85/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0140
Epoch 86/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0274 - val_loss: 0.0140
Epoch 87/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0274 - val_loss: 0.0141
Epoch 88/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0136
Epoch 89/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0275 - val_loss: 0.0138
Epoch 90/90
114/114 [==============================] - 1s 12ms/step - loss: 0.0274 - val_loss: 0.0140
Execution time:  131.45213794708252
DNN:
Mean Absolute Error: 0.0252
Root Mean Square Error: 0.0437
Mean Square Error: 0.0019

Train RMSE: 0.044
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  3d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_215&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_645 (Dense)            (None, 432, 12)           24        
_________________________________________________________________
dense_646 (Dense)            (None, 432, 16)           208       
_________________________________________________________________
dropout_215 (Dropout)        (None, 432, 16)           0         
_________________________________________________________________
dense_647 (Dense)            (None, 432, 1)            17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
50/50 [==============================] - 1s 26ms/step - loss: 0.1071 - val_loss: 0.0870
Epoch 2/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0537 - val_loss: 0.0442
Epoch 3/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0446 - val_loss: 0.0356
Epoch 4/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0424 - val_loss: 0.0344
Epoch 5/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0409 - val_loss: 0.0332
Epoch 6/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0397 - val_loss: 0.0322
Epoch 7/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0386 - val_loss: 0.0314
Epoch 8/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0377 - val_loss: 0.0301
Epoch 9/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0370 - val_loss: 0.0290
Epoch 10/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0363 - val_loss: 0.0282
Epoch 11/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0358 - val_loss: 0.0275
Epoch 12/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0353 - val_loss: 0.0270
Epoch 13/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0348 - val_loss: 0.0264
Epoch 14/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0343 - val_loss: 0.0258
Epoch 15/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0339 - val_loss: 0.0253
Epoch 16/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0334 - val_loss: 0.0247
Epoch 17/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0330 - val_loss: 0.0242
Epoch 18/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0326 - val_loss: 0.0236
Epoch 19/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0323 - val_loss: 0.0231
Epoch 20/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0319 - val_loss: 0.0226
Epoch 21/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0316 - val_loss: 0.0222
Epoch 22/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0313 - val_loss: 0.0219
Epoch 23/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0311 - val_loss: 0.0216
Epoch 24/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0308 - val_loss: 0.0215
Epoch 25/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0305 - val_loss: 0.0212
Epoch 26/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0303 - val_loss: 0.0209
Epoch 27/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0300 - val_loss: 0.0208
Epoch 28/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0298 - val_loss: 0.0206
Epoch 29/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0296 - val_loss: 0.0201
Epoch 30/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0294 - val_loss: 0.0197
Epoch 31/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0292 - val_loss: 0.0192
Epoch 32/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0291 - val_loss: 0.0188
Epoch 33/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0289 - val_loss: 0.0185
Epoch 34/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0288 - val_loss: 0.0181
Epoch 35/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0287 - val_loss: 0.0178
Epoch 36/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0285 - val_loss: 0.0175
Epoch 37/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0284 - val_loss: 0.0173
Epoch 38/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0283 - val_loss: 0.0171
Epoch 39/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0282 - val_loss: 0.0168
Epoch 40/52
50/50 [==============================] - 1s 17ms/step - loss: 0.0281 - val_loss: 0.0167
Epoch 41/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0166
Epoch 42/52
50/50 [==============================] - 1s 16ms/step - loss: 0.0280 - val_loss: 0.0164
Epoch 43/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0279 - val_loss: 0.0162
Epoch 44/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0278 - val_loss: 0.0160
Epoch 45/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0278 - val_loss: 0.0158
Epoch 46/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0277 - val_loss: 0.0156
Epoch 47/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0276 - val_loss: 0.0154
Epoch 48/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0276 - val_loss: 0.0152
Epoch 49/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0275 - val_loss: 0.0151
Epoch 50/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0275 - val_loss: 0.0151
Epoch 51/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0274 - val_loss: 0.0151
Epoch 52/52
50/50 [==============================] - 1s 15ms/step - loss: 0.0274 - val_loss: 0.0150
Execution time:  42.00743246078491
DNN:
Mean Absolute Error: 0.0250
Root Mean Square Error: 0.0426
Mean Square Error: 0.0018

Train RMSE: 0.043
Train MSE: 0.002
Train MAE: 0.025
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_216&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_648 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_649 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_216 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_650 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
87/87 [==============================] - 3s 37ms/step - loss: 0.1542 - val_loss: 0.0325
Epoch 2/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0914 - val_loss: 0.0081
Epoch 3/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0808 - val_loss: 0.0068
Epoch 4/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0708 - val_loss: 0.0069
Epoch 5/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0606 - val_loss: 0.0103
Epoch 6/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0509 - val_loss: 0.0132
Epoch 7/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0421 - val_loss: 0.0181
Epoch 8/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0354 - val_loss: 0.0190
Epoch 9/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0302 - val_loss: 0.0238
Epoch 10/80
87/87 [==============================] - 3s 39ms/step - loss: 0.0282 - val_loss: 0.0203
Epoch 11/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0269 - val_loss: 0.0138
Epoch 12/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0255 - val_loss: 0.0111
Epoch 13/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0245 - val_loss: 0.0111
Epoch 14/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0240 - val_loss: 0.0120
Epoch 15/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0239 - val_loss: 0.0129
Epoch 16/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0238 - val_loss: 0.0127
Epoch 17/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0237 - val_loss: 0.0128
Epoch 18/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0237 - val_loss: 0.0131
Epoch 19/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0237 - val_loss: 0.0130
Epoch 20/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0132
Epoch 21/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0137
Epoch 22/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0236 - val_loss: 0.0133
Epoch 23/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0236 - val_loss: 0.0134
Epoch 24/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0135
Epoch 25/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0136
Epoch 26/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0133
Epoch 27/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0236 - val_loss: 0.0133
Epoch 28/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0236 - val_loss: 0.0134
Epoch 29/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0135
Epoch 30/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0136
Epoch 31/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0132
Epoch 32/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 33/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 34/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 35/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 36/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 37/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 38/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 39/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 40/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 41/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 42/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 43/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 44/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 45/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 46/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 47/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 48/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 49/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 50/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 51/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 52/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 53/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 54/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 55/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 56/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 57/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 58/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 59/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 60/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 61/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 62/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 63/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 64/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 65/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 66/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 67/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 68/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 69/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 70/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 71/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 72/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 73/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 74/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 75/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 76/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0134
Epoch 77/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0135
Epoch 78/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0133
Epoch 79/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0235 - val_loss: 0.0136
Epoch 80/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0133
Execution time:  253.01840329170227
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0468
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_217&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_651 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_652 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_217 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_653 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
108/108 [==============================] - 3s 29ms/step - loss: 0.1233 - val_loss: 0.0232
Epoch 2/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0529 - val_loss: 0.0153
Epoch 3/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0498 - val_loss: 0.0090
Epoch 4/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0470 - val_loss: 0.0061
Epoch 5/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0441 - val_loss: 0.0062
Epoch 6/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0418 - val_loss: 0.0071
Epoch 7/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0397 - val_loss: 0.0081
Epoch 8/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0376 - val_loss: 0.0078
Epoch 9/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0359 - val_loss: 0.0082
Epoch 10/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0343 - val_loss: 0.0092
Epoch 11/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0323 - val_loss: 0.0094
Epoch 12/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0320 - val_loss: 0.0051
Epoch 13/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0311 - val_loss: 0.0084
Epoch 14/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0301 - val_loss: 0.0110
Epoch 15/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0290 - val_loss: 0.0130
Epoch 16/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0286 - val_loss: 0.0172
Epoch 17/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0281 - val_loss: 0.0211
Epoch 18/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0276 - val_loss: 0.0231
Epoch 19/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0272 - val_loss: 0.0144
Epoch 20/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0267 - val_loss: 0.0234
Epoch 21/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0266 - val_loss: 0.0227
Epoch 22/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0263 - val_loss: 0.0192
Epoch 23/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0259 - val_loss: 0.0227
Epoch 24/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0256 - val_loss: 0.0212
Epoch 25/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0255 - val_loss: 0.0193
Epoch 26/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0254 - val_loss: 0.0182
Epoch 27/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0252 - val_loss: 0.0175
Epoch 28/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0251 - val_loss: 0.0163
Epoch 29/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0249 - val_loss: 0.0158
Epoch 30/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0248 - val_loss: 0.0150
Epoch 31/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0247 - val_loss: 0.0141
Epoch 32/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0246 - val_loss: 0.0136
Epoch 33/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0244 - val_loss: 0.0134
Epoch 34/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0244 - val_loss: 0.0131
Epoch 35/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0244 - val_loss: 0.0124
Epoch 36/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0243 - val_loss: 0.0125
Epoch 37/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0243 - val_loss: 0.0122
Epoch 38/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0242 - val_loss: 0.0120
Epoch 39/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0241 - val_loss: 0.0123
Epoch 40/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0241 - val_loss: 0.0121
Epoch 41/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0239 - val_loss: 0.0117
Epoch 42/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0238 - val_loss: 0.0118
Epoch 43/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0237 - val_loss: 0.0115
Epoch 44/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0237 - val_loss: 0.0115
Epoch 45/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0237 - val_loss: 0.0113
Epoch 46/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0239 - val_loss: 0.0171
Epoch 47/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0238 - val_loss: 0.0112
Epoch 48/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0113
Epoch 49/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0120
Epoch 50/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0237 - val_loss: 0.0119
Epoch 51/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0237 - val_loss: 0.0124
Epoch 52/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0118
Epoch 53/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0123
Epoch 54/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0118
Epoch 55/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0122
Epoch 56/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0235 - val_loss: 0.0118
Epoch 57/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 58/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0118
Epoch 59/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0120
Epoch 60/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 61/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 62/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 63/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 64/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 65/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 66/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 67/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 68/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 69/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 70/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 71/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 72/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 73/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 74/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0120
Epoch 75/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 76/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 77/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0235 - val_loss: 0.0120
Epoch 78/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 79/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 80/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0119
Epoch 81/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0121
Epoch 82/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 83/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0122
Epoch 84/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0122
Epoch 85/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 86/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 87/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0122
Epoch 88/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0122
Epoch 89/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0121
Epoch 90/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0121
Execution time:  273.2143096923828
DNN:
Mean Absolute Error: 0.0261
Root Mean Square Error: 0.0474
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_218&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_654 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_655 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_218 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_656 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
48/48 [==============================] - 2s 33ms/step - loss: 0.3213 - val_loss: 0.1849
Epoch 2/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1496 - val_loss: 0.0079
Epoch 3/52
48/48 [==============================] - 2s 32ms/step - loss: 0.1076 - val_loss: 0.0182
Epoch 4/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0948 - val_loss: 0.0062
Epoch 5/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0835 - val_loss: 0.0177
Epoch 6/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0776 - val_loss: 0.0159
Epoch 7/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0722 - val_loss: 0.0145
Epoch 8/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0668 - val_loss: 0.0131
Epoch 9/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0613 - val_loss: 0.0118
Epoch 10/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0560 - val_loss: 0.0096
Epoch 11/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0509 - val_loss: 0.0072
Epoch 12/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0460 - val_loss: 0.0053
Epoch 13/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0413 - val_loss: 0.0049
Epoch 14/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0370 - val_loss: 0.0061
Epoch 15/52
48/48 [==============================] - 2s 31ms/step - loss: 0.0334 - val_loss: 0.0071
Epoch 16/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0307 - val_loss: 0.0074
Epoch 17/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0285 - val_loss: 0.0079
Epoch 18/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0265 - val_loss: 0.0092
Epoch 19/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0253 - val_loss: 0.0123
Epoch 20/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0253 - val_loss: 0.0140
Epoch 21/52
48/48 [==============================] - 1s 29ms/step - loss: 0.0242 - val_loss: 0.0158
Epoch 22/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0237 - val_loss: 0.0152
Epoch 23/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0234 - val_loss: 0.0147
Epoch 24/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0233 - val_loss: 0.0146
Epoch 25/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0233 - val_loss: 0.0145
Epoch 26/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0233 - val_loss: 0.0145
Epoch 27/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0233 - val_loss: 0.0144
Epoch 28/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0233 - val_loss: 0.0144
Epoch 29/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0233 - val_loss: 0.0143
Epoch 30/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0144
Epoch 31/52
48/48 [==============================] - 1s 29ms/step - loss: 0.0232 - val_loss: 0.0140
Epoch 32/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 33/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 34/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 35/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 36/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 37/52
48/48 [==============================] - 2s 31ms/step - loss: 0.0232 - val_loss: 0.0141
Epoch 38/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 39/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0141
Epoch 40/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 41/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 42/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0140
Epoch 43/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0142
Epoch 44/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0140
Epoch 45/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0141
Epoch 46/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0231 - val_loss: 0.0139
Epoch 47/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0232 - val_loss: 0.0141
Epoch 48/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0141
Epoch 49/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0139
Epoch 50/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0141
Epoch 51/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0139
Epoch 52/52
48/48 [==============================] - 1s 29ms/step - loss: 0.0231 - val_loss: 0.0140
Execution time:  78.57191014289856
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0465
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_219&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_657 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_658 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_219 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_659 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1639 - val_loss: 0.0417
Epoch 2/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0897 - val_loss: 0.0248
Epoch 3/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0806 - val_loss: 0.0174
Epoch 4/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0728 - val_loss: 0.0114
Epoch 5/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0621 - val_loss: 0.0134
Epoch 6/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0557 - val_loss: 0.0132
Epoch 7/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0499 - val_loss: 0.0137
Epoch 8/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0445 - val_loss: 0.0130
Epoch 9/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0401 - val_loss: 0.0131
Epoch 10/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0368 - val_loss: 0.0134
Epoch 11/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0343 - val_loss: 0.0107
Epoch 12/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0320 - val_loss: 0.0103
Epoch 13/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0303 - val_loss: 0.0104
Epoch 14/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0294 - val_loss: 0.0103
Epoch 15/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0272 - val_loss: 0.0102
Epoch 16/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0264 - val_loss: 0.0102
Epoch 17/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0260 - val_loss: 0.0115
Epoch 18/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0258 - val_loss: 0.0115
Epoch 19/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0257 - val_loss: 0.0119
Epoch 20/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0256 - val_loss: 0.0119
Epoch 21/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0255 - val_loss: 0.0120
Epoch 22/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0254 - val_loss: 0.0121
Epoch 23/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0254 - val_loss: 0.0122
Epoch 24/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0253 - val_loss: 0.0122
Epoch 25/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0253 - val_loss: 0.0123
Epoch 26/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0123
Epoch 27/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0123
Epoch 28/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0252 - val_loss: 0.0122
Epoch 29/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0123
Epoch 30/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0252 - val_loss: 0.0123
Epoch 31/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0123
Epoch 32/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0121
Epoch 33/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0122
Epoch 34/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0252 - val_loss: 0.0122
Epoch 35/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0123
Epoch 36/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 37/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0252 - val_loss: 0.0121
Epoch 38/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 39/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 40/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 41/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 42/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 43/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 44/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 45/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 46/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 47/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 48/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 49/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 50/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 51/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 52/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 53/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 54/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 55/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 56/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 57/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 58/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 59/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 60/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 61/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 62/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 63/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 64/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 65/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 66/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 67/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 68/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 69/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 70/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 71/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 72/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 73/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 74/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 75/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 76/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 77/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 78/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0123
Epoch 79/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 80/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Execution time:  232.92282390594482
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0465
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_220&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_660 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_661 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_220 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_662 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0968 - val_loss: 0.0135
Epoch 2/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0512 - val_loss: 0.0175
Epoch 3/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0479 - val_loss: 0.0198
Epoch 4/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0454 - val_loss: 0.0228
Epoch 5/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0434 - val_loss: 0.0243
Epoch 6/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0418 - val_loss: 0.0238
Epoch 7/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0404 - val_loss: 0.0231
Epoch 8/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0392 - val_loss: 0.0241
Epoch 9/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0381 - val_loss: 0.0222
Epoch 10/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0371 - val_loss: 0.0225
Epoch 11/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0362 - val_loss: 0.0212
Epoch 12/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0355 - val_loss: 0.0202
Epoch 13/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0346 - val_loss: 0.0184
Epoch 14/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0340 - val_loss: 0.0176
Epoch 15/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0333 - val_loss: 0.0169
Epoch 16/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0326 - val_loss: 0.0163
Epoch 17/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0321 - val_loss: 0.0167
Epoch 18/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0317 - val_loss: 0.0144
Epoch 19/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0316 - val_loss: 0.0127
Epoch 20/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0306 - val_loss: 0.0173
Epoch 21/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0315 - val_loss: 0.0181
Epoch 22/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0303 - val_loss: 0.0154
Epoch 23/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0302 - val_loss: 0.0131
Epoch 24/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0298 - val_loss: 0.0117
Epoch 25/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0295 - val_loss: 0.0106
Epoch 26/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0290 - val_loss: 0.0104
Epoch 27/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0286 - val_loss: 0.0104
Epoch 28/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0284 - val_loss: 0.0104
Epoch 29/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0281 - val_loss: 0.0103
Epoch 30/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0277 - val_loss: 0.0104
Epoch 31/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0276 - val_loss: 0.0102
Epoch 32/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0273 - val_loss: 0.0103
Epoch 33/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0271 - val_loss: 0.0103
Epoch 34/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0269 - val_loss: 0.0103
Epoch 35/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0268 - val_loss: 0.0103
Epoch 36/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0266 - val_loss: 0.0103
Epoch 37/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0265 - val_loss: 0.0103
Epoch 38/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0264 - val_loss: 0.0102
Epoch 39/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0264 - val_loss: 0.0101
Epoch 40/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0263 - val_loss: 0.0102
Epoch 41/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0262 - val_loss: 0.0102
Epoch 42/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0261 - val_loss: 0.0102
Epoch 43/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0260 - val_loss: 0.0102
Epoch 44/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0260 - val_loss: 0.0101
Epoch 45/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0259 - val_loss: 0.0101
Epoch 46/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0101
Epoch 47/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0101
Epoch 48/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0257 - val_loss: 0.0102
Epoch 49/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0257 - val_loss: 0.0101
Epoch 50/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0258 - val_loss: 0.0102
Epoch 51/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0257 - val_loss: 0.0102
Epoch 52/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0256 - val_loss: 0.0103
Epoch 53/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0256 - val_loss: 0.0104
Epoch 54/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0255 - val_loss: 0.0105
Epoch 55/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0102
Epoch 56/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0235 - val_loss: 0.0166
Epoch 57/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0274 - val_loss: 0.0107
Epoch 58/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0254 - val_loss: 0.0105
Epoch 59/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0106
Epoch 60/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0108
Epoch 61/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0112
Epoch 62/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0254 - val_loss: 0.0115
Epoch 63/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0254 - val_loss: 0.0113
Epoch 64/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0115
Epoch 65/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0112
Epoch 66/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0116
Epoch 67/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0252 - val_loss: 0.0111
Epoch 68/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0252 - val_loss: 0.0116
Epoch 69/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0112
Epoch 70/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0114
Epoch 71/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0115
Epoch 72/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0112
Epoch 73/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0116
Epoch 74/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0252 - val_loss: 0.0113
Epoch 75/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0114
Epoch 76/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0113
Epoch 77/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0114
Epoch 78/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0112
Epoch 79/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0251 - val_loss: 0.0117
Epoch 80/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0252 - val_loss: 0.0114
Epoch 81/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0115
Epoch 82/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0113
Epoch 83/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0114
Epoch 84/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0113
Epoch 85/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0251 - val_loss: 0.0114
Epoch 86/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0144
Epoch 87/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0255 - val_loss: 0.0107
Epoch 88/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0247 - val_loss: 0.0136
Epoch 89/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0255 - val_loss: 0.0117
Epoch 90/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0252 - val_loss: 0.0114
Execution time:  251.77310180664062
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0471
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_221&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_663 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_664 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_221 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_665 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
42/42 [==============================] - 1s 36ms/step - loss: 0.1956 - val_loss: 0.0539
Epoch 2/52
42/42 [==============================] - 1s 30ms/step - loss: 0.1290 - val_loss: 0.0327
Epoch 3/52
42/42 [==============================] - 1s 31ms/step - loss: 0.1178 - val_loss: 0.0256
Epoch 4/52
42/42 [==============================] - 1s 31ms/step - loss: 0.1073 - val_loss: 0.0132
Epoch 5/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0984 - val_loss: 0.0118
Epoch 6/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0906 - val_loss: 0.0115
Epoch 7/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0828 - val_loss: 0.0113
Epoch 8/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0751 - val_loss: 0.0112
Epoch 9/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0675 - val_loss: 0.0111
Epoch 10/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0604 - val_loss: 0.0110
Epoch 11/52
42/42 [==============================] - 1s 35ms/step - loss: 0.0537 - val_loss: 0.0109
Epoch 12/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0480 - val_loss: 0.0108
Epoch 13/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0429 - val_loss: 0.0112
Epoch 14/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0383 - val_loss: 0.0124
Epoch 15/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0344 - val_loss: 0.0117
Epoch 16/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0317 - val_loss: 0.0114
Epoch 17/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0305 - val_loss: 0.0130
Epoch 18/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0281 - val_loss: 0.0105
Epoch 19/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0269 - val_loss: 0.0155
Epoch 20/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0260 - val_loss: 0.0131
Epoch 21/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0252 - val_loss: 0.0136
Epoch 22/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0249 - val_loss: 0.0132
Epoch 23/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0249 - val_loss: 0.0129
Epoch 24/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0249 - val_loss: 0.0126
Epoch 25/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0249 - val_loss: 0.0124
Epoch 26/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0248 - val_loss: 0.0123
Epoch 27/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0249 - val_loss: 0.0127
Epoch 28/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0248 - val_loss: 0.0126
Epoch 29/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0248 - val_loss: 0.0124
Epoch 30/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0248 - val_loss: 0.0123
Epoch 31/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0248 - val_loss: 0.0122
Epoch 32/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0248 - val_loss: 0.0127
Epoch 33/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 34/52
42/42 [==============================] - 1s 35ms/step - loss: 0.0247 - val_loss: 0.0124
Epoch 35/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0247 - val_loss: 0.0123
Epoch 36/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0121
Epoch 37/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0248 - val_loss: 0.0126
Epoch 38/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 39/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0124
Epoch 40/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0123
Epoch 41/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0122
Epoch 42/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0247 - val_loss: 0.0121
Epoch 43/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 44/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0124
Epoch 45/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0247 - val_loss: 0.0122
Epoch 46/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0247 - val_loss: 0.0122
Epoch 47/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0247 - val_loss: 0.0121
Epoch 48/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0120
Epoch 49/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 50/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0124
Epoch 51/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0247 - val_loss: 0.0123
Epoch 52/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0246 - val_loss: 0.0123
Execution time:  72.3345251083374
DNN:
Mean Absolute Error: 0.0258
Root Mean Square Error: 0.0461
Mean Square Error: 0.0021

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_222&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_666 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_667 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_222 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_668 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0535 - val_loss: 0.0263
Epoch 2/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0369 - val_loss: 0.0235
Epoch 3/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0354 - val_loss: 0.0232
Epoch 4/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0342 - val_loss: 0.0225
Epoch 5/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0332 - val_loss: 0.0214
Epoch 6/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0322 - val_loss: 0.0207
Epoch 7/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0313 - val_loss: 0.0201
Epoch 8/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0301 - val_loss: 0.0235
Epoch 9/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0296 - val_loss: 0.0171
Epoch 10/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0289 - val_loss: 0.0171
Epoch 11/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0282 - val_loss: 0.0171
Epoch 12/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0276 - val_loss: 0.0167
Epoch 13/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0271 - val_loss: 0.0163
Epoch 14/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0266 - val_loss: 0.0168
Epoch 15/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0262 - val_loss: 0.0160
Epoch 16/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0259 - val_loss: 0.0161
Epoch 17/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0256 - val_loss: 0.0162
Epoch 18/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0253 - val_loss: 0.0163
Epoch 19/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0250 - val_loss: 0.0163
Epoch 20/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0248 - val_loss: 0.0164
Epoch 21/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0246 - val_loss: 0.0164
Epoch 22/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0244 - val_loss: 0.0163
Epoch 23/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0162
Epoch 24/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0241 - val_loss: 0.0162
Epoch 25/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0240 - val_loss: 0.0162
Epoch 26/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0239 - val_loss: 0.0161
Epoch 27/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0238 - val_loss: 0.0161
Epoch 28/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0237 - val_loss: 0.0161
Epoch 29/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0161
Epoch 30/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0160
Epoch 31/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0159
Epoch 32/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0158
Epoch 33/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0158
Epoch 34/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0233 - val_loss: 0.0158
Epoch 35/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0233 - val_loss: 0.0157
Epoch 36/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0233 - val_loss: 0.0157
Epoch 37/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0232 - val_loss: 0.0156
Epoch 38/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0232 - val_loss: 0.0156
Epoch 39/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0232 - val_loss: 0.0155
Epoch 40/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0231 - val_loss: 0.0154
Epoch 41/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0231 - val_loss: 0.0153
Epoch 42/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0231 - val_loss: 0.0152
Epoch 43/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0231 - val_loss: 0.0151
Epoch 44/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0230 - val_loss: 0.0150
Epoch 45/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0230 - val_loss: 0.0149
Epoch 46/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0230 - val_loss: 0.0148
Epoch 47/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0230 - val_loss: 0.0146
Epoch 48/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0230 - val_loss: 0.0145
Epoch 49/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0229 - val_loss: 0.0144
Epoch 50/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0229 - val_loss: 0.0142
Epoch 51/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0229 - val_loss: 0.0141
Epoch 52/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0229 - val_loss: 0.0140
Epoch 53/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0229 - val_loss: 0.0139
Epoch 54/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0228 - val_loss: 0.0137
Epoch 55/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0136
Epoch 56/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0135
Epoch 57/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0134
Epoch 58/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0228 - val_loss: 0.0133
Epoch 59/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0228 - val_loss: 0.0132
Epoch 60/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0131
Epoch 61/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0129
Epoch 62/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0227 - val_loss: 0.0128
Epoch 63/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0227 - val_loss: 0.0127
Epoch 64/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0227 - val_loss: 0.0127
Epoch 65/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0126
Epoch 66/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0126
Epoch 67/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0227 - val_loss: 0.0125
Epoch 68/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0226 - val_loss: 0.0125
Epoch 69/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0226 - val_loss: 0.0124
Epoch 70/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0124
Epoch 71/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0123
Epoch 72/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0126
Epoch 73/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0226 - val_loss: 0.0124
Epoch 74/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0226 - val_loss: 0.0123
Epoch 75/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0125
Epoch 76/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0124
Epoch 77/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0122
Epoch 78/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0226 - val_loss: 0.0126
Epoch 79/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0226 - val_loss: 0.0121
Epoch 80/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0126
Execution time:  252.10885977745056
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0472
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_223&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_669 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_670 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_223 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_671 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0456 - val_loss: 0.0247
Epoch 2/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0289 - val_loss: 0.0234
Epoch 3/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0288 - val_loss: 0.0233
Epoch 4/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0283 - val_loss: 0.0224
Epoch 5/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0281 - val_loss: 0.0215
Epoch 6/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0279 - val_loss: 0.0211
Epoch 7/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0276 - val_loss: 0.0205
Epoch 8/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0274 - val_loss: 0.0201
Epoch 9/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0272 - val_loss: 0.0194
Epoch 10/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0270 - val_loss: 0.0191
Epoch 11/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0268 - val_loss: 0.0189
Epoch 12/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0266 - val_loss: 0.0183
Epoch 13/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0255 - val_loss: 0.0234
Epoch 14/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0265 - val_loss: 0.0194
Epoch 15/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0262 - val_loss: 0.0180
Epoch 16/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0260 - val_loss: 0.0172
Epoch 17/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0169
Epoch 18/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0257 - val_loss: 0.0163
Epoch 19/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0256 - val_loss: 0.0160
Epoch 20/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0154
Epoch 21/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0154
Epoch 22/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0252 - val_loss: 0.0155
Epoch 23/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0251 - val_loss: 0.0150
Epoch 24/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0148
Epoch 25/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0249 - val_loss: 0.0149
Epoch 26/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0248 - val_loss: 0.0144
Epoch 27/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0248 - val_loss: 0.0146
Epoch 28/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0247 - val_loss: 0.0147
Epoch 29/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0246 - val_loss: 0.0146
Epoch 30/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0245 - val_loss: 0.0147
Epoch 31/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0244 - val_loss: 0.0147
Epoch 32/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0243 - val_loss: 0.0147
Epoch 33/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0242 - val_loss: 0.0149
Epoch 34/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0242 - val_loss: 0.0150
Epoch 35/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0241 - val_loss: 0.0151
Epoch 36/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0240 - val_loss: 0.0151
Epoch 37/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0240 - val_loss: 0.0152
Epoch 38/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0239 - val_loss: 0.0150
Epoch 39/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0239 - val_loss: 0.0150
Epoch 40/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0239 - val_loss: 0.0149
Epoch 41/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0239 - val_loss: 0.0147
Epoch 42/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0238 - val_loss: 0.0146
Epoch 43/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0238 - val_loss: 0.0146
Epoch 44/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0238 - val_loss: 0.0146
Epoch 45/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0237 - val_loss: 0.0145
Epoch 46/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0237 - val_loss: 0.0146
Epoch 47/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0145
Epoch 48/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0145
Epoch 49/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0145
Epoch 50/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0145
Epoch 51/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0235 - val_loss: 0.0145
Epoch 52/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0145
Epoch 53/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0145
Epoch 54/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0145
Epoch 55/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0145
Epoch 56/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0234 - val_loss: 0.0145
Epoch 57/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0226 - val_loss: 0.0205
Epoch 58/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0237 - val_loss: 0.0137
Epoch 59/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0149
Epoch 60/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0145
Epoch 61/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0145
Epoch 62/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0144
Epoch 63/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0144
Epoch 64/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0143
Epoch 65/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0144
Epoch 66/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0144
Epoch 67/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0144
Epoch 68/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0232 - val_loss: 0.0145
Epoch 69/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0232 - val_loss: 0.0145
Epoch 70/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0146
Epoch 71/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0232 - val_loss: 0.0145
Epoch 72/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0232 - val_loss: 0.0147
Epoch 73/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0146
Epoch 74/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0232 - val_loss: 0.0146
Epoch 75/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0147
Epoch 76/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0147
Epoch 77/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0232 - val_loss: 0.0146
Epoch 78/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0148
Epoch 79/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0232 - val_loss: 0.0145
Epoch 80/90
108/108 [==============================] - 3s 30ms/step - loss: 0.0232 - val_loss: 0.0147
Epoch 81/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0231 - val_loss: 0.0144
Epoch 82/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0231 - val_loss: 0.0145
Epoch 83/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0231 - val_loss: 0.0143
Epoch 84/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0231 - val_loss: 0.0143
Epoch 85/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0231 - val_loss: 0.0141
Epoch 86/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0231 - val_loss: 0.0141
Epoch 87/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0230 - val_loss: 0.0140
Epoch 88/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0230 - val_loss: 0.0139
Epoch 89/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0230 - val_loss: 0.0138
Epoch 90/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0230 - val_loss: 0.0138
Execution time:  274.5630130767822
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0467
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_224&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_672 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_673 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_224 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_674 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0561 - val_loss: 0.0374
Epoch 2/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0436 - val_loss: 0.0380
Epoch 3/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0408 - val_loss: 0.0347
Epoch 4/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0388 - val_loss: 0.0323
Epoch 5/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0371 - val_loss: 0.0290
Epoch 6/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0357 - val_loss: 0.0283
Epoch 7/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0347 - val_loss: 0.0275
Epoch 8/52
48/48 [==============================] - 1s 29ms/step - loss: 0.0339 - val_loss: 0.0271
Epoch 9/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0332 - val_loss: 0.0266
Epoch 10/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0325 - val_loss: 0.0261
Epoch 11/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0319 - val_loss: 0.0256
Epoch 12/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0310 - val_loss: 0.0205
Epoch 13/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0307 - val_loss: 0.0217
Epoch 14/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0302 - val_loss: 0.0224
Epoch 15/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0296 - val_loss: 0.0223
Epoch 16/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0291 - val_loss: 0.0220
Epoch 17/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0287 - val_loss: 0.0218
Epoch 18/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0283 - val_loss: 0.0216
Epoch 19/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0279 - val_loss: 0.0213
Epoch 20/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0275 - val_loss: 0.0210
Epoch 21/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0272 - val_loss: 0.0206: 0.027
Epoch 22/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0269 - val_loss: 0.0204
Epoch 23/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0266 - val_loss: 0.0201
Epoch 24/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0263 - val_loss: 0.0199
Epoch 25/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0261 - val_loss: 0.0196
Epoch 26/52
48/48 [==============================] - 1s 29ms/step - loss: 0.0258 - val_loss: 0.0194
Epoch 27/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0261 - val_loss: 0.0187
Epoch 28/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0254 - val_loss: 0.0189
Epoch 29/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0252 - val_loss: 0.0188
Epoch 30/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0251 - val_loss: 0.0186
Epoch 31/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0249 - val_loss: 0.0184
Epoch 32/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0247 - val_loss: 0.0182
Epoch 33/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0246 - val_loss: 0.0180
Epoch 34/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0244 - val_loss: 0.0178
Epoch 35/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0243 - val_loss: 0.0177
Epoch 36/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0242 - val_loss: 0.0176
Epoch 37/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0241 - val_loss: 0.0175
Epoch 38/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0240 - val_loss: 0.0174
Epoch 39/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0239 - val_loss: 0.0173
Epoch 40/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0238 - val_loss: 0.0172
Epoch 41/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0237 - val_loss: 0.0171
Epoch 42/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0237 - val_loss: 0.0171
Epoch 43/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0236 - val_loss: 0.0170
Epoch 44/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0235 - val_loss: 0.0169
Epoch 45/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0235 - val_loss: 0.0168
Epoch 46/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0234 - val_loss: 0.0167
Epoch 47/52
48/48 [==============================] - ETA: 0s - loss: 0.023 - 2s 32ms/step - loss: 0.0234 - val_loss: 0.0166
Epoch 48/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0233 - val_loss: 0.0166
Epoch 49/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0233 - val_loss: 0.0165
Epoch 50/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0233 - val_loss: 0.0164
Epoch 51/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0164
Epoch 52/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0162
Execution time:  78.71421003341675
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0462
Mean Square Error: 0.0021

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_225&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_675 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_676 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_225 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_677 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0649 - val_loss: 0.0238
Epoch 2/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0416 - val_loss: 0.0182
Epoch 3/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0387 - val_loss: 0.0170
Epoch 4/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0370 - val_loss: 0.0164
Epoch 5/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0359 - val_loss: 0.0164
Epoch 6/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0348 - val_loss: 0.0162
Epoch 7/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0338 - val_loss: 0.0162
Epoch 8/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0328 - val_loss: 0.0161
Epoch 9/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0319 - val_loss: 0.0161
Epoch 10/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0312 - val_loss: 0.0159
Epoch 11/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0304 - val_loss: 0.0159
Epoch 12/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0298 - val_loss: 0.0159
Epoch 13/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0292 - val_loss: 0.0158
Epoch 14/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0286 - val_loss: 0.0158
Epoch 15/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0281 - val_loss: 0.0157
Epoch 16/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0277 - val_loss: 0.0156
Epoch 17/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0275 - val_loss: 0.0150
Epoch 18/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0269 - val_loss: 0.0153
Epoch 19/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0266 - val_loss: 0.0151
Epoch 20/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0263 - val_loss: 0.0149
Epoch 21/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0260 - val_loss: 0.0148
Epoch 22/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0258 - val_loss: 0.0147
Epoch 23/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0256 - val_loss: 0.0146
Epoch 24/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0250 - val_loss: 0.0150
Epoch 25/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0254 - val_loss: 0.0144
Epoch 26/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0143
Epoch 27/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0142
Epoch 28/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0250 - val_loss: 0.0141
Epoch 29/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0249 - val_loss: 0.0140
Epoch 30/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0249 - val_loss: 0.0139
Epoch 31/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0248 - val_loss: 0.0138
Epoch 32/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0248 - val_loss: 0.0137
Epoch 33/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0247 - val_loss: 0.0137
Epoch 34/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0247 - val_loss: 0.0136
Epoch 35/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0247 - val_loss: 0.0136
Epoch 36/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0246 - val_loss: 0.0135
Epoch 37/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0246 - val_loss: 0.0135
Epoch 38/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0246 - val_loss: 0.0134
Epoch 39/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0246 - val_loss: 0.0134
Epoch 40/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0245 - val_loss: 0.0133
Epoch 41/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0245 - val_loss: 0.0132
Epoch 42/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0245 - val_loss: 0.0132
Epoch 43/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0245 - val_loss: 0.0131
Epoch 44/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0245 - val_loss: 0.0130
Epoch 45/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0245 - val_loss: 0.0129
Epoch 46/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0243 - val_loss: 0.0121
Epoch 47/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0245 - val_loss: 0.0144
Epoch 48/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0244 - val_loss: 0.0141
Epoch 49/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0137
Epoch 50/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0134
Epoch 51/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0131
Epoch 52/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0243 - val_loss: 0.0130
Epoch 53/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0128
Epoch 54/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0127
Epoch 55/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0126
Epoch 56/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0125
Epoch 57/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0124
Epoch 58/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0242 - val_loss: 0.0123
Epoch 59/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0242 - val_loss: 0.0123
Epoch 60/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0122
Epoch 61/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0121
Epoch 62/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0121
Epoch 63/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0242 - val_loss: 0.0120
Epoch 64/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0120
Epoch 65/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0242 - val_loss: 0.0119
Epoch 66/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0113
Epoch 67/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0124
Epoch 68/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0242 - val_loss: 0.0123
Epoch 69/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0242 - val_loss: 0.0122
Epoch 70/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0122
Epoch 71/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0121
Epoch 72/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0120
Epoch 73/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0119
Epoch 74/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0242 - val_loss: 0.0118
Epoch 75/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0118
Epoch 76/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0117
Epoch 77/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0242 - val_loss: 0.0116
Epoch 78/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0242 - val_loss: 0.0115
Epoch 79/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0242 - val_loss: 0.0115
Epoch 80/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0242 - val_loss: 0.0114
Execution time:  233.09254002571106
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0472
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_226&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_678 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_679 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_226 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_680 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0616 - val_loss: 0.0161
Epoch 2/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0317 - val_loss: 0.0178
Epoch 3/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0315 - val_loss: 0.0164
Epoch 4/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0311 - val_loss: 0.0155
Epoch 5/90
96/96 [==============================] - 3s 32ms/step - loss: 0.0308 - val_loss: 0.0154
Epoch 6/90
96/96 [==============================] - 3s 31ms/step - loss: 0.0305 - val_loss: 0.0154
Epoch 7/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0303 - val_loss: 0.0145
Epoch 8/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0300 - val_loss: 0.0145
Epoch 9/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0297 - val_loss: 0.0144
Epoch 10/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0295 - val_loss: 0.0138
Epoch 11/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0292 - val_loss: 0.0138
Epoch 12/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0290 - val_loss: 0.0138
Epoch 13/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0287 - val_loss: 0.0132
Epoch 14/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0285 - val_loss: 0.0133
Epoch 15/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0283 - val_loss: 0.0133
Epoch 16/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0281 - val_loss: 0.0144
Epoch 17/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0280 - val_loss: 0.0131
Epoch 18/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0277 - val_loss: 0.0133
Epoch 19/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0275 - val_loss: 0.0134
Epoch 20/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0273 - val_loss: 0.0134
Epoch 21/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0271 - val_loss: 0.0132
Epoch 22/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0271 - val_loss: 0.0131
Epoch 23/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0268 - val_loss: 0.0133
Epoch 24/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0267 - val_loss: 0.0133
Epoch 25/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0265 - val_loss: 0.0132
Epoch 26/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0263 - val_loss: 0.0132
Epoch 27/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0262 - val_loss: 0.0132
Epoch 28/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0261 - val_loss: 0.0132
Epoch 29/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0260 - val_loss: 0.0132
Epoch 30/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0258 - val_loss: 0.0132
Epoch 31/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0258 - val_loss: 0.0132
Epoch 32/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0257 - val_loss: 0.0132
Epoch 33/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0256 - val_loss: 0.0132
Epoch 34/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0255 - val_loss: 0.0132
Epoch 35/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0131
Epoch 36/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0130
Epoch 37/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0130
Epoch 38/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0252 - val_loss: 0.0130
Epoch 39/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0130
Epoch 40/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0250 - val_loss: 0.0129
Epoch 41/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0129
Epoch 42/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0249 - val_loss: 0.0129
Epoch 43/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0249 - val_loss: 0.0130
Epoch 44/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0249 - val_loss: 0.0130
Epoch 45/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0248 - val_loss: 0.0130
Epoch 46/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0248 - val_loss: 0.0130
Epoch 47/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0248 - val_loss: 0.0130
Epoch 48/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0248 - val_loss: 0.0130
Epoch 49/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0248 - val_loss: 0.0130
Epoch 50/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0248 - val_loss: 0.0130
Epoch 51/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0248 - val_loss: 0.0129
Epoch 52/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0248 - val_loss: 0.0128
Epoch 53/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0247 - val_loss: 0.0128
Epoch 54/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0247 - val_loss: 0.0127
Epoch 55/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0247 - val_loss: 0.0127
Epoch 56/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0241 - val_loss: 0.0144
Epoch 57/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0256 - val_loss: 0.0127
Epoch 58/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0248 - val_loss: 0.0135
Epoch 59/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0249 - val_loss: 0.0127
Epoch 60/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0247 - val_loss: 0.0126
Epoch 61/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 62/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0246 - val_loss: 0.0124
Epoch 63/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0246 - val_loss: 0.0124
Epoch 64/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0246 - val_loss: 0.0124
Epoch 65/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0246 - val_loss: 0.0123
Epoch 66/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0246 - val_loss: 0.0123
Epoch 67/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0246 - val_loss: 0.0123
Epoch 68/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0246 - val_loss: 0.0123
Epoch 69/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0246 - val_loss: 0.0122
Epoch 70/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0246 - val_loss: 0.0122
Epoch 71/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0245 - val_loss: 0.0122
Epoch 72/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0245 - val_loss: 0.0121
Epoch 73/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0245 - val_loss: 0.0121
Epoch 74/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0245 - val_loss: 0.0121
Epoch 75/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0245 - val_loss: 0.0121
Epoch 76/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0245 - val_loss: 0.0120
Epoch 77/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0245 - val_loss: 0.0120
Epoch 78/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0245 - val_loss: 0.0120
Epoch 79/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0245 - val_loss: 0.0119
Epoch 80/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0245 - val_loss: 0.0119
Epoch 81/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0245 - val_loss: 0.0119
Epoch 82/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0244 - val_loss: 0.0119
Epoch 83/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0244 - val_loss: 0.0118
Epoch 84/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0244 - val_loss: 0.0118
Epoch 85/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0244 - val_loss: 0.0118
Epoch 86/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0244 - val_loss: 0.0118
Epoch 87/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0244 - val_loss: 0.0117
Epoch 88/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0244 - val_loss: 0.0117
Epoch 89/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0244 - val_loss: 0.0117
Epoch 90/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0244 - val_loss: 0.0117
Execution time:  254.30602169036865
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0470
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adam
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_227&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_681 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_682 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_227 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_683 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
42/42 [==============================] - 1s 36ms/step - loss: 0.0528 - val_loss: 0.0425
Epoch 2/52
42/42 [==============================] - 1s 30ms/step - loss: 0.0461 - val_loss: 0.0375
Epoch 3/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0429 - val_loss: 0.0346
Epoch 4/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0405 - val_loss: 0.0323
Epoch 5/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0388 - val_loss: 0.0307
Epoch 6/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0375 - val_loss: 0.0295
Epoch 7/52
42/42 [==============================] - 2s 36ms/step - loss: 0.0366 - val_loss: 0.0286
Epoch 8/52
42/42 [==============================] - 2s 38ms/step - loss: 0.0358 - val_loss: 0.0278
Epoch 9/52
42/42 [==============================] - 2s 40ms/step - loss: 0.0351 - val_loss: 0.0272
Epoch 10/52
42/42 [==============================] - 2s 37ms/step - loss: 0.0345 - val_loss: 0.0266
Epoch 11/52
42/42 [==============================] - 2s 39ms/step - loss: 0.0339 - val_loss: 0.0261
Epoch 12/52
42/42 [==============================] - 2s 36ms/step - loss: 0.0333 - val_loss: 0.0255
Epoch 13/52
42/42 [==============================] - 1s 35ms/step - loss: 0.0328 - val_loss: 0.0250
Epoch 14/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0323 - val_loss: 0.0243
Epoch 15/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0318 - val_loss: 0.0237
Epoch 16/52
42/42 [==============================] - 2s 36ms/step - loss: 0.0313 - val_loss: 0.0232
Epoch 17/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0309 - val_loss: 0.0227
Epoch 18/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0304 - val_loss: 0.0222
Epoch 19/52
42/42 [==============================] - 1s 35ms/step - loss: 0.0300 - val_loss: 0.0217
Epoch 20/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0296 - val_loss: 0.0213
Epoch 21/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0293 - val_loss: 0.0208
Epoch 22/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0289 - val_loss: 0.0203
Epoch 23/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0286 - val_loss: 0.0199
Epoch 24/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0283 - val_loss: 0.0195
Epoch 25/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0280 - val_loss: 0.0192
Epoch 26/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0277 - val_loss: 0.0188
Epoch 27/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0276 - val_loss: 0.0184
Epoch 28/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0273 - val_loss: 0.0182
Epoch 29/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0270 - val_loss: 0.0179
Epoch 30/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0269 - val_loss: 0.0177
Epoch 31/52
42/42 [==============================] - 1s 35ms/step - loss: 0.0267 - val_loss: 0.0174
Epoch 32/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0265 - val_loss: 0.0172
Epoch 33/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0263 - val_loss: 0.0169
Epoch 34/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0262 - val_loss: 0.0167
Epoch 35/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0260 - val_loss: 0.0166
Epoch 36/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0259 - val_loss: 0.0164
Epoch 37/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0258 - val_loss: 0.0163
Epoch 38/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0257 - val_loss: 0.0161
Epoch 39/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0256 - val_loss: 0.0161
Epoch 40/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0254 - val_loss: 0.0159
Epoch 41/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0252 - val_loss: 0.0155
Epoch 42/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0251 - val_loss: 0.0153
Epoch 43/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0251 - val_loss: 0.0153
Epoch 44/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0250 - val_loss: 0.0152
Epoch 45/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0249 - val_loss: 0.0151
Epoch 46/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0249 - val_loss: 0.0151
Epoch 47/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0248 - val_loss: 0.0150
Epoch 48/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0248 - val_loss: 0.0151
Epoch 49/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0247 - val_loss: 0.0148
Epoch 50/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0247 - val_loss: 0.0148
Epoch 51/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0246 - val_loss: 0.0146- ETA: 0s
Epoch 52/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0245 - val_loss: 0.0145
Execution time:  75.68637752532959
DNN:
Mean Absolute Error: 0.0262
Root Mean Square Error: 0.0464
Mean Square Error: 0.0022

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_228&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_684 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_685 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_228 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_686 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
87/87 [==============================] - 3s 37ms/step - loss: 0.4494 - val_loss: 0.4161
Epoch 2/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4475 - val_loss: 0.4142
Epoch 3/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4455 - val_loss: 0.4121
Epoch 4/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4433 - val_loss: 0.4099
Epoch 5/80
87/87 [==============================] - 3s 36ms/step - loss: 0.4410 - val_loss: 0.4076
Epoch 6/80
87/87 [==============================] - 3s 36ms/step - loss: 0.4385 - val_loss: 0.4052
Epoch 7/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4360 - val_loss: 0.4026
Epoch 8/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4334 - val_loss: 0.4000
Epoch 9/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4306 - val_loss: 0.3972
Epoch 10/80
87/87 [==============================] - 3s 36ms/step - loss: 0.4277 - val_loss: 0.3944
Epoch 11/80
87/87 [==============================] - 3s 36ms/step - loss: 0.4248 - val_loss: 0.3914
Epoch 12/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4218 - val_loss: 0.3884
Epoch 13/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4186 - val_loss: 0.3853
Epoch 14/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4154 - val_loss: 0.3821
Epoch 15/80
87/87 [==============================] - 3s 36ms/step - loss: 0.4121 - val_loss: 0.3788
Epoch 16/80
87/87 [==============================] - 3s 36ms/step - loss: 0.4087 - val_loss: 0.3754
Epoch 17/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4052 - val_loss: 0.3720
Epoch 18/80
87/87 [==============================] - 3s 35ms/step - loss: 0.4016 - val_loss: 0.3685
Epoch 19/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3980 - val_loss: 0.3649
Epoch 20/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3943 - val_loss: 0.3612
Epoch 21/80
87/87 [==============================] - 3s 37ms/step - loss: 0.3905 - val_loss: 0.3574
Epoch 22/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3866 - val_loss: 0.3538
Epoch 23/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3831 - val_loss: 0.3506
Epoch 24/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3797 - val_loss: 0.3473
Epoch 25/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3764 - val_loss: 0.3440
Epoch 26/80
87/87 [==============================] - 3s 37ms/step - loss: 0.3730 - val_loss: 0.3406
Epoch 27/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3695 - val_loss: 0.3372
Epoch 28/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3659 - val_loss: 0.3338
Epoch 29/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3624 - val_loss: 0.3303
Epoch 30/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3589 - val_loss: 0.3268
Epoch 31/80
87/87 [==============================] - 3s 37ms/step - loss: 0.3553 - val_loss: 0.3233
Epoch 32/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3516 - val_loss: 0.3197
Epoch 33/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3479 - val_loss: 0.3160
Epoch 34/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3441 - val_loss: 0.3123
Epoch 35/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3403 - val_loss: 0.3085
Epoch 36/80
87/87 [==============================] - 3s 37ms/step - loss: 0.3364 - val_loss: 0.3047
Epoch 37/80
87/87 [==============================] - 3s 36ms/step - loss: 0.3325 - val_loss: 0.3009
Epoch 38/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3285 - val_loss: 0.2971
Epoch 39/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3249 - val_loss: 0.2942
Epoch 40/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3219 - val_loss: 0.2915
Epoch 41/80
87/87 [==============================] - 3s 37ms/step - loss: 0.3189 - val_loss: 0.2887
Epoch 42/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3160 - val_loss: 0.2859
Epoch 43/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3129 - val_loss: 0.2830
Epoch 44/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3099 - val_loss: 0.2800
Epoch 45/80
87/87 [==============================] - 3s 35ms/step - loss: 0.3068 - val_loss: 0.2770
Epoch 46/80
87/87 [==============================] - 3s 36ms/step - loss: 0.3037 - val_loss: 0.2739
Epoch 47/80
87/87 [==============================] - 3s 36ms/step - loss: 0.3004 - val_loss: 0.2708
Epoch 48/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2972 - val_loss: 0.2676
Epoch 49/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2939 - val_loss: 0.2644
Epoch 50/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2905 - val_loss: 0.2610
Epoch 51/80
87/87 [==============================] - 3s 36ms/step - loss: 0.2872 - val_loss: 0.2577
Epoch 52/80
87/87 [==============================] - 3s 36ms/step - loss: 0.2837 - val_loss: 0.2542
Epoch 53/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2801 - val_loss: 0.2506
Epoch 54/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2765 - val_loss: 0.2470
Epoch 55/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2727 - val_loss: 0.2433
Epoch 56/80
87/87 [==============================] - 3s 36ms/step - loss: 0.2689 - val_loss: 0.2395
Epoch 57/80
87/87 [==============================] - 3s 36ms/step - loss: 0.2651 - val_loss: 0.2356
Epoch 58/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2611 - val_loss: 0.2316
Epoch 59/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2570 - val_loss: 0.2275
Epoch 60/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2528 - val_loss: 0.2233
Epoch 61/80
87/87 [==============================] - 3s 36ms/step - loss: 0.2485 - val_loss: 0.2191
Epoch 62/80
87/87 [==============================] - 3s 37ms/step - loss: 0.2442 - val_loss: 0.2147
Epoch 63/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2398 - val_loss: 0.2102
Epoch 64/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2354 - val_loss: 0.2057
Epoch 65/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2307 - val_loss: 0.2010
Epoch 66/80
87/87 [==============================] - 3s 36ms/step - loss: 0.2253 - val_loss: 0.1940
Epoch 67/80
87/87 [==============================] - ETA: 0s - loss: 0.217 - 3s 37ms/step - loss: 0.2172 - val_loss: 0.1852
Epoch 68/80
87/87 [==============================] - 3s 35ms/step - loss: 0.2084 - val_loss: 0.1762
Epoch 69/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1994 - val_loss: 0.1672
Epoch 70/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1905 - val_loss: 0.1581
Epoch 71/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1814 - val_loss: 0.1489
Epoch 72/80
87/87 [==============================] - 3s 37ms/step - loss: 0.1724 - val_loss: 0.1397
Epoch 73/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1639 - val_loss: 0.1307
Epoch 74/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1563 - val_loss: 0.1225
Epoch 75/80
87/87 [==============================] - 3s 36ms/step - loss: 0.1503 - val_loss: 0.1154
Epoch 76/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1462 - val_loss: 0.1093
Epoch 77/80
87/87 [==============================] - 3s 37ms/step - loss: 0.1428 - val_loss: 0.1042
Epoch 78/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1403 - val_loss: 0.0996
Epoch 79/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1382 - val_loss: 0.0956
Epoch 80/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1363 - val_loss: 0.0920
Execution time:  250.89760971069336
DNN:
Mean Absolute Error: 0.1213
Root Mean Square Error: 0.1252
Mean Square Error: 0.0157

Train RMSE: 0.125
Train MSE: 0.016
Train MAE: 0.121
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_229&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_687 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_688 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_229 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_689 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
108/108 [==============================] - 3s 31ms/step - loss: 0.3898 - val_loss: 0.3594
Epoch 2/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3881 - val_loss: 0.3577
Epoch 3/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3863 - val_loss: 0.3558
Epoch 4/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3843 - val_loss: 0.3538
Epoch 5/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3822 - val_loss: 0.3517
Epoch 6/90
108/108 [==============================] - 3s 28ms/step - loss: 0.3800 - val_loss: 0.3495
Epoch 7/90
108/108 [==============================] - 3s 29ms/step - loss: 0.3776 - val_loss: 0.3471
Epoch 8/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3752 - val_loss: 0.3446
Epoch 9/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3726 - val_loss: 0.3421
Epoch 10/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3699 - val_loss: 0.3394
Epoch 11/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3672 - val_loss: 0.3367
Epoch 12/90
108/108 [==============================] - 3s 29ms/step - loss: 0.3643 - val_loss: 0.3339
Epoch 13/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3614 - val_loss: 0.3310
Epoch 14/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3584 - val_loss: 0.3280
Epoch 15/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3553 - val_loss: 0.3249
Epoch 16/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3521 - val_loss: 0.3217
Epoch 17/90
108/108 [==============================] - 3s 28ms/step - loss: 0.3488 - val_loss: 0.3184
Epoch 18/90
108/108 [==============================] - 3s 28ms/step - loss: 0.3454 - val_loss: 0.3150
Epoch 19/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3420 - val_loss: 0.3116
Epoch 20/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3384 - val_loss: 0.3080
Epoch 21/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3348 - val_loss: 0.3045
Epoch 22/90
108/108 [==============================] - 3s 28ms/step - loss: 0.3311 - val_loss: 0.3009
Epoch 23/90
108/108 [==============================] - 3s 29ms/step - loss: 0.3274 - val_loss: 0.2972
Epoch 24/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3236 - val_loss: 0.2934
Epoch 25/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3197 - val_loss: 0.2896
Epoch 26/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3158 - val_loss: 0.2858
Epoch 27/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3120 - val_loss: 0.2821
Epoch 28/90
108/108 [==============================] - 3s 29ms/step - loss: 0.3081 - val_loss: 0.2783
Epoch 29/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3042 - val_loss: 0.2745
Epoch 30/90
108/108 [==============================] - 3s 27ms/step - loss: 0.3002 - val_loss: 0.2705
Epoch 31/90
108/108 [==============================] - 3s 27ms/step - loss: 0.2962 - val_loss: 0.2665
Epoch 32/90
108/108 [==============================] - 3s 27ms/step - loss: 0.2921 - val_loss: 0.2624
Epoch 33/90
108/108 [==============================] - 3s 29ms/step - loss: 0.2878 - val_loss: 0.2582
Epoch 34/90
108/108 [==============================] - 3s 28ms/step - loss: 0.2835 - val_loss: 0.2539
Epoch 35/90
108/108 [==============================] - 3s 27ms/step - loss: 0.2769 - val_loss: 0.2443
Epoch 36/90
108/108 [==============================] - 3s 28ms/step - loss: 0.2665 - val_loss: 0.2339
Epoch 37/90
108/108 [==============================] - 3s 28ms/step - loss: 0.2560 - val_loss: 0.2234
Epoch 38/90
108/108 [==============================] - 3s 28ms/step - loss: 0.2453 - val_loss: 0.2127
Epoch 39/90
108/108 [==============================] - 3s 28ms/step - loss: 0.2343 - val_loss: 0.2018
Epoch 40/90
108/108 [==============================] - 3s 27ms/step - loss: 0.2232 - val_loss: 0.1906
Epoch 41/90
108/108 [==============================] - 3s 27ms/step - loss: 0.2119 - val_loss: 0.1792
Epoch 42/90
108/108 [==============================] - 3s 27ms/step - loss: 0.2001 - val_loss: 0.1675
Epoch 43/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1881 - val_loss: 0.1554
Epoch 44/90
108/108 [==============================] - 3s 29ms/step - loss: 0.1758 - val_loss: 0.1430
Epoch 45/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1631 - val_loss: 0.1303
Epoch 46/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1501 - val_loss: 0.1171
Epoch 47/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1367 - val_loss: 0.1035
Epoch 48/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1229 - val_loss: 0.0896
Epoch 49/90
108/108 [==============================] - 3s 29ms/step - loss: 0.1088 - val_loss: 0.0754
Epoch 50/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0946 - val_loss: 0.0609
Epoch 51/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0807 - val_loss: 0.0469
Epoch 52/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0695 - val_loss: 0.0354
Epoch 53/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0635 - val_loss: 0.0281
Epoch 54/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0611 - val_loss: 0.0253
Epoch 55/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0607 - val_loss: 0.0236
Epoch 56/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0605 - val_loss: 0.0223
Epoch 57/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0604 - val_loss: 0.0215
Epoch 58/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0603 - val_loss: 0.0211
Epoch 59/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0602 - val_loss: 0.0207
Epoch 60/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0602 - val_loss: 0.0204
Epoch 61/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0601 - val_loss: 0.0202
Epoch 62/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0601 - val_loss: 0.0201
Epoch 63/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0602 - val_loss: 0.0199
Epoch 64/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0601 - val_loss: 0.0198
Epoch 65/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0601 - val_loss: 0.0196
Epoch 66/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0602 - val_loss: 0.0195
Epoch 67/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0601 - val_loss: 0.0194
Epoch 68/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0601 - val_loss: 0.0193
Epoch 69/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0601 - val_loss: 0.0192
Epoch 70/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0601 - val_loss: 0.0191
Epoch 71/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0600 - val_loss: 0.0190
Epoch 72/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0600 - val_loss: 0.0190
Epoch 73/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0600 - val_loss: 0.0189
Epoch 74/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0600 - val_loss: 0.0189
Epoch 75/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0600 - val_loss: 0.0188
Epoch 76/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0600 - val_loss: 0.0187
Epoch 77/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0599 - val_loss: 0.0187
Epoch 78/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0599 - val_loss: 0.0186
Epoch 79/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0600 - val_loss: 0.0186
Epoch 80/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0599 - val_loss: 0.0185
Epoch 81/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0600 - val_loss: 0.0185
Epoch 82/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0600 - val_loss: 0.0184
Epoch 83/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0599 - val_loss: 0.0184
Epoch 84/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0598 - val_loss: 0.0183
Epoch 85/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0599 - val_loss: 0.0183
Epoch 86/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0598 - val_loss: 0.0182
Epoch 87/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0598 - val_loss: 0.0182
Epoch 88/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0598 - val_loss: 0.0181
Epoch 89/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0598 - val_loss: 0.0181
Epoch 90/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0598 - val_loss: 0.0181
Execution time:  272.74858951568604
DNN:
Mean Absolute Error: 0.0464
Root Mean Square Error: 0.0555
Mean Square Error: 0.0031

Train RMSE: 0.055
Train MSE: 0.003
Train MAE: 0.046
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_230&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_690 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_691 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_230 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_692 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
48/48 [==============================] - 2s 37ms/step - loss: 0.4199 - val_loss: 0.3890
Epoch 2/52
48/48 [==============================] - 1s 29ms/step - loss: 0.4192 - val_loss: 0.3883
Epoch 3/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4187 - val_loss: 0.3876
Epoch 4/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4179 - val_loss: 0.3869
Epoch 5/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4173 - val_loss: 0.3862
Epoch 6/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4164 - val_loss: 0.3855
Epoch 7/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4157 - val_loss: 0.3847
Epoch 8/52
48/48 [==============================] - 1s 31ms/step - loss: 0.4150 - val_loss: 0.3839
Epoch 9/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4142 - val_loss: 0.3831
Epoch 10/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4133 - val_loss: 0.3822
Epoch 11/52
48/48 [==============================] - 2s 32ms/step - loss: 0.4123 - val_loss: 0.3813
Epoch 12/52
48/48 [==============================] - 2s 33ms/step - loss: 0.4113 - val_loss: 0.3804
Epoch 13/52
48/48 [==============================] - 1s 31ms/step - loss: 0.4106 - val_loss: 0.3795
Epoch 14/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4097 - val_loss: 0.3786
Epoch 15/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4087 - val_loss: 0.3776
Epoch 16/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4077 - val_loss: 0.3766
Epoch 17/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4066 - val_loss: 0.3756
Epoch 18/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4058 - val_loss: 0.3746
Epoch 19/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4047 - val_loss: 0.3736
Epoch 20/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4036 - val_loss: 0.3725
Epoch 21/52
48/48 [==============================] - 1s 30ms/step - loss: 0.4026 - val_loss: 0.3715
Epoch 22/52
48/48 [==============================] - 2s 33ms/step - loss: 0.4015 - val_loss: 0.3704
Epoch 23/52
48/48 [==============================] - 2s 32ms/step - loss: 0.4004 - val_loss: 0.3693
Epoch 24/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3994 - val_loss: 0.3681
Epoch 25/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3982 - val_loss: 0.3670
Epoch 26/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3971 - val_loss: 0.3659
Epoch 27/52
48/48 [==============================] - 2s 32ms/step - loss: 0.3959 - val_loss: 0.3647
Epoch 28/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3947 - val_loss: 0.3635
Epoch 29/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3935 - val_loss: 0.3623
Epoch 30/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3922 - val_loss: 0.3611
Epoch 31/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3909 - val_loss: 0.3598
Epoch 32/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3897 - val_loss: 0.3586- ETA: 1s - loss - ETA: 0s - l
Epoch 33/52
48/48 [==============================] - 2s 33ms/step - loss: 0.3886 - val_loss: 0.3573
Epoch 34/52
48/48 [==============================] - 2s 31ms/step - loss: 0.3871 - val_loss: 0.3560
Epoch 35/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3859 - val_loss: 0.3547
Epoch 36/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3846 - val_loss: 0.3534
Epoch 37/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3833 - val_loss: 0.3521
Epoch 38/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3820 - val_loss: 0.3507
Epoch 39/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3806 - val_loss: 0.3494
Epoch 40/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3791 - val_loss: 0.3480
Epoch 41/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3778 - val_loss: 0.3466
Epoch 42/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3764 - val_loss: 0.3452
Epoch 43/52
48/48 [==============================] - 2s 32ms/step - loss: 0.3750 - val_loss: 0.3438
Epoch 44/52
48/48 [==============================] - 2s 32ms/step - loss: 0.3735 - val_loss: 0.3424
Epoch 45/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3722 - val_loss: 0.3409
Epoch 46/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3707 - val_loss: 0.3395
Epoch 47/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3692 - val_loss: 0.3380
Epoch 48/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3679 - val_loss: 0.3365
Epoch 49/52
48/48 [==============================] - 1s 31ms/step - loss: 0.3662 - val_loss: 0.3350
Epoch 50/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3648 - val_loss: 0.3335
Epoch 51/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3631 - val_loss: 0.3320
Epoch 52/52
48/48 [==============================] - 1s 30ms/step - loss: 0.3617 - val_loss: 0.3304
Execution time:  79.0238242149353
DNN:
Mean Absolute Error: 0.3645
Root Mean Square Error: 0.3668
Mean Square Error: 0.1346

Train RMSE: 0.367
Train MSE: 0.135
Train MAE: 0.364
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_231&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_693 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_694 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_231 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_695 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
77/77 [==============================] - 3s 41ms/step - loss: 0.3916 - val_loss: 0.3662
Epoch 2/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3895 - val_loss: 0.3641
Epoch 3/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3873 - val_loss: 0.3619
Epoch 4/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3849 - val_loss: 0.3595
Epoch 5/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3824 - val_loss: 0.3570
Epoch 6/80
77/77 [==============================] - 3s 38ms/step - loss: 0.3798 - val_loss: 0.3543
Epoch 7/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3770 - val_loss: 0.3516
Epoch 8/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3742 - val_loss: 0.3487
Epoch 9/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3712 - val_loss: 0.3457
Epoch 10/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3681 - val_loss: 0.3427
Epoch 11/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3650 - val_loss: 0.3397
Epoch 12/80
77/77 [==============================] - 3s 39ms/step - loss: 0.3619 - val_loss: 0.3367
Epoch 13/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3588 - val_loss: 0.3336
Epoch 14/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3556 - val_loss: 0.3303
Epoch 15/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3522 - val_loss: 0.3270
Epoch 16/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3488 - val_loss: 0.3239
Epoch 17/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3457 - val_loss: 0.3208
Epoch 18/80
77/77 [==============================] - 3s 38ms/step - loss: 0.3424 - val_loss: 0.3176
Epoch 19/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3391 - val_loss: 0.3143
Epoch 20/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3358 - val_loss: 0.3112
Epoch 21/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3327 - val_loss: 0.3083
Epoch 22/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3297 - val_loss: 0.3053
Epoch 23/80
77/77 [==============================] - 3s 39ms/step - loss: 0.3266 - val_loss: 0.3023
Epoch 24/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3234 - val_loss: 0.2991
Epoch 25/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3202 - val_loss: 0.2959
Epoch 26/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3169 - val_loss: 0.2927
Epoch 27/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3135 - val_loss: 0.2893
Epoch 28/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3102 - val_loss: 0.2861
Epoch 29/80
77/77 [==============================] - 3s 38ms/step - loss: 0.3068 - val_loss: 0.2828
Epoch 30/80
77/77 [==============================] - 3s 36ms/step - loss: 0.3035 - val_loss: 0.2794
Epoch 31/80
77/77 [==============================] - 3s 37ms/step - loss: 0.3000 - val_loss: 0.2760
Epoch 32/80
77/77 [==============================] - 3s 36ms/step - loss: 0.2965 - val_loss: 0.2726
Epoch 33/80
77/77 [==============================] - 3s 37ms/step - loss: 0.2930 - val_loss: 0.2690
Epoch 34/80
77/77 [==============================] - 3s 39ms/step - loss: 0.2893 - val_loss: 0.2654
Epoch 35/80
77/77 [==============================] - 3s 37ms/step - loss: 0.2847 - val_loss: 0.2583
Epoch 36/80
77/77 [==============================] - 3s 36ms/step - loss: 0.2759 - val_loss: 0.2490
Epoch 37/80
77/77 [==============================] - 3s 37ms/step - loss: 0.2667 - val_loss: 0.2397
Epoch 38/80
77/77 [==============================] - 3s 37ms/step - loss: 0.2572 - val_loss: 0.2304
Epoch 39/80
77/77 [==============================] - 3s 38ms/step - loss: 0.2478 - val_loss: 0.2211
Epoch 40/80
77/77 [==============================] - 3s 38ms/step - loss: 0.2385 - val_loss: 0.2118
Epoch 41/80
77/77 [==============================] - 3s 37ms/step - loss: 0.2292 - val_loss: 0.2024
Epoch 42/80
77/77 [==============================] - 3s 36ms/step - loss: 0.2196 - val_loss: 0.1929
Epoch 43/80
77/77 [==============================] - 3s 37ms/step - loss: 0.2099 - val_loss: 0.1832
Epoch 44/80
77/77 [==============================] - 3s 37ms/step - loss: 0.2002 - val_loss: 0.1733
Epoch 45/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1904 - val_loss: 0.1633
Epoch 46/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1804 - val_loss: 0.1533
Epoch 47/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1707 - val_loss: 0.1434
Epoch 48/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1618 - val_loss: 0.1340
Epoch 49/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1539 - val_loss: 0.1252
Epoch 50/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1474 - val_loss: 0.1170
Epoch 51/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1415 - val_loss: 0.1094
Epoch 52/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1366 - val_loss: 0.1024
Epoch 53/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1328 - val_loss: 0.0963
Epoch 54/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1295 - val_loss: 0.0908
Epoch 55/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1270 - val_loss: 0.0860
Epoch 56/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1248 - val_loss: 0.0819
Epoch 57/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1232 - val_loss: 0.0785
Epoch 58/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1221 - val_loss: 0.0757
Epoch 59/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1211 - val_loss: 0.0734
Epoch 60/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1204 - val_loss: 0.0715
Epoch 61/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1197 - val_loss: 0.0698
Epoch 62/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1192 - val_loss: 0.0683
Epoch 63/80
77/77 [==============================] - 3s 36ms/step - loss: 0.1188 - val_loss: 0.0669
Epoch 64/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1182 - val_loss: 0.0657
Epoch 65/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1179 - val_loss: 0.0645
Epoch 66/80
77/77 [==============================] - 3s 36ms/step - loss: 0.1175 - val_loss: 0.0634
Epoch 67/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1171 - val_loss: 0.0624
Epoch 68/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1167 - val_loss: 0.0614
Epoch 69/80
77/77 [==============================] - 3s 36ms/step - loss: 0.1163 - val_loss: 0.0604
Epoch 70/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1161 - val_loss: 0.0594
Epoch 71/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1157 - val_loss: 0.0585
Epoch 72/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1154 - val_loss: 0.0576
Epoch 73/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1150 - val_loss: 0.0568
Epoch 74/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1148 - val_loss: 0.0559
Epoch 75/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1144 - val_loss: 0.0551
Epoch 76/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1141 - val_loss: 0.0542
Epoch 77/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1138 - val_loss: 0.0534
Epoch 78/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1136 - val_loss: 0.0527
Epoch 79/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1133 - val_loss: 0.0519
Epoch 80/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1131 - val_loss: 0.0511
Execution time:  233.4064164161682
DNN:
Mean Absolute Error: 0.0731
Root Mean Square Error: 0.0784
Mean Square Error: 0.0061

Train RMSE: 0.078
Train MSE: 0.006
Train MAE: 0.073
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_232&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_696 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_697 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_232 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_698 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
96/96 [==============================] - 3s 30ms/step - loss: 0.3559 - val_loss: 0.3315
Epoch 2/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3525 - val_loss: 0.3280
Epoch 3/90
96/96 [==============================] - 3s 30ms/step - loss: 0.3488 - val_loss: 0.3242
Epoch 4/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3449 - val_loss: 0.3202
Epoch 5/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3407 - val_loss: 0.3160
Epoch 6/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3363 - val_loss: 0.3116
Epoch 7/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3316 - val_loss: 0.3069
Epoch 8/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3268 - val_loss: 0.3020
Epoch 9/90
96/96 [==============================] - 3s 30ms/step - loss: 0.3218 - val_loss: 0.2970
Epoch 10/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3165 - val_loss: 0.2917
Epoch 11/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3111 - val_loss: 0.2863
Epoch 12/90
96/96 [==============================] - 3s 28ms/step - loss: 0.3054 - val_loss: 0.2806
Epoch 13/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2996 - val_loss: 0.2748
Epoch 14/90
96/96 [==============================] - 3s 29ms/step - loss: 0.2936 - val_loss: 0.2688
Epoch 15/90
96/96 [==============================] - 3s 30ms/step - loss: 0.2874 - val_loss: 0.2627
Epoch 16/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2811 - val_loss: 0.2565
Epoch 17/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2747 - val_loss: 0.2501
Epoch 18/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2680 - val_loss: 0.2435
Epoch 19/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2613 - val_loss: 0.2368
Epoch 20/90
96/96 [==============================] - 3s 30ms/step - loss: 0.2544 - val_loss: 0.2299
Epoch 21/90
96/96 [==============================] - 3s 29ms/step - loss: 0.2473 - val_loss: 0.2229
Epoch 22/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2401 - val_loss: 0.2157
Epoch 23/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2326 - val_loss: 0.2083
Epoch 24/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2251 - val_loss: 0.2008
Epoch 25/90
96/96 [==============================] - 3s 28ms/step - loss: 0.2176 - val_loss: 0.1938
Epoch 26/90
96/96 [==============================] - 3s 30ms/step - loss: 0.2104 - val_loss: 0.1867
Epoch 27/90
96/96 [==============================] - 3s 29ms/step - loss: 0.2031 - val_loss: 0.1795
Epoch 28/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1956 - val_loss: 0.1721
Epoch 29/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1882 - val_loss: 0.1646
Epoch 30/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1804 - val_loss: 0.1570
Epoch 31/90
96/96 [==============================] - 3s 29ms/step - loss: 0.1726 - val_loss: 0.1493
Epoch 32/90
96/96 [==============================] - 3s 30ms/step - loss: 0.1647 - val_loss: 0.1414
Epoch 33/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1567 - val_loss: 0.1335
Epoch 34/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1485 - val_loss: 0.1253
Epoch 35/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1402 - val_loss: 0.1170
Epoch 36/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1317 - val_loss: 0.1086
Epoch 37/90
96/96 [==============================] - 3s 29ms/step - loss: 0.1232 - val_loss: 0.1000
Epoch 38/90
96/96 [==============================] - 3s 29ms/step - loss: 0.1144 - val_loss: 0.0913
Epoch 39/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1055 - val_loss: 0.0824
Epoch 40/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0968 - val_loss: 0.0736
Epoch 41/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0883 - val_loss: 0.0649
Epoch 42/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0805 - val_loss: 0.0567
Epoch 43/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0737 - val_loss: 0.0492
Epoch 44/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0681 - val_loss: 0.0426
Epoch 45/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0637 - val_loss: 0.0370
Epoch 46/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0607 - val_loss: 0.0332
Epoch 47/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0593 - val_loss: 0.0304
Epoch 48/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0583 - val_loss: 0.0281
Epoch 49/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0576 - val_loss: 0.0264
Epoch 50/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0571 - val_loss: 0.0250
Epoch 51/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0568 - val_loss: 0.0240
Epoch 52/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0566 - val_loss: 0.0231
Epoch 53/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0563 - val_loss: 0.0223
Epoch 54/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0561 - val_loss: 0.0216
Epoch 55/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0560 - val_loss: 0.0209
Epoch 56/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0558 - val_loss: 0.0203
Epoch 57/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0556 - val_loss: 0.0198
Epoch 58/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0555 - val_loss: 0.0192
Epoch 59/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0555 - val_loss: 0.0188
Epoch 60/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0554 - val_loss: 0.0183
Epoch 61/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0552 - val_loss: 0.0179
Epoch 62/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0552 - val_loss: 0.0175
Epoch 63/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0550 - val_loss: 0.0171
Epoch 64/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0550 - val_loss: 0.0167
Epoch 65/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0549 - val_loss: 0.0164
Epoch 66/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0549 - val_loss: 0.0161
Epoch 67/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0549 - val_loss: 0.0158
Epoch 68/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0548 - val_loss: 0.0156
Epoch 69/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0547 - val_loss: 0.0153
Epoch 70/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0547 - val_loss: 0.0150
Epoch 71/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0546 - val_loss: 0.0148
Epoch 72/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0546 - val_loss: 0.0146
Epoch 73/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0546 - val_loss: 0.0144
Epoch 74/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0546 - val_loss: 0.0141
Epoch 75/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0545 - val_loss: 0.0139
Epoch 76/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0545 - val_loss: 0.0137
Epoch 77/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0545 - val_loss: 0.0135
Epoch 78/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0545 - val_loss: 0.0133
Epoch 79/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0544 - val_loss: 0.0132
Epoch 80/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0544 - val_loss: 0.0131
Epoch 81/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0543 - val_loss: 0.0130
Epoch 82/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0543 - val_loss: 0.0130
Epoch 83/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0543 - val_loss: 0.0129
Epoch 84/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0543 - val_loss: 0.0129
Epoch 85/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0543 - val_loss: 0.0129
Epoch 86/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0542 - val_loss: 0.0128
Epoch 87/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0542 - val_loss: 0.0128
Epoch 88/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0542 - val_loss: 0.0127
Epoch 89/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0542 - val_loss: 0.0127
Epoch 90/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0542 - val_loss: 0.0127
Execution time:  252.02367758750916
DNN:
Mean Absolute Error: 0.0265
Root Mean Square Error: 0.0361
Mean Square Error: 0.0013

Train RMSE: 0.036
Train MSE: 0.001
Train MAE: 0.027
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_233&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_699 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_700 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_233 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_701 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
42/42 [==============================] - 1s 35ms/step - loss: 0.4052 - val_loss: 0.3800
Epoch 2/52
42/42 [==============================] - 1s 31ms/step - loss: 0.4046 - val_loss: 0.3795
Epoch 3/52
42/42 [==============================] - 1s 31ms/step - loss: 0.4042 - val_loss: 0.3791
Epoch 4/52
42/42 [==============================] - 1s 31ms/step - loss: 0.4037 - val_loss: 0.3786
Epoch 5/52
42/42 [==============================] - 1s 31ms/step - loss: 0.4032 - val_loss: 0.3781
Epoch 6/52
42/42 [==============================] - 1s 32ms/step - loss: 0.4027 - val_loss: 0.3776
Epoch 7/52
42/42 [==============================] - 1s 34ms/step - loss: 0.4023 - val_loss: 0.3771
Epoch 8/52
42/42 [==============================] - 1s 33ms/step - loss: 0.4017 - val_loss: 0.3765
Epoch 9/52
42/42 [==============================] - 1s 32ms/step - loss: 0.4011 - val_loss: 0.3760
Epoch 10/52
42/42 [==============================] - 1s 32ms/step - loss: 0.4006 - val_loss: 0.3754
Epoch 11/52
42/42 [==============================] - 1s 32ms/step - loss: 0.4000 - val_loss: 0.3748
Epoch 12/52
42/42 [==============================] - 1s 31ms/step - loss: 0.3994 - val_loss: 0.3742
Epoch 13/52
42/42 [==============================] - 1s 31ms/step - loss: 0.3988 - val_loss: 0.3736
Epoch 14/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3982 - val_loss: 0.3730
Epoch 15/52
42/42 [==============================] - 2s 37ms/step - loss: 0.3975 - val_loss: 0.3724
Epoch 16/52
42/42 [==============================] - 1s 36ms/step - loss: 0.3969 - val_loss: 0.3717
Epoch 17/52
42/42 [==============================] - 1s 35ms/step - loss: 0.3961 - val_loss: 0.3711
Epoch 18/52
42/42 [==============================] - 2s 38ms/step - loss: 0.3955 - val_loss: 0.3704
Epoch 19/52
42/42 [==============================] - 1s 35ms/step - loss: 0.3949 - val_loss: 0.3697
Epoch 20/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3942 - val_loss: 0.3690
Epoch 21/52
42/42 [==============================] - 1s 31ms/step - loss: 0.3934 - val_loss: 0.3683
Epoch 22/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3927 - val_loss: 0.3676
Epoch 23/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3920 - val_loss: 0.3669
Epoch 24/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3913 - val_loss: 0.3661
Epoch 25/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3905 - val_loss: 0.3654: 0s - loss: 0
Epoch 26/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3898 - val_loss: 0.3647
Epoch 27/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3890 - val_loss: 0.3639
Epoch 28/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3882 - val_loss: 0.3632
Epoch 29/52
42/42 [==============================] - 1s 34ms/step - loss: 0.3874 - val_loss: 0.3624
Epoch 30/52
42/42 [==============================] - 1s 35ms/step - loss: 0.3867 - val_loss: 0.3616
Epoch 31/52
42/42 [==============================] - 1s 34ms/step - loss: 0.3859 - val_loss: 0.3608
Epoch 32/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3851 - val_loss: 0.3600
Epoch 33/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3843 - val_loss: 0.3592
Epoch 34/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3836 - val_loss: 0.3584
Epoch 35/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3827 - val_loss: 0.3576
Epoch 36/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3817 - val_loss: 0.3567
Epoch 37/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3809 - val_loss: 0.3559
Epoch 38/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3800 - val_loss: 0.3550
Epoch 39/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3793 - val_loss: 0.3542
Epoch 40/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3782 - val_loss: 0.3533
Epoch 41/52
42/42 [==============================] - 1s 35ms/step - loss: 0.3773 - val_loss: 0.3524
Epoch 42/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3765 - val_loss: 0.3515
Epoch 43/52
42/42 [==============================] - ETA: 0s - loss: 0.375 - 1s 32ms/step - loss: 0.3756 - val_loss: 0.3506
Epoch 44/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3747 - val_loss: 0.3497
Epoch 45/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3737 - val_loss: 0.3488
Epoch 46/52
42/42 [==============================] - 1s 33ms/step - loss: 0.3728 - val_loss: 0.3478
Epoch 47/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3718 - val_loss: 0.3469
Epoch 48/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3709 - val_loss: 0.3460
Epoch 49/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3700 - val_loss: 0.3450
Epoch 50/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3690 - val_loss: 0.3440
Epoch 51/52
42/42 [==============================] - 1s 32ms/step - loss: 0.3680 - val_loss: 0.3431
Epoch 52/52
42/42 [==============================] - 1s 35ms/step - loss: 0.3670 - val_loss: 0.3421
Execution time:  74.60365986824036
DNN:
Mean Absolute Error: 0.3688
Root Mean Square Error: 0.3710
Mean Square Error: 0.1377

Train RMSE: 0.371
Train MSE: 0.138
Train MAE: 0.369
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_234&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_702 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_703 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_234 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_704 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
87/87 [==============================] - 3s 37ms/step - loss: 0.1041 - val_loss: 0.1309
Epoch 2/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1035 - val_loss: 0.1303
Epoch 3/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1028 - val_loss: 0.1296
Epoch 4/80
87/87 [==============================] - 3s 35ms/step - loss: 0.1021 - val_loss: 0.1288
Epoch 5/80
87/87 [==============================] - 3s 37ms/step - loss: 0.1013 - val_loss: 0.1280
Epoch 6/80
87/87 [==============================] - 3s 36ms/step - loss: 0.1006 - val_loss: 0.1272
Epoch 7/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0997 - val_loss: 0.1263
Epoch 8/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0989 - val_loss: 0.1254
Epoch 9/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0980 - val_loss: 0.1245
Epoch 10/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0971 - val_loss: 0.1235
Epoch 11/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0962 - val_loss: 0.1226
Epoch 12/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0953 - val_loss: 0.1216
Epoch 13/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0944 - val_loss: 0.1206
Epoch 14/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0935 - val_loss: 0.1197
Epoch 15/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0926 - val_loss: 0.1188
Epoch 16/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0917 - val_loss: 0.1178
Epoch 17/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0908 - val_loss: 0.1169
Epoch 18/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0899 - val_loss: 0.1159
Epoch 19/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0890 - val_loss: 0.1149
Epoch 20/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0881 - val_loss: 0.1139
Epoch 21/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0872 - val_loss: 0.1128
Epoch 22/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0862 - val_loss: 0.1117
Epoch 23/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0851 - val_loss: 0.1104
Epoch 24/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0838 - val_loss: 0.1089
Epoch 25/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0826 - val_loss: 0.1075
Epoch 26/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0813 - val_loss: 0.1061
Epoch 27/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0800 - val_loss: 0.1046
Epoch 28/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0788 - val_loss: 0.1034
Epoch 29/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0777 - val_loss: 0.1022
Epoch 30/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0766 - val_loss: 0.1010
Epoch 31/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0756 - val_loss: 0.0998
Epoch 32/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0745 - val_loss: 0.0986
Epoch 33/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0735 - val_loss: 0.0974
Epoch 34/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0725 - val_loss: 0.0962
Epoch 35/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0715 - val_loss: 0.0950
Epoch 36/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0704 - val_loss: 0.0938
Epoch 37/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0694 - val_loss: 0.0926
Epoch 38/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0684 - val_loss: 0.0914
Epoch 39/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0675 - val_loss: 0.0902
Epoch 40/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0665 - val_loss: 0.0890
Epoch 41/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0656 - val_loss: 0.0878
Epoch 42/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0646 - val_loss: 0.0867
Epoch 43/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0637 - val_loss: 0.0855
Epoch 44/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0628 - val_loss: 0.0844
Epoch 45/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0619 - val_loss: 0.0832
Epoch 46/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0610 - val_loss: 0.0821
Epoch 47/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0601 - val_loss: 0.0809
Epoch 48/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0592 - val_loss: 0.0798
Epoch 49/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0584 - val_loss: 0.0787
Epoch 50/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0575 - val_loss: 0.0776
Epoch 51/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0567 - val_loss: 0.0765
Epoch 52/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0559 - val_loss: 0.0754
Epoch 53/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0551 - val_loss: 0.0743
Epoch 54/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0543 - val_loss: 0.0732
Epoch 55/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0536 - val_loss: 0.0722
Epoch 56/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0528 - val_loss: 0.0712
Epoch 57/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0521 - val_loss: 0.0700
Epoch 58/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0513 - val_loss: 0.0687
Epoch 59/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0504 - val_loss: 0.0673
Epoch 60/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0495 - val_loss: 0.0660
Epoch 61/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0486 - val_loss: 0.0646
Epoch 62/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0478 - val_loss: 0.0634
Epoch 63/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0470 - val_loss: 0.0622
Epoch 64/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0463 - val_loss: 0.0611
Epoch 65/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0457 - val_loss: 0.0600
Epoch 66/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0451 - val_loss: 0.0589
Epoch 67/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0446 - val_loss: 0.0579
Epoch 68/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0440 - val_loss: 0.0570
Epoch 69/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0436 - val_loss: 0.0562
Epoch 70/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0432 - val_loss: 0.0553
Epoch 71/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0429 - val_loss: 0.0545
Epoch 72/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0425 - val_loss: 0.0538
Epoch 73/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0422 - val_loss: 0.0531
Epoch 74/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0419 - val_loss: 0.0524
Epoch 75/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0416 - val_loss: 0.0517
Epoch 76/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0414 - val_loss: 0.0510
Epoch 77/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0411 - val_loss: 0.0503
Epoch 78/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0409 - val_loss: 0.0495
Epoch 79/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0406 - val_loss: 0.0488
Epoch 80/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0404 - val_loss: 0.0481
Execution time:  251.3837537765503
DNN:
Mean Absolute Error: 0.0362
Root Mean Square Error: 0.0476
Mean Square Error: 0.0023

Train RMSE: 0.048
Train MSE: 0.002
Train MAE: 0.036
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_235&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_705 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_706 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_235 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_707 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
108/108 [==============================] - 3s 32ms/step - loss: 0.1139 - val_loss: 0.1409
Epoch 2/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1135 - val_loss: 0.1405
Epoch 3/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1131 - val_loss: 0.1401
Epoch 4/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1127 - val_loss: 0.1397
Epoch 5/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1122 - val_loss: 0.1392
Epoch 6/90
108/108 [==============================] - 3s 28ms/step - loss: 0.1117 - val_loss: 0.1387
Epoch 7/90
108/108 [==============================] - 3s 28ms/step - loss: 0.1112 - val_loss: 0.1382
Epoch 8/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1107 - val_loss: 0.1376
Epoch 9/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1102 - val_loss: 0.1371
Epoch 10/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1096 - val_loss: 0.1365
Epoch 11/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1091 - val_loss: 0.1359
Epoch 12/90
108/108 [==============================] - 3s 29ms/step - loss: 0.1085 - val_loss: 0.1353
Epoch 13/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1079 - val_loss: 0.1347
Epoch 14/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1074 - val_loss: 0.1342
Epoch 15/90
108/108 [==============================] - 3s 29ms/step - loss: 0.1068 - val_loss: 0.1336
Epoch 16/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1062 - val_loss: 0.1329
Epoch 17/90
108/108 [==============================] - 3s 29ms/step - loss: 0.1056 - val_loss: 0.1324
Epoch 18/90
108/108 [==============================] - 3s 28ms/step - loss: 0.1051 - val_loss: 0.1318
Epoch 19/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1045 - val_loss: 0.1312
Epoch 20/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1039 - val_loss: 0.1306
Epoch 21/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1033 - val_loss: 0.1300
Epoch 22/90
108/108 [==============================] - 3s 29ms/step - loss: 0.1028 - val_loss: 0.1294
Epoch 23/90
108/108 [==============================] - 3s 28ms/step - loss: 0.1022 - val_loss: 0.1288
Epoch 24/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1016 - val_loss: 0.1282
Epoch 25/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1010 - val_loss: 0.1275
Epoch 26/90
108/108 [==============================] - 3s 27ms/step - loss: 0.1003 - val_loss: 0.1269
Epoch 27/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0997 - val_loss: 0.1263
Epoch 28/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0991 - val_loss: 0.1256
Epoch 29/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0985 - val_loss: 0.1249
Epoch 30/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0978 - val_loss: 0.1242
Epoch 31/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0972 - val_loss: 0.1236
Epoch 32/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0965 - val_loss: 0.1229
Epoch 33/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0959 - val_loss: 0.1222
Epoch 34/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0952 - val_loss: 0.1215
Epoch 35/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0946 - val_loss: 0.1208
Epoch 36/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0939 - val_loss: 0.1201
Epoch 37/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0932 - val_loss: 0.1194
Epoch 38/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0926 - val_loss: 0.1187
Epoch 39/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0919 - val_loss: 0.1180
Epoch 40/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0913 - val_loss: 0.1173
Epoch 41/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0906 - val_loss: 0.1166
Epoch 42/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0900 - val_loss: 0.1159
Epoch 43/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0893 - val_loss: 0.1152
Epoch 44/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0887 - val_loss: 0.1145
Epoch 45/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0880 - val_loss: 0.1137
Epoch 46/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0873 - val_loss: 0.1130
Epoch 47/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0867 - val_loss: 0.1123
Epoch 48/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0860 - val_loss: 0.1116
Epoch 49/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0854 - val_loss: 0.1109
Epoch 50/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0847 - val_loss: 0.1102
Epoch 51/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0841 - val_loss: 0.1095
Epoch 52/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0835 - val_loss: 0.1088
Epoch 53/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0828 - val_loss: 0.1081
Epoch 54/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0822 - val_loss: 0.1074
Epoch 55/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0816 - val_loss: 0.1067
Epoch 56/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0809 - val_loss: 0.1060
Epoch 57/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0803 - val_loss: 0.1052
Epoch 58/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0796 - val_loss: 0.1045
Epoch 59/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0790 - val_loss: 0.1038
Epoch 60/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0783 - val_loss: 0.1030
Epoch 61/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0777 - val_loss: 0.1023
Epoch 62/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0770 - val_loss: 0.1016
Epoch 63/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0764 - val_loss: 0.1008
Epoch 64/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0758 - val_loss: 0.1001
Epoch 65/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0751 - val_loss: 0.0993
Epoch 66/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0744 - val_loss: 0.0985
Epoch 67/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0737 - val_loss: 0.0977
Epoch 68/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0730 - val_loss: 0.0968
Epoch 69/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0723 - val_loss: 0.0960
Epoch 70/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0716 - val_loss: 0.0952
Epoch 71/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0709 - val_loss: 0.0944
Epoch 72/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0701 - val_loss: 0.0935
Epoch 73/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0694 - val_loss: 0.0927
Epoch 74/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0687 - val_loss: 0.0918
Epoch 75/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0680 - val_loss: 0.0910
Epoch 76/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0673 - val_loss: 0.0901
Epoch 77/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0666 - val_loss: 0.0893
Epoch 78/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0659 - val_loss: 0.0884
Epoch 79/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0651 - val_loss: 0.0876
Epoch 80/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0644 - val_loss: 0.0867
Epoch 81/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0637 - val_loss: 0.0858
Epoch 82/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0629 - val_loss: 0.0849
Epoch 83/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0622 - val_loss: 0.0840
Epoch 84/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0615 - val_loss: 0.0832
Epoch 85/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0608 - val_loss: 0.0823
Epoch 86/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0601 - val_loss: 0.0814
Epoch 87/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0593 - val_loss: 0.0805
Epoch 88/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0587 - val_loss: 0.0796
Epoch 89/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0579 - val_loss: 0.0788
Epoch 90/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0573 - val_loss: 0.0779
Execution time:  272.5813035964966
DNN:
Mean Absolute Error: 0.0531
Root Mean Square Error: 0.0614
Mean Square Error: 0.0038

Train RMSE: 0.061
Train MSE: 0.004
Train MAE: 0.053
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_236&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_708 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_709 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_236 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_710 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
48/48 [==============================] - 2s 33ms/step - loss: 0.1197 - val_loss: 0.1461
Epoch 2/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1195 - val_loss: 0.1458
Epoch 3/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1192 - val_loss: 0.1456
Epoch 4/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1190 - val_loss: 0.1453
Epoch 5/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1188 - val_loss: 0.1450
Epoch 6/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1185 - val_loss: 0.1448
Epoch 7/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1182 - val_loss: 0.1445
Epoch 8/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1179 - val_loss: 0.1442
Epoch 9/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1176 - val_loss: 0.1439
Epoch 10/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1173 - val_loss: 0.1436
Epoch 11/52
48/48 [==============================] - 2s 32ms/step - loss: 0.1170 - val_loss: 0.1432
Epoch 12/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1167 - val_loss: 0.1429
Epoch 13/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1164 - val_loss: 0.1426
Epoch 14/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1161 - val_loss: 0.1422
Epoch 15/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1157 - val_loss: 0.1419
Epoch 16/52
48/48 [==============================] - ETA: 0s - loss: 0.115 - 1s 30ms/step - loss: 0.1154 - val_loss: 0.1415
Epoch 17/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1150 - val_loss: 0.1412
Epoch 18/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1147 - val_loss: 0.1408
Epoch 19/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1143 - val_loss: 0.1404
Epoch 20/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1140 - val_loss: 0.1401
Epoch 21/52
48/48 [==============================] - 2s 31ms/step - loss: 0.1136 - val_loss: 0.1397
Epoch 22/52
48/48 [==============================] - 2s 32ms/step - loss: 0.1133 - val_loss: 0.1393
Epoch 23/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1129 - val_loss: 0.1389
Epoch 24/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1125 - val_loss: 0.1385
Epoch 25/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1122 - val_loss: 0.1381
Epoch 26/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1118 - val_loss: 0.1377
Epoch 27/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1114 - val_loss: 0.1373
Epoch 28/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1110 - val_loss: 0.1369
Epoch 29/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1107 - val_loss: 0.1365
Epoch 30/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1102 - val_loss: 0.1361
Epoch 31/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1099 - val_loss: 0.1357
Epoch 32/52
48/48 [==============================] - 2s 32ms/step - loss: 0.1094 - val_loss: 0.1353
Epoch 33/52
48/48 [==============================] - 2s 32ms/step - loss: 0.1091 - val_loss: 0.1349
Epoch 34/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1086 - val_loss: 0.1344
Epoch 35/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1082 - val_loss: 0.1340
Epoch 36/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1078 - val_loss: 0.1336
Epoch 37/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1074 - val_loss: 0.1331
Epoch 38/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1070 - val_loss: 0.1327
Epoch 39/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1066 - val_loss: 0.1322
Epoch 40/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1062 - val_loss: 0.1318
Epoch 41/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1057 - val_loss: 0.1313
Epoch 42/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1053 - val_loss: 0.1309
Epoch 43/52
48/48 [==============================] - 2s 32ms/step - loss: 0.1049 - val_loss: 0.1304
Epoch 44/52
48/48 [==============================] - 2s 32ms/step - loss: 0.1044 - val_loss: 0.1300
Epoch 45/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1040 - val_loss: 0.1295
Epoch 46/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1036 - val_loss: 0.1290
Epoch 47/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1031 - val_loss: 0.1286
Epoch 48/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1027 - val_loss: 0.1281
Epoch 49/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1022 - val_loss: 0.1276
Epoch 50/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1018 - val_loss: 0.1272
Epoch 51/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1013 - val_loss: 0.1267
Epoch 52/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1009 - val_loss: 0.1262
Execution time:  78.73348450660706
DNN:
Mean Absolute Error: 0.0949
Root Mean Square Error: 0.1012
Mean Square Error: 0.0103

Train RMSE: 0.101
Train MSE: 0.010
Train MAE: 0.095
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_237&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_711 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_712 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_237 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_713 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1023 - val_loss: 0.1238
Epoch 2/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1020 - val_loss: 0.1236
Epoch 3/80
77/77 [==============================] - 3s 36ms/step - loss: 0.1018 - val_loss: 0.1233
Epoch 4/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1015 - val_loss: 0.1230
Epoch 5/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1013 - val_loss: 0.1228
Epoch 6/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1010 - val_loss: 0.1225
Epoch 7/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1007 - val_loss: 0.1221
Epoch 8/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1004 - val_loss: 0.1218
Epoch 9/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1001 - val_loss: 0.1215
Epoch 10/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0998 - val_loss: 0.1211
Epoch 11/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0994 - val_loss: 0.1208
Epoch 12/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0991 - val_loss: 0.1204
Epoch 13/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0987 - val_loss: 0.1200
Epoch 14/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0984 - val_loss: 0.1197
Epoch 15/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0980 - val_loss: 0.1193
Epoch 16/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0976 - val_loss: 0.1189
Epoch 17/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0973 - val_loss: 0.1185
Epoch 18/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0970 - val_loss: 0.1182
Epoch 19/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0967 - val_loss: 0.1179
Epoch 20/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0964 - val_loss: 0.1176
Epoch 21/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0961 - val_loss: 0.1173
Epoch 22/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0957 - val_loss: 0.1169
Epoch 23/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0954 - val_loss: 0.1166
Epoch 24/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0951 - val_loss: 0.1163
Epoch 25/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0948 - val_loss: 0.1159
Epoch 26/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0945 - val_loss: 0.1156
Epoch 27/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0941 - val_loss: 0.1152
Epoch 28/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0938 - val_loss: 0.1148
Epoch 29/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0935 - val_loss: 0.1145
Epoch 30/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0931 - val_loss: 0.1141
Epoch 31/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0928 - val_loss: 0.1137
Epoch 32/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0924 - val_loss: 0.1133
Epoch 33/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0921 - val_loss: 0.1130
Epoch 34/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0917 - val_loss: 0.1126
Epoch 35/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0913 - val_loss: 0.1122
Epoch 36/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0910 - val_loss: 0.1118
Epoch 37/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0906 - val_loss: 0.1114
Epoch 38/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0902 - val_loss: 0.1110
Epoch 39/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0899 - val_loss: 0.1106
Epoch 40/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0895 - val_loss: 0.1101
Epoch 41/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0891 - val_loss: 0.1097
Epoch 42/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0886 - val_loss: 0.1089
Epoch 43/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0878 - val_loss: 0.1081
Epoch 44/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0871 - val_loss: 0.1073
Epoch 45/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0864 - val_loss: 0.1065
Epoch 46/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0857 - val_loss: 0.1058
Epoch 47/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0850 - val_loss: 0.1050
Epoch 48/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0843 - val_loss: 0.1042
Epoch 49/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0836 - val_loss: 0.1034
Epoch 50/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0829 - val_loss: 0.1027
Epoch 51/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0822 - val_loss: 0.1019
Epoch 52/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0816 - val_loss: 0.1011
Epoch 53/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0809 - val_loss: 0.1004
Epoch 54/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0803 - val_loss: 0.0997
Epoch 55/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0796 - val_loss: 0.0989
Epoch 56/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0790 - val_loss: 0.0982
Epoch 57/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0783 - val_loss: 0.0975
Epoch 58/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0777 - val_loss: 0.0967
Epoch 59/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0771 - val_loss: 0.0960
Epoch 60/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0765 - val_loss: 0.0953
Epoch 61/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0758 - val_loss: 0.0945
Epoch 62/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0752 - val_loss: 0.0938
Epoch 63/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0746 - val_loss: 0.0931
Epoch 64/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0740 - val_loss: 0.0923
Epoch 65/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0734 - val_loss: 0.0916
Epoch 66/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0728 - val_loss: 0.0909
Epoch 67/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0722 - val_loss: 0.0901
Epoch 68/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0716 - val_loss: 0.0894
Epoch 69/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0709 - val_loss: 0.0887
Epoch 70/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0704 - val_loss: 0.0880
Epoch 71/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0698 - val_loss: 0.0872
Epoch 72/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0692 - val_loss: 0.0865
Epoch 73/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0686 - val_loss: 0.0858
Epoch 74/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0680 - val_loss: 0.0850
Epoch 75/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0674 - val_loss: 0.0843
Epoch 76/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0669 - val_loss: 0.0836
Epoch 77/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0663 - val_loss: 0.0829
Epoch 78/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0657 - val_loss: 0.0821
Epoch 79/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0652 - val_loss: 0.0814
Epoch 80/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0646 - val_loss: 0.0807
Execution time:  233.5884566307068
DNN:
Mean Absolute Error: 0.0610
Root Mean Square Error: 0.0690
Mean Square Error: 0.0048

Train RMSE: 0.069
Train MSE: 0.005
Train MAE: 0.061
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_238&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_714 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_715 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_238 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_716 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
96/96 [==============================] - 3s 30ms/step - loss: 0.1214 - val_loss: 0.1425
Epoch 2/90
96/96 [==============================] - 3s 30ms/step - loss: 0.1206 - val_loss: 0.1417
Epoch 3/90
96/96 [==============================] - 3s 29ms/step - loss: 0.1197 - val_loss: 0.1408
Epoch 4/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1188 - val_loss: 0.1399
Epoch 5/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1179 - val_loss: 0.1389
Epoch 6/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1168 - val_loss: 0.1378
Epoch 7/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1158 - val_loss: 0.1367
Epoch 8/90
96/96 [==============================] - 3s 31ms/step - loss: 0.1147 - val_loss: 0.1356
Epoch 9/90
96/96 [==============================] - 3s 29ms/step - loss: 0.1135 - val_loss: 0.1344
Epoch 10/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1123 - val_loss: 0.1332
Epoch 11/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1111 - val_loss: 0.1319
Epoch 12/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1099 - val_loss: 0.1307
Epoch 13/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1086 - val_loss: 0.1294
Epoch 14/90
96/96 [==============================] - 3s 30ms/step - loss: 0.1074 - val_loss: 0.1280
Epoch 15/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1060 - val_loss: 0.1267
Epoch 16/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1047 - val_loss: 0.1253
Epoch 17/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1034 - val_loss: 0.1239
Epoch 18/90
96/96 [==============================] - 3s 28ms/step - loss: 0.1020 - val_loss: 0.1225
Epoch 19/90
96/96 [==============================] - 3s 29ms/step - loss: 0.1006 - val_loss: 0.1210
Epoch 20/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0993 - val_loss: 0.1196
Epoch 21/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0979 - val_loss: 0.1182
Epoch 22/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0965 - val_loss: 0.1167
Epoch 23/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0952 - val_loss: 0.1152
Epoch 24/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0938 - val_loss: 0.1138
Epoch 25/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0924 - val_loss: 0.1123
Epoch 26/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0910 - val_loss: 0.1108
Epoch 27/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0896 - val_loss: 0.1093
Epoch 28/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0882 - val_loss: 0.1077
Epoch 29/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0868 - val_loss: 0.1062
Epoch 30/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0854 - val_loss: 0.1047
Epoch 31/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0840 - val_loss: 0.1032
Epoch 32/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0826 - val_loss: 0.1017
Epoch 33/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0813 - val_loss: 0.1002
Epoch 34/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0801 - val_loss: 0.0988
Epoch 35/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0789 - val_loss: 0.0976
Epoch 36/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0777 - val_loss: 0.0963
Epoch 37/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0767 - val_loss: 0.0951
Epoch 38/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0756 - val_loss: 0.0939
Epoch 39/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0745 - val_loss: 0.0927
Epoch 40/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0734 - val_loss: 0.0915
Epoch 41/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0724 - val_loss: 0.0902
Epoch 42/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0713 - val_loss: 0.0890
Epoch 43/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0703 - val_loss: 0.0878
Epoch 44/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0693 - val_loss: 0.0866
Epoch 45/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0683 - val_loss: 0.0854
Epoch 46/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0673 - val_loss: 0.0842
Epoch 47/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0662 - val_loss: 0.0829
Epoch 48/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0652 - val_loss: 0.0817
Epoch 49/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0642 - val_loss: 0.0805
Epoch 50/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0632 - val_loss: 0.0793
Epoch 51/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0623 - val_loss: 0.0781
Epoch 52/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0613 - val_loss: 0.0769
Epoch 53/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0604 - val_loss: 0.0758
Epoch 54/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0595 - val_loss: 0.0747
Epoch 55/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0587 - val_loss: 0.0736
Epoch 56/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0578 - val_loss: 0.0725
Epoch 57/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0570 - val_loss: 0.0715
Epoch 58/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0561 - val_loss: 0.0704
Epoch 59/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0553 - val_loss: 0.0693
Epoch 60/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0546 - val_loss: 0.0683
Epoch 61/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0537 - val_loss: 0.0672
Epoch 62/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0529 - val_loss: 0.0661
Epoch 63/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0522 - val_loss: 0.0651 0s
Epoch 64/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0514 - val_loss: 0.0640
Epoch 65/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0506 - val_loss: 0.0630
Epoch 66/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0499 - val_loss: 0.0619
Epoch 67/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0491 - val_loss: 0.0609
Epoch 68/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0484 - val_loss: 0.0598
Epoch 69/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0477 - val_loss: 0.0587
Epoch 70/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0469 - val_loss: 0.0577
Epoch 71/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0462 - val_loss: 0.0566
Epoch 72/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0455 - val_loss: 0.0556
Epoch 73/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0448 - val_loss: 0.0545
Epoch 74/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0441 - val_loss: 0.0535
Epoch 75/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0434 - val_loss: 0.0525
Epoch 76/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0428 - val_loss: 0.0515
Epoch 77/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0421 - val_loss: 0.0505
Epoch 78/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0415 - val_loss: 0.0495
Epoch 79/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0409 - val_loss: 0.0485
Epoch 80/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0403 - val_loss: 0.0476
Epoch 81/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0397 - val_loss: 0.0466
Epoch 82/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0391 - val_loss: 0.0457
Epoch 83/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0386 - val_loss: 0.0448
Epoch 84/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0381 - val_loss: 0.0440
Epoch 85/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0376 - val_loss: 0.0432
Epoch 86/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0371 - val_loss: 0.0425
Epoch 87/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0367 - val_loss: 0.0418
Epoch 88/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0363 - val_loss: 0.0411
Epoch 89/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0359 - val_loss: 0.0404
Epoch 90/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0356 - val_loss: 0.0397
Execution time:  252.25831508636475
DNN:
Mean Absolute Error: 0.0361
Root Mean Square Error: 0.0479
Mean Square Error: 0.0023

Train RMSE: 0.048
Train MSE: 0.002
Train MAE: 0.036
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adadelta
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_239&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_717 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_718 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_239 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_719 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
42/42 [==============================] - 1s 35ms/step - loss: 0.0616 - val_loss: 0.0758
Epoch 2/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0615 - val_loss: 0.0756
Epoch 3/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0615 - val_loss: 0.0754
Epoch 4/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0613 - val_loss: 0.0753
Epoch 5/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0612 - val_loss: 0.0751
Epoch 6/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0611 - val_loss: 0.0750
Epoch 7/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0610 - val_loss: 0.0748
Epoch 8/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0608 - val_loss: 0.0746
Epoch 9/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0608 - val_loss: 0.0745
Epoch 10/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0606 - val_loss: 0.0743
Epoch 11/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0605 - val_loss: 0.0741
Epoch 12/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0604 - val_loss: 0.0739
Epoch 13/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0602 - val_loss: 0.0738
Epoch 14/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0601 - val_loss: 0.0736
Epoch 15/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0600 - val_loss: 0.0734
Epoch 16/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0599 - val_loss: 0.0732
Epoch 17/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0597 - val_loss: 0.0730
Epoch 18/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0597 - val_loss: 0.0728
Epoch 19/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0595 - val_loss: 0.0727
Epoch 20/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0594 - val_loss: 0.0725
Epoch 21/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0593 - val_loss: 0.0723
Epoch 22/52
42/42 [==============================] - 1s 30ms/step - loss: 0.0591 - val_loss: 0.0721
Epoch 23/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0590 - val_loss: 0.0719
Epoch 24/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0589 - val_loss: 0.0717
Epoch 25/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0588 - val_loss: 0.0715
Epoch 26/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0587 - val_loss: 0.0713
Epoch 27/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0585 - val_loss: 0.0711
Epoch 28/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0584 - val_loss: 0.0709
Epoch 29/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0582 - val_loss: 0.0708
Epoch 30/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0581 - val_loss: 0.0706
Epoch 31/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0580 - val_loss: 0.0704
Epoch 32/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0579 - val_loss: 0.0702
Epoch 33/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0577 - val_loss: 0.0700
Epoch 34/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0576 - val_loss: 0.0698
Epoch 35/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0575 - val_loss: 0.0696
Epoch 36/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0574 - val_loss: 0.0694
Epoch 37/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0572 - val_loss: 0.0692
Epoch 38/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0571 - val_loss: 0.0690
Epoch 39/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0570 - val_loss: 0.0688
Epoch 40/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0569 - val_loss: 0.0686
Epoch 41/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0568 - val_loss: 0.0684
Epoch 42/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0566 - val_loss: 0.0682
Epoch 43/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0565 - val_loss: 0.0680
Epoch 44/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0564 - val_loss: 0.0678
Epoch 45/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0562 - val_loss: 0.0676
Epoch 46/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0561 - val_loss: 0.0674
Epoch 47/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0560 - val_loss: 0.0672
Epoch 48/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0559 - val_loss: 0.0670
Epoch 49/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0558 - val_loss: 0.0668
Epoch 50/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0556 - val_loss: 0.0666
Epoch 51/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0555 - val_loss: 0.0664
Epoch 52/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0554 - val_loss: 0.0662
Execution time:  72.07327222824097
DNN:
Mean Absolute Error: 0.0514
Root Mean Square Error: 0.0607
Mean Square Error: 0.0037

Train RMSE: 0.061
Train MSE: 0.004
Train MAE: 0.051
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_240&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_720 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_721 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_240 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_722 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
87/87 [==============================] - 3s 37ms/step - loss: 0.1544 - val_loss: 0.0303
Epoch 2/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0973 - val_loss: 0.0213
Epoch 3/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0882 - val_loss: 0.0140
Epoch 4/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0814 - val_loss: 0.0114
Epoch 5/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0757 - val_loss: 0.0101
Epoch 6/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0706 - val_loss: 0.0091
Epoch 7/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0657 - val_loss: 0.0079
Epoch 8/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0610 - val_loss: 0.0062
Epoch 9/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0565 - val_loss: 0.0056
Epoch 10/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0521 - val_loss: 0.0068
Epoch 11/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0473 - val_loss: 0.0084
Epoch 12/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0425 - val_loss: 0.0147
Epoch 13/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0380 - val_loss: 0.0179
Epoch 14/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0355 - val_loss: 0.0194
Epoch 15/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0333 - val_loss: 0.0215
Epoch 16/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0317 - val_loss: 0.0236
Epoch 17/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0302 - val_loss: 0.0242
Epoch 18/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0290 - val_loss: 0.0248
Epoch 19/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0281 - val_loss: 0.0203
Epoch 20/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0273 - val_loss: 0.0146
Epoch 21/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0261 - val_loss: 0.0129
Epoch 22/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0252 - val_loss: 0.0107
Epoch 23/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0242 - val_loss: 0.0114
Epoch 24/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0241 - val_loss: 0.0120
Epoch 25/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0239 - val_loss: 0.0166
Epoch 26/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0236 - val_loss: 0.0122
Epoch 27/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0237 - val_loss: 0.0113
Epoch 28/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0139
Epoch 29/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0132
Epoch 30/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 31/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 32/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 33/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 34/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 35/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 36/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 37/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 38/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 39/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 40/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 41/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 42/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 43/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 44/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 45/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 46/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 47/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 48/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 49/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 50/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 51/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 52/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 53/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 54/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0135
Epoch 55/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0138
Epoch 56/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 57/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 58/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 59/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 60/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 61/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 62/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 63/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 64/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 65/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 66/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 67/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 68/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 69/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 70/80
87/87 [==============================] - ETA: 0s - loss: 0.023 - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 71/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 72/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 73/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 74/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 75/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 76/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 77/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 78/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Epoch 79/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0136
Epoch 80/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0234 - val_loss: 0.0137
Execution time:  251.38306307792664
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0468
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_241&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_723 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_724 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_241 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_725 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
108/108 [==============================] - 3s 32ms/step - loss: 0.0841 - val_loss: 0.0147
Epoch 2/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0556 - val_loss: 0.0101
Epoch 3/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0536 - val_loss: 0.0074
Epoch 4/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0515 - val_loss: 0.0073
Epoch 5/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0498 - val_loss: 0.0081
Epoch 6/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0480 - val_loss: 0.0077
Epoch 7/90
108/108 [==============================] - ETA: 0s - loss: 0.046 - 3s 29ms/step - loss: 0.0465 - val_loss: 0.0091
Epoch 8/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0450 - val_loss: 0.0085
Epoch 9/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0437 - val_loss: 0.0093
Epoch 10/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0425 - val_loss: 0.0102
Epoch 11/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0412 - val_loss: 0.0093
Epoch 12/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0401 - val_loss: 0.0090
Epoch 13/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0391 - val_loss: 0.0085
Epoch 14/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0382 - val_loss: 0.0087
Epoch 15/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0372 - val_loss: 0.0095
Epoch 16/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0362 - val_loss: 0.0099
Epoch 17/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0354 - val_loss: 0.0101
Epoch 18/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0344 - val_loss: 0.0096
Epoch 19/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0337 - val_loss: 0.0097
Epoch 20/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0331 - val_loss: 0.0093
Epoch 21/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0323 - val_loss: 0.0108
Epoch 22/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0316 - val_loss: 0.0096- los
Epoch 23/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0313 - val_loss: 0.0115
Epoch 24/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0309 - val_loss: 0.0118
Epoch 25/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0307 - val_loss: 0.0129
Epoch 26/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0303 - val_loss: 0.0130
Epoch 27/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0297 - val_loss: 0.0132
Epoch 28/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0291 - val_loss: 0.0141
Epoch 29/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0289 - val_loss: 0.0159
Epoch 30/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0283 - val_loss: 0.0175
Epoch 31/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0279 - val_loss: 0.0178
Epoch 32/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0277 - val_loss: 0.0193
Epoch 33/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0273 - val_loss: 0.0206
Epoch 34/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0271 - val_loss: 0.0224
Epoch 35/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0269 - val_loss: 0.0222
Epoch 36/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0268 - val_loss: 0.0248
Epoch 37/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0265 - val_loss: 0.0237
Epoch 38/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0262 - val_loss: 0.0232
Epoch 39/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0233
Epoch 40/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0257 - val_loss: 0.0224
Epoch 41/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0256 - val_loss: 0.0208
Epoch 42/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0254 - val_loss: 0.0183
Epoch 43/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0160
Epoch 44/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0161
Epoch 45/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0249 - val_loss: 0.0147
Epoch 46/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0246 - val_loss: 0.0147
Epoch 47/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0245 - val_loss: 0.0133
Epoch 48/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0243 - val_loss: 0.0131
Epoch 49/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0243 - val_loss: 0.0124
Epoch 50/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0244 - val_loss: 0.0121
Epoch 51/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0241 - val_loss: 0.0136
Epoch 52/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0243 - val_loss: 0.0121
Epoch 53/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0240 - val_loss: 0.0123
Epoch 54/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0238 - val_loss: 0.0112
Epoch 55/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0237 - val_loss: 0.0113
Epoch 56/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0115
Epoch 57/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0118
Epoch 58/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0120
Epoch 59/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0123
Epoch 60/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0124
Epoch 61/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 62/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0128
Epoch 63/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0125
Epoch 64/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 65/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 66/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 67/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 68/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0131
Epoch 69/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 70/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 71/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 72/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 73/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0127
Epoch 74/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0130
Epoch 75/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0125
Epoch 76/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0128
Epoch 77/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0131
Epoch 78/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 79/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 80/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0126
Epoch 81/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 82/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0132
Epoch 83/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0125
Epoch 84/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 85/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0132
Epoch 86/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0234 - val_loss: 0.0125
Epoch 87/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0129
Epoch 88/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0132
Epoch 89/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0125
Epoch 90/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0128
Execution time:  273.0604820251465
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0470
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_242&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_726 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_727 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_242 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_728 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
48/48 [==============================] - 2s 34ms/step - loss: 0.3995 - val_loss: 0.1772
Epoch 2/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1722 - val_loss: 0.0489
Epoch 3/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1416 - val_loss: 0.0081
Epoch 4/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1256 - val_loss: 0.0075
Epoch 5/52
48/48 [==============================] - 1s 30ms/step - loss: 0.1138 - val_loss: 0.0170
Epoch 6/52
48/48 [==============================] - 1s 31ms/step - loss: 0.1033 - val_loss: 0.0248
Epoch 7/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0938 - val_loss: 0.0264
Epoch 8/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0856 - val_loss: 0.0208
Epoch 9/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0778 - val_loss: 0.0142
Epoch 10/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0703 - val_loss: 0.0087
Epoch 11/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0635 - val_loss: 0.0049
Epoch 12/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0570 - val_loss: 0.0064
Epoch 13/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0519 - val_loss: 0.0069
Epoch 14/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0478 - val_loss: 0.0061
Epoch 15/52
48/48 [==============================] - 1s 29ms/step - loss: 0.0442 - val_loss: 0.0061
Epoch 16/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0410 - val_loss: 0.0068
Epoch 17/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0382 - val_loss: 0.0076
Epoch 18/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0356 - val_loss: 0.0084
Epoch 19/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0332 - val_loss: 0.0092
Epoch 20/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0312 - val_loss: 0.0092
Epoch 21/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0295 - val_loss: 0.0088
Epoch 22/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0284 - val_loss: 0.0087
Epoch 23/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0274 - val_loss: 0.0109
Epoch 24/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0245 - val_loss: 0.0163
Epoch 25/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0159
Epoch 26/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0152
Epoch 27/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0232 - val_loss: 0.0150
Epoch 28/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0231 - val_loss: 0.0143
Epoch 29/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0141
Epoch 30/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0128
Epoch 31/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0144
Epoch 32/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0229 - val_loss: 0.0130
Epoch 33/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0231 - val_loss: 0.0142
Epoch 34/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0229 - val_loss: 0.0128
Epoch 35/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0145
Epoch 36/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0229 - val_loss: 0.0130
Epoch 37/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0231 - val_loss: 0.0143
Epoch 38/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0229 - val_loss: 0.0129
Epoch 39/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0145 ETA: 0s - loss: 0.02
Epoch 40/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0229 - val_loss: 0.0130
Epoch 41/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0142
Epoch 42/52
48/48 [==============================] - 2s 31ms/step - loss: 0.0229 - val_loss: 0.0128
Epoch 43/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0230 - val_loss: 0.0144
Epoch 44/52
48/48 [==============================] - 2s 31ms/step - loss: 0.0229 - val_loss: 0.0129
Epoch 45/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0145
Epoch 46/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0229 - val_loss: 0.0130
Epoch 47/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0142
Epoch 48/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0228 - val_loss: 0.0127
Epoch 49/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0143
Epoch 50/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0229 - val_loss: 0.0119
Epoch 51/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0230 - val_loss: 0.0139
Epoch 52/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0228 - val_loss: 0.0124
Execution time:  79.42036509513855
DNN:
Mean Absolute Error: 0.0261
Root Mean Square Error: 0.0475
Mean Square Error: 0.0023

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_243&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_729 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_730 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_243 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_731 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
77/77 [==============================] - 3s 39ms/step - loss: 0.1409 - val_loss: 0.0363
Epoch 2/80
77/77 [==============================] - 3s 37ms/step - loss: 0.1084 - val_loss: 0.0225
Epoch 3/80
77/77 [==============================] - 3s 38ms/step - loss: 0.1021 - val_loss: 0.0201
Epoch 4/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0956 - val_loss: 0.0197
Epoch 5/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0887 - val_loss: 0.0197
Epoch 6/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0821 - val_loss: 0.0193
Epoch 7/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0759 - val_loss: 0.0201
Epoch 8/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0700 - val_loss: 0.0206
Epoch 9/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0645 - val_loss: 0.0222
Epoch 10/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0594 - val_loss: 0.0241
Epoch 11/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0220
Epoch 12/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0502 - val_loss: 0.0177
Epoch 13/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0460 - val_loss: 0.0143
Epoch 14/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0421 - val_loss: 0.0113
Epoch 15/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0387 - val_loss: 0.0106
Epoch 16/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0357 - val_loss: 0.0102
Epoch 17/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0329 - val_loss: 0.0105
Epoch 18/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0311 - val_loss: 0.0114
Epoch 19/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0302 - val_loss: 0.0104
Epoch 20/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0283 - val_loss: 0.0101
Epoch 21/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0262 - val_loss: 0.0106
Epoch 22/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0249 - val_loss: 0.0129
Epoch 23/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 24/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0123
Epoch 25/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0124
Epoch 26/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 27/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0122
Epoch 28/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 29/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 30/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 31/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 32/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 33/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 34/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 35/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 36/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 37/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 38/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 39/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 40/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 41/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 42/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 43/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 44/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 45/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 46/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 47/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 48/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 49/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 50/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 51/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0123
Epoch 52/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0124
Epoch 53/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 54/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0123
Epoch 55/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 56/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 57/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 58/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 59/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 60/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 61/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 62/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0123
Epoch 63/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0124
Epoch 64/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 65/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0122
Epoch 66/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 67/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 68/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 69/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 70/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 71/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 72/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 73/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 74/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 75/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 76/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 77/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 78/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0251 - val_loss: 0.0122
Epoch 79/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 80/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0123
Execution time:  234.0260627269745
DNN:
Mean Absolute Error: 0.0258
Root Mean Square Error: 0.0464
Mean Square Error: 0.0022

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_244&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_732 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_733 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_244 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_734 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
96/96 [==============================] - 3s 30ms/step - loss: 0.1522 - val_loss: 0.0277
Epoch 2/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0599 - val_loss: 0.0230
Epoch 3/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0585 - val_loss: 0.0228
Epoch 4/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0566 - val_loss: 0.0208
Epoch 5/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0550 - val_loss: 0.0193
Epoch 6/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0533 - val_loss: 0.0190
Epoch 7/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0518 - val_loss: 0.0173
Epoch 8/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0504 - val_loss: 0.0174
Epoch 9/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0489 - val_loss: 0.0160
Epoch 10/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0475 - val_loss: 0.0162
Epoch 11/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0460 - val_loss: 0.0117
Epoch 12/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0448 - val_loss: 0.0167
Epoch 13/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0434 - val_loss: 0.0155
Epoch 14/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0421 - val_loss: 0.0126
Epoch 15/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0410 - val_loss: 0.0118
Epoch 16/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0400 - val_loss: 0.0113
Epoch 17/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0391 - val_loss: 0.0110
Epoch 18/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0380 - val_loss: 0.0107
Epoch 19/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0373 - val_loss: 0.0107
Epoch 20/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0363 - val_loss: 0.0103
Epoch 21/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0349 - val_loss: 0.0102
Epoch 22/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0343 - val_loss: 0.0102
Epoch 23/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0336 - val_loss: 0.0102
Epoch 24/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0330 - val_loss: 0.0102
Epoch 25/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0322 - val_loss: 0.0105
Epoch 26/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0318 - val_loss: 0.0102
Epoch 27/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0312 - val_loss: 0.0101
Epoch 28/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0307 - val_loss: 0.0103
Epoch 29/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0303 - val_loss: 0.0102
Epoch 30/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0297 - val_loss: 0.0102
Epoch 31/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0292 - val_loss: 0.0101
Epoch 32/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0289 - val_loss: 0.0102
Epoch 33/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0286 - val_loss: 0.0103
Epoch 34/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0283 - val_loss: 0.0101
Epoch 35/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0278 - val_loss: 0.0103
Epoch 36/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0278 - val_loss: 0.0102
Epoch 37/90
96/96 [==============================] - 3s 31ms/step - loss: 0.0274 - val_loss: 0.0102
Epoch 38/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0271 - val_loss: 0.0105
Epoch 39/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0269 - val_loss: 0.0102
Epoch 40/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0267 - val_loss: 0.0101
Epoch 41/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0265 - val_loss: 0.0101
Epoch 42/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0264 - val_loss: 0.0102
Epoch 43/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0263 - val_loss: 0.0102
Epoch 44/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0263 - val_loss: 0.0101
Epoch 45/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0263 - val_loss: 0.0101
Epoch 46/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0261 - val_loss: 0.0102
Epoch 47/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0259 - val_loss: 0.0114
Epoch 48/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0262 - val_loss: 0.0103
Epoch 49/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0260 - val_loss: 0.0104
Epoch 50/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0258 - val_loss: 0.0105
Epoch 51/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0258 - val_loss: 0.0106
Epoch 52/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0105
Epoch 53/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0255 - val_loss: 0.0107
Epoch 54/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0255 - val_loss: 0.0106
Epoch 55/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0107
Epoch 56/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0254 - val_loss: 0.0107
Epoch 57/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0109
Epoch 58/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0111
Epoch 59/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0253 - val_loss: 0.0111
Epoch 60/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0253 - val_loss: 0.0113
Epoch 61/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0113
Epoch 62/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0112
Epoch 63/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0108
Epoch 64/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0121
Epoch 65/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0251 - val_loss: 0.0114
Epoch 66/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0250 - val_loss: 0.0115
Epoch 67/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0117
Epoch 68/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 69/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0118
Epoch 70/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 71/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0251 - val_loss: 0.0117
Epoch 72/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 73/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 74/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0118
Epoch 75/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0118
Epoch 76/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 77/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 78/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0117
Epoch 79/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 80/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 81/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0117
Epoch 82/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 83/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0118
Epoch 84/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 85/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 86/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 87/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0118
Epoch 88/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0250 - val_loss: 0.0116
Epoch 89/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0118
Epoch 90/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0250 - val_loss: 0.0116
Execution time:  252.53930497169495
DNN:
Mean Absolute Error: 0.0263
Root Mean Square Error: 0.0479
Mean Square Error: 0.0023

Train RMSE: 0.048
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: tanh
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_245&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_735 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_736 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_245 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_737 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
42/42 [==============================] - 1s 36ms/step - loss: 0.3376 - val_loss: 0.1624
Epoch 2/52
42/42 [==============================] - 1s 31ms/step - loss: 0.1480 - val_loss: 0.0330
Epoch 3/52
42/42 [==============================] - 1s 31ms/step - loss: 0.1211 - val_loss: 0.0200
Epoch 4/52
42/42 [==============================] - 1s 31ms/step - loss: 0.1095 - val_loss: 0.0173
Epoch 5/52
42/42 [==============================] - 1s 33ms/step - loss: 0.1014 - val_loss: 0.0144
Epoch 6/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0953 - val_loss: 0.0124
Epoch 7/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0905 - val_loss: 0.0128
Epoch 8/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0860 - val_loss: 0.0137
Epoch 9/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0820 - val_loss: 0.0133
Epoch 10/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0780 - val_loss: 0.0121
Epoch 11/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0740 - val_loss: 0.0116
Epoch 12/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0701 - val_loss: 0.0112
Epoch 13/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0665 - val_loss: 0.0109
Epoch 14/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0630 - val_loss: 0.0108
Epoch 15/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0596 - val_loss: 0.0108
Epoch 16/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0564 - val_loss: 0.0108
Epoch 17/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0532 - val_loss: 0.0111
Epoch 18/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0504 - val_loss: 0.0113
Epoch 19/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0476 - val_loss: 0.0116
Epoch 20/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0450 - val_loss: 0.0121
Epoch 21/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0425 - val_loss: 0.0125
Epoch 22/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0402 - val_loss: 0.0129
Epoch 23/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0381 - val_loss: 0.0131
Epoch 24/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0361 - val_loss: 0.0132
Epoch 25/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0343 - val_loss: 0.0129
Epoch 26/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0327 - val_loss: 0.0128
Epoch 27/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0313 - val_loss: 0.0125
Epoch 28/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0302 - val_loss: 0.0125
Epoch 29/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0294 - val_loss: 0.0128
Epoch 30/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0287 - val_loss: 0.0131
Epoch 31/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0280 - val_loss: 0.0130
Epoch 32/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0276 - val_loss: 0.0128
Epoch 33/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0271 - val_loss: 0.0130
Epoch 34/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0266 - val_loss: 0.0135
Epoch 35/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0258 - val_loss: 0.0130
Epoch 36/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0258 - val_loss: 0.0133
Epoch 37/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0253 - val_loss: 0.0132
Epoch 38/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0250 - val_loss: 0.0132
Epoch 39/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0249 - val_loss: 0.0131
Epoch 40/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 41/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0246 - val_loss: 0.0124
Epoch 42/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0246 - val_loss: 0.0123
Epoch 43/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0246 - val_loss: 0.0122
Epoch 44/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0246 - val_loss: 0.0125
Epoch 45/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0245 - val_loss: 0.0123
Epoch 46/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0245 - val_loss: 0.0122
Epoch 47/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0245 - val_loss: 0.0121
Epoch 48/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0245 - val_loss: 0.0119
Epoch 49/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0245 - val_loss: 0.0118
Epoch 50/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0245 - val_loss: 0.0117
Epoch 51/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0245 - val_loss: 0.0122
Epoch 52/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0245 - val_loss: 0.0121
Execution time:  72.59026646614075
DNN:
Mean Absolute Error: 0.0259
Root Mean Square Error: 0.0470
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.1
Model: &#34;sequential_246&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_738 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_739 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_246 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_740 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0562 - val_loss: 0.0326
Epoch 2/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0391 - val_loss: 0.0283
Epoch 3/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0370 - val_loss: 0.0262
Epoch 4/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0358 - val_loss: 0.0248
Epoch 5/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0348 - val_loss: 0.0238
Epoch 6/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0340 - val_loss: 0.0234
Epoch 7/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0332 - val_loss: 0.0227
Epoch 8/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0326 - val_loss: 0.0220
Epoch 9/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0320 - val_loss: 0.0214
Epoch 10/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0314 - val_loss: 0.0208
Epoch 11/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0309 - val_loss: 0.0203
Epoch 12/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0303 - val_loss: 0.0201
Epoch 13/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0299 - val_loss: 0.0199
Epoch 14/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0294 - val_loss: 0.0196
Epoch 15/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0289 - val_loss: 0.0190
Epoch 16/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0285 - val_loss: 0.0188
Epoch 17/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0281 - val_loss: 0.0186
Epoch 18/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0277 - val_loss: 0.0183
Epoch 19/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0273 - val_loss: 0.0183
Epoch 20/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0270 - val_loss: 0.0182
Epoch 21/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0266 - val_loss: 0.0177
Epoch 22/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0263 - val_loss: 0.0175
Epoch 23/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0260 - val_loss: 0.0172
Epoch 24/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0257 - val_loss: 0.0170
Epoch 25/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0255 - val_loss: 0.0169
Epoch 26/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0252 - val_loss: 0.0170
Epoch 27/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0250 - val_loss: 0.0169
Epoch 28/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0248 - val_loss: 0.0169
Epoch 29/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0246 - val_loss: 0.0169
Epoch 30/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0245 - val_loss: 0.0169
Epoch 31/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0243 - val_loss: 0.0170
Epoch 32/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0242 - val_loss: 0.0169
Epoch 33/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0240 - val_loss: 0.0167
Epoch 34/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0241 - val_loss: 0.0165
Epoch 35/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0238 - val_loss: 0.0164
Epoch 36/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0237 - val_loss: 0.0163
Epoch 37/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0237 - val_loss: 0.0163
Epoch 38/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0236 - val_loss: 0.0162
Epoch 39/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0162
Epoch 40/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0235 - val_loss: 0.0162
Epoch 41/80
87/87 [==============================] - 3s 38ms/step - loss: 0.0234 - val_loss: 0.0162
Epoch 42/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0233 - val_loss: 0.0161
Epoch 43/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0233 - val_loss: 0.0162
Epoch 44/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0233 - val_loss: 0.0166
Epoch 45/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0231 - val_loss: 0.0158
Epoch 46/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0231 - val_loss: 0.0158
Epoch 47/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0231 - val_loss: 0.0157
Epoch 48/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0231 - val_loss: 0.0156
Epoch 49/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0230 - val_loss: 0.0155
Epoch 50/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0230 - val_loss: 0.0154
Epoch 51/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0230 - val_loss: 0.0157
Epoch 52/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0229 - val_loss: 0.0156
Epoch 53/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0150
Epoch 54/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0148
Epoch 55/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0146
Epoch 56/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0228 - val_loss: 0.0144
Epoch 57/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0228 - val_loss: 0.0143
Epoch 58/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0142
Epoch 59/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0142
Epoch 60/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0228 - val_loss: 0.0141
Epoch 61/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0228 - val_loss: 0.0140
Epoch 62/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0228 - val_loss: 0.0139
Epoch 63/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0138
Epoch 64/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0136
Epoch 65/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0136
Epoch 66/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0227 - val_loss: 0.0135
Epoch 67/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0227 - val_loss: 0.0133
Epoch 68/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0132
Epoch 69/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0131
Epoch 70/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0227 - val_loss: 0.0131
Epoch 71/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0227 - val_loss: 0.0130
Epoch 72/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0226 - val_loss: 0.0130
Epoch 73/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0125
Epoch 74/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0126
Epoch 75/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0125
Epoch 76/80
87/87 [==============================] - 3s 36ms/step - loss: 0.0226 - val_loss: 0.0126
Epoch 77/80
87/87 [==============================] - 3s 37ms/step - loss: 0.0226 - val_loss: 0.0122
Epoch 78/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0128
Epoch 79/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0123
Epoch 80/80
87/87 [==============================] - 3s 35ms/step - loss: 0.0226 - val_loss: 0.0121
Execution time:  251.10111236572266
DNN:
Mean Absolute Error: 0.0261
Root Mean Square Error: 0.0474
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.1
Model: &#34;sequential_247&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_741 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_742 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_247 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_743 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
108/108 [==============================] - 4s 34ms/step - loss: 0.0419 - val_loss: 0.0233
Epoch 2/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0287 - val_loss: 0.0219
Epoch 3/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0286 - val_loss: 0.0214
Epoch 4/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0284 - val_loss: 0.0212
Epoch 5/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0282 - val_loss: 0.0207
Epoch 6/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0280 - val_loss: 0.0205
Epoch 7/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0279 - val_loss: 0.0203
Epoch 8/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0277 - val_loss: 0.0201
Epoch 9/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0276 - val_loss: 0.0199
Epoch 10/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0275 - val_loss: 0.0203
Epoch 11/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0274 - val_loss: 0.0199
Epoch 12/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0272 - val_loss: 0.0191
Epoch 13/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0270 - val_loss: 0.0190
Epoch 14/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0269 - val_loss: 0.0189
Epoch 15/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0268 - val_loss: 0.0189
Epoch 16/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0267 - val_loss: 0.0184
Epoch 17/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0266 - val_loss: 0.0182
Epoch 18/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0265 - val_loss: 0.0184
Epoch 19/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0263 - val_loss: 0.0184
Epoch 20/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0263 - val_loss: 0.0177
Epoch 21/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0262 - val_loss: 0.0178
Epoch 22/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0261 - val_loss: 0.0178
Epoch 23/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0259 - val_loss: 0.0178
Epoch 24/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0178
Epoch 25/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0257 - val_loss: 0.0171
Epoch 26/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0257 - val_loss: 0.0172
Epoch 27/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0255 - val_loss: 0.0172
Epoch 28/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0255 - val_loss: 0.0169
Epoch 29/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0254 - val_loss: 0.0163
Epoch 30/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0253 - val_loss: 0.0163
Epoch 31/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0252 - val_loss: 0.0164
Epoch 32/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0164
Epoch 33/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0249 - val_loss: 0.0159
Epoch 34/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0251 - val_loss: 0.0150
Epoch 35/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0250 - val_loss: 0.0161
Epoch 36/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0248 - val_loss: 0.0160
Epoch 37/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0248 - val_loss: 0.0159
Epoch 38/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0247 - val_loss: 0.0159
Epoch 39/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0246 - val_loss: 0.0160
Epoch 40/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0246 - val_loss: 0.0158
Epoch 41/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0245 - val_loss: 0.0159
Epoch 42/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0244 - val_loss: 0.0156
Epoch 43/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0246 - val_loss: 0.0161
Epoch 44/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0244 - val_loss: 0.0156
Epoch 45/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0243 - val_loss: 0.0159
Epoch 46/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0243 - val_loss: 0.0159
Epoch 47/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0242 - val_loss: 0.0159
Epoch 48/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0241 - val_loss: 0.0152
Epoch 49/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0241 - val_loss: 0.0144
Epoch 50/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0242 - val_loss: 0.0156
Epoch 51/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0241 - val_loss: 0.0154
Epoch 52/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0240 - val_loss: 0.0155
Epoch 53/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0240 - val_loss: 0.0155
Epoch 54/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0239 - val_loss: 0.0151
Epoch 55/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0239 - val_loss: 0.0152
Epoch 56/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0239 - val_loss: 0.0151
Epoch 57/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0238 - val_loss: 0.0150
Epoch 58/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0238 - val_loss: 0.0150
Epoch 59/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0238 - val_loss: 0.0150
Epoch 60/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0237 - val_loss: 0.0147
Epoch 61/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0237 - val_loss: 0.0146
Epoch 62/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0237 - val_loss: 0.0146
Epoch 63/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0237 - val_loss: 0.0146
Epoch 64/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0146
Epoch 65/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0236 - val_loss: 0.0146
Epoch 66/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0236 - val_loss: 0.0145
Epoch 67/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0235 - val_loss: 0.0147
Epoch 68/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0147
Epoch 69/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0146
Epoch 70/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0235 - val_loss: 0.0136
Epoch 71/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0236 - val_loss: 0.0156
Epoch 72/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0148
Epoch 73/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0146
Epoch 74/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0146
Epoch 75/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0234 - val_loss: 0.0146
Epoch 76/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0234 - val_loss: 0.0146
Epoch 77/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0234 - val_loss: 0.0146
Epoch 78/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0233 - val_loss: 0.0146
Epoch 79/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0233 - val_loss: 0.0146
Epoch 80/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0147
Epoch 81/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0146
Epoch 82/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0233 - val_loss: 0.0147
Epoch 83/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0233 - val_loss: 0.0147
Epoch 84/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0233 - val_loss: 0.0147
Epoch 85/90
108/108 [==============================] - 3s 28ms/step - loss: 0.0233 - val_loss: 0.0148
Epoch 86/90
108/108 [==============================] - 3s 29ms/step - loss: 0.0233 - val_loss: 0.0148
Epoch 87/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0148
Epoch 88/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0139
Epoch 89/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0233 - val_loss: 0.0149
Epoch 90/90
108/108 [==============================] - 3s 27ms/step - loss: 0.0232 - val_loss: 0.0150
Execution time:  273.47749066352844
DNN:
Mean Absolute Error: 0.0260
Root Mean Square Error: 0.0466
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.1
Model: &#34;sequential_248&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_744 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_745 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_248 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_746 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
48/48 [==============================] - 2s 34ms/step - loss: 0.0531 - val_loss: 0.0413
Epoch 2/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0427 - val_loss: 0.0397
Epoch 3/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0407 - val_loss: 0.0371
Epoch 4/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0391 - val_loss: 0.0356
Epoch 5/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0380 - val_loss: 0.0348
Epoch 6/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0370 - val_loss: 0.0341
Epoch 7/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0362 - val_loss: 0.0335
Epoch 8/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0356 - val_loss: 0.0330
Epoch 9/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0350 - val_loss: 0.0324
Epoch 10/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0345 - val_loss: 0.0320
Epoch 11/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0340 - val_loss: 0.0316
Epoch 12/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0336 - val_loss: 0.0312
Epoch 13/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0330 - val_loss: 0.0297
Epoch 14/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0324 - val_loss: 0.0295
Epoch 15/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0319 - val_loss: 0.0289
Epoch 16/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0315 - val_loss: 0.0284
Epoch 17/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0312 - val_loss: 0.0279
Epoch 18/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0308 - val_loss: 0.0275
Epoch 19/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0305 - val_loss: 0.0272
Epoch 20/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0302 - val_loss: 0.0270
Epoch 21/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0300 - val_loss: 0.0267
Epoch 22/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0297 - val_loss: 0.0266
Epoch 23/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0294 - val_loss: 0.0262
Epoch 24/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0291 - val_loss: 0.0259
Epoch 25/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0288 - val_loss: 0.0256
Epoch 26/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0286 - val_loss: 0.0254
Epoch 27/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0284 - val_loss: 0.0251
Epoch 28/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0282 - val_loss: 0.0248
Epoch 29/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0279 - val_loss: 0.0245
Epoch 30/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0277 - val_loss: 0.0242
Epoch 31/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0275 - val_loss: 0.0239
Epoch 32/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0273 - val_loss: 0.0236
Epoch 33/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0271 - val_loss: 0.0233
Epoch 34/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0268 - val_loss: 0.0230
Epoch 35/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0267 - val_loss: 0.0226
Epoch 36/52
48/48 [==============================] - 2s 33ms/step - loss: 0.0265 - val_loss: 0.0224
Epoch 37/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0263 - val_loss: 0.0220
Epoch 38/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0261 - val_loss: 0.0218
Epoch 39/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0260 - val_loss: 0.0215
Epoch 40/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0258 - val_loss: 0.0212
Epoch 41/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0257 - val_loss: 0.0210
Epoch 42/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0255 - val_loss: 0.0208
Epoch 43/52
48/48 [==============================] - 2s 32ms/step - loss: 0.0254 - val_loss: 0.0206
Epoch 44/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0253 - val_loss: 0.0204
Epoch 45/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0251 - val_loss: 0.0202
Epoch 46/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0250 - val_loss: 0.0200
Epoch 47/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0249 - val_loss: 0.0198
Epoch 48/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0248 - val_loss: 0.0197
Epoch 49/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0247 - val_loss: 0.0194
Epoch 50/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0246 - val_loss: 0.0193
Epoch 51/52
48/48 [==============================] - 1s 30ms/step - loss: 0.0245 - val_loss: 0.0191
Epoch 52/52
48/48 [==============================] - 1s 31ms/step - loss: 0.0244 - val_loss: 0.0188
Execution time:  79.1154522895813
DNN:
Mean Absolute Error: 0.0266
Root Mean Square Error: 0.0465
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.027
###########################

MODEL:  DNN
sequence:  7d
units:  87
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 80
batchsize: 31
validation_split: 0.2
Model: &#34;sequential_249&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_747 (Dense)            (None, 1008, 87)          174       
_________________________________________________________________
dense_748 (Dense)            (None, 1008, 16)          1408      
_________________________________________________________________
dropout_249 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_749 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,599
Trainable params: 1,599
Non-trainable params: 0
_________________________________________________________________
Epoch 1/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0544 - val_loss: 0.0251
Epoch 2/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0404 - val_loss: 0.0224
Epoch 3/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0385 - val_loss: 0.0198
Epoch 4/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0368 - val_loss: 0.0160
Epoch 5/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0353 - val_loss: 0.0154
Epoch 6/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0346 - val_loss: 0.0154
Epoch 7/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0340 - val_loss: 0.0151
Epoch 8/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0336 - val_loss: 0.0148
Epoch 9/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0332 - val_loss: 0.0146
Epoch 10/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0328 - val_loss: 0.0145
Epoch 11/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0325 - val_loss: 0.0144
Epoch 12/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0322 - val_loss: 0.0143
Epoch 13/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0318 - val_loss: 0.0142
Epoch 14/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0315 - val_loss: 0.0143
Epoch 15/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0312 - val_loss: 0.0143
Epoch 16/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0309 - val_loss: 0.0143
Epoch 17/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0306 - val_loss: 0.0145
Epoch 18/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0303 - val_loss: 0.0145
Epoch 19/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0300 - val_loss: 0.0144
Epoch 20/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0298 - val_loss: 0.0138
Epoch 21/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0295 - val_loss: 0.0146
Epoch 22/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0292 - val_loss: 0.0145
Epoch 23/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0290 - val_loss: 0.0145
Epoch 24/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0287 - val_loss: 0.0145
Epoch 25/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0285 - val_loss: 0.0145
Epoch 26/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0283 - val_loss: 0.0145
Epoch 27/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0281 - val_loss: 0.0145
Epoch 28/80
77/77 [==============================] - 3s 40ms/step - loss: 0.0279 - val_loss: 0.0145
Epoch 29/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0276 - val_loss: 0.0146
Epoch 30/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0275 - val_loss: 0.0139
Epoch 31/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0272 - val_loss: 0.0147
Epoch 32/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0271 - val_loss: 0.0148
Epoch 33/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0269 - val_loss: 0.0147
Epoch 34/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0267 - val_loss: 0.0147
Epoch 35/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0266 - val_loss: 0.0146
Epoch 36/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0264 - val_loss: 0.0138
Epoch 37/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0264 - val_loss: 0.0139
Epoch 38/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0262 - val_loss: 0.0147
Epoch 39/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0261 - val_loss: 0.0144
Epoch 40/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0259 - val_loss: 0.0143
Epoch 41/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0258 - val_loss: 0.0139
Epoch 42/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0258 - val_loss: 0.0141
Epoch 43/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0257 - val_loss: 0.0143
Epoch 44/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0256 - val_loss: 0.0142
Epoch 45/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0254 - val_loss: 0.0127
Epoch 46/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0255 - val_loss: 0.0143
Epoch 47/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0254 - val_loss: 0.0141
Epoch 48/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0253 - val_loss: 0.0140
Epoch 49/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0253 - val_loss: 0.0136
Epoch 50/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0252 - val_loss: 0.0141
Epoch 51/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0129
Epoch 52/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0252 - val_loss: 0.0140
Epoch 53/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0251 - val_loss: 0.0139
Epoch 54/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0250 - val_loss: 0.0138
Epoch 55/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0250 - val_loss: 0.0138
Epoch 56/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0249 - val_loss: 0.0133
Epoch 57/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0249 - val_loss: 0.0136
Epoch 58/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0249 - val_loss: 0.0138
Epoch 59/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0248 - val_loss: 0.0130
Epoch 60/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0249 - val_loss: 0.0138
Epoch 61/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0248 - val_loss: 0.0137
Epoch 62/80
77/77 [==============================] - 3s 36ms/step - loss: 0.0248 - val_loss: 0.0136
Epoch 63/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0248 - val_loss: 0.0136
Epoch 64/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0247 - val_loss: 0.0125
Epoch 65/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0249 - val_loss: 0.0139
Epoch 66/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0247 - val_loss: 0.0137
Epoch 67/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0247 - val_loss: 0.0135
Epoch 68/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0247 - val_loss: 0.0135
Epoch 69/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0247 - val_loss: 0.0135
Epoch 70/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0246 - val_loss: 0.0134
Epoch 71/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0246 - val_loss: 0.0134
Epoch 72/80
77/77 [==============================] - 3s 39ms/step - loss: 0.0246 - val_loss: 0.0134
Epoch 73/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0246 - val_loss: 0.0133
Epoch 74/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0246 - val_loss: 0.0133
Epoch 75/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0246 - val_loss: 0.0133
Epoch 76/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0245 - val_loss: 0.0132
Epoch 77/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0245 - val_loss: 0.0130
Epoch 78/80
77/77 [==============================] - 3s 38ms/step - loss: 0.0245 - val_loss: 0.0131
Epoch 79/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0245 - val_loss: 0.0132
Epoch 80/80
77/77 [==============================] - 3s 37ms/step - loss: 0.0245 - val_loss: 0.0131
Execution time:  233.4667613506317
DNN:
Mean Absolute Error: 0.0258
Root Mean Square Error: 0.0462
Mean Square Error: 0.0021

Train RMSE: 0.046
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  80
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 90
batchsize: 25
validation_split: 0.2
Model: &#34;sequential_250&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_750 (Dense)            (None, 1008, 80)          160       
_________________________________________________________________
dense_751 (Dense)            (None, 1008, 16)          1296      
_________________________________________________________________
dropout_250 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_752 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 1,473
Trainable params: 1,473
Non-trainable params: 0
_________________________________________________________________
Epoch 1/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0404 - val_loss: 0.0147
Epoch 2/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0301 - val_loss: 0.0135
Epoch 3/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0297 - val_loss: 0.0130
Epoch 4/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0295 - val_loss: 0.0129
Epoch 5/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0293 - val_loss: 0.0126
Epoch 6/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0292 - val_loss: 0.0126
Epoch 7/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0288 - val_loss: 0.0124
Epoch 8/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0288 - val_loss: 0.0121
Epoch 9/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0287 - val_loss: 0.0120
Epoch 10/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0285 - val_loss: 0.0119
Epoch 11/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0284 - val_loss: 0.0118
Epoch 12/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0284 - val_loss: 0.0116
Epoch 13/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0282 - val_loss: 0.0116
Epoch 14/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0281 - val_loss: 0.0116
Epoch 15/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0281 - val_loss: 0.0115
Epoch 16/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0280 - val_loss: 0.0117
Epoch 17/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0279 - val_loss: 0.0116
Epoch 18/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0278 - val_loss: 0.0114
Epoch 19/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0278 - val_loss: 0.0114
Epoch 20/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0277 - val_loss: 0.0114
Epoch 21/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0276 - val_loss: 0.0114
Epoch 22/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0275 - val_loss: 0.0114
Epoch 23/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0275 - val_loss: 0.0114
Epoch 24/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0274 - val_loss: 0.0115
Epoch 25/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0274 - val_loss: 0.0113
Epoch 26/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0273 - val_loss: 0.0113
Epoch 27/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0273 - val_loss: 0.0113
Epoch 28/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0272 - val_loss: 0.0115
Epoch 29/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0271 - val_loss: 0.0115
Epoch 30/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0271 - val_loss: 0.0115
Epoch 31/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0271 - val_loss: 0.0113
Epoch 32/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0269 - val_loss: 0.0110
Epoch 33/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0270 - val_loss: 0.0116
Epoch 34/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0269 - val_loss: 0.0117
Epoch 35/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0268 - val_loss: 0.0116
Epoch 36/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0268 - val_loss: 0.0116
Epoch 37/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0267 - val_loss: 0.0116
Epoch 38/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0267 - val_loss: 0.0115
Epoch 39/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0267 - val_loss: 0.0116
Epoch 40/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0266 - val_loss: 0.0116
Epoch 41/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0266 - val_loss: 0.0116
Epoch 42/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0265 - val_loss: 0.0117
Epoch 43/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0265 - val_loss: 0.0118
Epoch 44/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0264 - val_loss: 0.0118
Epoch 45/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0264 - val_loss: 0.0117
Epoch 46/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0264 - val_loss: 0.0117
Epoch 47/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0263 - val_loss: 0.0118
Epoch 48/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0263 - val_loss: 0.0118
Epoch 49/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0261 - val_loss: 0.0108
Epoch 50/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0263 - val_loss: 0.0121
Epoch 51/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0261 - val_loss: 0.0120
Epoch 52/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0261 - val_loss: 0.0120
Epoch 53/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0260 - val_loss: 0.0121
Epoch 54/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0260 - val_loss: 0.0119
Epoch 55/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0260 - val_loss: 0.0120
Epoch 56/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0260 - val_loss: 0.0121
Epoch 57/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0259 - val_loss: 0.0122
Epoch 58/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0110
Epoch 59/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0259 - val_loss: 0.0127
Epoch 60/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0258 - val_loss: 0.0124
Epoch 61/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0125
Epoch 62/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0258 - val_loss: 0.0126
Epoch 63/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0258 - val_loss: 0.0126
Epoch 64/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0257 - val_loss: 0.0126
Epoch 65/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0257 - val_loss: 0.0126
Epoch 66/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0257 - val_loss: 0.0126
Epoch 67/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0256 - val_loss: 0.0126
Epoch 68/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0256 - val_loss: 0.0127
Epoch 69/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0256 - val_loss: 0.0127
Epoch 70/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0255 - val_loss: 0.0126
Epoch 71/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0255 - val_loss: 0.0127
Epoch 72/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0255 - val_loss: 0.0127
Epoch 73/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0127
Epoch 74/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0127
Epoch 75/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0254 - val_loss: 0.0127
Epoch 76/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0254 - val_loss: 0.0127
Epoch 77/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0253 - val_loss: 0.0127
Epoch 78/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0127
Epoch 79/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0253 - val_loss: 0.0127
Epoch 80/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0114
Epoch 81/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0254 - val_loss: 0.0134
Epoch 82/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0252 - val_loss: 0.0127
Epoch 83/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0252 - val_loss: 0.0126
Epoch 84/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0127
Epoch 85/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0252 - val_loss: 0.0127
Epoch 86/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0252 - val_loss: 0.0126
Epoch 87/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0120
Epoch 88/90
96/96 [==============================] - 3s 30ms/step - loss: 0.0252 - val_loss: 0.0129
Epoch 89/90
96/96 [==============================] - 3s 29ms/step - loss: 0.0251 - val_loss: 0.0126
Epoch 90/90
96/96 [==============================] - 3s 28ms/step - loss: 0.0251 - val_loss: 0.0126
Execution time:  252.21988654136658
DNN:
Mean Absolute Error: 0.0261
Root Mean Square Error: 0.0469
Mean Square Error: 0.0022

Train RMSE: 0.047
Train MSE: 0.002
Train MAE: 0.026
###########################

MODEL:  DNN
sequence:  7d
units:  12
dropout1:  0.48476373451509647
optimizer: adamax
activationDense: sigmoid
epochs: 52
batchsize: 57
validation_split: 0.2
Model: &#34;sequential_251&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_753 (Dense)            (None, 1008, 12)          24        
_________________________________________________________________
dense_754 (Dense)            (None, 1008, 16)          208       
_________________________________________________________________
dropout_251 (Dropout)        (None, 1008, 16)          0         
_________________________________________________________________
dense_755 (Dense)            (None, 1008, 1)           17        
=================================================================
Total params: 249
Trainable params: 249
Non-trainable params: 0
_________________________________________________________________
Epoch 1/52
42/42 [==============================] - 2s 36ms/step - loss: 0.0601 - val_loss: 0.0470- ETA: 0s - loss: 0.
Epoch 2/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0446 - val_loss: 0.0354
Epoch 3/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0430 - val_loss: 0.0352
Epoch 4/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0419 - val_loss: 0.0341
Epoch 5/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0408 - val_loss: 0.0330
Epoch 6/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0399 - val_loss: 0.0321
Epoch 7/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0391 - val_loss: 0.0311
Epoch 8/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0381 - val_loss: 0.0290
Epoch 9/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0370 - val_loss: 0.0285
Epoch 10/52
42/42 [==============================] - 1s 30ms/step - loss: 0.0362 - val_loss: 0.0281
Epoch 11/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0355 - val_loss: 0.0273
Epoch 12/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0350 - val_loss: 0.0269
Epoch 13/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0345 - val_loss: 0.0266
Epoch 14/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0341 - val_loss: 0.0261
Epoch 15/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0337 - val_loss: 0.0258
Epoch 16/52
42/42 [==============================] - 1s 30ms/step - loss: 0.0333 - val_loss: 0.0253
Epoch 17/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0330 - val_loss: 0.0248
Epoch 18/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0326 - val_loss: 0.0243
Epoch 19/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0323 - val_loss: 0.0240
Epoch 20/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0319 - val_loss: 0.0235
Epoch 21/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0315 - val_loss: 0.0231
Epoch 22/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0312 - val_loss: 0.0228
Epoch 23/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0308 - val_loss: 0.0225
Epoch 24/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0305 - val_loss: 0.0221
Epoch 25/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0303 - val_loss: 0.0218
Epoch 26/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0299 - val_loss: 0.0215
Epoch 27/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0297 - val_loss: 0.0212
Epoch 28/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0295 - val_loss: 0.0209
Epoch 29/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0292 - val_loss: 0.0206
Epoch 30/52
42/42 [==============================] - 1s 34ms/step - loss: 0.0290 - val_loss: 0.0204
Epoch 31/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0288 - val_loss: 0.0201
Epoch 32/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0286 - val_loss: 0.0200
Epoch 33/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0285 - val_loss: 0.0198
Epoch 34/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0283 - val_loss: 0.0196
Epoch 35/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0282 - val_loss: 0.0194
Epoch 36/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0280 - val_loss: 0.0193
Epoch 37/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0279 - val_loss: 0.0191
Epoch 38/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0277 - val_loss: 0.0189
Epoch 39/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0276 - val_loss: 0.0187
Epoch 40/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0274 - val_loss: 0.0185
Epoch 41/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0273 - val_loss: 0.0183
Epoch 42/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0272 - val_loss: 0.0181
Epoch 43/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0271 - val_loss: 0.0179
Epoch 44/52
42/42 [==============================] - 1s 32ms/step - loss: 0.0269 - val_loss: 0.0177
Epoch 45/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0268 - val_loss: 0.0176
Epoch 46/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0267 - val_loss: 0.0174
Epoch 47/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0266 - val_loss: 0.0172
Epoch 48/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0265 - val_loss: 0.0171
Epoch 49/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0264 - val_loss: 0.0169
Epoch 50/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0263 - val_loss: 0.0168TA: 0s - loss: 0.0
Epoch 51/52
42/42 [==============================] - 1s 31ms/step - loss: 0.0262 - val_loss: 0.0167
Epoch 52/52
42/42 [==============================] - 1s 33ms/step - loss: 0.0262 - val_loss: 0.0165
Execution time:  72.02425909042358
DNN:
Mean Absolute Error: 0.0263
Root Mean Square Error: 0.0454
Mean Square Error: 0.0021

Train RMSE: 0.045
Train MSE: 0.002
Train MAE: 0.026
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">list_results</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;resultats-cerca-optim-dnn-v1.csv&quot;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">list_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[11]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>sequence</th>
      <th>activationDense</th>
      <th>optimizer</th>
      <th>dropout1</th>
      <th>units</th>
      <th>epochs</th>
      <th>batchsize</th>
      <th>validation_split</th>
      <th>RMSE</th>
      <th>MSE</th>
      <th>MAE</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.1</td>
      <td>0.019200</td>
      <td>0.000369</td>
      <td>0.009905</td>
      <td>17.237772</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.1</td>
      <td>0.018856</td>
      <td>0.000356</td>
      <td>0.005032</td>
      <td>22.277235</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.1</td>
      <td>0.017678</td>
      <td>0.000313</td>
      <td>0.011355</td>
      <td>6.903105</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.2</td>
      <td>0.021332</td>
      <td>0.000455</td>
      <td>0.013577</td>
      <td>16.152798</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.2</td>
      <td>0.021047</td>
      <td>0.000443</td>
      <td>0.008638</td>
      <td>20.035282</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.2</td>
      <td>0.024989</td>
      <td>0.000624</td>
      <td>0.016478</td>
      <td>6.411896</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.1</td>
      <td>0.018717</td>
      <td>0.000350</td>
      <td>0.010021</td>
      <td>16.140609</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.1</td>
      <td>0.020069</td>
      <td>0.000403</td>
      <td>0.005485</td>
      <td>20.425318</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.1</td>
      <td>0.019351</td>
      <td>0.000374</td>
      <td>0.010890</td>
      <td>6.711839</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.2</td>
      <td>0.028214</td>
      <td>0.000796</td>
      <td>0.014225</td>
      <td>15.500949</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.2</td>
      <td>0.023030</td>
      <td>0.000530</td>
      <td>0.006963</td>
      <td>19.581539</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.2</td>
      <td>0.024698</td>
      <td>0.000610</td>
      <td>0.014355</td>
      <td>6.615545</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.1</td>
      <td>0.061576</td>
      <td>0.003792</td>
      <td>0.055773</td>
      <td>15.818824</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.1</td>
      <td>0.038144</td>
      <td>0.001455</td>
      <td>0.031027</td>
      <td>19.608741</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.1</td>
      <td>0.328606</td>
      <td>0.107982</td>
      <td>0.326552</td>
      <td>6.342408</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.2</td>
      <td>0.073644</td>
      <td>0.005423</td>
      <td>0.069398</td>
      <td>15.335560</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.2</td>
      <td>0.048324</td>
      <td>0.002335</td>
      <td>0.041429</td>
      <td>19.509650</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.2</td>
      <td>0.327068</td>
      <td>0.106973</td>
      <td>0.324953</td>
      <td>6.494782</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adadelta</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.1</td>
      <td>0.059135</td>
      <td>0.003497</td>
      <td>0.051032</td>
      <td>16.379211</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adadelta</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.1</td>
      <td>0.046956</td>
      <td>0.002205</td>
      <td>0.036599</td>
      <td>19.777558</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">list_results</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;RMSE&#39;</span><span class="p">,</span> <span class="s1">&#39;sequence&#39;</span><span class="p">])</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[12]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>sequence</th>
      <th>activationDense</th>
      <th>optimizer</th>
      <th>dropout1</th>
      <th>units</th>
      <th>epochs</th>
      <th>batchsize</th>
      <th>validation_split</th>
      <th>RMSE</th>
      <th>MSE</th>
      <th>MAE</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adamax</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.2</td>
      <td>0.013449</td>
      <td>0.000181</td>
      <td>0.005524</td>
      <td>19.100332</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adamax</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.2</td>
      <td>0.013451</td>
      <td>0.000181</td>
      <td>0.007040</td>
      <td>15.485010</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.405196</td>
      <td>87</td>
      <td>80</td>
      <td>31</td>
      <td>0.1</td>
      <td>0.013571</td>
      <td>0.000184</td>
      <td>0.007138</td>
      <td>15.828002</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.1</td>
      <td>0.013672</td>
      <td>0.000187</td>
      <td>0.007954</td>
      <td>6.807474</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.118148</td>
      <td>80</td>
      <td>90</td>
      <td>25</td>
      <td>0.2</td>
      <td>0.014728</td>
      <td>0.000217</td>
      <td>0.009229</td>
      <td>19.297134</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>7d</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.2</td>
      <td>0.371034</td>
      <td>0.137667</td>
      <td>0.368752</td>
      <td>74.603660</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>12h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.2</td>
      <td>0.380825</td>
      <td>0.145028</td>
      <td>0.378488</td>
      <td>10.224256</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>1d</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.1</td>
      <td>0.443665</td>
      <td>0.196839</td>
      <td>0.440807</td>
      <td>17.952344</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>6h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.2</td>
      <td>0.452307</td>
      <td>0.204582</td>
      <td>0.449499</td>
      <td>9.281137</td>
    </tr>
    <tr>
      <th>0</th>
      <td>DNN</td>
      <td>3d</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.484764</td>
      <td>12</td>
      <td>52</td>
      <td>57</td>
      <td>0.1</td>
      <td>0.473704</td>
      <td>0.224396</td>
      <td>0.470815</td>
      <td>44.018067</td>
    </tr>
  </tbody>
</table>
<p>252 rows × 13 columns</p>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[&nbsp;]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[&nbsp;]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div>
</body>







</html>
