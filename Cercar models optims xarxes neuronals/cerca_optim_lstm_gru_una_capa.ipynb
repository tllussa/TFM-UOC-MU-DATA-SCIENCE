{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cercador model òptim de LSTM i GRU de una capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lMCuRV3SJK6w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dj_kr\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from time import time\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d9z8yxsJK6x"
   },
   "source": [
    "## Càrrega de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "seq5Q3gjJK6y",
    "outputId": "72e82bf7-9c19-4d0d-da4a-42c4aff349b3"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/SentDATA.csv')\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df = df.set_index('Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VLv_DJhJK6y"
   },
   "source": [
    "## Transformació de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NUx3mg0RJK6y"
   },
   "outputs": [],
   "source": [
    "columns = ['PM1','PM25','PM10','PM1ATM','PM25ATM','PM10ATM']\n",
    "\n",
    "df1 = df.copy();\n",
    "\n",
    "df1 = df1.rename(columns={\"PM 1\":\"PM1\",\"PM 2.5\":\"PM25\",\"PM 10\":\"PM10\",\"PM 1 ATM\":\"PM1ATM\",\"PM 2.5 ATM\":\"PM25ATM\",\"PM 10 ATM\":\"PM10ATM\"})\n",
    "\n",
    "df1['PM1'] = df['PM 1'].astype(np.float32)\n",
    "df1['PM25'] = df['PM 2.5'].astype(np.float32)\n",
    "df1['PM10'] = df['PM 10'].astype(np.float32)\n",
    "df1['PM1ATM'] = df['PM 1 ATM'].astype(np.float32)\n",
    "df1['PM25ATM'] = df['PM 2.5 ATM'].astype(np.float32)\n",
    "df1['PM10ATM'] = df['PM 10 ATM'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eh0Xz9k-IHtg"
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLJVxlmlJK6z"
   },
   "source": [
    "## Crear dades d'entrenament i de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1Dwm-E9JK6z",
    "outputId": "620f3cbe-6354-40b1-88c3-a80206b571f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3991, 7), (998, 7))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(df2) * 0.8)\n",
    "test_size = len(df2) - train_size\n",
    "train, test = df2.iloc[0:train_size], df2.iloc[train_size:len(df2)]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuMi9G1LJK6z"
   },
   "source": [
    "## Normalitzar les dades d'entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqcC5lWvJK6z",
    "outputId": "ca0f0c98-e1f3-4e86-dc5f-65dcb992a209"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-13-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-13-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-13-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-13-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-13-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Standardize the data\n",
    "for col in columns:\n",
    "    scaler = StandardScaler()\n",
    "    train[col] = scaler.fit_transform(train[[col]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LsNLCHSJK6z"
   },
   "source": [
    "## Crear finestra de temps PM 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEH4EmxcJK6z",
    "outputId": "b8c5f2c4-adca-47ef-fb6b-c734f0160dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1h shape: (3847, 144, 1)\n",
      "X_train3d shape: (3973, 18, 1)\n",
      "X_train6h shape: (3955, 36, 1)\n",
      "X_train12h shape: (3919, 72, 1)\n",
      "X_train1d shape: (3847, 144, 1)\n",
      "X_train3d shape: (3559, 432, 1)\n",
      "X_train7d shape: (2983, 1008, 1)\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS=144 #6 registres hora x 24h x 3 --> equival a una finestra d'un dia\n",
    "\n",
    "def create_sequences(X, y, time_steps=TIME_STEPS):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        Xs.append(X.iloc[i:(i+time_steps)].values)\n",
    "        ys.append(y.iloc[i+time_steps])\n",
    "    \n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train1h, y_train1h = create_sequences(train[[columns[1]]], train[columns[1]], 6) #1 hour\n",
    "\n",
    "X_train3h, y_train3h = create_sequences(train[[columns[1]]], train[columns[1]], 18) #3 hours\n",
    "\n",
    "X_train6h, y_train6h = create_sequences(train[[columns[1]]], train[columns[1]], 36) #6 hours\n",
    "\n",
    "X_train12h, y_train12h = create_sequences(train[[columns[1]]], train[columns[1]], 72) #12 hours\n",
    "\n",
    "X_train1d, y_train1d = create_sequences(train[[columns[1]]], train[columns[1]], 144) #1 day\n",
    "\n",
    "X_train3d, y_train3d = create_sequences(train[[columns[1]]], train[columns[1]], 432) #3 days\n",
    "\n",
    "X_train7d, y_train7d = create_sequences(train[[columns[1]]], train[columns[1]], 1008) #7 days\n",
    "#X_test, y_test = create_sequences(test[[columns[1]]], test[columns[1]])\n",
    "\n",
    "print(f'X_train1h shape: {X_train1d.shape}')\n",
    "print(f'X_train3d shape: {X_train3h.shape}')\n",
    "print(f'X_train6h shape: {X_train6h.shape}')\n",
    "print(f'X_train12h shape: {X_train12h.shape}')\n",
    "print(f'X_train1d shape: {X_train1d.shape}')\n",
    "print(f'X_train3d shape: {X_train3d.shape}')\n",
    "print(f'X_train7d shape: {X_train7d.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "\n",
    "    print(model_name + ':')\n",
    "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
    "    print('Root Mean Square Error: {:.4f}'.format(rmse))\n",
    "    print('Mean Square Error: {:.4f}'.format(mse))\n",
    "    print('')\n",
    "    return mae,rmse,mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9caadjyJK6z"
   },
   "source": [
    "## Cerca dels models òptims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7znr_i72OjVb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 6, 40)             6720      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5607 - val_loss: 0.3392\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3897 - val_loss: 0.3060\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3470 - val_loss: 0.2439\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3207 - val_loss: 0.2145\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3083 - val_loss: 0.1999\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2997 - val_loss: 0.1878\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2932 - val_loss: 0.1743\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2871 - val_loss: 0.1666\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2831 - val_loss: 0.1602\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2797 - val_loss: 0.1566\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2774 - val_loss: 0.1505\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2761 - val_loss: 0.1522\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2759 - val_loss: 0.1484\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2744 - val_loss: 0.1477\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2737 - val_loss: 0.1459\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2734 - val_loss: 0.1442\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2729 - val_loss: 0.1432\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2723 - val_loss: 0.1418\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2723 - val_loss: 0.1429\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2722 - val_loss: 0.1414\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2722 - val_loss: 0.1408\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2713 - val_loss: 0.1382\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2719 - val_loss: 0.1373\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2711 - val_loss: 0.1376\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2707 - val_loss: 0.1393\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2706 - val_loss: 0.1385\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2708 - val_loss: 0.1372\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2707 - val_loss: 0.1344\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2697 - val_loss: 0.1372\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 0.1331\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2700 - val_loss: 0.1343\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.1345\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2697 - val_loss: 0.1346\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2691 - val_loss: 0.1342\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2692 - val_loss: 0.1324\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2694 - val_loss: 0.1301\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2688 - val_loss: 0.1310\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2686 - val_loss: 0.1305\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2684 - val_loss: 0.1274\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2684 - val_loss: 0.1280\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2679 - val_loss: 0.1250\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2676 - val_loss: 0.1278\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2674 - val_loss: 0.1245\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2670 - val_loss: 0.1267\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2673 - val_loss: 0.1233\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2666 - val_loss: 0.1192\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2669 - val_loss: 0.1214\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2669 - val_loss: 0.1239\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2666 - val_loss: 0.1187\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2662 - val_loss: 0.1213\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2658 - val_loss: 0.1182\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2657 - val_loss: 0.1202\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2654 - val_loss: 0.1169\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2656 - val_loss: 0.1139\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2650 - val_loss: 0.1136\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2648 - val_loss: 0.1155\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2644 - val_loss: 0.1138\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2644 - val_loss: 0.1169\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2648 - val_loss: 0.1144\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2643 - val_loss: 0.1115\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2639 - val_loss: 0.1131\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2642 - val_loss: 0.1099\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2636 - val_loss: 0.1092\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2636 - val_loss: 0.1069\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 0.1059\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 0.1104\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2632 - val_loss: 0.1086\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2629 - val_loss: 0.1084\n",
      "Execution time:  27.412817239761353\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1593\n",
      "Root Mean Square Error: 0.5818\n",
      "Mean Square Error: 0.3385\n",
      "\n",
      "Train RMSE: 0.582\n",
      "Train MSE: 0.338\n",
      "Train MAE: 0.159\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 6, 55)             12540     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 2s 6ms/step - loss: 0.3993 - val_loss: 0.2529\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2935 - val_loss: 0.2201\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2740 - val_loss: 0.2121\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2690 - val_loss: 0.2104\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2678 - val_loss: 0.2091\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2673 - val_loss: 0.2083\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2670 - val_loss: 0.2075\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2667 - val_loss: 0.2064\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2662 - val_loss: 0.2054\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2659 - val_loss: 0.2051\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2654 - val_loss: 0.2026\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2002\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2644 - val_loss: 0.1984\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.1973\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2630 - val_loss: 0.1934\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2624 - val_loss: 0.1915\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2613 - val_loss: 0.1889\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2610 - val_loss: 0.1872\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2605 - val_loss: 0.1875\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2598 - val_loss: 0.1867\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1840\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2592 - val_loss: 0.1835\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2589 - val_loss: 0.1808\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2585 - val_loss: 0.1827\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2583 - val_loss: 0.1829\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2580 - val_loss: 0.1816\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2576 - val_loss: 0.1804\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2578 - val_loss: 0.1793\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2572 - val_loss: 0.1785\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2571 - val_loss: 0.1803\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2569 - val_loss: 0.1778\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2570 - val_loss: 0.1786\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2568 - val_loss: 0.1758\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2566 - val_loss: 0.1780\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2564 - val_loss: 0.1779\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2567 - val_loss: 0.1775\n",
      "Execution time:  51.12305450439453\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1511\n",
      "Root Mean Square Error: 0.5819\n",
      "Mean Square Error: 0.3386\n",
      "\n",
      "Train RMSE: 0.582\n",
      "Train MSE: 0.339\n",
      "Train MAE: 0.151\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 6, 40)             6720      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8414 - val_loss: 1.0555\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6386 - val_loss: 0.8866\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5859 - val_loss: 0.8566\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5654 - val_loss: 0.8405\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5545 - val_loss: 0.8302\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5469 - val_loss: 0.8226\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5412 - val_loss: 0.8167\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5362 - val_loss: 0.8120\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5314 - val_loss: 0.8082\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5243 - val_loss: 0.8049\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5164 - val_loss: 0.8022\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5111 - val_loss: 0.8002\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5073 - val_loss: 0.7985\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5042 - val_loss: 0.7972\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5023 - val_loss: 0.7962\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5003 - val_loss: 0.7955\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4990 - val_loss: 0.7949\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4980 - val_loss: 0.7945\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4972 - val_loss: 0.7941\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4966 - val_loss: 0.7939\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4961 - val_loss: 0.7937\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7935\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7934\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 0.7933\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4945 - val_loss: 0.7932\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4944 - val_loss: 0.7931\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7931\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4939 - val_loss: 0.7930\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4938 - val_loss: 0.7930\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4936 - val_loss: 0.7929\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4937 - val_loss: 0.7929\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4935 - val_loss: 0.7929\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4933 - val_loss: 0.7929\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4934 - val_loss: 0.7928\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4931 - val_loss: 0.7928\n",
      "Epoch 36/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4933 - val_loss: 0.7928\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4931 - val_loss: 0.7928\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4929 - val_loss: 0.7928\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4932 - val_loss: 0.7928\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4929 - val_loss: 0.7927\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.7927\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4928 - val_loss: 0.7927\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.7927\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4928 - val_loss: 0.7927\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.7927\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7927\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7927\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4922 - val_loss: 0.7927\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7927\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4924 - val_loss: 0.7926\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4924 - val_loss: 0.7926\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4922 - val_loss: 0.7926\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7926\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4921 - val_loss: 0.7926\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4921 - val_loss: 0.7926\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4921 - val_loss: 0.7926\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4922 - val_loss: 0.7926\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4922 - val_loss: 0.7926\n",
      "Execution time:  28.099231719970703\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4837\n",
      "Root Mean Square Error: 0.7525\n",
      "Mean Square Error: 0.5663\n",
      "\n",
      "Train RMSE: 0.753\n",
      "Train MSE: 0.566\n",
      "Train MAE: 0.484\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 6, 55)             12540     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.6488 - val_loss: 0.7089\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5468 - val_loss: 0.6836\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5270 - val_loss: 0.6669\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5125 - val_loss: 0.6599\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5022 - val_loss: 0.6556\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4968 - val_loss: 0.6533\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4946 - val_loss: 0.6523\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4934 - val_loss: 0.6516\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4928 - val_loss: 0.6510\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4923 - val_loss: 0.6514\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4920 - val_loss: 0.6510\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4917 - val_loss: 0.6508\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.6510\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4914 - val_loss: 0.6506\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4913 - val_loss: 0.6510\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4911 - val_loss: 0.6511\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4909 - val_loss: 0.6513\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4908 - val_loss: 0.6507\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4904 - val_loss: 0.6510\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4902 - val_loss: 0.6514\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4904 - val_loss: 0.6516\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4902 - val_loss: 0.6511\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4902 - val_loss: 0.6512\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4900 - val_loss: 0.6510\n",
      "Execution time:  35.80128622055054\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4832\n",
      "Root Mean Square Error: 0.7581\n",
      "Mean Square Error: 0.5746\n",
      "\n",
      "Train RMSE: 0.758\n",
      "Train MSE: 0.575\n",
      "Train MAE: 0.483\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 6, 40)             6720      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6788 - val_loss: 0.7690\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6787 - val_loss: 0.7687\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6784 - val_loss: 0.7684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6784 - val_loss: 0.7680\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6778 - val_loss: 0.7677\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6777 - val_loss: 0.7674\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6776 - val_loss: 0.7670\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6774 - val_loss: 0.7667\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6770 - val_loss: 0.7663\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6769 - val_loss: 0.7660\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6767 - val_loss: 0.7656\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6763 - val_loss: 0.7653\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6763 - val_loss: 0.7649\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6758 - val_loss: 0.7646\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6757 - val_loss: 0.7642\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6755 - val_loss: 0.7638\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6754 - val_loss: 0.7635\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6751 - val_loss: 0.7631\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6748 - val_loss: 0.7628\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6743 - val_loss: 0.7624\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6744 - val_loss: 0.7620\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6743 - val_loss: 0.7616\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6740 - val_loss: 0.7613\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6737 - val_loss: 0.7609\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6734 - val_loss: 0.7605\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6731 - val_loss: 0.7601\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6729 - val_loss: 0.7598\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6729 - val_loss: 0.7594\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6725 - val_loss: 0.7590\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6723 - val_loss: 0.7586\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6721 - val_loss: 0.7582\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6716 - val_loss: 0.7579\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6717 - val_loss: 0.7575\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6713 - val_loss: 0.7571\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6709 - val_loss: 0.7567\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6709 - val_loss: 0.7563\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6706 - val_loss: 0.7559\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6703 - val_loss: 0.7556\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6701 - val_loss: 0.7552\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6698 - val_loss: 0.7548\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6696 - val_loss: 0.7544\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6695 - val_loss: 0.7540\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6691 - val_loss: 0.7536\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6690 - val_loss: 0.7532\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6685 - val_loss: 0.7528\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6683 - val_loss: 0.7524\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6683 - val_loss: 0.7520\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6680 - val_loss: 0.7516\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6677 - val_loss: 0.7512\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6676 - val_loss: 0.7508\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6672 - val_loss: 0.7504\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6670 - val_loss: 0.7500\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6669 - val_loss: 0.7496\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6667 - val_loss: 0.7492\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6662 - val_loss: 0.7488\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6662 - val_loss: 0.7484\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6656 - val_loss: 0.7480\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6655 - val_loss: 0.7476\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6652 - val_loss: 0.7472\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6651 - val_loss: 0.7468\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6649 - val_loss: 0.7464\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6645 - val_loss: 0.7460\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6646 - val_loss: 0.7456\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6639 - val_loss: 0.7452\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6639 - val_loss: 0.7448\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6636 - val_loss: 0.7444\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6634 - val_loss: 0.7440\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6629 - val_loss: 0.7436\n",
      "Execution time:  27.464905261993408\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6691\n",
      "Root Mean Square Error: 0.9589\n",
      "Mean Square Error: 0.9195\n",
      "\n",
      "Train RMSE: 0.959\n",
      "Train MSE: 0.919\n",
      "Train MAE: 0.669\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 6, 55)             12540     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.7034 - val_loss: 0.6855\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7019 - val_loss: 0.6838\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7005 - val_loss: 0.6821\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6992 - val_loss: 0.6804\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6979 - val_loss: 0.6786\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6965 - val_loss: 0.6768\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6951 - val_loss: 0.6750\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6938 - val_loss: 0.6731\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6924 - val_loss: 0.6713\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - ETA: 0s - loss: 0.693 - 1s 4ms/step - loss: 0.6910 - val_loss: 0.6694\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6895 - val_loss: 0.6674\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6880 - val_loss: 0.6655\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6867 - val_loss: 0.6635\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6852 - val_loss: 0.6616\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6838 - val_loss: 0.6595\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6823 - val_loss: 0.6575\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6808 - val_loss: 0.6555\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6794 - val_loss: 0.6534\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6778 - val_loss: 0.6514\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6763 - val_loss: 0.6493\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6747 - val_loss: 0.6472\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6733 - val_loss: 0.6450\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6716 - val_loss: 0.6429\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6704 - val_loss: 0.6407\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6688 - val_loss: 0.6386\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6671 - val_loss: 0.6364\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6655 - val_loss: 0.6342\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6639 - val_loss: 0.6320\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6623 - val_loss: 0.6297\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6607 - val_loss: 0.6275\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6591 - val_loss: 0.6253\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6575 - val_loss: 0.6230\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6559 - val_loss: 0.6207\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6543 - val_loss: 0.6184\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6526 - val_loss: 0.6161\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.6510 - val_loss: 0.6138\n",
      "Execution time:  51.92017316818237\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6398\n",
      "Root Mean Square Error: 0.9327\n",
      "Mean Square Error: 0.8699\n",
      "\n",
      "Train RMSE: 0.933\n",
      "Train MSE: 0.870\n",
      "Train MAE: 0.640\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 6, 40)             6720      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8879 - val_loss: 1.2982\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8878 - val_loss: 1.2982\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8877 - val_loss: 1.2981\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8878 - val_loss: 1.2980\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8876 - val_loss: 1.2979\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8876 - val_loss: 1.2978\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8876 - val_loss: 1.2977\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8875 - val_loss: 1.2976\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8874 - val_loss: 1.2975\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8874 - val_loss: 1.2974\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8873 - val_loss: 1.2974\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8873 - val_loss: 1.2973\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8872 - val_loss: 1.2972\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8872 - val_loss: 1.2971\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8870 - val_loss: 1.2970\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8870 - val_loss: 1.2969\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8870 - val_loss: 1.2968\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8869 - val_loss: 1.2967\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8868 - val_loss: 1.2966\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8868 - val_loss: 1.2965\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8867 - val_loss: 1.2964\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8867 - val_loss: 1.2963\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8866 - val_loss: 1.2962\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8866 - val_loss: 1.2961\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8865 - val_loss: 1.2960\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8865 - val_loss: 1.2959\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8864 - val_loss: 1.2958\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8863 - val_loss: 1.2957\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8862 - val_loss: 1.2956\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8863 - val_loss: 1.2955\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8862 - val_loss: 1.2954\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8861 - val_loss: 1.2953\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8860 - val_loss: 1.2952\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8860 - val_loss: 1.2951\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8859 - val_loss: 1.2950\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8859 - val_loss: 1.2949\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8858 - val_loss: 1.2948\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8857 - val_loss: 1.2947\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8857 - val_loss: 1.2946\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8856 - val_loss: 1.2945\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8856 - val_loss: 1.2944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8856 - val_loss: 1.2943\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8855 - val_loss: 1.2942\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8854 - val_loss: 1.2941\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8853 - val_loss: 1.2940\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8853 - val_loss: 1.2939\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8852 - val_loss: 1.2938\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8852 - val_loss: 1.2937\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8851 - val_loss: 1.2936\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8851 - val_loss: 1.2935\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8850 - val_loss: 1.2934\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8849 - val_loss: 1.2933\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8848 - val_loss: 1.2932\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8847 - val_loss: 1.2931\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8848 - val_loss: 1.2930\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8847 - val_loss: 1.2929\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8846 - val_loss: 1.2928\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8846 - val_loss: 1.2927\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.8845 - val_loss: 1.2926\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.8844 - val_loss: 1.2925\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.8844 - val_loss: 1.2924\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.8843 - val_loss: 1.2923\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.8843 - val_loss: 1.2922\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.8842 - val_loss: 1.2921\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.8841 - val_loss: 1.2920\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.8841 - val_loss: 1.2919\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8840 - val_loss: 1.2918\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8839 - val_loss: 1.2917\n",
      "Execution time:  29.166857481002808\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9237\n",
      "Root Mean Square Error: 1.1187\n",
      "Mean Square Error: 1.2514\n",
      "\n",
      "Train RMSE: 1.119\n",
      "Train MSE: 1.251\n",
      "Train MAE: 0.924\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 6, 55)             12540     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 7ms/step - loss: 0.8733 - val_loss: 1.1280\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8730 - val_loss: 1.1276\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8727 - val_loss: 1.1271\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8724 - val_loss: 1.1267\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8721 - val_loss: 1.1262\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8718 - val_loss: 1.1258\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8715 - val_loss: 1.1253\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8712 - val_loss: 1.1248\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8709 - val_loss: 1.1243\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8705 - val_loss: 1.1238\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8702 - val_loss: 1.1233\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8699 - val_loss: 1.1228\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8696 - val_loss: 1.1222\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8692 - val_loss: 1.1217\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8689 - val_loss: 1.1212\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8685 - val_loss: 1.1206\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8682 - val_loss: 1.1201\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8678 - val_loss: 1.1195\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8674 - val_loss: 1.1190\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8671 - val_loss: 1.1184\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8667 - val_loss: 1.1179\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8664 - val_loss: 1.1173\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8661 - val_loss: 1.1167\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8657 - val_loss: 1.1162\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8653 - val_loss: 1.1156\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8649 - val_loss: 1.1150\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8646 - val_loss: 1.1144\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8642 - val_loss: 1.1138\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8638 - val_loss: 1.1133\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8634 - val_loss: 1.1127\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8631 - val_loss: 1.1121\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8627 - val_loss: 1.1115\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8623 - val_loss: 1.1109\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8619 - val_loss: 1.1102\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8616 - val_loss: 1.1096\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8612 - val_loss: 1.1090\n",
      "Execution time:  52.62877559661865\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9090\n",
      "Root Mean Square Error: 1.1032\n",
      "Mean Square Error: 1.2170\n",
      "\n",
      "Train RMSE: 1.103\n",
      "Train MSE: 1.217\n",
      "Train MAE: 0.909\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 6, 40)             6720      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6451 - val_loss: 0.6118\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.3694\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4148 - val_loss: 0.3375\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3864 - val_loss: 0.3173\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3664 - val_loss: 0.2920\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3493 - val_loss: 0.2645\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3356 - val_loss: 0.2392\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3236 - val_loss: 0.2228\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3162 - val_loss: 0.2104\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3105 - val_loss: 0.2025\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3052 - val_loss: 0.1949\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3013 - val_loss: 0.1914\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2980 - val_loss: 0.1857\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2938 - val_loss: 0.1820\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2912 - val_loss: 0.1771\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2883 - val_loss: 0.1739\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2862 - val_loss: 0.1724\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2839 - val_loss: 0.1654\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2823 - val_loss: 0.1638\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2808 - val_loss: 0.1631\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2793 - val_loss: 0.1598\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2777 - val_loss: 0.1578\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2772 - val_loss: 0.1560\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2765 - val_loss: 0.1553\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2756 - val_loss: 0.1552\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2754 - val_loss: 0.1553\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2751 - val_loss: 0.1556\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1532\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2739 - val_loss: 0.1545\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2743 - val_loss: 0.1517\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2741 - val_loss: 0.1505\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2733 - val_loss: 0.1515\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2732 - val_loss: 0.1510\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2726 - val_loss: 0.1504\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2728 - val_loss: 0.1515\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2727 - val_loss: 0.1488\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2729 - val_loss: 0.1503\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2721 - val_loss: 0.1484\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2723 - val_loss: 0.1489\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2726 - val_loss: 0.1486\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2717 - val_loss: 0.1483\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2718 - val_loss: 0.1477\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2716 - val_loss: 0.1462\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2718 - val_loss: 0.1462\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2714 - val_loss: 0.1462\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2711 - val_loss: 0.1463\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2716 - val_loss: 0.1461\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2715 - val_loss: 0.1460\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2714 - val_loss: 0.1445\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2712 - val_loss: 0.1433\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2709 - val_loss: 0.1442\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2708 - val_loss: 0.1438\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2706 - val_loss: 0.1458\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2708 - val_loss: 0.1422\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2704 - val_loss: 0.1425\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2705 - val_loss: 0.1417\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 0.1419\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 0.1416\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2704 - val_loss: 0.1397\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2703 - val_loss: 0.1416\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2701 - val_loss: 0.1416\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2700 - val_loss: 0.1398\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2701 - val_loss: 0.1394\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2703 - val_loss: 0.1391\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.1371\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2698 - val_loss: 0.1381\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2698 - val_loss: 0.1382\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.1380\n",
      "Execution time:  27.472850561141968\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1616\n",
      "Root Mean Square Error: 0.5849\n",
      "Mean Square Error: 0.3421\n",
      "\n",
      "Train RMSE: 0.585\n",
      "Train MSE: 0.342\n",
      "Train MAE: 0.162\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 6, 55)             12540     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4727 - val_loss: 0.3076\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.3310 - val_loss: 0.2575\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.3028 - val_loss: 0.2365\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2874 - val_loss: 0.2255\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2788 - val_loss: 0.2181\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2736 - val_loss: 0.2146\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2703 - val_loss: 0.2120\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2686 - val_loss: 0.2102\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2671 - val_loss: 0.2097\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2664 - val_loss: 0.2089\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2662 - val_loss: 0.2084\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2660 - val_loss: 0.2078\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2656 - val_loss: 0.2068\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2654 - val_loss: 0.2064\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2060\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2055\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2645 - val_loss: 0.2044\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2642 - val_loss: 0.2044\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2640 - val_loss: 0.2039\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.2020\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.2019\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2632 - val_loss: 0.2015\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2629 - val_loss: 0.2006\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2626 - val_loss: 0.1999\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2624 - val_loss: 0.1998\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2622 - val_loss: 0.1981\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2617 - val_loss: 0.1977\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2615 - val_loss: 0.1966\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2612 - val_loss: 0.1952\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2611 - val_loss: 0.1949\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2607 - val_loss: 0.1946\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2606 - val_loss: 0.1932\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2606 - val_loss: 0.1932\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1919\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1912\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1907\n",
      "Execution time:  51.26910877227783\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1536\n",
      "Root Mean Square Error: 0.5828\n",
      "Mean Square Error: 0.3397\n",
      "\n",
      "Train RMSE: 0.583\n",
      "Train MSE: 0.340\n",
      "Train MAE: 0.154\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 6, 40)             6720      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8701 - val_loss: 1.2355\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8039 - val_loss: 1.0838\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6868 - val_loss: 0.9458\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6250 - val_loss: 0.8996\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6000 - val_loss: 0.8773\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5847 - val_loss: 0.8631\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5742 - val_loss: 0.8530\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5663 - val_loss: 0.8451\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5601 - val_loss: 0.8389\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5552 - val_loss: 0.8336\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5506 - val_loss: 0.8289\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5471 - val_loss: 0.8250\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5438 - val_loss: 0.8217\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5410 - val_loss: 0.8187\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5384 - val_loss: 0.8160\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5361 - val_loss: 0.8135\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5339 - val_loss: 0.8112\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5318 - val_loss: 0.8091\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5298 - val_loss: 0.8073\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5279 - val_loss: 0.8056\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5259 - val_loss: 0.8041\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5234 - val_loss: 0.8028\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5211 - val_loss: 0.8017\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5186 - val_loss: 0.8008\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5159 - val_loss: 0.7999\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5135 - val_loss: 0.7992\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5121 - val_loss: 0.7984\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5108 - val_loss: 0.7978\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5098 - val_loss: 0.7973\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5089 - val_loss: 0.7968\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5079 - val_loss: 0.7963\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5069 - val_loss: 0.7959\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5061 - val_loss: 0.7956\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5050 - val_loss: 0.7953\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5038 - val_loss: 0.7950\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5031 - val_loss: 0.7948\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5021 - val_loss: 0.7946\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5013 - val_loss: 0.7944\n",
      "Epoch 39/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5004 - val_loss: 0.7942\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4996 - val_loss: 0.7941\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4989 - val_loss: 0.7940\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4982 - val_loss: 0.7938\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4977 - val_loss: 0.7937\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4974 - val_loss: 0.7936\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4968 - val_loss: 0.7935\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4966 - val_loss: 0.7935\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4964 - val_loss: 0.7934\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4962 - val_loss: 0.7933\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4958 - val_loss: 0.7933\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4957 - val_loss: 0.7932\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4955 - val_loss: 0.7932\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7932\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4951 - val_loss: 0.7931\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4952 - val_loss: 0.7931\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4950 - val_loss: 0.7931\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4949 - val_loss: 0.7930\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4947 - val_loss: 0.7930\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4948 - val_loss: 0.7930\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4945 - val_loss: 0.7930\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4945 - val_loss: 0.7929\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4944 - val_loss: 0.7929\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4945 - val_loss: 0.7929\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4943 - val_loss: 0.7929\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7929\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7929\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7929\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4941 - val_loss: 0.7929\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4940 - val_loss: 0.7928\n",
      "Execution time:  28.038204193115234\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4847\n",
      "Root Mean Square Error: 0.7526\n",
      "Mean Square Error: 0.5665\n",
      "\n",
      "Train RMSE: 0.753\n",
      "Train MSE: 0.566\n",
      "Train MAE: 0.485\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 6, 55)             12540     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.7271 - val_loss: 0.7620\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5800 - val_loss: 0.7183\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5584 - val_loss: 0.7008\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5478 - val_loss: 0.6892\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5404 - val_loss: 0.6806\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5346 - val_loss: 0.6744\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5303 - val_loss: 0.6696\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5265 - val_loss: 0.6660\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5237 - val_loss: 0.6634\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5217 - val_loss: 0.6613\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5200 - val_loss: 0.6598\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5187 - val_loss: 0.6589\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5177 - val_loss: 0.6579\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5165 - val_loss: 0.6569\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5147 - val_loss: 0.6561\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5118 - val_loss: 0.6548\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5080 - val_loss: 0.6539\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.5035 - val_loss: 0.6527\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4994 - val_loss: 0.6521\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4967 - val_loss: 0.6520\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4951 - val_loss: 0.6517\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4940 - val_loss: 0.6513\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4934 - val_loss: 0.6511\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4930 - val_loss: 0.6509\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4927 - val_loss: 0.6508\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4924 - val_loss: 0.6507\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4921 - val_loss: 0.6506\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4921 - val_loss: 0.6505\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4920 - val_loss: 0.6504\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4918 - val_loss: 0.6504\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.6503\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4917 - val_loss: 0.6503\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4914 - val_loss: 0.6502\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4915 - val_loss: 0.6502\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4913 - val_loss: 0.6502\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.6502\n",
      "Execution time:  52.554423809051514\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4827\n",
      "Root Mean Square Error: 0.7519\n",
      "Mean Square Error: 0.5653\n",
      "\n",
      "Train RMSE: 0.752\n",
      "Train MSE: 0.565\n",
      "Train MAE: 0.483\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 18, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 2s 19ms/step - loss: 0.5428 - val_loss: 0.3229\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4200 - val_loss: 0.2475\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3916 - val_loss: 0.2248\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3778 - val_loss: 0.2098\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3680 - val_loss: 0.1974\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3617 - val_loss: 0.1904\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3571 - val_loss: 0.1847\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3538 - val_loss: 0.1804\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3509 - val_loss: 0.1786\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3490 - val_loss: 0.1762\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3468 - val_loss: 0.1723\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3449 - val_loss: 0.1685\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3437 - val_loss: 0.1688\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3422 - val_loss: 0.1660\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3415 - val_loss: 0.1640\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3404 - val_loss: 0.1632\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3395 - val_loss: 0.1623\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3388 - val_loss: 0.1609\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3381 - val_loss: 0.1598\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3377 - val_loss: 0.1587\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3371 - val_loss: 0.1576\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3365 - val_loss: 0.1577\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.1583\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.1581\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3352 - val_loss: 0.1572\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3347 - val_loss: 0.1578\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3342 - val_loss: 0.1564\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3344 - val_loss: 0.1565\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3339 - val_loss: 0.1558\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3339 - val_loss: 0.1559\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3332 - val_loss: 0.1558\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3334 - val_loss: 0.1545\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3330 - val_loss: 0.1555\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3334 - val_loss: 0.1544\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3325 - val_loss: 0.1540\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3324 - val_loss: 0.1558\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3326 - val_loss: 0.1557\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3325 - val_loss: 0.1557\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3319 - val_loss: 0.1551\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3323 - val_loss: 0.1556\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3321 - val_loss: 0.1557\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3321 - val_loss: 0.1550\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3322 - val_loss: 0.1551\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3319 - val_loss: 0.1550\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3314 - val_loss: 0.1567\n",
      "Execution time:  34.46935534477234\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1715\n",
      "Root Mean Square Error: 0.5867\n",
      "Mean Square Error: 0.3442\n",
      "\n",
      "Train RMSE: 0.587\n",
      "Train MSE: 0.344\n",
      "Train MAE: 0.171\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 18, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.4433 - val_loss: 0.2909\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 7ms/step - loss: 0.3632 - val_loss: 0.2717\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 7ms/step - loss: 0.3469 - val_loss: 0.2630\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 7ms/step - loss: 0.3401 - val_loss: 0.2583\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3361 - val_loss: 0.2559\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3335 - val_loss: 0.2545\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3318 - val_loss: 0.2551\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3311 - val_loss: 0.2539\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3306 - val_loss: 0.2543\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2540\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3300 - val_loss: 0.2534\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3294 - val_loss: 0.2542\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3296 - val_loss: 0.2541\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3295 - val_loss: 0.2539\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3291 - val_loss: 0.2539\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3289 - val_loss: 0.2542\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3287 - val_loss: 0.2526\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3285 - val_loss: 0.2525\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3283 - val_loss: 0.2521\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3281 - val_loss: 0.2519\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3280 - val_loss: 0.2523\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3276 - val_loss: 0.2510\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3274 - val_loss: 0.2494\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3272 - val_loss: 0.2498\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3271 - val_loss: 0.2485\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3269 - val_loss: 0.2489\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3266 - val_loss: 0.2480\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3265 - val_loss: 0.2471\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3264 - val_loss: 0.2454\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3262 - val_loss: 0.2457\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3259 - val_loss: 0.2440\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3261 - val_loss: 0.2437\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3260 - val_loss: 0.2434\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3253 - val_loss: 0.2416\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3254 - val_loss: 0.2408\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2404\n",
      "Execution time:  103.10356855392456\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1657\n",
      "Root Mean Square Error: 0.5856\n",
      "Mean Square Error: 0.3429\n",
      "\n",
      "Train RMSE: 0.586\n",
      "Train MSE: 0.343\n",
      "Train MAE: 0.166\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 18, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8012 - val_loss: 0.9284\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6322 - val_loss: 0.8511\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6136 - val_loss: 0.8369\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6012 - val_loss: 0.8284\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5904 - val_loss: 0.8228\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5821 - val_loss: 0.8178\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5756 - val_loss: 0.8145\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5690 - val_loss: 0.8117\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5650 - val_loss: 0.8091\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5594 - val_loss: 0.8071\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5547 - val_loss: 0.8050\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5518 - val_loss: 0.8033\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5491 - val_loss: 0.8019\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5469 - val_loss: 0.8006\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5456 - val_loss: 0.7996\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5440 - val_loss: 0.7987\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5430 - val_loss: 0.7979\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5418 - val_loss: 0.7972\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5407 - val_loss: 0.7966\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5401 - val_loss: 0.7961\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5391 - val_loss: 0.7956\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5383 - val_loss: 0.7952\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5377 - val_loss: 0.7949\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5374 - val_loss: 0.7946\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5368 - val_loss: 0.7943\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5362 - val_loss: 0.7941\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5358 - val_loss: 0.7939\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7937\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5352 - val_loss: 0.7936\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5349 - val_loss: 0.7935\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5344 - val_loss: 0.7933\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5343 - val_loss: 0.7932\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5338 - val_loss: 0.7931\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5338 - val_loss: 0.7931\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5336 - val_loss: 0.7930\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7929\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5332 - val_loss: 0.7929\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5329 - val_loss: 0.7928\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5329 - val_loss: 0.7928\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5329 - val_loss: 0.7927\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5327 - val_loss: 0.7927\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5324 - val_loss: 0.7926\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5325 - val_loss: 0.7926\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5323 - val_loss: 0.7926\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5323 - val_loss: 0.7926\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5321 - val_loss: 0.7925\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5322 - val_loss: 0.7925\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5320 - val_loss: 0.7925\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5319 - val_loss: 0.7925\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5317 - val_loss: 0.7925\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5317 - val_loss: 0.7925\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7924\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7924\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7924\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7924\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5312 - val_loss: 0.7924\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5314 - val_loss: 0.7924\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5314 - val_loss: 0.7924\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5312 - val_loss: 0.7924\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5310 - val_loss: 0.7924\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5310 - val_loss: 0.7924\n",
      "Epoch 62/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5311 - val_loss: 0.7924\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5309 - val_loss: 0.7924\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5311 - val_loss: 0.7923\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5309 - val_loss: 0.7923\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5308 - val_loss: 0.7923\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5305 - val_loss: 0.7923\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5307 - val_loss: 0.7923\n",
      "Execution time:  50.67752957344055\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4956\n",
      "Root Mean Square Error: 0.7571\n",
      "Mean Square Error: 0.5732\n",
      "\n",
      "Train RMSE: 0.757\n",
      "Train MSE: 0.573\n",
      "Train MAE: 0.496\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 18, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 4s 11ms/step - loss: 0.6584 - val_loss: 0.7140\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5911 - val_loss: 0.6992\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5688 - val_loss: 0.6932\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5558 - val_loss: 0.6841\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5472 - val_loss: 0.6802\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5421 - val_loss: 0.6754\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5387 - val_loss: 0.6735\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5366 - val_loss: 0.6718\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5352 - val_loss: 0.6703\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5344 - val_loss: 0.6694\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5337 - val_loss: 0.6686\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5334 - val_loss: 0.6683\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5329 - val_loss: 0.6678\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5326 - val_loss: 0.6674\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5323 - val_loss: 0.6670\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5323 - val_loss: 0.6667\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5318 - val_loss: 0.6668\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5316 - val_loss: 0.6665\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5314 - val_loss: 0.6667\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5312 - val_loss: 0.6665\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5310 - val_loss: 0.6663\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5311 - val_loss: 0.6659\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5310 - val_loss: 0.6662\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5306 - val_loss: 0.6662\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5305 - val_loss: 0.6661\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5303 - val_loss: 0.6657\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5302 - val_loss: 0.6659\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5301 - val_loss: 0.6655\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5300 - val_loss: 0.6660\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5298 - val_loss: 0.6658\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5298 - val_loss: 0.6659\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5296 - val_loss: 0.6660\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5294 - val_loss: 0.6658\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5293 - val_loss: 0.6661\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5294 - val_loss: 0.6656\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5292 - val_loss: 0.6660\n",
      "Execution time:  104.57192826271057\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4969\n",
      "Root Mean Square Error: 0.7660\n",
      "Mean Square Error: 0.5868\n",
      "\n",
      "Train RMSE: 0.766\n",
      "Train MSE: 0.587\n",
      "Train MAE: 0.497\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 18, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6935 - val_loss: 0.7820\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6931 - val_loss: 0.7815\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6929 - val_loss: 0.7810\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6925 - val_loss: 0.7804\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6921 - val_loss: 0.7799\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6918 - val_loss: 0.7793\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6915 - val_loss: 0.7788\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6911 - val_loss: 0.7782\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6907 - val_loss: 0.7777\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6903 - val_loss: 0.7771\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6901 - val_loss: 0.7765\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6897 - val_loss: 0.7759\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6893 - val_loss: 0.7753\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6890 - val_loss: 0.7747\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6886 - val_loss: 0.7741\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6883 - val_loss: 0.7735\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6878 - val_loss: 0.7729\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6873 - val_loss: 0.7723\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6869 - val_loss: 0.7717\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6867 - val_loss: 0.7711\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6862 - val_loss: 0.7705\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6859 - val_loss: 0.7699\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6856 - val_loss: 0.7693\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6852 - val_loss: 0.7687\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6847 - val_loss: 0.7680\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6844 - val_loss: 0.7674\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6841 - val_loss: 0.7668\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6835 - val_loss: 0.7662\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6833 - val_loss: 0.7655\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6828 - val_loss: 0.7649\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6825 - val_loss: 0.7643\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6822 - val_loss: 0.7636\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6819 - val_loss: 0.7630\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6814 - val_loss: 0.7623\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6809 - val_loss: 0.7617\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6805 - val_loss: 0.7610\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6802 - val_loss: 0.7604\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6796 - val_loss: 0.7597\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6793 - val_loss: 0.7591\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6789 - val_loss: 0.7584\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6786 - val_loss: 0.7578\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6780 - val_loss: 0.7571\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6775 - val_loss: 0.7565\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6772 - val_loss: 0.7558\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6769 - val_loss: 0.7551\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6765 - val_loss: 0.7545\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6760 - val_loss: 0.7538\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6757 - val_loss: 0.7531\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6753 - val_loss: 0.7525\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6747 - val_loss: 0.7518\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6744 - val_loss: 0.7511\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6741 - val_loss: 0.7504\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6737 - val_loss: 0.7497\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6732 - val_loss: 0.7491\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6729 - val_loss: 0.7484\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6724 - val_loss: 0.7477\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6720 - val_loss: 0.7470\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6715 - val_loss: 0.7463\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6710 - val_loss: 0.7456\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6706 - val_loss: 0.7449\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6704 - val_loss: 0.7442\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6698 - val_loss: 0.7435\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6695 - val_loss: 0.7428\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6689 - val_loss: 0.7421\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6687 - val_loss: 0.7414\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6681 - val_loss: 0.7407\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6678 - val_loss: 0.7400\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6673 - val_loss: 0.7392\n",
      "Execution time:  50.471102476119995\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6701\n",
      "Root Mean Square Error: 0.9662\n",
      "Mean Square Error: 0.9336\n",
      "\n",
      "Train RMSE: 0.966\n",
      "Train MSE: 0.934\n",
      "Train MAE: 0.670\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 18, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6988 - val_loss: 0.6786\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6967 - val_loss: 0.6762\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6947 - val_loss: 0.6737\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6927 - val_loss: 0.6713\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6907 - val_loss: 0.6687\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6887 - val_loss: 0.6662\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6868 - val_loss: 0.6636\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6848 - val_loss: 0.6610\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6828 - val_loss: 0.6583\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6807 - val_loss: 0.6557\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6785 - val_loss: 0.6529\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6765 - val_loss: 0.6502\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6744 - val_loss: 0.6474\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6722 - val_loss: 0.6445\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6701 - val_loss: 0.6416\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6679 - val_loss: 0.6387\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6656 - val_loss: 0.6358\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6634 - val_loss: 0.6328\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6612 - val_loss: 0.6297\n",
      "Epoch 20/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6588 - val_loss: 0.6267\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6565 - val_loss: 0.6236\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6541 - val_loss: 0.6204\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6517 - val_loss: 0.6172\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6493 - val_loss: 0.6140\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6468 - val_loss: 0.6107\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6443 - val_loss: 0.6074\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6418 - val_loss: 0.6040\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6391 - val_loss: 0.6005\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6364 - val_loss: 0.5970\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6337 - val_loss: 0.5934\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6309 - val_loss: 0.5897\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6280 - val_loss: 0.5860\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6251 - val_loss: 0.5821\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6221 - val_loss: 0.5782\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6189 - val_loss: 0.5742\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6158 - val_loss: 0.5701\n",
      "Execution time:  103.2042829990387\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5930\n",
      "Root Mean Square Error: 0.8943\n",
      "Mean Square Error: 0.7998\n",
      "\n",
      "Train RMSE: 0.894\n",
      "Train MSE: 0.800\n",
      "Train MAE: 0.593\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 18, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8942 - val_loss: 1.3055\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8941 - val_loss: 1.3053\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8940 - val_loss: 1.3052\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8939 - val_loss: 1.3050\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8938 - val_loss: 1.3048\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8937 - val_loss: 1.3046\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8936 - val_loss: 1.3044\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8935 - val_loss: 1.3042\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8934 - val_loss: 1.3040\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8933 - val_loss: 1.3038\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8932 - val_loss: 1.3036\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8931 - val_loss: 1.3034\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8929 - val_loss: 1.3032\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8929 - val_loss: 1.3030\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8927 - val_loss: 1.3028\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8926 - val_loss: 1.3026\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8925 - val_loss: 1.3024\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8924 - val_loss: 1.3022\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8923 - val_loss: 1.3020\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8922 - val_loss: 1.3018\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8921 - val_loss: 1.3016\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8919 - val_loss: 1.3014\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8918 - val_loss: 1.3012\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8918 - val_loss: 1.3010\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8916 - val_loss: 1.3007\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8915 - val_loss: 1.3005\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8914 - val_loss: 1.3003\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8913 - val_loss: 1.3001\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8911 - val_loss: 1.2999\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8910 - val_loss: 1.2997\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8909 - val_loss: 1.2995\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.894 - 1s 8ms/step - loss: 0.8908 - val_loss: 1.2993\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8907 - val_loss: 1.2991\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8906 - val_loss: 1.2989\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8905 - val_loss: 1.2987\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8904 - val_loss: 1.2985\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8903 - val_loss: 1.2983\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8902 - val_loss: 1.2980\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8900 - val_loss: 1.2978\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8899 - val_loss: 1.2976\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8898 - val_loss: 1.2974\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8897 - val_loss: 1.2972\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8896 - val_loss: 1.2970\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8895 - val_loss: 1.2968\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8894 - val_loss: 1.2966\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8892 - val_loss: 1.2964\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8891 - val_loss: 1.2962\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8890 - val_loss: 1.2960\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8889 - val_loss: 1.2957\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8888 - val_loss: 1.2955\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8887 - val_loss: 1.2953\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8886 - val_loss: 1.2951\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8884 - val_loss: 1.2949\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8883 - val_loss: 1.2947\n",
      "Epoch 55/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8882 - val_loss: 1.2945\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8881 - val_loss: 1.2943\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8880 - val_loss: 1.2941\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8879 - val_loss: 1.2939\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8878 - val_loss: 1.2936\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8877 - val_loss: 1.2934\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8875 - val_loss: 1.2932\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8874 - val_loss: 1.2930\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8873 - val_loss: 1.2928\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8872 - val_loss: 1.2926\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8870 - val_loss: 1.2924\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8869 - val_loss: 1.2922\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8868 - val_loss: 1.2919\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8867 - val_loss: 1.2917\n",
      "Execution time:  51.19877076148987\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9248\n",
      "Root Mean Square Error: 1.1194\n",
      "Mean Square Error: 1.2530\n",
      "\n",
      "Train RMSE: 1.119\n",
      "Train MSE: 1.253\n",
      "Train MAE: 0.925\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 18, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8759 - val_loss: 1.1276\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 7ms/step - loss: 0.8755 - val_loss: 1.1270\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 7ms/step - loss: 0.8750 - val_loss: 1.1264\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8746 - val_loss: 1.1257\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8741 - val_loss: 1.1250\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8736 - val_loss: 1.1242\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8732 - val_loss: 1.1235\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8727 - val_loss: 1.1227\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8722 - val_loss: 1.1220\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8717 - val_loss: 1.1212\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8712 - val_loss: 1.1204\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8707 - val_loss: 1.1196\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8701 - val_loss: 1.1187\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8696 - val_loss: 1.1179\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8691 - val_loss: 1.1171\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8685 - val_loss: 1.1162\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8680 - val_loss: 1.1153\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8674 - val_loss: 1.1145\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8668 - val_loss: 1.1136\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8663 - val_loss: 1.1127\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8657 - val_loss: 1.1118\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8651 - val_loss: 1.1108\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8645 - val_loss: 1.1099\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8639 - val_loss: 1.1089\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8633 - val_loss: 1.1080\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8627 - val_loss: 1.1070\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8621 - val_loss: 1.1060\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8614 - val_loss: 1.1050\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8608 - val_loss: 1.1040\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8601 - val_loss: 1.1030\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8595 - val_loss: 1.1019\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8588 - val_loss: 1.1008\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8582 - val_loss: 1.0998\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8574 - val_loss: 1.0987\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8567 - val_loss: 1.0975\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.8560 - val_loss: 1.0964\n",
      "Execution time:  102.7319438457489\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8990\n",
      "Root Mean Square Error: 1.0935\n",
      "Mean Square Error: 1.1957\n",
      "\n",
      "Train RMSE: 1.093\n",
      "Train MSE: 1.196\n",
      "Train MAE: 0.899\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 18, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5782 - val_loss: 0.3338\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4444 - val_loss: 0.2973\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4154 - val_loss: 0.2611\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3995 - val_loss: 0.2405\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3904 - val_loss: 0.2291\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3832 - val_loss: 0.2204\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3778 - val_loss: 0.2129\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3727 - val_loss: 0.2076\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3684 - val_loss: 0.2021\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3649 - val_loss: 0.1980\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3615 - val_loss: 0.1936\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3585 - val_loss: 0.1909\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3565 - val_loss: 0.1880\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3541 - val_loss: 0.1861\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3528 - val_loss: 0.1838\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3511 - val_loss: 0.1826\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3499 - val_loss: 0.1807\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3488 - val_loss: 0.1790\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3478 - val_loss: 0.1775\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3466 - val_loss: 0.1767\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3458 - val_loss: 0.1755\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3449 - val_loss: 0.1747\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3440 - val_loss: 0.1742\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3438 - val_loss: 0.1733\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3428 - val_loss: 0.1713\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3422 - val_loss: 0.1712\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3416 - val_loss: 0.1711\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3411 - val_loss: 0.1700\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3405 - val_loss: 0.1691\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3400 - val_loss: 0.1693\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3393 - val_loss: 0.1681\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3393 - val_loss: 0.1673\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3387 - val_loss: 0.1680\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3387 - val_loss: 0.1664\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3380 - val_loss: 0.1651\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3375 - val_loss: 0.1659\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3373 - val_loss: 0.1645\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3373 - val_loss: 0.1643\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3364 - val_loss: 0.1639\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3367 - val_loss: 0.1634\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3365 - val_loss: 0.1637\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3362 - val_loss: 0.1624\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3360 - val_loss: 0.1618\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.1617\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3353 - val_loss: 0.1617\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3351 - val_loss: 0.1608\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3350 - val_loss: 0.1599\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3347 - val_loss: 0.1603\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3347 - val_loss: 0.1596\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3345 - val_loss: 0.1589\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3343 - val_loss: 0.1597\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3340 - val_loss: 0.1590\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3340 - val_loss: 0.1585\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3339 - val_loss: 0.1579\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3336 - val_loss: 0.1580\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3335 - val_loss: 0.1571\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3331 - val_loss: 0.1567\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3336 - val_loss: 0.1561\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3332 - val_loss: 0.1564\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3329 - val_loss: 0.1559\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3328 - val_loss: 0.1564\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3327 - val_loss: 0.1557\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3326 - val_loss: 0.1555\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3323 - val_loss: 0.1554\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3321 - val_loss: 0.1550\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3324 - val_loss: 0.1544\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3320 - val_loss: 0.1547\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3322 - val_loss: 0.1540\n",
      "Execution time:  50.554858684539795\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1753\n",
      "Root Mean Square Error: 0.5879\n",
      "Mean Square Error: 0.3456\n",
      "\n",
      "Train RMSE: 0.588\n",
      "Train MSE: 0.346\n",
      "Train MAE: 0.175\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 18, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.4806 - val_loss: 0.3197\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3831 - val_loss: 0.2917\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3632 - val_loss: 0.2785\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3533 - val_loss: 0.2720\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3472 - val_loss: 0.2672\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3431 - val_loss: 0.2638\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3404 - val_loss: 0.2614\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3381 - val_loss: 0.2596\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3365 - val_loss: 0.2580\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3349 - val_loss: 0.2571\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3337 - val_loss: 0.2565\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3327 - val_loss: 0.2558\n",
      "Epoch 13/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3320 - val_loss: 0.2555\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3313 - val_loss: 0.2549\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3309 - val_loss: 0.2544\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2540\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3296 - val_loss: 0.2537\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3292 - val_loss: 0.2532\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3289 - val_loss: 0.2528\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3284 - val_loss: 0.2527\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3279 - val_loss: 0.2520\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3276 - val_loss: 0.2515\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3271 - val_loss: 0.2512\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3268 - val_loss: 0.2505\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3266 - val_loss: 0.2502\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3263 - val_loss: 0.2500\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3260 - val_loss: 0.2496\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3260 - val_loss: 0.2492\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3257 - val_loss: 0.2486\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3255 - val_loss: 0.2486\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3255 - val_loss: 0.2482\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3255 - val_loss: 0.2482\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2480\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2472\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2471\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3250 - val_loss: 0.2470\n",
      "Execution time:  105.72694277763367\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1691\n",
      "Root Mean Square Error: 0.5855\n",
      "Mean Square Error: 0.3428\n",
      "\n",
      "Train RMSE: 0.586\n",
      "Train MSE: 0.343\n",
      "Train MAE: 0.169\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 18, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.8514 - val_loss: 1.1396\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7094 - val_loss: 0.9193\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6436 - val_loss: 0.8747\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6275 - val_loss: 0.8582\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6188 - val_loss: 0.8487\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6130 - val_loss: 0.8421\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6080 - val_loss: 0.8376\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6042 - val_loss: 0.8342\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6009 - val_loss: 0.8314\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5980 - val_loss: 0.8288\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5949 - val_loss: 0.8267\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5918 - val_loss: 0.8246\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5882 - val_loss: 0.8226\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5849 - val_loss: 0.8209\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5816 - val_loss: 0.8192\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5791 - val_loss: 0.8176\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5767 - val_loss: 0.8161\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5748 - val_loss: 0.8147\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5730 - val_loss: 0.8133\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5715 - val_loss: 0.8121\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5696 - val_loss: 0.8109\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5684 - val_loss: 0.8098\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5667 - val_loss: 0.8088\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5649 - val_loss: 0.8079\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5630 - val_loss: 0.8069\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5610 - val_loss: 0.8060\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5588 - val_loss: 0.8052\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5561 - val_loss: 0.8043\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5536 - val_loss: 0.8035\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5520 - val_loss: 0.8027\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5504 - val_loss: 0.8019\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5493 - val_loss: 0.8012\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5482 - val_loss: 0.8006\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5472 - val_loss: 0.7999\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5462 - val_loss: 0.7994\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5453 - val_loss: 0.7988\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5445 - val_loss: 0.7983\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5438 - val_loss: 0.7979\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5430 - val_loss: 0.7975\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5423 - val_loss: 0.7971\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5416 - val_loss: 0.7967\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5411 - val_loss: 0.7964\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5407 - val_loss: 0.7961\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5402 - val_loss: 0.7958\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5398 - val_loss: 0.7956\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5392 - val_loss: 0.7954\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5389 - val_loss: 0.7952\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5383 - val_loss: 0.7950\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5381 - val_loss: 0.7948\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5376 - val_loss: 0.7946\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5374 - val_loss: 0.7944\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5371 - val_loss: 0.7943\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5368 - val_loss: 0.7942\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5366 - val_loss: 0.7941\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5362 - val_loss: 0.7939\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5361 - val_loss: 0.7938\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5356 - val_loss: 0.7937\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5354 - val_loss: 0.7937\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7936\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5351 - val_loss: 0.7935\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5351 - val_loss: 0.7934\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5347 - val_loss: 0.7934\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7933\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7932\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5344 - val_loss: 0.7932\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5342 - val_loss: 0.7931\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5340 - val_loss: 0.7931\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5340 - val_loss: 0.7931\n",
      "Execution time:  51.192992210388184\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4960\n",
      "Root Mean Square Error: 0.7579\n",
      "Mean Square Error: 0.5745\n",
      "\n",
      "Train RMSE: 0.758\n",
      "Train MSE: 0.574\n",
      "Train MAE: 0.496\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 18, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.7157 - val_loss: 0.7534\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 7ms/step - loss: 0.6180 - val_loss: 0.7329\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6036 - val_loss: 0.7206\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5895 - val_loss: 0.7102\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5811 - val_loss: 0.7042\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5754 - val_loss: 0.6996\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5706 - val_loss: 0.6955\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5664 - val_loss: 0.6920\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5626 - val_loss: 0.6891\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5592 - val_loss: 0.6864\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5556 - val_loss: 0.6841\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5524 - val_loss: 0.6820\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5493 - val_loss: 0.6802\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5468 - val_loss: 0.6787\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5451 - val_loss: 0.6773\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5436 - val_loss: 0.6760\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5420 - val_loss: 0.6751\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5407 - val_loss: 0.6744\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5396 - val_loss: 0.6734\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5387 - val_loss: 0.6730\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5380 - val_loss: 0.6724\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5374 - val_loss: 0.6718\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5369 - val_loss: 0.6710\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5363 - val_loss: 0.6707\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5358 - val_loss: 0.6702\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5353 - val_loss: 0.6697\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5349 - val_loss: 0.6690\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5346 - val_loss: 0.6686\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5342 - val_loss: 0.6683\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5339 - val_loss: 0.6684\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5334 - val_loss: 0.6682\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5333 - val_loss: 0.6676\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5331 - val_loss: 0.6676\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5327 - val_loss: 0.6676\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5326 - val_loss: 0.6673\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5324 - val_loss: 0.6674\n",
      "Execution time:  102.66195869445801\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4942\n",
      "Root Mean Square Error: 0.7569\n",
      "Mean Square Error: 0.5729\n",
      "\n",
      "Train RMSE: 0.757\n",
      "Train MSE: 0.573\n",
      "Train MAE: 0.494\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 36, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.5578 - val_loss: 0.2927\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4657 - val_loss: 0.2469\n",
      "Epoch 3/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4475 - val_loss: 0.2243\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4379 - val_loss: 0.2104\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4307 - val_loss: 0.2036\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4255 - val_loss: 0.1971\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4220 - val_loss: 0.1915\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4194 - val_loss: 0.1899\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4170 - val_loss: 0.1875\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4154 - val_loss: 0.1867\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4138 - val_loss: 0.1843\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4126 - val_loss: 0.1837\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4114 - val_loss: 0.1828\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4106 - val_loss: 0.1816\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4097 - val_loss: 0.1820\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4090 - val_loss: 0.1805\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4085 - val_loss: 0.1800\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4080 - val_loss: 0.1797\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4072 - val_loss: 0.1788\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4070 - val_loss: 0.1792\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4062 - val_loss: 0.1792\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4058 - val_loss: 0.1780\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4052 - val_loss: 0.1781\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4047 - val_loss: 0.1781\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4045 - val_loss: 0.1781\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4041 - val_loss: 0.1775\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4037 - val_loss: 0.1778\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4034 - val_loss: 0.1774\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4030 - val_loss: 0.1770\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4030 - val_loss: 0.1771\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4028 - val_loss: 0.1771\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4026 - val_loss: 0.1768\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4023 - val_loss: 0.1780\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4021 - val_loss: 0.1770\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4016 - val_loss: 0.1776\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4015 - val_loss: 0.1773\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4013 - val_loss: 0.1776\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4012 - val_loss: 0.1774\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4007 - val_loss: 0.1771\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4007 - val_loss: 0.1774\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4005 - val_loss: 0.1767\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4009 - val_loss: 0.1778\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4006 - val_loss: 0.1777\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4000 - val_loss: 0.1777\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4001 - val_loss: 0.1769\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4002 - val_loss: 0.1785\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.3998 - val_loss: 0.1775\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.3998 - val_loss: 0.1777\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.3996 - val_loss: 0.1777\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.3991 - val_loss: 0.1781\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.3991 - val_loss: 0.1782\n",
      "Execution time:  67.7232837677002\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1886\n",
      "Root Mean Square Error: 0.6016\n",
      "Mean Square Error: 0.3620\n",
      "\n",
      "Train RMSE: 0.602\n",
      "Train MSE: 0.362\n",
      "Train MAE: 0.189\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 36, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4842 - val_loss: 0.3335\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 4s 13ms/step - loss: 0.4217 - val_loss: 0.3220\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 4s 13ms/step - loss: 0.4111 - val_loss: 0.3180\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 4s 13ms/step - loss: 0.4062 - val_loss: 0.3151\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.4031 - val_loss: 0.3124\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4007 - val_loss: 0.3117\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3989 - val_loss: 0.3120\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3978 - val_loss: 0.3113\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3975 - val_loss: 0.3117\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3968 - val_loss: 0.3123\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3965 - val_loss: 0.3128\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3961 - val_loss: 0.3127\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3958 - val_loss: 0.3121\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3962 - val_loss: 0.3116\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3960 - val_loss: 0.3125\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3954 - val_loss: 0.3122\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3954 - val_loss: 0.3123\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3948 - val_loss: 0.3131\n",
      "Execution time:  87.66285276412964\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1884\n",
      "Root Mean Square Error: 0.6122\n",
      "Mean Square Error: 0.3748\n",
      "\n",
      "Train RMSE: 0.612\n",
      "Train MSE: 0.375\n",
      "Train MAE: 0.188\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 36, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 2s 21ms/step - loss: 0.8001 - val_loss: 0.9025\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6505 - val_loss: 0.8378\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6324 - val_loss: 0.8215\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6271 - val_loss: 0.8158\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6221 - val_loss: 0.8119\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6197 - val_loss: 0.8094\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6173 - val_loss: 0.8076\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6111 - val_loss: 0.8064\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6090 - val_loss: 0.8043\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6078 - val_loss: 0.8032\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6032 - val_loss: 0.8026\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6010 - val_loss: 0.8011\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5942 - val_loss: 0.8004\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5904 - val_loss: 0.7996\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5878 - val_loss: 0.7989\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5865 - val_loss: 0.7983\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5852 - val_loss: 0.7977\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5842 - val_loss: 0.7972\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5833 - val_loss: 0.7968\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5825 - val_loss: 0.7964\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5816 - val_loss: 0.7960\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5809 - val_loss: 0.7957\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5805 - val_loss: 0.7954\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5796 - val_loss: 0.7952\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5794 - val_loss: 0.7949\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5791 - val_loss: 0.7947\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5783 - val_loss: 0.7945\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5782 - val_loss: 0.7943\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5777 - val_loss: 0.7941\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5773 - val_loss: 0.7939\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5772 - val_loss: 0.7938\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5768 - val_loss: 0.7936\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5764 - val_loss: 0.7935\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5761 - val_loss: 0.7934\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5761 - val_loss: 0.7933\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5757 - val_loss: 0.7932\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5757 - val_loss: 0.7931\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5752 - val_loss: 0.7930\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5751 - val_loss: 0.7929\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5747 - val_loss: 0.7928\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5746 - val_loss: 0.7927\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5745 - val_loss: 0.7927\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5745 - val_loss: 0.7926\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5741 - val_loss: 0.7925\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5741 - val_loss: 0.7925\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5739 - val_loss: 0.7924\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5740 - val_loss: 0.7924\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5735 - val_loss: 0.7923\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5735 - val_loss: 0.7923\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5730 - val_loss: 0.7922\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5731 - val_loss: 0.7922\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5730 - val_loss: 0.7922\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5729 - val_loss: 0.7921\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5728 - val_loss: 0.7921\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5725 - val_loss: 0.7921\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.7921\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.7920\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5722 - val_loss: 0.7920\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5721 - val_loss: 0.7920\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5717 - val_loss: 0.7920\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5718 - val_loss: 0.7919\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5718 - val_loss: 0.7919\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5715 - val_loss: 0.7919\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5714 - val_loss: 0.7919\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5714 - val_loss: 0.7919\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5713 - val_loss: 0.7919\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5710 - val_loss: 0.7919\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5708 - val_loss: 0.7918\n",
      "Execution time:  87.32253789901733\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5057\n",
      "Root Mean Square Error: 0.7654\n",
      "Mean Square Error: 0.5858\n",
      "\n",
      "Train RMSE: 0.765\n",
      "Train MSE: 0.586\n",
      "Train MAE: 0.506\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 36, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "  1/352 [..............................] - ETA: 0s - loss: 0.3995WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6817 - val_loss: 0.7408\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6186 - val_loss: 0.7273\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6109 - val_loss: 0.7218\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6131 - val_loss: 0.7188\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6108 - val_loss: 0.7176\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6069 - val_loss: 0.7164\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6065 - val_loss: 0.7156\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6031 - val_loss: 0.7145\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6064 - val_loss: 0.7128\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6028 - val_loss: 0.7123\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6010 - val_loss: 0.7110\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6009 - val_loss: 0.7096\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5988 - val_loss: 0.7031\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5906 - val_loss: 0.6931ss: 0.59\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5789 - val_loss: 0.6925\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5754 - val_loss: 0.6921\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5748 - val_loss: 0.6910\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5736 - val_loss: 0.6909\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5737 - val_loss: 0.6904\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5732 - val_loss: 0.6902\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5728 - val_loss: 0.6899\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5723 - val_loss: 0.6895\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5725 - val_loss: 0.6899\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5717 - val_loss: 0.6891\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5715 - val_loss: 0.6895\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5717 - val_loss: 0.6889\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5713 - val_loss: 0.6886\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5709 - val_loss: 0.6893\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5705 - val_loss: 0.6887\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5702 - val_loss: 0.6898\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5701 - val_loss: 0.6889\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5700 - val_loss: 0.6888\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5698 - val_loss: 0.6890\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5697 - val_loss: 0.6889\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5694 - val_loss: 0.6887\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5693 - val_loss: 0.6887\n",
      "Execution time:  173.4124629497528\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5046\n",
      "Root Mean Square Error: 0.7699\n",
      "Mean Square Error: 0.5928\n",
      "\n",
      "Train RMSE: 0.770\n",
      "Train MSE: 0.593\n",
      "Train MAE: 0.505\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 36, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.7075 - val_loss: 0.8010\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7072 - val_loss: 0.8004\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.7069 - val_loss: 0.7999\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.7067 - val_loss: 0.7993\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7063 - val_loss: 0.7987\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7060 - val_loss: 0.7980\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7057 - val_loss: 0.7974\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7053 - val_loss: 0.7968\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7050 - val_loss: 0.7961\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7047 - val_loss: 0.7955\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7043 - val_loss: 0.7948\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7039 - val_loss: 0.7941\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7036 - val_loss: 0.7935\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7032 - val_loss: 0.7928\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7029 - val_loss: 0.7921\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.7026 - val_loss: 0.7914\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.7022 - val_loss: 0.7907\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7018 - val_loss: 0.7900\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7014 - val_loss: 0.7893\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7012 - val_loss: 0.7886\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7008 - val_loss: 0.7879\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7004 - val_loss: 0.7872\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7001 - val_loss: 0.7865\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6997 - val_loss: 0.7858\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6993 - val_loss: 0.7851\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6990 - val_loss: 0.7844\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6986 - val_loss: 0.7837\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6983 - val_loss: 0.7830\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6979 - val_loss: 0.7822\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6976 - val_loss: 0.7815\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6972 - val_loss: 0.7808\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6968 - val_loss: 0.7801\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6965 - val_loss: 0.7793\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6961 - val_loss: 0.7786\n",
      "Epoch 35/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6958 - val_loss: 0.7779\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6954 - val_loss: 0.7771\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6950 - val_loss: 0.7764\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6946 - val_loss: 0.7756\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6943 - val_loss: 0.7749\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6939 - val_loss: 0.7741\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6935 - val_loss: 0.7734\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6932 - val_loss: 0.7726\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6927 - val_loss: 0.7719\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6923 - val_loss: 0.7711\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6919 - val_loss: 0.7703\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6916 - val_loss: 0.7696\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6912 - val_loss: 0.7688\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6909 - val_loss: 0.7680\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6905 - val_loss: 0.7673\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6901 - val_loss: 0.7665\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6898 - val_loss: 0.7657\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6893 - val_loss: 0.7649\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6888 - val_loss: 0.7641\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6885 - val_loss: 0.7634\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6882 - val_loss: 0.7626\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6878 - val_loss: 0.7618\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6874 - val_loss: 0.7610\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6869 - val_loss: 0.7602\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6866 - val_loss: 0.7594\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6861 - val_loss: 0.7586\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6858 - val_loss: 0.7578\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6854 - val_loss: 0.7570\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6850 - val_loss: 0.7562\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6846 - val_loss: 0.7553\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6841 - val_loss: 0.7545\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6838 - val_loss: 0.7537\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6833 - val_loss: 0.7529\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6829 - val_loss: 0.7520\n",
      "Execution time:  89.1095700263977\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6842\n",
      "Root Mean Square Error: 0.9809\n",
      "Mean Square Error: 0.9621\n",
      "\n",
      "Train RMSE: 0.981\n",
      "Train MSE: 0.962\n",
      "Train MAE: 0.684\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 36, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.7094 - val_loss: 0.6888\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.7059 - val_loss: 0.6844\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.7025 - val_loss: 0.6799\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6989 - val_loss: 0.6751\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6952 - val_loss: 0.6703\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6915 - val_loss: 0.6653\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6877 - val_loss: 0.6601\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6838 - val_loss: 0.6548\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6798 - val_loss: 0.6495\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6758 - val_loss: 0.6440\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6716 - val_loss: 0.6384\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6674 - val_loss: 0.6327\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6631 - val_loss: 0.6268\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6587 - val_loss: 0.6209\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6541 - val_loss: 0.6147\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6496 - val_loss: 0.6085\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6449 - val_loss: 0.6020\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6401 - val_loss: 0.5954\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6349 - val_loss: 0.5885\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6298 - val_loss: 0.5815\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6243 - val_loss: 0.5742\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6188 - val_loss: 0.5666\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6132 - val_loss: 0.5589\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6074 - val_loss: 0.5510\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6015 - val_loss: 0.5430\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5955 - val_loss: 0.5348\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5895 - val_loss: 0.5266\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5835 - val_loss: 0.5184\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5776 - val_loss: 0.5103\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5719 - val_loss: 0.5024\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5662 - val_loss: 0.4948\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5609 - val_loss: 0.4875\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5558 - val_loss: 0.4807\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5512 - val_loss: 0.4742\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5465 - val_loss: 0.4680\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5424 - val_loss: 0.4621\n",
      "Execution time:  173.8960039615631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "Mean Absolute Error: 0.4821\n",
      "Root Mean Square Error: 0.8295\n",
      "Mean Square Error: 0.6881\n",
      "\n",
      "Train RMSE: 0.830\n",
      "Train MSE: 0.688\n",
      "Train MAE: 0.482\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 36, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.8916 - val_loss: 1.2948\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8915 - val_loss: 1.2946\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8914 - val_loss: 1.2945\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8913 - val_loss: 1.2944\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8913 - val_loss: 1.2943\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8912 - val_loss: 1.2941\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8912 - val_loss: 1.2940\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8911 - val_loss: 1.2939\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8910 - val_loss: 1.2937\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8909 - val_loss: 1.2936\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8909 - val_loss: 1.2935\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8908 - val_loss: 1.2933\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8907 - val_loss: 1.2932\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8907 - val_loss: 1.2930\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8906 - val_loss: 1.2929\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8905 - val_loss: 1.2928\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8905 - val_loss: 1.2926\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8904 - val_loss: 1.2925\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8903 - val_loss: 1.2923\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8902 - val_loss: 1.2922\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8901 - val_loss: 1.2921\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8901 - val_loss: 1.2919\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8900 - val_loss: 1.2918\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8899 - val_loss: 1.2916\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8898 - val_loss: 1.2915\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8898 - val_loss: 1.2913\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8897 - val_loss: 1.2912 0s - los\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8896 - val_loss: 1.2910\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8895 - val_loss: 1.2909\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8895 - val_loss: 1.2907\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.889 - 1s 15ms/step - loss: 0.8894 - val_loss: 1.2906\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8893 - val_loss: 1.2904\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8893 - val_loss: 1.2903\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8891 - val_loss: 1.2901\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8891 - val_loss: 1.2900\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8890 - val_loss: 1.2898\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8890 - val_loss: 1.2897\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8889 - val_loss: 1.2895\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8888 - val_loss: 1.2894\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8887 - val_loss: 1.2892\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8886 - val_loss: 1.2891\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8886 - val_loss: 1.2889\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8885 - val_loss: 1.2888\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8884 - val_loss: 1.2886\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8884 - val_loss: 1.2885\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8883 - val_loss: 1.2883\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8882 - val_loss: 1.2882\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8881 - val_loss: 1.2880\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8880 - val_loss: 1.2879\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8879 - val_loss: 1.2877\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8879 - val_loss: 1.2876\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8878 - val_loss: 1.2874\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8877 - val_loss: 1.2873\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8876 - val_loss: 1.2871\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8875 - val_loss: 1.2870\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8875 - val_loss: 1.2868\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8874 - val_loss: 1.2867\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8873 - val_loss: 1.2865\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8872 - val_loss: 1.2863\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8872 - val_loss: 1.2862\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8871 - val_loss: 1.2860\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8870 - val_loss: 1.2859\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8869 - val_loss: 1.2857\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8869 - val_loss: 1.2856\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8868 - val_loss: 1.2854\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8867 - val_loss: 1.2853\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8866 - val_loss: 1.2851\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8866 - val_loss: 1.2849\n",
      "Execution time:  88.27953171730042\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9210\n",
      "Root Mean Square Error: 1.1156\n",
      "Mean Square Error: 1.2446\n",
      "\n",
      "Train RMSE: 1.116\n",
      "Train MSE: 1.245\n",
      "Train MAE: 0.921\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, 36, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8758 - val_loss: 1.1230\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8753 - val_loss: 1.1222\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8747 - val_loss: 1.1213\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8741 - val_loss: 1.1205\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8736 - val_loss: 1.1196\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8730 - val_loss: 1.1186\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8723 - val_loss: 1.1177\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.8717 - val_loss: 1.1167\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8711 - val_loss: 1.1157\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8704 - val_loss: 1.1147\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.8698 - val_loss: 1.1137\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8691 - val_loss: 1.1126\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8684 - val_loss: 1.1115\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8677 - val_loss: 1.1104\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8670 - val_loss: 1.1093\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8663 - val_loss: 1.1082\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8655 - val_loss: 1.1070\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.8648 - val_loss: 1.1058\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8640 - val_loss: 1.1046\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8632 - val_loss: 1.1034\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8624 - val_loss: 1.1021\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8616 - val_loss: 1.1008\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8607 - val_loss: 1.0995\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8599 - val_loss: 1.0981\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8590 - val_loss: 1.0967\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8581 - val_loss: 1.0953\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8572 - val_loss: 1.0938\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.8562 - val_loss: 1.0923\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8553 - val_loss: 1.0908\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8543 - val_loss: 1.0892\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8532 - val_loss: 1.0876\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.8522 - val_loss: 1.0859\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8511 - val_loss: 1.0842\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8499 - val_loss: 1.0824\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.8488 - val_loss: 1.0806\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.8476 - val_loss: 1.0787\n",
      "Execution time:  171.20195198059082\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8821\n",
      "Root Mean Square Error: 1.0774\n",
      "Mean Square Error: 1.1609\n",
      "\n",
      "Train RMSE: 1.077\n",
      "Train MSE: 1.161\n",
      "Train MAE: 0.882\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 36, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.5675 - val_loss: 0.2891\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4786 - val_loss: 0.2690\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4616 - val_loss: 0.2483\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4529 - val_loss: 0.2355\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4468 - val_loss: 0.2278\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4416 - val_loss: 0.2208\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4374 - val_loss: 0.2145\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4336 - val_loss: 0.2116\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4302 - val_loss: 0.2076\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4276 - val_loss: 0.2053\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.4247 - val_loss: 0.2021\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4226 - val_loss: 0.1996\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4207 - val_loss: 0.1979\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4189 - val_loss: 0.1971\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4177 - val_loss: 0.1959\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4165 - val_loss: 0.1936\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4155 - val_loss: 0.1924\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4144 - val_loss: 0.1916\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4134 - val_loss: 0.1913\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4129 - val_loss: 0.1912\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4118 - val_loss: 0.1916\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4113 - val_loss: 0.1903\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4105 - val_loss: 0.1902\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4100 - val_loss: 0.1901\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4097 - val_loss: 0.1891\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4093 - val_loss: 0.1890\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4085 - val_loss: 0.1897\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4082 - val_loss: 0.1890\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4078 - val_loss: 0.1887\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4074 - val_loss: 0.1884\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4072 - val_loss: 0.1886\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4070 - val_loss: 0.1881\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4067 - val_loss: 0.1887\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4061 - val_loss: 0.1876\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4059 - val_loss: 0.1881\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4057 - val_loss: 0.1878\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4052 - val_loss: 0.1877\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4050 - val_loss: 0.1871\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4046 - val_loss: 0.1870\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4046 - val_loss: 0.1873\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4043 - val_loss: 0.1865\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4043 - val_loss: 0.1874\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4042 - val_loss: 0.1868\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4037 - val_loss: 0.1863\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4036 - val_loss: 0.1858\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4035 - val_loss: 0.1864\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4035 - val_loss: 0.1860\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4032 - val_loss: 0.1861\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4029 - val_loss: 0.1860\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4025 - val_loss: 0.1859\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4026 - val_loss: 0.1857\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4025 - val_loss: 0.1854\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4025 - val_loss: 0.1851\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4023 - val_loss: 0.1852\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4019 - val_loss: 0.1846\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4021 - val_loss: 0.1851\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4019 - val_loss: 0.1847\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4017 - val_loss: 0.1848\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4015 - val_loss: 0.1845\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4014 - val_loss: 0.1842\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4013 - val_loss: 0.1844\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4014 - val_loss: 0.1838\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4015 - val_loss: 0.1843\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4010 - val_loss: 0.1842\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4010 - val_loss: 0.1832\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4010 - val_loss: 0.1838\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4007 - val_loss: 0.1839\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4004 - val_loss: 0.1836\n",
      "Execution time:  88.11387300491333\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1945\n",
      "Root Mean Square Error: 0.5975\n",
      "Mean Square Error: 0.3570\n",
      "\n",
      "Train RMSE: 0.598\n",
      "Train MSE: 0.357\n",
      "Train MAE: 0.194\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 36, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.4970 - val_loss: 0.3536\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 0.4343 - val_loss: 0.3346\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4207 - val_loss: 0.3275\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4140 - val_loss: 0.3238\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4095 - val_loss: 0.3216\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4067 - val_loss: 0.3197\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4045 - val_loss: 0.3185\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4025 - val_loss: 0.3175\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.4010 - val_loss: 0.3164\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3994 - val_loss: 0.3160\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3984 - val_loss: 0.3154\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3975 - val_loss: 0.3153\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3969 - val_loss: 0.3148\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3963 - val_loss: 0.3143\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3956 - val_loss: 0.3141\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3952 - val_loss: 0.3137\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3948 - val_loss: 0.3137\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3944 - val_loss: 0.3137\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3941 - val_loss: 0.3134\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3938 - val_loss: 0.3135\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3934 - val_loss: 0.3130\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3932 - val_loss: 0.3132\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3929 - val_loss: 0.3133\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3927 - val_loss: 0.3135\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3924 - val_loss: 0.3134\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3920 - val_loss: 0.3133\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3916 - val_loss: 0.3137\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3913 - val_loss: 0.3135ETA\n",
      "Epoch 29/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3911 - val_loss: 0.3137\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3909 - val_loss: 0.3138\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.3909 - val_loss: 0.3140\n",
      "Execution time:  146.72795796394348\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1893\n",
      "Root Mean Square Error: 0.5927\n",
      "Mean Square Error: 0.3513\n",
      "\n",
      "Train RMSE: 0.593\n",
      "Train MSE: 0.351\n",
      "Train MAE: 0.189\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 36, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.8534 - val_loss: 1.0939\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7103 - val_loss: 0.8974\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6594 - val_loss: 0.8561\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6467 - val_loss: 0.8423\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6407 - val_loss: 0.8356\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6360 - val_loss: 0.8307\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6324 - val_loss: 0.8269\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6291 - val_loss: 0.8238\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6264 - val_loss: 0.8217\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6240 - val_loss: 0.8201\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6220 - val_loss: 0.8183\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6200 - val_loss: 0.8167\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6180 - val_loss: 0.8156\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6160 - val_loss: 0.8144\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6137 - val_loss: 0.8133\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6111 - val_loss: 0.8122\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6091 - val_loss: 0.8110\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6068 - val_loss: 0.8100\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6048 - val_loss: 0.8090\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6032 - val_loss: 0.8081\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6015 - val_loss: 0.8072\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6002 - val_loss: 0.8063\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5988 - val_loss: 0.8055\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5975 - val_loss: 0.8048\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5962 - val_loss: 0.8041\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5952 - val_loss: 0.8035\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5940 - val_loss: 0.8029\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5929 - val_loss: 0.8023\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5919 - val_loss: 0.8018\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5911 - val_loss: 0.8013\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5901 - val_loss: 0.8008\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5895 - val_loss: 0.8004\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5887 - val_loss: 0.7999\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5879 - val_loss: 0.7996\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5874 - val_loss: 0.7992\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5866 - val_loss: 0.7989TA: 0s - loss: 0.60\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5860 - val_loss: 0.7986\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5855 - val_loss: 0.7983\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5848 - val_loss: 0.7980\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5844 - val_loss: 0.7977\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5839 - val_loss: 0.7975\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5835 - val_loss: 0.7972\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5830 - val_loss: 0.7970\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.7968\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5822 - val_loss: 0.7966\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5819 - val_loss: 0.7964\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5816 - val_loss: 0.7962\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5811 - val_loss: 0.7960\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5808 - val_loss: 0.7959\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5803 - val_loss: 0.7957- ETA: 0s - loss:\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5802 - val_loss: 0.7956\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5799 - val_loss: 0.7954\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5796 - val_loss: 0.7953\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5794 - val_loss: 0.7952\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5791 - val_loss: 0.7950\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5789 - val_loss: 0.7949\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5786 - val_loss: 0.7948\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5783 - val_loss: 0.7947\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5782 - val_loss: 0.7946\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5780 - val_loss: 0.7945\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5778 - val_loss: 0.7944\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5776 - val_loss: 0.7943\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5774 - val_loss: 0.7942\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5772 - val_loss: 0.7941\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5771 - val_loss: 0.7940\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5769 - val_loss: 0.7940\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5767 - val_loss: 0.7939\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5765 - val_loss: 0.7938\n",
      "Execution time:  88.48160099983215\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5115\n",
      "Root Mean Square Error: 0.7669\n",
      "Mean Square Error: 0.5882\n",
      "\n",
      "Train RMSE: 0.767\n",
      "Train MSE: 0.588\n",
      "Train MAE: 0.511\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_35 (LSTM)               (None, 36, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.7059 - val_loss: 0.7600\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 4s 13ms/step - loss: 0.6331 - val_loss: 0.7443\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6238 - val_loss: 0.7372\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6166 - val_loss: 0.7337\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6125 - val_loss: 0.7309\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6094 - val_loss: 0.7285\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6066 - val_loss: 0.7258\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6038 - val_loss: 0.7239\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6018 - val_loss: 0.7220\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.6002 - val_loss: 0.7191\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5976 - val_loss: 0.7163\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5951 - val_loss: 0.7119\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5911 - val_loss: 0.7081\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5873 - val_loss: 0.7044\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5851 - val_loss: 0.7027\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5837 - val_loss: 0.7013\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5824 - val_loss: 0.6999\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5812 - val_loss: 0.6989\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5803 - val_loss: 0.6981\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5795 - val_loss: 0.6972\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5787 - val_loss: 0.6966\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5779 - val_loss: 0.6960\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5773 - val_loss: 0.6957\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5767 - val_loss: 0.6951\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5760 - val_loss: 0.6948\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5754 - val_loss: 0.6955\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5751 - val_loss: 0.6954\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5748 - val_loss: 0.6954\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5742 - val_loss: 0.6952\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5740 - val_loss: 0.6952\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5737 - val_loss: 0.6951\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5733 - val_loss: 0.6950\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5730 - val_loss: 0.6949\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5727 - val_loss: 0.6948\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5727 - val_loss: 0.6946\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 13ms/step - loss: 0.5725 - val_loss: 0.6944\n",
      "Execution time:  170.76853561401367\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5091\n",
      "Root Mean Square Error: 0.7645\n",
      "Mean Square Error: 0.5844\n",
      "\n",
      "Train RMSE: 0.764\n",
      "Train MSE: 0.584\n",
      "Train MAE: 0.509\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_36 (LSTM)               (None, 72, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 0.6086 - val_loss: 0.2800\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5312 - val_loss: 0.2414\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.5193 - val_loss: 0.2284\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5139 - val_loss: 0.2201\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5108 - val_loss: 0.2167\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5083 - val_loss: 0.2138\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5058 - val_loss: 0.2129\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5043 - val_loss: 0.2115\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5024 - val_loss: 0.2111\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5009 - val_loss: 0.2106\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4997 - val_loss: 0.2106\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4985 - val_loss: 0.2101\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4974 - val_loss: 0.2102\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4968 - val_loss: 0.2100\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4951 - val_loss: 0.2106\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4974 - val_loss: 0.2098\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4917 - val_loss: 0.2101\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4963 - val_loss: 0.2098\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4882 - val_loss: 0.2094\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4878 - val_loss: 0.2102\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4778 - val_loss: 0.2092\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4952 - val_loss: 0.2108\n",
      "Epoch 23/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4932 - val_loss: 0.2105\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4919 - val_loss: 0.2097\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4887 - val_loss: 0.2085\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4784 - val_loss: 0.2093\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4765 - val_loss: 0.2063\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4952 - val_loss: 0.2096\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4855 - val_loss: 0.2080\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4776 - val_loss: 0.2091\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4762 - val_loss: 0.2061\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4735 - val_loss: 0.2066\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4719 - val_loss: 0.2074\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4704 - val_loss: 0.2079\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4694 - val_loss: 0.2079\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4690 - val_loss: 0.2085\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4707 - val_loss: 0.2080\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4691 - val_loss: 0.2080\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4707 - val_loss: 0.2078\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4689 - val_loss: 0.2082\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4680 - val_loss: 0.2082\n",
      "Execution time:  97.28828144073486\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2927\n",
      "Root Mean Square Error: 0.7776\n",
      "Mean Square Error: 0.6047\n",
      "\n",
      "Train RMSE: 0.778\n",
      "Train MSE: 0.605\n",
      "Train MAE: 0.293\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_37 (LSTM)               (None, 72, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_37 (TimeDis (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.5565 - val_loss: 0.3951\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.5028 - val_loss: 0.3865\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4961 - val_loss: 0.3850\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4923 - val_loss: 0.3843\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4893 - val_loss: 0.3835\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4871 - val_loss: 0.3829\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4860 - val_loss: 0.3828\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4846 - val_loss: 0.3811\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4839 - val_loss: 0.3815\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4815 - val_loss: 0.3800\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4805 - val_loss: 0.3797\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4882 - val_loss: 0.3864\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4811 - val_loss: 0.3784\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4780 - val_loss: 0.3792\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4739 - val_loss: 0.3762\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4737 - val_loss: 0.3722\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4699 - val_loss: 0.3695\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4670 - val_loss: 0.3678\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4648 - val_loss: 0.3663\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4735 - val_loss: 0.3720\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4683 - val_loss: 0.3631\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4611 - val_loss: 0.3642\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4621 - val_loss: 0.3643\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4617 - val_loss: 0.3656\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4599 - val_loss: 0.3667\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4570 - val_loss: 0.3661\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4591 - val_loss: 0.3665\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4560 - val_loss: 0.3667\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4697 - val_loss: 0.3635\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4611 - val_loss: 0.3657\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4583 - val_loss: 0.3677\n",
      "Execution time:  278.48469734191895\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2832\n",
      "Root Mean Square Error: 0.7731\n",
      "Mean Square Error: 0.5976\n",
      "\n",
      "Train RMSE: 0.773\n",
      "Train MSE: 0.598\n",
      "Train MAE: 0.283\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_38 (LSTM)               (None, 72, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_38 (TimeDis (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 0.8061 - val_loss: 0.8870\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6826 - val_loss: 0.8276\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6697 - val_loss: 0.8162\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6634 - val_loss: 0.8115\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6601 - val_loss: 0.8081\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6589 - val_loss: 0.8059\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6570 - val_loss: 0.8042\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6555 - val_loss: 0.8028\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6545 - val_loss: 0.8016\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6538 - val_loss: 0.8008\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6530 - val_loss: 0.7999\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6523 - val_loss: 0.7993\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6520 - val_loss: 0.7987\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6513 - val_loss: 0.7983\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.651 - 2s 27ms/step - loss: 0.6509 - val_loss: 0.7976\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6515 - val_loss: 0.7974\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6501 - val_loss: 0.7968\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6504 - val_loss: 0.7966\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6500 - val_loss: 0.7963\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6497 - val_loss: 0.7961\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6494 - val_loss: 0.7958\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6493 - val_loss: 0.7955\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6504 - val_loss: 0.7953\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6487 - val_loss: 0.7951\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6491 - val_loss: 0.7949\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6483 - val_loss: 0.7947\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6481 - val_loss: 0.7946\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6488 - val_loss: 0.7943\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6506 - val_loss: 0.7942\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6491 - val_loss: 0.7941\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6478 - val_loss: 0.7940\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6473 - val_loss: 0.7938\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6476 - val_loss: 0.7938\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6471 - val_loss: 0.7936\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6467 - val_loss: 0.7932\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6562 - val_loss: 0.7932\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6512 - val_loss: 0.7931\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6533 - val_loss: 0.7931\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6494 - val_loss: 0.7930\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6491 - val_loss: 0.7930\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6495 - val_loss: 0.7929\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6493 - val_loss: 0.7928\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6490 - val_loss: 0.7927\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6491 - val_loss: 0.7927\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6484 - val_loss: 0.7926\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6473 - val_loss: 0.7926\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6465 - val_loss: 0.7926\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6462 - val_loss: 0.7925\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6454 - val_loss: 0.7924\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6448 - val_loss: 0.7924\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6440 - val_loss: 0.7923\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6435 - val_loss: 0.7923\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6430 - val_loss: 0.7922\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6424 - val_loss: 0.7923\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6411 - val_loss: 0.7923\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6394 - val_loss: 0.7922\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6393 - val_loss: 0.7922\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6389 - val_loss: 0.7921\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6401 - val_loss: 0.7921\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6377 - val_loss: 0.7920\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6383 - val_loss: 0.7920\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6382 - val_loss: 0.7919\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6375 - val_loss: 0.7919\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6369 - val_loss: 0.7919\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6384 - val_loss: 0.7919\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6354 - val_loss: 0.7918\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6349 - val_loss: 0.7918\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6355 - val_loss: 0.7917\n",
      "Execution time:  156.2325563430786\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6051\n",
      "Root Mean Square Error: 0.9010\n",
      "Mean Square Error: 0.8119\n",
      "\n",
      "Train RMSE: 0.901\n",
      "Train MSE: 0.812\n",
      "Train MAE: 0.605\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_39 (LSTM)               (None, 72, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_39 (TimeDis (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.7196 - val_loss: 0.7421\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6585 - val_loss: 0.7309\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6561 - val_loss: 0.7255\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6539 - val_loss: 0.7243\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6534 - val_loss: 0.7231\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6515 - val_loss: 0.7209\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6558 - val_loss: 0.7191\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6537 - val_loss: 0.7195\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6512 - val_loss: 0.7238\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6501 - val_loss: 0.7215\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6478 - val_loss: 0.7185\n",
      "Epoch 12/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6487 - val_loss: 0.7185\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6476 - val_loss: 0.7170\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6471 - val_loss: 0.7163\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6469 - val_loss: 0.7160\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6477 - val_loss: 0.7172\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6473 - val_loss: 0.7195\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6460 - val_loss: 0.7170\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6457 - val_loss: 0.7153\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6457 - val_loss: 0.7155\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6454 - val_loss: 0.7162\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6500 - val_loss: 0.7141\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7135\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6446 - val_loss: 0.7131\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6568 - val_loss: 0.7118\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6508 - val_loss: 0.7161\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6369 - val_loss: 0.7144\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6243 - val_loss: 0.7131\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6223 - val_loss: 0.7115\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6176 - val_loss: 0.7107\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6156 - val_loss: 0.7089\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6129 - val_loss: 0.7079\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6120 - val_loss: 0.7091\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6077 - val_loss: 0.7068\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6159 - val_loss: 0.7079\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6051 - val_loss: 0.7051\n",
      "Execution time:  321.54596757888794\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5525\n",
      "Root Mean Square Error: 0.8496\n",
      "Mean Square Error: 0.7219\n",
      "\n",
      "Train RMSE: 0.850\n",
      "Train MSE: 0.722\n",
      "Train MAE: 0.552\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_40 (LSTM)               (None, 72, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_40 (TimeDis (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.6976 - val_loss: 0.7943\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.6975 - val_loss: 0.7937\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6972 - val_loss: 0.7931\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6969 - val_loss: 0.7925\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6967 - val_loss: 0.7919\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6964 - val_loss: 0.7913\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6960 - val_loss: 0.7906\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6958 - val_loss: 0.7900\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6955 - val_loss: 0.7893\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6952 - val_loss: 0.7886\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6948 - val_loss: 0.7879\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6947 - val_loss: 0.7872\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6942 - val_loss: 0.7865\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6939 - val_loss: 0.7858\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6936 - val_loss: 0.7851\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6933 - val_loss: 0.7844\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6929 - val_loss: 0.7837\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6926 - val_loss: 0.7830\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6923 - val_loss: 0.7823\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6920 - val_loss: 0.7815\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6917 - val_loss: 0.7808\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6914 - val_loss: 0.7801\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6910 - val_loss: 0.7793\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6907 - val_loss: 0.7786\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6904 - val_loss: 0.7779\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6900 - val_loss: 0.7771\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6897 - val_loss: 0.7764\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6893 - val_loss: 0.7757\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6890 - val_loss: 0.7749\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6888 - val_loss: 0.7742\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6883 - val_loss: 0.7734\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6880 - val_loss: 0.7727\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6877 - val_loss: 0.7719\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6874 - val_loss: 0.7712\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6870 - val_loss: 0.7704\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6867 - val_loss: 0.7697\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6864 - val_loss: 0.7689\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6860 - val_loss: 0.7681\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6858 - val_loss: 0.7674\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6854 - val_loss: 0.7666\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6850 - val_loss: 0.7658\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6847 - val_loss: 0.7651\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6843 - val_loss: 0.7643\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6839 - val_loss: 0.7635\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6836 - val_loss: 0.7628\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6832 - val_loss: 0.7620\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6829 - val_loss: 0.7612\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6826 - val_loss: 0.7604\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6822 - val_loss: 0.7596\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6819 - val_loss: 0.7588\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6816 - val_loss: 0.7581\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6812 - val_loss: 0.7573\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6809 - val_loss: 0.7565\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6805 - val_loss: 0.7557\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6801 - val_loss: 0.7549\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6798 - val_loss: 0.7541\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6794 - val_loss: 0.7533\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6791 - val_loss: 0.7525\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6787 - val_loss: 0.7517\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6784 - val_loss: 0.7509\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6781 - val_loss: 0.7501\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6777 - val_loss: 0.7493\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6772 - val_loss: 0.7485\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6769 - val_loss: 0.7476\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6766 - val_loss: 0.7468\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6763 - val_loss: 0.7460\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6759 - val_loss: 0.7452\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.6754 - val_loss: 0.7444\n",
      "Execution time:  158.29453372955322\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6766\n",
      "Root Mean Square Error: 0.9757\n",
      "Mean Square Error: 0.9520\n",
      "\n",
      "Train RMSE: 0.976\n",
      "Train MSE: 0.952\n",
      "Train MAE: 0.677\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_41 (LSTM)               (None, 72, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_41 (TimeDis (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6778 - val_loss: 0.6595\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6759 - val_loss: 0.6569\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6739 - val_loss: 0.6540\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6717 - val_loss: 0.6510\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6695 - val_loss: 0.6478\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6672 - val_loss: 0.6446\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6649 - val_loss: 0.6413\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6625 - val_loss: 0.6379\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6601 - val_loss: 0.6345\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6577 - val_loss: 0.6310\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6552 - val_loss: 0.6274\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6526 - val_loss: 0.6237\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6500 - val_loss: 0.6200\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6473 - val_loss: 0.6161\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6446 - val_loss: 0.6121\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6418 - val_loss: 0.6079\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6388 - val_loss: 0.6037\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6359 - val_loss: 0.5993\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6328 - val_loss: 0.5948\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6297 - val_loss: 0.5902\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6264 - val_loss: 0.5854\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6230 - val_loss: 0.5804\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6196 - val_loss: 0.5752\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6160 - val_loss: 0.5699\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6123 - val_loss: 0.5644\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6086 - val_loss: 0.5588\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6047 - val_loss: 0.5531\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6008 - val_loss: 0.5472\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.5969 - val_loss: 0.5412\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.5929 - val_loss: 0.5352\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.5889 - val_loss: 0.5292\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.5849 - val_loss: 0.5231\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.5809 - val_loss: 0.5170\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.5771 - val_loss: 0.5109\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.5730 - val_loss: 0.5048\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.5695 - val_loss: 0.4991\n",
      "Execution time:  324.0750644207001\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4830\n",
      "Root Mean Square Error: 0.8228\n",
      "Mean Square Error: 0.6770\n",
      "\n",
      "Train RMSE: 0.823\n",
      "Train MSE: 0.677\n",
      "Train MAE: 0.483\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 72, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_42 (TimeDis (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 3s 36ms/step - loss: 0.8904 - val_loss: 1.2875\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8904 - val_loss: 1.2874\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8903 - val_loss: 1.2873\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8903 - val_loss: 1.2871\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8902 - val_loss: 1.2870\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8902 - val_loss: 1.2868\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8901 - val_loss: 1.2867\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8900 - val_loss: 1.2865\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8900 - val_loss: 1.2864\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8899 - val_loss: 1.2862\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8898 - val_loss: 1.2861\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8898 - val_loss: 1.2859\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8897 - val_loss: 1.2858\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8896 - val_loss: 1.2856\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8895 - val_loss: 1.2854\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8895 - val_loss: 1.2853\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8894 - val_loss: 1.2851\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8893 - val_loss: 1.2850\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8893 - val_loss: 1.2848\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8892 - val_loss: 1.2846\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8891 - val_loss: 1.2845\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8891 - val_loss: 1.2843\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8890 - val_loss: 1.2841\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8889 - val_loss: 1.2840\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8889 - val_loss: 1.2838\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8888 - val_loss: 1.2836\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8887 - val_loss: 1.2835\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8886 - val_loss: 1.2833\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8886 - val_loss: 1.2831\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8885 - val_loss: 1.2830\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8884 - val_loss: 1.2828\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8883 - val_loss: 1.2826\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8883 - val_loss: 1.2824\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8882 - val_loss: 1.2823\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8881 - val_loss: 1.2821\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8880 - val_loss: 1.2819\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8880 - val_loss: 1.2817\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8879 - val_loss: 1.2816\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8878 - val_loss: 1.2814\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8877 - val_loss: 1.2812\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8876 - val_loss: 1.2810\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8876 - val_loss: 1.2809\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8875 - val_loss: 1.2807\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.8874 - val_loss: 1.2805\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8874 - val_loss: 1.2803\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8872 - val_loss: 1.2802\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8872 - val_loss: 1.2800\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8871 - val_loss: 1.2798\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8870 - val_loss: 1.2796\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8869 - val_loss: 1.2794\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8869 - val_loss: 1.2793\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8868 - val_loss: 1.2791\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8867 - val_loss: 1.2789\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8867 - val_loss: 1.2787\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8866 - val_loss: 1.2785\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.8865 - val_loss: 1.2784\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8864 - val_loss: 1.2782\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8863 - val_loss: 1.2780\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8862 - val_loss: 1.2778\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8862 - val_loss: 1.2776\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8861 - val_loss: 1.2774\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8860 - val_loss: 1.2772\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.8859 - val_loss: 1.2771\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8858 - val_loss: 1.2769\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8857 - val_loss: 1.2767\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8857 - val_loss: 1.2765\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8856 - val_loss: 1.2763\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8855 - val_loss: 1.2761\n",
      "Execution time:  163.17893505096436\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9169\n",
      "Root Mean Square Error: 1.1115\n",
      "Mean Square Error: 1.2354\n",
      "\n",
      "Train RMSE: 1.111\n",
      "Train MSE: 1.235\n",
      "Train MAE: 0.917\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_43 (LSTM)               (None, 72, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_43 (TimeDis (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8813 - val_loss: 1.1289\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8809 - val_loss: 1.1283\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8806 - val_loss: 1.1277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8802 - val_loss: 1.1271\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8798 - val_loss: 1.1264\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8794 - val_loss: 1.1258\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8790 - val_loss: 1.1251\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8786 - val_loss: 1.1243\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8781 - val_loss: 1.1236\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8777 - val_loss: 1.1228\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8772 - val_loss: 1.1221\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8767 - val_loss: 1.1213\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8763 - val_loss: 1.1205\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8758 - val_loss: 1.1196\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8753 - val_loss: 1.1188\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8748 - val_loss: 1.1179\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8743 - val_loss: 1.1170\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8737 - val_loss: 1.1162\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8732 - val_loss: 1.1152\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8726 - val_loss: 1.1143\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8721 - val_loss: 1.1133\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8715 - val_loss: 1.1124\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8709 - val_loss: 1.1114\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8703 - val_loss: 1.1103\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8697 - val_loss: 1.1093\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8691 - val_loss: 1.1082\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8684 - val_loss: 1.1071\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8678 - val_loss: 1.1060\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8671 - val_loss: 1.1049\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8664 - val_loss: 1.1037\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8657 - val_loss: 1.1025\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8650 - val_loss: 1.1013\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8643 - val_loss: 1.1000\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8635 - val_loss: 1.0987\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.8627 - val_loss: 1.0974\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.8619 - val_loss: 1.0960\n",
      "Execution time:  323.8963716030121\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8968\n",
      "Root Mean Square Error: 1.0926\n",
      "Mean Square Error: 1.1938\n",
      "\n",
      "Train RMSE: 1.093\n",
      "Train MSE: 1.194\n",
      "Train MAE: 0.897\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_44 (LSTM)               (None, 72, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_44 (TimeDis (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.6281 - val_loss: 0.2935\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5470 - val_loss: 0.2725\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5319 - val_loss: 0.2593\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5222 - val_loss: 0.2501\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5171 - val_loss: 0.2421\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5137 - val_loss: 0.2374\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5113 - val_loss: 0.2335\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5097 - val_loss: 0.2300\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5081 - val_loss: 0.2269\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5068 - val_loss: 0.2249\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5059 - val_loss: 0.2233\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5046 - val_loss: 0.2218\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.5036 - val_loss: 0.2202\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5027 - val_loss: 0.2192\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5021 - val_loss: 0.2184\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5009 - val_loss: 0.2175\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5002 - val_loss: 0.2169\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4991 - val_loss: 0.2167\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4984 - val_loss: 0.2163\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4976 - val_loss: 0.2160\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4966 - val_loss: 0.2163\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4954 - val_loss: 0.2169\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4945 - val_loss: 0.2169\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4932 - val_loss: 0.2176\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4919 - val_loss: 0.2187\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4910 - val_loss: 0.2186\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4899 - val_loss: 0.2188\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4892 - val_loss: 0.2191\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.4883 - val_loss: 0.2193\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4877 - val_loss: 0.2193\n",
      "Execution time:  72.93636870384216\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2586\n",
      "Root Mean Square Error: 0.6588\n",
      "Mean Square Error: 0.4341\n",
      "\n",
      "Train RMSE: 0.659\n",
      "Train MSE: 0.434\n",
      "Train MAE: 0.259\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_45 (LSTM)               (None, 72, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_45 (TimeDis (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 9s 26ms/step - loss: 0.5411 - val_loss: 0.4008\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.5039 - val_loss: 0.3932\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4976 - val_loss: 0.3910\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4927 - val_loss: 0.3904\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4893 - val_loss: 0.3884\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4867 - val_loss: 0.3856\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4850 - val_loss: 0.3800\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4838 - val_loss: 0.3774\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4814 - val_loss: 0.3731\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4789 - val_loss: 0.3697\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4741 - val_loss: 0.3660\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4717 - val_loss: 0.3641\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4697 - val_loss: 0.3634\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4694 - val_loss: 0.3626\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4684 - val_loss: 0.3622\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4676 - val_loss: 0.3619\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4676 - val_loss: 0.3615\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4666 - val_loss: 0.3612\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4659 - val_loss: 0.3608\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4657 - val_loss: 0.3607\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4656 - val_loss: 0.3604\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4654 - val_loss: 0.3601\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4652 - val_loss: 0.3597\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4645 - val_loss: 0.3592\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4637 - val_loss: 0.3591\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4629 - val_loss: 0.3580\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4611 - val_loss: 0.3580\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4601 - val_loss: 0.3582\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4598 - val_loss: 0.3583\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4588 - val_loss: 0.3586\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4584 - val_loss: 0.3583\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4575 - val_loss: 0.3588\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4566 - val_loss: 0.3584\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4560 - val_loss: 0.3590\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.4546 - val_loss: 0.3590\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.4549 - val_loss: 0.3587\n",
      "Execution time:  323.77091455459595\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2971\n",
      "Root Mean Square Error: 0.7749\n",
      "Mean Square Error: 0.6005\n",
      "\n",
      "Train RMSE: 0.775\n",
      "Train MSE: 0.601\n",
      "Train MAE: 0.297\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_46 (LSTM)               (None, 72, 40)            6720      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_46 (TimeDis (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.8535 - val_loss: 1.0639\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.7285 - val_loss: 0.8927\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6871 - val_loss: 0.8505\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6710 - val_loss: 0.8344\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6630 - val_loss: 0.8256\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6582 - val_loss: 0.8204\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6550 - val_loss: 0.8168\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6527 - val_loss: 0.8140\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6506 - val_loss: 0.8119\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6488 - val_loss: 0.8101\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6472 - val_loss: 0.8086\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6457 - val_loss: 0.8073\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6445 - val_loss: 0.8062\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6434 - val_loss: 0.8053\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6425 - val_loss: 0.8044\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6416 - val_loss: 0.8037\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6410 - val_loss: 0.8031\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6401 - val_loss: 0.8025\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6395 - val_loss: 0.8020\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6389 - val_loss: 0.8015\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6383 - val_loss: 0.8010\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6379 - val_loss: 0.8006\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6375 - val_loss: 0.8003\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.6371 - val_loss: 0.7999\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6367 - val_loss: 0.7996\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6365 - val_loss: 0.7993\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6360 - val_loss: 0.7991\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6356 - val_loss: 0.7988\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6353 - val_loss: 0.7986\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6349 - val_loss: 0.7984\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6346 - val_loss: 0.7981\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6343 - val_loss: 0.7979\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6341 - val_loss: 0.7977\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6337 - val_loss: 0.7976\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6335 - val_loss: 0.7974\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6333 - val_loss: 0.7972\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6328 - val_loss: 0.7971\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6327 - val_loss: 0.7969\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6325 - val_loss: 0.7968\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6324 - val_loss: 0.7966\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6320 - val_loss: 0.7965\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6319 - val_loss: 0.7964\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6316 - val_loss: 0.7962\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6312 - val_loss: 0.7961\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6314 - val_loss: 0.7960\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6307 - val_loss: 0.7959\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6306 - val_loss: 0.7958\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6306 - val_loss: 0.7957\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6301 - val_loss: 0.7956\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6301 - val_loss: 0.7955\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6297 - val_loss: 0.7954\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6296 - val_loss: 0.7953\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6294 - val_loss: 0.7952\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6306 - val_loss: 0.7951\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6296 - val_loss: 0.7951\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6290 - val_loss: 0.7950\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 0.6288 - val_loss: 0.7949\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6293 - val_loss: 0.7948\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6286 - val_loss: 0.7948\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6289 - val_loss: 0.7947\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6286 - val_loss: 0.7946\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6283 - val_loss: 0.7946\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6280 - val_loss: 0.7945\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 0.6292 - val_loss: 0.7944\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6282 - val_loss: 0.7944\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6277 - val_loss: 0.7943\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6274 - val_loss: 0.7943\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6285 - val_loss: 0.7942\n",
      "Execution time:  166.01413583755493\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5579\n",
      "Root Mean Square Error: 0.8281\n",
      "Mean Square Error: 0.6858\n",
      "\n",
      "Train RMSE: 0.828\n",
      "Train MSE: 0.686\n",
      "Train MAE: 0.558\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_47 (LSTM)               (None, 72, 55)            12540     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_47 (TimeDis (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.7222 - val_loss: 0.7592\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6680 - val_loss: 0.7469\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6619 - val_loss: 0.7404\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6591 - val_loss: 0.7391\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6583 - val_loss: 0.7375\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6573 - val_loss: 0.7361\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6566 - val_loss: 0.7351\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6561 - val_loss: 0.7331\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6556 - val_loss: 0.7323\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6546 - val_loss: 0.7314\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6539 - val_loss: 0.7303\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6533 - val_loss: 0.7297\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6525 - val_loss: 0.7287\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6525 - val_loss: 0.7282\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6517 - val_loss: 0.7276\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6516 - val_loss: 0.7271\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6515 - val_loss: 0.7269\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6511 - val_loss: 0.7262\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6511 - val_loss: 0.7259\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6505 - val_loss: 0.7253\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6506 - val_loss: 0.7250\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6503 - val_loss: 0.7247\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6495 - val_loss: 0.7245\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6494 - val_loss: 0.7242\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6493 - val_loss: 0.7239\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7236\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7233\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6489 - val_loss: 0.7229\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7224\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6488 - val_loss: 0.7222\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6487 - val_loss: 0.7221\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6485 - val_loss: 0.7217\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6481 - val_loss: 0.7216\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6481 - val_loss: 0.7213\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 9s 25ms/step - loss: 0.6482 - val_loss: 0.7209\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 9s 26ms/step - loss: 0.6480 - val_loss: 0.7208\n",
      "Execution time:  329.16143250465393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "Mean Absolute Error: 0.6018\n",
      "Root Mean Square Error: 0.8835\n",
      "Mean Square Error: 0.7806\n",
      "\n",
      "Train RMSE: 0.884\n",
      "Train MSE: 0.781\n",
      "Train MAE: 0.602\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_48 (LSTM)               (None, 144, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_48 (TimeDis (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 70ms/step - loss: 0.6232 - val_loss: 0.2472\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5862 - val_loss: 0.2524\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5795 - val_loss: 0.2529\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5746 - val_loss: 0.2520\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5711 - val_loss: 0.2506\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5673 - val_loss: 0.2513\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.5655 - val_loss: 0.2521\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5607 - val_loss: 0.2501\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5960 - val_loss: 0.2546\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5774 - val_loss: 0.2470\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5631 - val_loss: 0.2495\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5620 - val_loss: 0.2398\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5603 - val_loss: 0.2433\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5428 - val_loss: 0.2463\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5451 - val_loss: 0.2430\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5362 - val_loss: 0.2405\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5365 - val_loss: 0.2440\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5344 - val_loss: 0.2427\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5298 - val_loss: 0.2419\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5292 - val_loss: 0.2429\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5282 - val_loss: 0.2424\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5279 - val_loss: 0.2424\n",
      "Execution time:  94.87232208251953\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3893\n",
      "Root Mean Square Error: 0.8575\n",
      "Mean Square Error: 0.7353\n",
      "\n",
      "Train RMSE: 0.858\n",
      "Train MSE: 0.735\n",
      "Train MAE: 0.389\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_49 (LSTM)               (None, 144, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_49 (TimeDis (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 16s 48ms/step - loss: 0.6048 - val_loss: 0.4179\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 16s 47ms/step - loss: 0.5798 - val_loss: 0.4155\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 16s 47ms/step - loss: 0.5738 - val_loss: 0.4137\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5685 - val_loss: 0.4116\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5651 - val_loss: 0.4098\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5630 - val_loss: 0.4085\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5605 - val_loss: 0.4061\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5595 - val_loss: 0.4038\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5582 - val_loss: 0.3985\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5567 - val_loss: 0.3928\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5548 - val_loss: 0.3927\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5480 - val_loss: 0.3855\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5602 - val_loss: 0.3961\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5422 - val_loss: 0.3817\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5411 - val_loss: 0.3816\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5395 - val_loss: 0.3834\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5357 - val_loss: 0.3781\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5303 - val_loss: 0.3805\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5287 - val_loss: 0.3823\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5276 - val_loss: 0.3822\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5229 - val_loss: 0.3817\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5306 - val_loss: 0.3804\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5210 - val_loss: 0.3811\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5224 - val_loss: 0.3830\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5228 - val_loss: 0.3795\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5183 - val_loss: 0.3792\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5198 - val_loss: 0.3764\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5176 - val_loss: 0.3749\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5211 - val_loss: 0.3749\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5159 - val_loss: 0.3725\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5156 - val_loss: 0.3749\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5106 - val_loss: 0.3746\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5101 - val_loss: 0.3756\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5089 - val_loss: 0.3793\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5097 - val_loss: 0.3785\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5112 - val_loss: 0.3758\n",
      "Execution time:  602.3874583244324\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3919\n",
      "Root Mean Square Error: 0.8977\n",
      "Mean Square Error: 0.8058\n",
      "\n",
      "Train RMSE: 0.898\n",
      "Train MSE: 0.806\n",
      "Train MAE: 0.392\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_50 (LSTM)               (None, 144, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_50 (TimeDis (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 60ms/step - loss: 0.8295 - val_loss: 0.8891\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6965 - val_loss: 0.8180\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6926 - val_loss: 0.8076\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6892 - val_loss: 0.8031\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6877 - val_loss: 0.8006\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6801 - val_loss: 0.7987\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6784 - val_loss: 0.7974\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6839 - val_loss: 0.7966\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6775 - val_loss: 0.7958\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6810 - val_loss: 0.7953\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6814 - val_loss: 0.7952\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6988 - val_loss: 0.7949\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6976 - val_loss: 0.7946\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6971 - val_loss: 0.7944\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6969 - val_loss: 0.7942\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6967 - val_loss: 0.7940\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6966 - val_loss: 0.7938\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6965 - val_loss: 0.7936\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6964 - val_loss: 0.7934\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6963 - val_loss: 0.7933\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6962 - val_loss: 0.7931\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6961 - val_loss: 0.7930\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6960 - val_loss: 0.7929\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6959 - val_loss: 0.7927\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6959 - val_loss: 0.7926\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6958 - val_loss: 0.7925\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6957 - val_loss: 0.7924\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6945 - val_loss: 0.7922\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6656 - val_loss: 0.7919\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6752 - val_loss: 0.7919\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6624 - val_loss: 0.7918\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6674 - val_loss: 0.7916\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6717 - val_loss: 0.7916\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6680 - val_loss: 0.7916\n",
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6649 - val_loss: 0.7915\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6603 - val_loss: 0.7915\n",
      "Epoch 37/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6693 - val_loss: 0.7914\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6815 - val_loss: 0.7914\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6951 - val_loss: 0.7913\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6951 - val_loss: 0.7913\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6950 - val_loss: 0.7912\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6950 - val_loss: 0.7912\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6950 - val_loss: 0.7912\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6949 - val_loss: 0.7911\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6949 - val_loss: 0.7911\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6948 - val_loss: 0.7910\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6948 - val_loss: 0.7910\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6855 - val_loss: 0.7910\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6659 - val_loss: 0.7908\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6741 - val_loss: 0.7908\n",
      "Epoch 51/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6672 - val_loss: 0.7907\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6815 - val_loss: 0.7909\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6795 - val_loss: 0.7909\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6761 - val_loss: 0.7910\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6722 - val_loss: 0.7910\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6710 - val_loss: 0.7910\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6713 - val_loss: 0.7910\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6705 - val_loss: 0.7910\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6708 - val_loss: 0.7910\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6699 - val_loss: 0.7910\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6703 - val_loss: 0.7909\n",
      "Execution time:  253.6297149658203\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6309\n",
      "Root Mean Square Error: 0.9198\n",
      "Mean Square Error: 0.8460\n",
      "\n",
      "Train RMSE: 0.920\n",
      "Train MSE: 0.846\n",
      "Train MAE: 0.631\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_51 (LSTM)               (None, 144, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_51 (TimeDis (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.7332 - val_loss: 0.7220\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 17s 50ms/step - loss: 0.7004 - val_loss: 0.7191\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.6934 - val_loss: 0.7159\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6928 - val_loss: 0.7138\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.6942 - val_loss: 0.7150\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6906 - val_loss: 0.7136\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.6900 - val_loss: 0.7126\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6901 - val_loss: 0.7118\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6889 - val_loss: 0.7116\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6888 - val_loss: 0.7112\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6878 - val_loss: 0.7105\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6946 - val_loss: 0.7107\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6888 - val_loss: 0.7100\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6877 - val_loss: 0.7090\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6951 - val_loss: 0.7073\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6912 - val_loss: 0.7080\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6825 - val_loss: 0.7076\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6858 - val_loss: 0.7070\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6849 - val_loss: 0.7066\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6845 - val_loss: 0.7066\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6997 - val_loss: 0.7051\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6894 - val_loss: 0.7048\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6836 - val_loss: 0.7046\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6829 - val_loss: 0.7046\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6833 - val_loss: 0.7040\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 17s 50ms/step - loss: 0.6806 - val_loss: 0.7032\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6817 - val_loss: 0.7041\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6793 - val_loss: 0.7024\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6890 - val_loss: 0.7029\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6766 - val_loss: 0.7027\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6762 - val_loss: 0.7025\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6756 - val_loss: 0.7022\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6876 - val_loss: 0.7021\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6781 - val_loss: 0.7022\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6771 - val_loss: 0.7020\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6759 - val_loss: 0.7019\n",
      "Execution time:  606.6077654361725\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6711\n",
      "Root Mean Square Error: 0.9762\n",
      "Mean Square Error: 0.9530\n",
      "\n",
      "Train RMSE: 0.976\n",
      "Train MSE: 0.953\n",
      "Train MAE: 0.671\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_52 (LSTM)               (None, 144, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_52 (TimeDis (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 61ms/step - loss: 0.6840 - val_loss: 0.7594\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.6838 - val_loss: 0.7590\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6838 - val_loss: 0.7586\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6835 - val_loss: 0.7582\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6833 - val_loss: 0.7577\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6832 - val_loss: 0.7572\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6830 - val_loss: 0.7567\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6829 - val_loss: 0.7562\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6827 - val_loss: 0.7557\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6826 - val_loss: 0.7552\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6824 - val_loss: 0.7547\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6822 - val_loss: 0.7541\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6820 - val_loss: 0.7536\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.6819 - val_loss: 0.7531\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6816 - val_loss: 0.7525\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6814 - val_loss: 0.7520\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6813 - val_loss: 0.7514\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6811 - val_loss: 0.7509\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6809 - val_loss: 0.7503\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6807 - val_loss: 0.7498\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6806 - val_loss: 0.7492\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6804 - val_loss: 0.7487\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6801 - val_loss: 0.7481\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6800 - val_loss: 0.7476\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6798 - val_loss: 0.7470\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6795 - val_loss: 0.7464\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6794 - val_loss: 0.7459\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6792 - val_loss: 0.7453\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6790 - val_loss: 0.7447\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6788 - val_loss: 0.7441\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6786 - val_loss: 0.7436\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6784 - val_loss: 0.7430\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6782 - val_loss: 0.7424\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6780 - val_loss: 0.7418\n",
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6778 - val_loss: 0.7413\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6776 - val_loss: 0.7407\n",
      "Epoch 37/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6774 - val_loss: 0.7401\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6772 - val_loss: 0.7395\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6770 - val_loss: 0.7389\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6768 - val_loss: 0.7383\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6766 - val_loss: 0.7377\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6764 - val_loss: 0.7372\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6763 - val_loss: 0.7366\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6760 - val_loss: 0.7360\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6758 - val_loss: 0.7354\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6756 - val_loss: 0.7348\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6754 - val_loss: 0.7342\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6753 - val_loss: 0.7336\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6751 - val_loss: 0.7330\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6748 - val_loss: 0.7324\n",
      "Epoch 51/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6746 - val_loss: 0.7318\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6745 - val_loss: 0.7312\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6742 - val_loss: 0.7306\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6741 - val_loss: 0.7300\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6738 - val_loss: 0.7294\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6736 - val_loss: 0.7288\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6735 - val_loss: 0.7282\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6732 - val_loss: 0.7276\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6731 - val_loss: 0.7270\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6729 - val_loss: 0.7264\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6726 - val_loss: 0.7258\n",
      "Epoch 62/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6724 - val_loss: 0.7252\n",
      "Epoch 63/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6723 - val_loss: 0.7245\n",
      "Epoch 64/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6720 - val_loss: 0.7239\n",
      "Epoch 65/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6719 - val_loss: 0.7233\n",
      "Epoch 66/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6716 - val_loss: 0.7227\n",
      "Epoch 67/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6715 - val_loss: 0.7221\n",
      "Epoch 68/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6712 - val_loss: 0.7215\n",
      "Execution time:  283.50412583351135\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6551\n",
      "Root Mean Square Error: 0.9536\n",
      "Mean Square Error: 0.9094\n",
      "\n",
      "Train RMSE: 0.954\n",
      "Train MSE: 0.909\n",
      "Train MAE: 0.655\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_53 (LSTM)               (None, 144, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_53 (TimeDis (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 16s 48ms/step - loss: 0.6932 - val_loss: 0.6855\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6915 - val_loss: 0.6824\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6896 - val_loss: 0.6791\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6875 - val_loss: 0.6756\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6854 - val_loss: 0.6720\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6832 - val_loss: 0.6683\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6810 - val_loss: 0.6646\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6789 - val_loss: 0.6609\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6767 - val_loss: 0.6572\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6746 - val_loss: 0.6536\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6725 - val_loss: 0.6499\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6704 - val_loss: 0.6461\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6683 - val_loss: 0.6424\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6662 - val_loss: 0.6386\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6641 - val_loss: 0.6347\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6619 - val_loss: 0.6308\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6597 - val_loss: 0.6268\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6575 - val_loss: 0.6227\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6552 - val_loss: 0.6186\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6529 - val_loss: 0.6143\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6505 - val_loss: 0.6099\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6482 - val_loss: 0.6055\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6457 - val_loss: 0.6009\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6432 - val_loss: 0.5963\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6406 - val_loss: 0.5914\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6381 - val_loss: 0.5865\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 17s 50ms/step - loss: 0.6355 - val_loss: 0.5815\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6328 - val_loss: 0.5764\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6302 - val_loss: 0.5713\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6275 - val_loss: 0.5660\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6248 - val_loss: 0.5607\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6221 - val_loss: 0.5553\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6194 - val_loss: 0.5499\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6167 - val_loss: 0.5445\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 17s 50ms/step - loss: 0.6141 - val_loss: 0.5391\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.6115 - val_loss: 0.5337\n",
      "Execution time:  608.9561471939087\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5098\n",
      "Root Mean Square Error: 0.8439\n",
      "Mean Square Error: 0.7122\n",
      "\n",
      "Train RMSE: 0.844\n",
      "Train MSE: 0.712\n",
      "Train MAE: 0.510\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_54 (LSTM)               (None, 144, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_54 (TimeDis (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 60ms/step - loss: 0.9011 - val_loss: 1.2813\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9010 - val_loss: 1.2811\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9010 - val_loss: 1.2810\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9009 - val_loss: 1.2808\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9009 - val_loss: 1.2806\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9008 - val_loss: 1.2805\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9007 - val_loss: 1.2803\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9007 - val_loss: 1.2801\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9006 - val_loss: 1.2799\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9005 - val_loss: 1.2797\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9004 - val_loss: 1.2796\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9004 - val_loss: 1.2794\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9003 - val_loss: 1.2792\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9002 - val_loss: 1.2790\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9001 - val_loss: 1.2788\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9001 - val_loss: 1.2786\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9000 - val_loss: 1.2784\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8999 - val_loss: 1.2782\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8998 - val_loss: 1.2780\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8998 - val_loss: 1.2778\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8997 - val_loss: 1.2776\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8996 - val_loss: 1.2774\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8995 - val_loss: 1.2771\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8994 - val_loss: 1.2769\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8994 - val_loss: 1.2767\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8993 - val_loss: 1.2765\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8992 - val_loss: 1.2763\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8991 - val_loss: 1.2761\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8990 - val_loss: 1.2759\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2757\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2754\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8988 - val_loss: 1.2752\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8987 - val_loss: 1.2750\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8986 - val_loss: 1.2748\n",
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8985 - val_loss: 1.2746\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8984 - val_loss: 1.2743\n",
      "Epoch 37/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.8984 - val_loss: 1.2741\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8983 - val_loss: 1.2739\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8982 - val_loss: 1.2737\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8981 - val_loss: 1.2735\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8980 - val_loss: 1.2732\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8979 - val_loss: 1.2730\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8978 - val_loss: 1.2728\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8977 - val_loss: 1.2726\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8976 - val_loss: 1.2723\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8976 - val_loss: 1.2721\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8975 - val_loss: 1.2719\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8974 - val_loss: 1.2716\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8973 - val_loss: 1.2714\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.8972 - val_loss: 1.2712\n",
      "Epoch 51/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8971 - val_loss: 1.2709\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.8970 - val_loss: 1.2707\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8969 - val_loss: 1.2705\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8968 - val_loss: 1.2702\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8967 - val_loss: 1.2700\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8966 - val_loss: 1.2698\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8965 - val_loss: 1.2695\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8964 - val_loss: 1.2693\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8964 - val_loss: 1.2691\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8963 - val_loss: 1.2688\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8962 - val_loss: 1.2686\n",
      "Epoch 62/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8961 - val_loss: 1.2683\n",
      "Epoch 63/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8959 - val_loss: 1.2681\n",
      "Epoch 64/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8959 - val_loss: 1.2679\n",
      "Epoch 65/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8958 - val_loss: 1.2676\n",
      "Epoch 66/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8957 - val_loss: 1.2674\n",
      "Epoch 67/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8956 - val_loss: 1.2671\n",
      "Epoch 68/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8955 - val_loss: 1.2669\n",
      "Execution time:  281.46847128868103\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9132\n",
      "Root Mean Square Error: 1.1095\n",
      "Mean Square Error: 1.2309\n",
      "\n",
      "Train RMSE: 1.109\n",
      "Train MSE: 1.231\n",
      "Train MAE: 0.913\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_55 (LSTM)               (None, 144, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_55 (TimeDis (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8935 - val_loss: 1.1304\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8932 - val_loss: 1.1299\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8929 - val_loss: 1.1293\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8926 - val_loss: 1.1287\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8922 - val_loss: 1.1280\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8919 - val_loss: 1.1273\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8915 - val_loss: 1.1267\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8911 - val_loss: 1.1259\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8907 - val_loss: 1.1252\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8903 - val_loss: 1.1245\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8899 - val_loss: 1.1237\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 16s 45ms/step - loss: 0.8895 - val_loss: 1.1229\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8890 - val_loss: 1.1221\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8886 - val_loss: 1.1213\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8881 - val_loss: 1.1205\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8877 - val_loss: 1.1196\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 16s 45ms/step - loss: 0.8872 - val_loss: 1.1188\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 16s 45ms/step - loss: 0.8867 - val_loss: 1.1179\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8862 - val_loss: 1.1170\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8857 - val_loss: 1.1160\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8852 - val_loss: 1.1151\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8847 - val_loss: 1.1141\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8841 - val_loss: 1.1131\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8836 - val_loss: 1.1121\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8830 - val_loss: 1.1111\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8824 - val_loss: 1.1100\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 16s 45ms/step - loss: 0.8819 - val_loss: 1.1089\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 16s 45ms/step - loss: 0.8812 - val_loss: 1.1078\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8806 - val_loss: 1.1066\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8800 - val_loss: 1.1055\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8793 - val_loss: 1.1042\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8786 - val_loss: 1.1030\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8779 - val_loss: 1.1017\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.8772 - val_loss: 1.1004\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8765 - val_loss: 1.0990\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 15s 45ms/step - loss: 0.8757 - val_loss: 1.0976\n",
      "Execution time:  562.3713281154633\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8973\n",
      "Root Mean Square Error: 1.0944\n",
      "Mean Square Error: 1.1978\n",
      "\n",
      "Train RMSE: 1.094\n",
      "Train MSE: 1.198\n",
      "Train MAE: 0.897\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_56 (LSTM)               (None, 144, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_56 (TimeDis (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 64ms/step - loss: 0.6613 - val_loss: 0.4499\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5872 - val_loss: 0.2570\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5855 - val_loss: 0.2596\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5816 - val_loss: 0.2567\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5791 - val_loss: 0.2550\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5767 - val_loss: 0.2532\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5746 - val_loss: 0.2519\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5727 - val_loss: 0.2515\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5705 - val_loss: 0.2518\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5679 - val_loss: 0.2522\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5641 - val_loss: 0.2536\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5590 - val_loss: 0.2566\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5518 - val_loss: 0.2621\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5453 - val_loss: 0.2629\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5414 - val_loss: 0.2630\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5390 - val_loss: 0.2629\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5373 - val_loss: 0.2622\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5360 - val_loss: 0.2614\n",
      "Execution time:  77.334885597229\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3890\n",
      "Root Mean Square Error: 0.8297\n",
      "Mean Square Error: 0.6883\n",
      "\n",
      "Train RMSE: 0.830\n",
      "Train MSE: 0.688\n",
      "Train MAE: 0.389\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_57 (LSTM)               (None, 144, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_57 (TimeDis (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 16s 48ms/step - loss: 0.6009 - val_loss: 0.4123\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 16s 47ms/step - loss: 0.5773 - val_loss: 0.4180\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 16s 48ms/step - loss: 0.5724 - val_loss: 0.4196\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5684 - val_loss: 0.4192\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 16s 48ms/step - loss: 0.5659 - val_loss: 0.4184\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 16s 48ms/step - loss: 0.5640 - val_loss: 0.4170\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5621 - val_loss: 0.4151\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5604 - val_loss: 0.4139\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5590 - val_loss: 0.4109\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5571 - val_loss: 0.4063\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5552 - val_loss: 0.3969\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5515 - val_loss: 0.3851\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5448 - val_loss: 0.3754\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 16s 48ms/step - loss: 0.5350 - val_loss: 0.3748\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5289 - val_loss: 0.3764\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5188 - val_loss: 0.3771\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5179 - val_loss: 0.3774\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5188 - val_loss: 0.3771\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5202 - val_loss: 0.3761\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5175 - val_loss: 0.3768\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5179 - val_loss: 0.3767\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5162 - val_loss: 0.3767\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 17s 49ms/step - loss: 0.5158 - val_loss: 0.3768\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 17s 48ms/step - loss: 0.5136 - val_loss: 0.3785\n",
      "Execution time:  400.83725786209106\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3808\n",
      "Root Mean Square Error: 0.8723\n",
      "Mean Square Error: 0.7609\n",
      "\n",
      "Train RMSE: 0.872\n",
      "Train MSE: 0.761\n",
      "Train MAE: 0.381\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_58 (LSTM)               (None, 144, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_58 (TimeDis (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 59ms/step - loss: 0.8856 - val_loss: 1.1479\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7761 - val_loss: 0.9216\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7284 - val_loss: 0.8666\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7135 - val_loss: 0.8446\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7041 - val_loss: 0.8320\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6973 - val_loss: 0.8239\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6928 - val_loss: 0.8182\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6897 - val_loss: 0.8140\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6873 - val_loss: 0.8109\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6852 - val_loss: 0.8086\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6832 - val_loss: 0.8069\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6812 - val_loss: 0.8055\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6792 - val_loss: 0.8044\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6768 - val_loss: 0.8034\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6747 - val_loss: 0.8026\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6724 - val_loss: 0.8019\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6696 - val_loss: 0.8013\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.6677 - val_loss: 0.8009\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6622 - val_loss: 0.8004\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6617 - val_loss: 0.8000\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6606 - val_loss: 0.7996\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6597 - val_loss: 0.7993\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6593 - val_loss: 0.7990\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6577 - val_loss: 0.7987\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6575 - val_loss: 0.7985\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6568 - val_loss: 0.7983\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6561 - val_loss: 0.7981\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6555 - val_loss: 0.7978\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6551 - val_loss: 0.7976\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6533 - val_loss: 0.7975\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6523 - val_loss: 0.7972\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6528 - val_loss: 0.7972\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6502 - val_loss: 0.7971\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6498 - val_loss: 0.7969\n",
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6496 - val_loss: 0.7968\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6494 - val_loss: 0.7967\n",
      "Epoch 37/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6491 - val_loss: 0.7966\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6486 - val_loss: 0.7965\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6482 - val_loss: 0.7964\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6481 - val_loss: 0.7963\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6478 - val_loss: 0.7963\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6478 - val_loss: 0.7962\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6482 - val_loss: 0.7961\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6486 - val_loss: 0.7960\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6469 - val_loss: 0.7960\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6472 - val_loss: 0.7959\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.6467 - val_loss: 0.7958\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6468 - val_loss: 0.7958\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6468 - val_loss: 0.7957\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6465 - val_loss: 0.7957\n",
      "Epoch 51/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6467 - val_loss: 0.7957\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6452 - val_loss: 0.7956\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6448 - val_loss: 0.7955\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6474 - val_loss: 0.7955\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6473 - val_loss: 0.7954\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6458 - val_loss: 0.7954\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6480 - val_loss: 0.7954\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6454 - val_loss: 0.7953\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6450 - val_loss: 0.7953\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6441 - val_loss: 0.7952\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6438 - val_loss: 0.7952\n",
      "Epoch 62/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6437 - val_loss: 0.7952\n",
      "Epoch 63/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6439 - val_loss: 0.7951\n",
      "Epoch 64/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6437 - val_loss: 0.7951\n",
      "Epoch 65/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6436 - val_loss: 0.7950\n",
      "Epoch 66/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6430 - val_loss: 0.7950\n",
      "Epoch 67/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6416 - val_loss: 0.7949\n",
      "Epoch 68/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6429 - val_loss: 0.7949\n",
      "Execution time:  280.67818784713745\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6319\n",
      "Root Mean Square Error: 0.9309\n",
      "Mean Square Error: 0.8666\n",
      "\n",
      "Train RMSE: 0.931\n",
      "Train MSE: 0.867\n",
      "Train MAE: 0.632\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_59 (LSTM)               (None, 144, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 16s 47ms/step - loss: 0.7371 - val_loss: 0.7355\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6908 - val_loss: 0.7269\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6881 - val_loss: 0.7233\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6892 - val_loss: 0.7220\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6854 - val_loss: 0.7205\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6848 - val_loss: 0.7194\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6834 - val_loss: 0.7189\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6827 - val_loss: 0.7185\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6833 - val_loss: 0.7179\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6830 - val_loss: 0.7174\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6827 - val_loss: 0.7171\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6830 - val_loss: 0.7166\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6826 - val_loss: 0.7160\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6816 - val_loss: 0.7158\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6812 - val_loss: 0.7155\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6814 - val_loss: 0.7149\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6827 - val_loss: 0.7148\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 16s 47ms/step - loss: 0.6802 - val_loss: 0.7145\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6801 - val_loss: 0.7140\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6801 - val_loss: 0.7138\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6804 - val_loss: 0.7134\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6801 - val_loss: 0.7133\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7130\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7128\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7125\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7123\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6791 - val_loss: 0.7121\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6788 - val_loss: 0.7118\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7112\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6789 - val_loss: 0.7109\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7105\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6666 - val_loss: 0.7103\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 16s 47ms/step - loss: 0.6666 - val_loss: 0.7101\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6667 - val_loss: 0.7099\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6668 - val_loss: 0.7097\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 16s 46ms/step - loss: 0.6667 - val_loss: 0.7095\n",
      "Execution time:  571.1410245895386\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6455\n",
      "Root Mean Square Error: 0.9380\n",
      "Mean Square Error: 0.8798\n",
      "\n",
      "Train RMSE: 0.938\n",
      "Train MSE: 0.880\n",
      "Train MAE: 0.646\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_60 (LSTM)               (None, 432, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_60 (TimeDis (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6438 - val_loss: 0.3788\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6266 - val_loss: 0.3196\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6227 - val_loss: 0.3657\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6168 - val_loss: 0.3678\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6146 - val_loss: 0.3690\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6130 - val_loss: 0.3703\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6117 - val_loss: 0.3709\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6106 - val_loss: 0.3711\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6098 - val_loss: 0.3714\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6090 - val_loss: 0.3716\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6084 - val_loss: 0.3716\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6078 - val_loss: 0.3719\n",
      "Execution time:  163.69914937019348\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5339\n",
      "Root Mean Square Error: 0.9933\n",
      "Mean Square Error: 0.9867\n",
      "\n",
      "Train RMSE: 0.993\n",
      "Train MSE: 0.987\n",
      "Train MAE: 0.534\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_61 (LSTM)               (None, 432, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_61 (TimeDis (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 43s 136ms/step - loss: 0.6733 - val_loss: 0.3087\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.6685 - val_loss: 0.2731\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.6677 - val_loss: 0.2682\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 43s 136ms/step - loss: 0.6562 - val_loss: 0.2692\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 43s 136ms/step - loss: 0.6542 - val_loss: 0.2694\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 43s 137ms/step - loss: 0.6546 - val_loss: 0.2722\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6545 - val_loss: 0.2748\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6540 - val_loss: 0.2811\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6527 - val_loss: 0.2806\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6521 - val_loss: 0.2844\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6513 - val_loss: 0.2840\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6506 - val_loss: 0.2725\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6507 - val_loss: 0.2721\n",
      "Execution time:  558.6593346595764\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5364\n",
      "Root Mean Square Error: 1.0084\n",
      "Mean Square Error: 1.0168\n",
      "\n",
      "Train RMSE: 1.008\n",
      "Train MSE: 1.017\n",
      "Train MAE: 0.536\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_62 (LSTM)               (None, 432, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_62 (TimeDis (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.8786 - val_loss: 1.0165\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.7079 - val_loss: 0.8192\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6808 - val_loss: 0.7994\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6767 - val_loss: 0.7932\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6749 - val_loss: 0.7900\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6738 - val_loss: 0.7881\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6731 - val_loss: 0.7867\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 187ms/step - loss: 0.6726 - val_loss: 0.7858\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 14s 191ms/step - loss: 0.6723 - val_loss: 0.7850\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 187ms/step - loss: 0.6720 - val_loss: 0.7845\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6717 - val_loss: 0.7840\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 14s 190ms/step - loss: 0.6715 - val_loss: 0.7836\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6714 - val_loss: 0.7833\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6712 - val_loss: 0.7830\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6711 - val_loss: 0.7828\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6710 - val_loss: 0.7826\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6709 - val_loss: 0.7824\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6708 - val_loss: 0.7822\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 13s 187ms/step - loss: 0.6708 - val_loss: 0.7821\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6707 - val_loss: 0.7820\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6706 - val_loss: 0.7818\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6706 - val_loss: 0.7817\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6705 - val_loss: 0.7816\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 13s 187ms/step - loss: 0.6705 - val_loss: 0.7815\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6705 - val_loss: 0.7815\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6704 - val_loss: 0.7814\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 14s 189ms/step - loss: 0.6704 - val_loss: 0.7813\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6703 - val_loss: 0.7812\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7812\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7811\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6703 - val_loss: 0.7811\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7810\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7809\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6702 - val_loss: 0.7809\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7809\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6701 - val_loss: 0.7808\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7808\n",
      "Epoch 38/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7806\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 14s 197ms/step - loss: 0.6700 - val_loss: 0.7806\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7806\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6700 - val_loss: 0.7804\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6699 - val_loss: 0.7804\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6699 - val_loss: 0.7804\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6699 - val_loss: 0.7804\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 14s 198ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 14s 198ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7802\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7802\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7802\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7802\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 14s 198ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Execution time:  965.790694475174\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6863\n",
      "Root Mean Square Error: 0.9969\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_63 (LSTM)               (None, 432, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_63 (TimeDis (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.7745 - val_loss: 0.6149\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.7011 - val_loss: 0.6123\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.7005 - val_loss: 0.6114\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.7002 - val_loss: 0.6109\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.7000 - val_loss: 0.6105\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6999 - val_loss: 0.6102\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6998 - val_loss: 0.6100\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6997 - val_loss: 0.6097\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6996 - val_loss: 0.6095\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6995 - val_loss: 0.6093\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6995 - val_loss: 0.6092\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 42s 131ms/step - loss: 0.6994 - val_loss: 0.6090\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6993 - val_loss: 0.6089\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6993 - val_loss: 0.6087\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6992 - val_loss: 0.6086\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6992 - val_loss: 0.6085\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 42s 131ms/step - loss: 0.6991 - val_loss: 0.6084\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6991 - val_loss: 0.6083\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6991 - val_loss: 0.6082\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6990 - val_loss: 0.6081\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6990 - val_loss: 0.6080\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6990 - val_loss: 0.6079\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6990 - val_loss: 0.6079\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6989 - val_loss: 0.6078\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6078\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Execution time:  1521.076640844345\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6862\n",
      "Root Mean Square Error: 0.9968\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_64 (LSTM)               (None, 432, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_64 (TimeDis (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6702 - val_loss: 0.7792\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 14s 192ms/step - loss: 0.6702 - val_loss: 0.7788\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.6700 - val_loss: 0.7785\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 14s 192ms/step - loss: 0.6700 - val_loss: 0.7781\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 14s 198ms/step - loss: 0.6699 - val_loss: 0.7778\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 14s 197ms/step - loss: 0.6697 - val_loss: 0.7774\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 14s 197ms/step - loss: 0.6696 - val_loss: 0.7770\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 14s 197ms/step - loss: 0.6696 - val_loss: 0.7766\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6694 - val_loss: 0.7763\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6693 - val_loss: 0.7759\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6692 - val_loss: 0.7755\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6691 - val_loss: 0.7751\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.6690 - val_loss: 0.7747\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6689 - val_loss: 0.7743\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6687 - val_loss: 0.7739\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6687 - val_loss: 0.7735\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6685 - val_loss: 0.7731\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6684 - val_loss: 0.7727\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6683 - val_loss: 0.7723\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6682 - val_loss: 0.7719\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6680 - val_loss: 0.7715\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.6679 - val_loss: 0.7711\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6678 - val_loss: 0.7706\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6677 - val_loss: 0.7702\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6676 - val_loss: 0.7698\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6674 - val_loss: 0.7694\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6673 - val_loss: 0.7690\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6672 - val_loss: 0.7686\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6671 - val_loss: 0.7681\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6670 - val_loss: 0.7677\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6668 - val_loss: 0.7673\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6667 - val_loss: 0.7669\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6666 - val_loss: 0.7664\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6664 - val_loss: 0.7660\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6663 - val_loss: 0.7656\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6662 - val_loss: 0.7652\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6661 - val_loss: 0.7647\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6659 - val_loss: 0.7643\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6658 - val_loss: 0.7639\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6657 - val_loss: 0.7635\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6656 - val_loss: 0.7630\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6655 - val_loss: 0.7626\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6653 - val_loss: 0.7622\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6652 - val_loss: 0.7617\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6651 - val_loss: 0.7613\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.6650 - val_loss: 0.7609\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6648 - val_loss: 0.7604\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6647 - val_loss: 0.7600\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6645 - val_loss: 0.7596\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6644 - val_loss: 0.7591\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6643 - val_loss: 0.7587\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6642 - val_loss: 0.7583\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6640 - val_loss: 0.7578\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.6639 - val_loss: 0.7574\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6638 - val_loss: 0.7570\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6637 - val_loss: 0.7565\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6635 - val_loss: 0.7561\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6634 - val_loss: 0.7557\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6633 - val_loss: 0.7552\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6632 - val_loss: 0.7548\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6630 - val_loss: 0.7543\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6629 - val_loss: 0.7539\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6627 - val_loss: 0.7535\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 15s 207ms/step - loss: 0.6626 - val_loss: 0.7530\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6625 - val_loss: 0.7526\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6624 - val_loss: 0.7522\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6622 - val_loss: 0.7517\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6621 - val_loss: 0.7513\n",
      "Execution time:  1001.7487716674805\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6632\n",
      "Root Mean Square Error: 0.9771\n",
      "Mean Square Error: 0.9546\n",
      "\n",
      "Train RMSE: 0.977\n",
      "Train MSE: 0.955\n",
      "Train MAE: 0.663\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_65 (LSTM)               (None, 432, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_65 (TimeDis (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6969 - val_loss: 0.5985\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 43s 134ms/step - loss: 0.6965 - val_loss: 0.5970\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6961 - val_loss: 0.5955\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6956 - val_loss: 0.5939\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 42s 131ms/step - loss: 0.6951 - val_loss: 0.5921\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6945 - val_loss: 0.5904\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6940 - val_loss: 0.5885\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6934 - val_loss: 0.5866\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6928 - val_loss: 0.5847\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6922 - val_loss: 0.5827\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6916 - val_loss: 0.5807\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6910 - val_loss: 0.5786\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6904 - val_loss: 0.5765\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6898 - val_loss: 0.5744\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6892 - val_loss: 0.5723\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6886 - val_loss: 0.5702\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6880 - val_loss: 0.5682\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6874 - val_loss: 0.5663\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6868 - val_loss: 0.5643\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6863 - val_loss: 0.5624\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6857 - val_loss: 0.5604\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6851 - val_loss: 0.5584\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6846 - val_loss: 0.5565\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6840 - val_loss: 0.5545\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6835 - val_loss: 0.5525\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6829 - val_loss: 0.5505\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6824 - val_loss: 0.5484\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6818 - val_loss: 0.5463\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6812 - val_loss: 0.5442\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.6807 - val_loss: 0.5421\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6801 - val_loss: 0.5399\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.6795 - val_loss: 0.5377\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6789 - val_loss: 0.5355\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6783 - val_loss: 0.5332\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6777 - val_loss: 0.5309\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6770 - val_loss: 0.5285\n",
      "Execution time:  1523.2913496494293\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6121\n",
      "Root Mean Square Error: 0.9406\n",
      "Mean Square Error: 0.8848\n",
      "\n",
      "Train RMSE: 0.941\n",
      "Train MSE: 0.885\n",
      "Train MAE: 0.612\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_66 (LSTM)               (None, 432, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_66 (TimeDis (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.9013 - val_loss: 1.2782\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 14s 197ms/step - loss: 0.9013 - val_loss: 1.2782\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9012 - val_loss: 1.2781\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9012 - val_loss: 1.2780\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9012 - val_loss: 1.2780\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9012 - val_loss: 1.2779\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9011 - val_loss: 1.2778\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9011 - val_loss: 1.2777\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9011 - val_loss: 1.2776\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9010 - val_loss: 1.2776\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9010 - val_loss: 1.2775\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9009 - val_loss: 1.2774\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9009 - val_loss: 1.2773\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9009 - val_loss: 1.2772\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9008 - val_loss: 1.2771\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.9008 - val_loss: 1.2771\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9008 - val_loss: 1.2770\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9007 - val_loss: 1.2769\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9007 - val_loss: 1.2768\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9006 - val_loss: 1.2767\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9006 - val_loss: 1.2766\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9006 - val_loss: 1.2765\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9005 - val_loss: 1.2764\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9005 - val_loss: 1.2763\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9005 - val_loss: 1.2763\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9004 - val_loss: 1.2762\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9004 - val_loss: 1.2761\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9003 - val_loss: 1.2760\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.9003 - val_loss: 1.2759\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.9002 - val_loss: 1.2758\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9002 - val_loss: 1.2757\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9002 - val_loss: 1.2756\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.9001 - val_loss: 1.2755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.9001 - val_loss: 1.2754\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.9000 - val_loss: 1.2753\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.9000 - val_loss: 1.2752\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8999 - val_loss: 1.2751\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8999 - val_loss: 1.2750\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8999 - val_loss: 1.2749\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8998 - val_loss: 1.2748\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8998 - val_loss: 1.2747\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8997 - val_loss: 1.2746\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8997 - val_loss: 1.2745\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8996 - val_loss: 1.2744\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8996 - val_loss: 1.2743\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8995 - val_loss: 1.2742\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8995 - val_loss: 1.2741\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8994 - val_loss: 1.2740\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8994 - val_loss: 1.2739\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8994 - val_loss: 1.2738\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8993 - val_loss: 1.2737\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.8993 - val_loss: 1.2736\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8992 - val_loss: 1.2735\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8992 - val_loss: 1.2734\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8991 - val_loss: 1.2733\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8991 - val_loss: 1.2732\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8990 - val_loss: 1.2731\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.8990 - val_loss: 1.2730\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.8989 - val_loss: 1.2729\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.8989 - val_loss: 1.2728\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8988 - val_loss: 1.2727\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8988 - val_loss: 1.2726\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8988 - val_loss: 1.2725\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8987 - val_loss: 1.2724\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.8987 - val_loss: 1.2723\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8986 - val_loss: 1.2722\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8986 - val_loss: 1.2720\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.8985 - val_loss: 1.2719\n",
      "Execution time:  1012.0301132202148\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9118\n",
      "Root Mean Square Error: 1.1124\n",
      "Mean Square Error: 1.2373\n",
      "\n",
      "Train RMSE: 1.112\n",
      "Train MSE: 1.237\n",
      "Train MAE: 0.912\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_67 (LSTM)               (None, 432, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_67 (TimeDis (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 42s 131ms/step - loss: 0.9022 - val_loss: 1.0921\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 40s 127ms/step - loss: 0.9021 - val_loss: 1.0918\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.9019 - val_loss: 1.0915\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.9018 - val_loss: 1.0912\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 42s 131ms/step - loss: 0.9016 - val_loss: 1.0908\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.9014 - val_loss: 1.0905\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.9013 - val_loss: 1.0901\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.9011 - val_loss: 1.0897\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 43s 134ms/step - loss: 0.9009 - val_loss: 1.0892\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.9007 - val_loss: 1.0888\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.9005 - val_loss: 1.0884\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.9003 - val_loss: 1.0879\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.9001 - val_loss: 1.0874\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8998 - val_loss: 1.0870\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8996 - val_loss: 1.0865\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8994 - val_loss: 1.0860\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.8992 - val_loss: 1.0855\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8989 - val_loss: 1.0850\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8987 - val_loss: 1.0845\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8984 - val_loss: 1.0839\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8982 - val_loss: 1.0834\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8979 - val_loss: 1.0829\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8977 - val_loss: 1.0823\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8974 - val_loss: 1.0817\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.8971 - val_loss: 1.0812\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8969 - val_loss: 1.0806\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 42s 132ms/step - loss: 0.8966 - val_loss: 1.0800\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8963 - val_loss: 1.0794\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 43s 134ms/step - loss: 0.8960 - val_loss: 1.0788\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8957 - val_loss: 1.0782\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.8954 - val_loss: 1.0776\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 42s 134ms/step - loss: 0.8951 - val_loss: 1.0770\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8948 - val_loss: 1.0763\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.8945 - val_loss: 1.0757\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 42s 131ms/step - loss: 0.8942 - val_loss: 1.0750\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 42s 131ms/step - loss: 0.8939 - val_loss: 1.0743\n",
      "Execution time:  1524.1113595962524\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9095\n",
      "Root Mean Square Error: 1.1113\n",
      "Mean Square Error: 1.2350\n",
      "\n",
      "Train RMSE: 1.111\n",
      "Train MSE: 1.235\n",
      "Train MAE: 0.909\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_68 (LSTM)               (None, 432, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_68 (TimeDis (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 14s 196ms/step - loss: 0.6511 - val_loss: 0.5898\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6148 - val_loss: 0.3710\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6157 - val_loss: 0.3619\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 178ms/step - loss: 0.6125 - val_loss: 0.3605\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6104 - val_loss: 0.3624\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6088 - val_loss: 0.3623\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6079 - val_loss: 0.3635\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6072 - val_loss: 0.3644\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6066 - val_loss: 0.3657\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6061 - val_loss: 0.3664\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6056 - val_loss: 0.3670\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6051 - val_loss: 0.3678\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6048 - val_loss: 0.3682\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6044 - val_loss: 0.3687\n",
      "Execution time:  189.3432776927948\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5275\n",
      "Root Mean Square Error: 0.9867\n",
      "Mean Square Error: 0.9735\n",
      "\n",
      "Train RMSE: 0.987\n",
      "Train MSE: 0.974\n",
      "Train MAE: 0.528\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_69 (LSTM)               (None, 432, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 42s 133ms/step - loss: 0.6639 - val_loss: 0.3176\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.6491 - val_loss: 0.2959\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.6439 - val_loss: 0.2798\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.6456 - val_loss: 0.2750\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 47s 148ms/step - loss: 0.6475 - val_loss: 0.2747\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6485 - val_loss: 0.2769\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.6479 - val_loss: 0.2753\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6476 - val_loss: 0.2815\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6470 - val_loss: 0.2801\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 49s 154ms/step - loss: 0.6467 - val_loss: 0.2845\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6458 - val_loss: 0.2784\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.6457 - val_loss: 0.2860\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 49s 154ms/step - loss: 0.6449 - val_loss: 0.2836\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.6446 - val_loss: 0.2829\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 49s 154ms/step - loss: 0.6441 - val_loss: 0.2812\n",
      "Execution time:  707.2453815937042\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5238\n",
      "Root Mean Square Error: 1.0161\n",
      "Mean Square Error: 1.0325\n",
      "\n",
      "Train RMSE: 1.016\n",
      "Train MSE: 1.032\n",
      "Train MAE: 0.524\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_70 (LSTM)               (None, 432, 40)           6720      \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_70 (TimeDis (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 14s 191ms/step - loss: 0.8934 - val_loss: 1.2299\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.8167 - val_loss: 0.9677\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.7171 - val_loss: 0.8530\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6911 - val_loss: 0.8188\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6831 - val_loss: 0.8061\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6796 - val_loss: 0.7998\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6776 - val_loss: 0.7961\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6764 - val_loss: 0.7936\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6755 - val_loss: 0.7918\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6748 - val_loss: 0.7904\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6742 - val_loss: 0.7893\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6738 - val_loss: 0.7884\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 13s 178ms/step - loss: 0.6734 - val_loss: 0.7876\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6731 - val_loss: 0.7870\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6729 - val_loss: 0.7865\n",
      "Epoch 16/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 13s 178ms/step - loss: 0.6726 - val_loss: 0.7860\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6724 - val_loss: 0.7856\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6722 - val_loss: 0.7852\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6721 - val_loss: 0.7848\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 13s 178ms/step - loss: 0.6719 - val_loss: 0.7845\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6718 - val_loss: 0.7843\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6717 - val_loss: 0.7840\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6716 - val_loss: 0.7838\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 14s 196ms/step - loss: 0.6715 - val_loss: 0.7836\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 14s 197ms/step - loss: 0.6713 - val_loss: 0.7834\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6713 - val_loss: 0.7832\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6712 - val_loss: 0.7830\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6711 - val_loss: 0.7828\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6710 - val_loss: 0.7827\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6710 - val_loss: 0.7825\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6709 - val_loss: 0.7824\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6708 - val_loss: 0.7823\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6708 - val_loss: 0.7822\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6707 - val_loss: 0.7820\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6707 - val_loss: 0.7819\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6706 - val_loss: 0.7818\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6706 - val_loss: 0.7817\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6705 - val_loss: 0.7816\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6705 - val_loss: 0.7815\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6704 - val_loss: 0.7815\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6704 - val_loss: 0.7814\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6704 - val_loss: 0.7813\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6703 - val_loss: 0.7812\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6703 - val_loss: 0.7811\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6702 - val_loss: 0.7811\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6702 - val_loss: 0.7810\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7809\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6702 - val_loss: 0.7809\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7808\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6701 - val_loss: 0.7808\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6700 - val_loss: 0.7806\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6700 - val_loss: 0.7806\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7804\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7804\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7802\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7802\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Execution time:  963.6103754043579\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6863\n",
      "Root Mean Square Error: 0.9969\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_71 (LSTM)               (None, 432, 55)           12540     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_71 (TimeDis (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 45s 141ms/step - loss: 0.8145 - val_loss: 0.6359\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 43s 137ms/step - loss: 0.7157 - val_loss: 0.6212\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 43s 135ms/step - loss: 0.7051 - val_loss: 0.6175\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 43s 136ms/step - loss: 0.7035 - val_loss: 0.6155\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.7026 - val_loss: 0.6142\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.7019 - val_loss: 0.6133\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.7015 - val_loss: 0.6126\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.7011 - val_loss: 0.6121\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.7008 - val_loss: 0.6116\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.7005 - val_loss: 0.6112\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.7003 - val_loss: 0.6108\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.7001 - val_loss: 0.6104\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 49s 153ms/step - loss: 0.6999 - val_loss: 0.6101\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6997 - val_loss: 0.6097\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6996 - val_loss: 0.6094\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6995 - val_loss: 0.6091\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6993 - val_loss: 0.6088\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6992 - val_loss: 0.6086\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6992 - val_loss: 0.6084\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6991 - val_loss: 0.6082\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6990 - val_loss: 0.6081\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6990 - val_loss: 0.6080\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6990 - val_loss: 0.6079\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6078\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 48s 153ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Execution time:  1727.5521295070648\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6862\n",
      "Root Mean Square Error: 0.9968\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_72 (LSTM)               (None, 1008, 40)          6720      \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_72 (TimeDis (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6410 - val_loss: 0.4830\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6079 - val_loss: 0.6219\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5764 - val_loss: 0.4767\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5729 - val_loss: 0.4666\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5678 - val_loss: 0.4491\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5650 - val_loss: 0.4424\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5633 - val_loss: 0.4348\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5619 - val_loss: 0.4298\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5608 - val_loss: 0.4249\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5600 - val_loss: 0.4211\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5592 - val_loss: 0.4175\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5585 - val_loss: 0.4145\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.5580 - val_loss: 0.4115\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.5575 - val_loss: 0.4092\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5570 - val_loss: 0.4067\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5566 - val_loss: 0.4050\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.5562 - val_loss: 0.4026\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.5559 - val_loss: 0.4020\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.5556 - val_loss: 0.3998\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.5553 - val_loss: 0.3987\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.5550 - val_loss: 0.3971\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.5548 - val_loss: 0.3958\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5545 - val_loss: 0.3946\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.5544 - val_loss: 0.3933\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5542 - val_loss: 0.3932\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.5539 - val_loss: 0.3921\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.5536 - val_loss: 0.3914\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5535 - val_loss: 0.3903\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5533 - val_loss: 0.3896\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.5531 - val_loss: 0.3886\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5530 - val_loss: 0.3882\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.5528 - val_loss: 0.3871\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.5527 - val_loss: 0.3866\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.5526 - val_loss: 0.3856\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5524 - val_loss: 0.3854\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.5523 - val_loss: 0.3844\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5522 - val_loss: 0.3845\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.5521 - val_loss: 0.3836\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.5519 - val_loss: 0.3835\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5518 - val_loss: 0.3827\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5517 - val_loss: 0.3827\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5516 - val_loss: 0.3819\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.5515 - val_loss: 0.3817\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.5514 - val_loss: 0.3811\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5512 - val_loss: 0.3808\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.5511 - val_loss: 0.3802\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5509 - val_loss: 0.3799\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5510 - val_loss: 0.3792\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5509 - val_loss: 0.3790\n",
      "Epoch 50/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 24s 407ms/step - loss: 0.5508 - val_loss: 0.3785\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5507 - val_loss: 0.3784\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5507 - val_loss: 0.3778\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5506 - val_loss: 0.3779\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5505 - val_loss: 0.3771\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5504 - val_loss: 0.3771\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5504 - val_loss: 0.3765\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.5503 - val_loss: 0.3765\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 26s 431ms/step - loss: 0.5502 - val_loss: 0.3757\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 25s 421ms/step - loss: 0.5501 - val_loss: 0.3758\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5501 - val_loss: 0.3750\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5500 - val_loss: 0.3755\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5500 - val_loss: 0.3749\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.5498 - val_loss: 0.3751\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5498 - val_loss: 0.3744\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5496 - val_loss: 0.3743\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5497 - val_loss: 0.3737\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5496 - val_loss: 0.3736\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5496 - val_loss: 0.3731\n",
      "Execution time:  1682.1987023353577\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6438\n",
      "Root Mean Square Error: 1.1469\n",
      "Mean Square Error: 1.3153\n",
      "\n",
      "Train RMSE: 1.147\n",
      "Train MSE: 1.315\n",
      "Train MAE: 0.644\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_73 (LSTM)               (None, 1008, 55)          12540     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_73 (TimeDis (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 88s 329ms/step - loss: 0.6362 - val_loss: 0.4384\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 89s 335ms/step - loss: 0.6149 - val_loss: 0.3621\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 91s 340ms/step - loss: 0.6031 - val_loss: 0.2902\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 89s 335ms/step - loss: 0.5936 - val_loss: 0.2951\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.5927 - val_loss: 0.2913\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5905 - val_loss: 0.2906\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 87s 329ms/step - loss: 0.5895 - val_loss: 0.2906\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 89s 333ms/step - loss: 0.5890 - val_loss: 0.2908\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 89s 335ms/step - loss: 0.5888 - val_loss: 0.2883\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.5891 - val_loss: 0.2878\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 93s 348ms/step - loss: 0.5891 - val_loss: 0.2883\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 92s 346ms/step - loss: 0.5886 - val_loss: 0.2865\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 94s 352ms/step - loss: 0.5886 - val_loss: 0.2870\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.5893 - val_loss: 0.2984\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 94s 353ms/step - loss: 0.5892 - val_loss: 0.2970\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 92s 346ms/step - loss: 0.5890 - val_loss: 0.2989\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 92s 346ms/step - loss: 0.5886 - val_loss: 0.2970\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 92s 345ms/step - loss: 0.5887 - val_loss: 0.2995\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 93s 348ms/step - loss: 0.5885 - val_loss: 0.2973\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 93s 350ms/step - loss: 0.5884 - val_loss: 0.3020\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.5884 - val_loss: 0.2998\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 91s 342ms/step - loss: 0.5883 - val_loss: 0.3021\n",
      "Execution time:  2014.745056629181\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6489\n",
      "Root Mean Square Error: 1.1585\n",
      "Mean Square Error: 1.3421\n",
      "\n",
      "Train RMSE: 1.158\n",
      "Train MSE: 1.342\n",
      "Train MAE: 0.649\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_74 (LSTM)               (None, 1008, 40)          6720      \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_74 (TimeDis (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9254 - val_loss: 0.9972\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.7074 - val_loss: 0.8439\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6824 - val_loss: 0.8327\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6787 - val_loss: 0.8288\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6771 - val_loss: 0.8269\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6761 - val_loss: 0.8256\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6755 - val_loss: 0.8248\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6751 - val_loss: 0.8242\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6747 - val_loss: 0.8238\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6745 - val_loss: 0.8235\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6743 - val_loss: 0.8232\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6741 - val_loss: 0.8229\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6740 - val_loss: 0.8227\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6739 - val_loss: 0.8226\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6737 - val_loss: 0.8224\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6737 - val_loss: 0.8223\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6736 - val_loss: 0.8222\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6735 - val_loss: 0.8221\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6735 - val_loss: 0.8220\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6734 - val_loss: 0.8219\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6734 - val_loss: 0.8219\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6733 - val_loss: 0.8218\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6733 - val_loss: 0.8217\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 0.6732 - val_loss: 0.8217\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6732 - val_loss: 0.8216\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6732 - val_loss: 0.8216\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 25s 415ms/step - loss: 0.6731 - val_loss: 0.8215\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6731 - val_loss: 0.8215\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 0.6731 - val_loss: 0.8214\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6731 - val_loss: 0.8214\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6730 - val_loss: 0.8214\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6730 - val_loss: 0.8214\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6730 - val_loss: 0.8213\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6730 - val_loss: 0.8213\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 0.6730 - val_loss: 0.8213\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8212\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6729 - val_loss: 0.8212\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8212\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6729 - val_loss: 0.8212\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 0.6729 - val_loss: 0.8211\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8211\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6729 - val_loss: 0.8211\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6729 - val_loss: 0.8211\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6728 - val_loss: 0.8211\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 25s 415ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6727 - val_loss: 0.8209\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6727 - val_loss: 0.8209\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 25s 415ms/step - loss: 0.6727 - val_loss: 0.8209\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6727 - val_loss: 0.8209\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6727 - val_loss: 0.8209\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6727 - val_loss: 0.8209\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 25s 415ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Execution time:  1707.1715667247772\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6894\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0466\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.047\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_75 (LSTM)               (None, 1008, 55)          12540     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_75 (TimeDis (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 91s 344ms/step - loss: 0.7783 - val_loss: 0.6613\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 93s 350ms/step - loss: 0.6974 - val_loss: 0.6586\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.6965 - val_loss: 0.6577\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.6962 - val_loss: 0.6573\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6960 - val_loss: 0.6570\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6959 - val_loss: 0.6568\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6958 - val_loss: 0.6566\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 98s 368ms/step - loss: 0.6957 - val_loss: 0.6565\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 98s 367ms/step - loss: 0.6957 - val_loss: 0.6564\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6956 - val_loss: 0.6563\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6956 - val_loss: 0.6562\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 98s 368ms/step - loss: 0.6955 - val_loss: 0.6562\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 99s 372ms/step - loss: 0.6955 - val_loss: 0.6561\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6955 - val_loss: 0.6561\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6955 - val_loss: 0.6560\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6954 - val_loss: 0.6560\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 98s 369ms/step - loss: 0.6954 - val_loss: 0.6560\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 19/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - 97s 364ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 100s 376ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 99s 374ms/step - loss: 0.6954 - val_loss: 0.6558\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 98s 367ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 98s 367ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 98s 369ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 98s 367ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 98s 370ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 98s 370ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 32/36\n",
      "266/266 [==============================] - 96s 359ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 98s 369ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Execution time:  3504.303931236267\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6893\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0465\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.046\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_76 (LSTM)               (None, 1008, 40)          6720      \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_76 (TimeDis (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6708 - val_loss: 0.8231\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.6707 - val_loss: 0.8229\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6706 - val_loss: 0.8227\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.6705 - val_loss: 0.8225\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6705 - val_loss: 0.8223\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6703 - val_loss: 0.8221\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6703 - val_loss: 0.8219\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6701 - val_loss: 0.8217\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6700 - val_loss: 0.8215\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6699 - val_loss: 0.8213\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6698 - val_loss: 0.8211\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.6697 - val_loss: 0.8209\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.6696 - val_loss: 0.8207\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.6695 - val_loss: 0.8205\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6694 - val_loss: 0.8202\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.6692 - val_loss: 0.8200\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6691 - val_loss: 0.8198\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6690 - val_loss: 0.8196\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6689 - val_loss: 0.8193\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6687 - val_loss: 0.8191\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6686 - val_loss: 0.8188\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6685 - val_loss: 0.8186\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6684 - val_loss: 0.8184\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6682 - val_loss: 0.8181\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6681 - val_loss: 0.8179\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6680 - val_loss: 0.8176\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6678 - val_loss: 0.8174\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6677 - val_loss: 0.8171\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6676 - val_loss: 0.8169\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6674 - val_loss: 0.8166\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6673 - val_loss: 0.8164\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6672 - val_loss: 0.8161\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6670 - val_loss: 0.8159\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6669 - val_loss: 0.8156\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6668 - val_loss: 0.8154\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6666 - val_loss: 0.8151\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6665 - val_loss: 0.8148\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6664 - val_loss: 0.8146\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6662 - val_loss: 0.8143\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6661 - val_loss: 0.8141\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6660 - val_loss: 0.8138\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6658 - val_loss: 0.8135\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6657 - val_loss: 0.8133\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6655 - val_loss: 0.8130\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6654 - val_loss: 0.8127\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6653 - val_loss: 0.8125\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6651 - val_loss: 0.8122\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6650 - val_loss: 0.8119\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6648 - val_loss: 0.8117\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6647 - val_loss: 0.8114\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6646 - val_loss: 0.8111\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6644 - val_loss: 0.8109\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6643 - val_loss: 0.8106\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6641 - val_loss: 0.8103\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6640 - val_loss: 0.8100\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6639 - val_loss: 0.8098\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6637 - val_loss: 0.8095\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6636 - val_loss: 0.8092\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6634 - val_loss: 0.8090\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6633 - val_loss: 0.8087\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6631 - val_loss: 0.8084\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6630 - val_loss: 0.8081\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6629 - val_loss: 0.8078\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6627 - val_loss: 0.8076\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6626 - val_loss: 0.8073\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6624 - val_loss: 0.8070\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6623 - val_loss: 0.8067\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6622 - val_loss: 0.8065\n",
      "Execution time:  1653.1658599376678\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6908\n",
      "Root Mean Square Error: 1.0334\n",
      "Mean Square Error: 1.0678\n",
      "\n",
      "Train RMSE: 1.033\n",
      "Train MSE: 1.068\n",
      "Train MAE: 0.691\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_77 (LSTM)               (None, 1008, 55)          12540     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_77 (TimeDis (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.6932 - val_loss: 0.6754\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.6927 - val_loss: 0.6748\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 94s 354ms/step - loss: 0.6922 - val_loss: 0.6742\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 96s 361ms/step - loss: 0.6916 - val_loss: 0.6736\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6910 - val_loss: 0.6728\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 98s 370ms/step - loss: 0.6904 - val_loss: 0.6720\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 100s 375ms/step - loss: 0.6897 - val_loss: 0.6712\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.6891 - val_loss: 0.6702\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6884 - val_loss: 0.6693\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.6877 - val_loss: 0.6683\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 98s 368ms/step - loss: 0.6869 - val_loss: 0.6673\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.6862 - val_loss: 0.6662\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 96s 361ms/step - loss: 0.6854 - val_loss: 0.6652\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6846 - val_loss: 0.6640\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6839 - val_loss: 0.6629\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.6831 - val_loss: 0.6617\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 98s 370ms/step - loss: 0.6822 - val_loss: 0.6605\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 98s 367ms/step - loss: 0.6814 - val_loss: 0.6593\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 96s 361ms/step - loss: 0.6805 - val_loss: 0.6581\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.6797 - val_loss: 0.6568\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6788 - val_loss: 0.6555\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.6779 - val_loss: 0.6542\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6770 - val_loss: 0.6528\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6760 - val_loss: 0.6514\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6751 - val_loss: 0.6500\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.6741 - val_loss: 0.6486\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.6732 - val_loss: 0.6471\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 98s 369ms/step - loss: 0.6722 - val_loss: 0.6456\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.6712 - val_loss: 0.6441\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 96s 359ms/step - loss: 0.6702 - val_loss: 0.6426\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6691 - val_loss: 0.6410\n",
      "Epoch 32/36\n",
      "266/266 [==============================] - 98s 368ms/step - loss: 0.6681 - val_loss: 0.6394\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.6670 - val_loss: 0.6377\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.6659 - val_loss: 0.6360\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6648 - val_loss: 0.6343\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.6637 - val_loss: 0.6325\n",
      "Execution time:  3482.5707383155823\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.7240\n",
      "Root Mean Square Error: 1.0838\n",
      "Mean Square Error: 1.1747\n",
      "\n",
      "Train RMSE: 1.084\n",
      "Train MSE: 1.175\n",
      "Train MAE: 0.724\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_78 (LSTM)               (None, 1008, 40)          6720      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_78 (TimeDis (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.9761 - val_loss: 1.3214\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 23s 384ms/step - loss: 0.9760 - val_loss: 1.3213\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9760 - val_loss: 1.3213\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9760 - val_loss: 1.3212\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.9759 - val_loss: 1.3212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.9759 - val_loss: 1.3211\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 23s 388ms/step - loss: 0.9759 - val_loss: 1.3211\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.9758 - val_loss: 1.3210\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9758 - val_loss: 1.3209\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 23s 385ms/step - loss: 0.9757 - val_loss: 1.3209\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.9757 - val_loss: 1.3208\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.9757 - val_loss: 1.3208\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.9756 - val_loss: 1.3207\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 23s 386ms/step - loss: 0.9756 - val_loss: 1.3207\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 23s 386ms/step - loss: 0.9755 - val_loss: 1.3206\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.9755 - val_loss: 1.3205\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.9754 - val_loss: 1.3205\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 23s 386ms/step - loss: 0.9754 - val_loss: 1.3204\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.9753 - val_loss: 1.3203\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.9753 - val_loss: 1.3203\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9753 - val_loss: 1.3202\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.9752 - val_loss: 1.3201\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9752 - val_loss: 1.3201\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.9751 - val_loss: 1.3200\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9751 - val_loss: 1.3199\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.9750 - val_loss: 1.3199\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.9750 - val_loss: 1.3198\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.9749 - val_loss: 1.3197\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.9749 - val_loss: 1.3196\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.9748 - val_loss: 1.3196\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9748 - val_loss: 1.3195\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.9747 - val_loss: 1.3194\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9747 - val_loss: 1.3194\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.9746 - val_loss: 1.3193\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9746 - val_loss: 1.3192\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9745 - val_loss: 1.3191\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9745 - val_loss: 1.3191\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.9744 - val_loss: 1.3190\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9743 - val_loss: 1.3189\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9743 - val_loss: 1.3188\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9742 - val_loss: 1.3188\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.9742 - val_loss: 1.3187\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.9741 - val_loss: 1.3186\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.9741 - val_loss: 1.3185\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.9740 - val_loss: 1.3185\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.9740 - val_loss: 1.3184\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9739 - val_loss: 1.3183\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9738 - val_loss: 1.3182\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.9738 - val_loss: 1.3181\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.9737 - val_loss: 1.3181\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9737 - val_loss: 1.3180\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.9736 - val_loss: 1.3179\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.9736 - val_loss: 1.3178\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9735 - val_loss: 1.3178\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9735 - val_loss: 1.3177\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9734 - val_loss: 1.3176\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.9733 - val_loss: 1.3175\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.9733 - val_loss: 1.3174\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9732 - val_loss: 1.3174\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9732 - val_loss: 1.3173\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.9731 - val_loss: 1.3172\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.9731 - val_loss: 1.3171\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.9730 - val_loss: 1.3170\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.9729 - val_loss: 1.3169\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.9729 - val_loss: 1.3169\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.9728 - val_loss: 1.3168\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.9728 - val_loss: 1.3167\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.9727 - val_loss: 1.3166\n",
      "Execution time:  1637.7548382282257\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9131\n",
      "Root Mean Square Error: 1.1321\n",
      "Mean Square Error: 1.2817\n",
      "\n",
      "Train RMSE: 1.132\n",
      "Train MSE: 1.282\n",
      "Train MAE: 0.913\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_79 (LSTM)               (None, 1008, 55)          12540     \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_79 (TimeDis (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.9769 - val_loss: 1.1420\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.9767 - val_loss: 1.1417\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 94s 353ms/step - loss: 0.9764 - val_loss: 1.1414\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 94s 352ms/step - loss: 0.9762 - val_loss: 1.1411\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.9759 - val_loss: 1.1408\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.9757 - val_loss: 1.1404\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 95s 358ms/step - loss: 0.9754 - val_loss: 1.1400\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.9751 - val_loss: 1.1396\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.9748 - val_loss: 1.1392\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.9744 - val_loss: 1.1388\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.9741 - val_loss: 1.1383\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 96s 363ms/step - loss: 0.9737 - val_loss: 1.1378\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 98s 368ms/step - loss: 0.9734 - val_loss: 1.1373\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 95s 357ms/step - loss: 0.9730 - val_loss: 1.1368\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.9726 - val_loss: 1.1363\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.9722 - val_loss: 1.1357\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.9718 - val_loss: 1.1351\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 98s 367ms/step - loss: 0.9714 - val_loss: 1.1345\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.9710 - val_loss: 1.1339\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.9706 - val_loss: 1.1333\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.9701 - val_loss: 1.1326\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 98s 369ms/step - loss: 0.9696 - val_loss: 1.1319\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 96s 361ms/step - loss: 0.9692 - val_loss: 1.1312\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.9687 - val_loss: 1.1305\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 95s 359ms/step - loss: 0.9682 - val_loss: 1.1297\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 96s 359ms/step - loss: 0.9677 - val_loss: 1.1289\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 98s 367ms/step - loss: 0.9671 - val_loss: 1.1281\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 96s 363ms/step - loss: 0.9666 - val_loss: 1.1273\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.9660 - val_loss: 1.1264\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 96s 363ms/step - loss: 0.9654 - val_loss: 1.1255\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.9648 - val_loss: 1.1246\n",
      "Epoch 32/36\n",
      "266/266 [==============================] - 96s 362ms/step - loss: 0.9642 - val_loss: 1.1236\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 95s 357ms/step - loss: 0.9636 - val_loss: 1.1226\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.9629 - val_loss: 1.1215\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.9622 - val_loss: 1.1204\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.9615 - val_loss: 1.1193\n",
      "Execution time:  3482.1177275180817\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8975\n",
      "Root Mean Square Error: 1.1198\n",
      "Mean Square Error: 1.2539\n",
      "\n",
      "Train RMSE: 1.120\n",
      "Train MSE: 1.254\n",
      "Train MAE: 0.897\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_80 (LSTM)               (None, 1008, 40)          6720      \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_80 (TimeDis (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6533 - val_loss: 0.7524\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.5911 - val_loss: 0.5156\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.5720 - val_loss: 0.4768\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.5663 - val_loss: 0.4574\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.5645 - val_loss: 0.4485\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.5629 - val_loss: 0.4418\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5618 - val_loss: 0.4370\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5609 - val_loss: 0.4330\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5601 - val_loss: 0.4296\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5594 - val_loss: 0.4268\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.5589 - val_loss: 0.4242\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5582 - val_loss: 0.4216\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5578 - val_loss: 0.4191\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5574 - val_loss: 0.4174\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5569 - val_loss: 0.4155\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5565 - val_loss: 0.4138\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5562 - val_loss: 0.4121\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5558 - val_loss: 0.4106\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5556 - val_loss: 0.4092\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5552 - val_loss: 0.4078\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.5549 - val_loss: 0.4065\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5546 - val_loss: 0.4052\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5543 - val_loss: 0.4039\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5541 - val_loss: 0.4028\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5539 - val_loss: 0.4019\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.5536 - val_loss: 0.4009\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5534 - val_loss: 0.3998\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5531 - val_loss: 0.3990\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.5529 - val_loss: 0.3981\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.5527 - val_loss: 0.3973\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.5525 - val_loss: 0.3965\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.5522 - val_loss: 0.3958\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5520 - val_loss: 0.3952\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.5517 - val_loss: 0.3949\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5514 - val_loss: 0.3950\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.5510 - val_loss: 0.3958\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.5507 - val_loss: 0.3972\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.5502 - val_loss: 0.3995\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5499 - val_loss: 0.4035\n",
      "Epoch 40/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 24s 395ms/step - loss: 0.5494 - val_loss: 0.4056\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5481 - val_loss: 0.3929\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 23s 388ms/step - loss: 0.5479 - val_loss: 0.3913\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.5408 - val_loss: 0.4078\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5442 - val_loss: 0.4222\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5334 - val_loss: 0.4460\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.5312 - val_loss: 0.4634\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.5302 - val_loss: 0.4718\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.5299 - val_loss: 0.4763\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5297 - val_loss: 0.4807\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.5296 - val_loss: 0.4813\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5295 - val_loss: 0.4829\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.5294 - val_loss: 0.4832\n",
      "Execution time:  1262.1854693889618\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6645\n",
      "Root Mean Square Error: 1.1549\n",
      "Mean Square Error: 1.3337\n",
      "\n",
      "Train RMSE: 1.155\n",
      "Train MSE: 1.334\n",
      "Train MAE: 0.665\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_81 (LSTM)               (None, 1008, 55)          12540     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_81 (TimeDis (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 92s 345ms/step - loss: 0.6391 - val_loss: 0.4254\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 99s 373ms/step - loss: 0.6013 - val_loss: 0.3253\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 99s 371ms/step - loss: 0.5972 - val_loss: 0.3108\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 99s 373ms/step - loss: 0.5951 - val_loss: 0.3018\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 98s 368ms/step - loss: 0.5940 - val_loss: 0.2989\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.5934 - val_loss: 0.3008\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 91s 343ms/step - loss: 0.5929 - val_loss: 0.3040\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.5921 - val_loss: 0.3025\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.5910 - val_loss: 0.3030\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.5902 - val_loss: 0.3036\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 92s 347ms/step - loss: 0.5896 - val_loss: 0.3022\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 94s 352ms/step - loss: 0.5891 - val_loss: 0.3032\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 93s 348ms/step - loss: 0.5887 - val_loss: 0.3041\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.5884 - val_loss: 0.3026\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.5881 - val_loss: 0.3050\n",
      "Execution time:  1424.8833267688751\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6535\n",
      "Root Mean Square Error: 1.1601\n",
      "Mean Square Error: 1.3459\n",
      "\n",
      "Train RMSE: 1.160\n",
      "Train MSE: 1.346\n",
      "Train MAE: 0.653\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_82 (LSTM)               (None, 1008, 40)          6720      \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_82 (TimeDis (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 6,761\n",
      "Trainable params: 6,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.9654 - val_loss: 1.2824\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.8706 - val_loss: 1.0379\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.7462 - val_loss: 0.9000\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.7017 - val_loss: 0.8584\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6893 - val_loss: 0.8451\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.6844 - val_loss: 0.8387\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6817 - val_loss: 0.8350\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6801 - val_loss: 0.8325\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.6789 - val_loss: 0.8308\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6781 - val_loss: 0.8295\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6774 - val_loss: 0.8285\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6769 - val_loss: 0.8276\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6765 - val_loss: 0.8270\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6761 - val_loss: 0.8264\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6758 - val_loss: 0.8259\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6756 - val_loss: 0.8255\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6753 - val_loss: 0.8251\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6751 - val_loss: 0.8248\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6750 - val_loss: 0.8245\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6748 - val_loss: 0.8243\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6747 - val_loss: 0.8241\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6745 - val_loss: 0.8239\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6744 - val_loss: 0.8237\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6743 - val_loss: 0.8235\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6742 - val_loss: 0.8233\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6741 - val_loss: 0.8232\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6741 - val_loss: 0.8231\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6740 - val_loss: 0.8229\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6739 - val_loss: 0.8228\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6738 - val_loss: 0.8227\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6738 - val_loss: 0.8226\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6737 - val_loss: 0.8225\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6737 - val_loss: 0.8224\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6736 - val_loss: 0.8223\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6736 - val_loss: 0.8223\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6735 - val_loss: 0.8222\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6735 - val_loss: 0.8221\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6734 - val_loss: 0.8221\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6734 - val_loss: 0.8220\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6734 - val_loss: 0.8219\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6733 - val_loss: 0.8219\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6733 - val_loss: 0.8218\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6733 - val_loss: 0.8218\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6732 - val_loss: 0.8217\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6732 - val_loss: 0.8217\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6732 - val_loss: 0.8216\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6731 - val_loss: 0.8216\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6731 - val_loss: 0.8215\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6731 - val_loss: 0.8215\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6731 - val_loss: 0.8214\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6730 - val_loss: 0.8214\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6730 - val_loss: 0.8214\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6730 - val_loss: 0.8214\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6730 - val_loss: 0.8213\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6730 - val_loss: 0.8213\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6729 - val_loss: 0.8213\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6729 - val_loss: 0.8212\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6729 - val_loss: 0.8212\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6729 - val_loss: 0.8212\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6729 - val_loss: 0.8211\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6729 - val_loss: 0.8211\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6728 - val_loss: 0.8211\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6728 - val_loss: 0.8211\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6728 - val_loss: 0.8211\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8210\n",
      "Execution time:  1665.0534462928772\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6895\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0466\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.047\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_83 (LSTM)               (None, 1008, 55)          12540     \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_83 (TimeDis (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 12,596\n",
      "Trainable params: 12,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 96s 359ms/step - loss: 0.8270 - val_loss: 0.6715\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.7005 - val_loss: 0.6626\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 94s 353ms/step - loss: 0.6982 - val_loss: 0.6603\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 92s 344ms/step - loss: 0.6973 - val_loss: 0.6592\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 93s 348ms/step - loss: 0.6969 - val_loss: 0.6585\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 93s 348ms/step - loss: 0.6966 - val_loss: 0.6580\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 92s 346ms/step - loss: 0.6963 - val_loss: 0.6576\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 95s 355ms/step - loss: 0.6962 - val_loss: 0.6573\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.6960 - val_loss: 0.6571\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.6959 - val_loss: 0.6569\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.6958 - val_loss: 0.6567\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.6957 - val_loss: 0.6566\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 95s 358ms/step - loss: 0.6957 - val_loss: 0.6564\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 94s 354ms/step - loss: 0.6956 - val_loss: 0.6563\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 96s 359ms/step - loss: 0.6956 - val_loss: 0.6562\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6955 - val_loss: 0.6561\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 95s 359ms/step - loss: 0.6955 - val_loss: 0.6561\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6954 - val_loss: 0.6560\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 94s 355ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6954 - val_loss: 0.6558\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 94s 354ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 97s 363ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 94s 355ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 95s 357ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 97s 364ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 93s 351ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 97s 366ms/step - loss: 0.6953 - val_loss: 0.6556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/36\n",
      "266/266 [==============================] - 95s 357ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 95s 356ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 96s 363ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Execution time:  3432.49423289299\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6893\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0465\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.046\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 6, 40)             5160      \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_84 (TimeDis (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5337 - val_loss: 0.2678\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3511 - val_loss: 0.2222\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3131 - val_loss: 0.1869\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2993 - val_loss: 0.1687\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2911 - val_loss: 0.1550\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2846 - val_loss: 0.1484\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2815 - val_loss: 0.1408\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2780 - val_loss: 0.1386\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2773 - val_loss: 0.1396\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2760 - val_loss: 0.1379\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.1380\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1397\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2749 - val_loss: 0.1388\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2742 - val_loss: 0.1371\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.1374\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.1371\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2729 - val_loss: 0.1391\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2724 - val_loss: 0.1357\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.1375\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2723 - val_loss: 0.1374\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.1371\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2711 - val_loss: 0.1351\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2718 - val_loss: 0.1343\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2709 - val_loss: 0.1356\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2706 - val_loss: 0.1348\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2701 - val_loss: 0.1348\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2704 - val_loss: 0.1342\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2703 - val_loss: 0.1321\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2694 - val_loss: 0.1341\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2701 - val_loss: 0.1318\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2698 - val_loss: 0.1322\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2693 - val_loss: 0.1325\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2693 - val_loss: 0.1326\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2691 - val_loss: 0.1316\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2689 - val_loss: 0.1313\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2690 - val_loss: 0.1280\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2688 - val_loss: 0.1297\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2682 - val_loss: 0.1285\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2681 - val_loss: 0.1257\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2681 - val_loss: 0.1265\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2675 - val_loss: 0.1259\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2675 - val_loss: 0.1287\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2672 - val_loss: 0.1268\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2668 - val_loss: 0.1276\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2671 - val_loss: 0.1245\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2665 - val_loss: 0.1238\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2664 - val_loss: 0.1226\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2668 - val_loss: 0.1249\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2662 - val_loss: 0.1213\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2662 - val_loss: 0.1220\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2654 - val_loss: 0.1216\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2655 - val_loss: 0.1191\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2651 - val_loss: 0.1192\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2653 - val_loss: 0.1165\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2647 - val_loss: 0.1172\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2646 - val_loss: 0.1155\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2641 - val_loss: 0.1190\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2645 - val_loss: 0.1187\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2643 - val_loss: 0.1175\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2641 - val_loss: 0.1144\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2636 - val_loss: 0.1151\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2638 - val_loss: 0.1162\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2638 - val_loss: 0.1161\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2643 - val_loss: 0.1110\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2632 - val_loss: 0.1122\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2630 - val_loss: 0.1127\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2633 - val_loss: 0.1137\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2629 - val_loss: 0.1113\n",
      "Execution time:  30.32288146018982\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1603\n",
      "Root Mean Square Error: 0.5823\n",
      "Mean Square Error: 0.3391\n",
      "\n",
      "Train RMSE: 0.582\n",
      "Train MSE: 0.339\n",
      "Train MAE: 0.160\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 6, 55)             9570      \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_85 (TimeDis (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 6ms/step - loss: 0.3530 - val_loss: 0.2207\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2762 - val_loss: 0.2055\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2702 - val_loss: 0.2060\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2690 - val_loss: 0.2056\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.2684 - val_loss: 0.2063\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2677 - val_loss: 0.2053\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2671 - val_loss: 0.2049\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2669 - val_loss: 0.2040\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2660 - val_loss: 0.2033\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2659 - val_loss: 0.2035\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2653 - val_loss: 0.2020\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2005\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2645 - val_loss: 0.1994\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2639 - val_loss: 0.1982\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2633 - val_loss: 0.1975\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.2628 - val_loss: 0.1964\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2625 - val_loss: 0.1946\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2623 - val_loss: 0.1931\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2617 - val_loss: 0.1935\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2611 - val_loss: 0.1907\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2612 - val_loss: 0.1890\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2606 - val_loss: 0.1890\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2600 - val_loss: 0.1872\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2600 - val_loss: 0.1869\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2596 - val_loss: 0.1865\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.2594 - val_loss: 0.1847\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2589 - val_loss: 0.1837\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2589 - val_loss: 0.1825\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2583 - val_loss: 0.1832\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2585 - val_loss: 0.1818\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2581 - val_loss: 0.1815\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2581 - val_loss: 0.1810\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2577 - val_loss: 0.1798\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2580 - val_loss: 0.1812\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2578 - val_loss: 0.1800\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.2579 - val_loss: 0.1794\n",
      "Execution time:  58.817262411117554\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1541\n",
      "Root Mean Square Error: 0.5817\n",
      "Mean Square Error: 0.3383\n",
      "\n",
      "Train RMSE: 0.582\n",
      "Train MSE: 0.338\n",
      "Train MAE: 0.154\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (None, 6, 40)             5160      \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_86 (TimeDis (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8019 - val_loss: 0.9501\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5860 - val_loss: 0.8503\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5540 - val_loss: 0.8299\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5399 - val_loss: 0.8183\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5285 - val_loss: 0.8105\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5181 - val_loss: 0.8049\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5125 - val_loss: 0.8013\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5076 - val_loss: 0.7987\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5043 - val_loss: 0.7970\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5020 - val_loss: 0.7958\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5002 - val_loss: 0.7950\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4991 - val_loss: 0.7944\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4981 - val_loss: 0.7941\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4970 - val_loss: 0.7938\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4966 - val_loss: 0.7936\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4958 - val_loss: 0.7934\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4957 - val_loss: 0.7933\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7932\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4949 - val_loss: 0.7931\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 0.7930\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4947 - val_loss: 0.7930\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4941 - val_loss: 0.7929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4944 - val_loss: 0.7929\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4942 - val_loss: 0.7929\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7928\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7928\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7928\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7928\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7928\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4930 - val_loss: 0.7927\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927\n",
      "Execution time:  29.47386884689331\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4832\n",
      "Root Mean Square Error: 0.7518\n",
      "Mean Square Error: 0.5651\n",
      "\n",
      "Train RMSE: 0.752\n",
      "Train MSE: 0.565\n",
      "Train MAE: 0.483\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_3 (GRU)                  (None, 6, 55)             9570      \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_87 (TimeDis (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 6ms/step - loss: 0.6176 - val_loss: 0.6844\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5212 - val_loss: 0.6629\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5022 - val_loss: 0.6550\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4961 - val_loss: 0.6523\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4938 - val_loss: 0.6514\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4925 - val_loss: 0.6506\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4922 - val_loss: 0.6503\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4920 - val_loss: 0.6501\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4920 - val_loss: 0.6498\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4918 - val_loss: 0.6498\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6496\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6498\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6495\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6496\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6494\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6498\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6497\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6495\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4915 - val_loss: 0.6496\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4911 - val_loss: 0.6496\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6495\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4908 - val_loss: 0.6498\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4907 - val_loss: 0.6496\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4908 - val_loss: 0.6494\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4908 - val_loss: 0.6496\n",
      "Epoch 26/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4907 - val_loss: 0.6494\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4905 - val_loss: 0.6499\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4903 - val_loss: 0.6496\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4904 - val_loss: 0.6494\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4905 - val_loss: 0.6499\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4904 - val_loss: 0.6496\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4901 - val_loss: 0.6495\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4899 - val_loss: 0.6497\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4901 - val_loss: 0.6495\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4900 - val_loss: 0.6495\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4902 - val_loss: 0.6495\n",
      "Execution time:  60.4005651473999\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4832\n",
      "Root Mean Square Error: 0.7517\n",
      "Mean Square Error: 0.5650\n",
      "\n",
      "Train RMSE: 0.752\n",
      "Train MSE: 0.565\n",
      "Train MAE: 0.483\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_4 (GRU)                  (None, 6, 40)             5160      \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_88 (TimeDis (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.7307 - val_loss: 0.8310\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7300 - val_loss: 0.8305\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7302 - val_loss: 0.8300\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7292 - val_loss: 0.8295\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7292 - val_loss: 0.8290\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7287 - val_loss: 0.8285\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7284 - val_loss: 0.8279\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7281 - val_loss: 0.8274\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7277 - val_loss: 0.8269\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7275 - val_loss: 0.8264\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7270 - val_loss: 0.8258\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7267 - val_loss: 0.8253\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7264 - val_loss: 0.8247\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7259 - val_loss: 0.8242\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7257 - val_loss: 0.8237\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7253 - val_loss: 0.8231\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7249 - val_loss: 0.8226\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7244 - val_loss: 0.8220\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7242 - val_loss: 0.8215\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7238 - val_loss: 0.8209\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7238 - val_loss: 0.8204\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7231 - val_loss: 0.8198\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7229 - val_loss: 0.8193\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7225 - val_loss: 0.8187\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7222 - val_loss: 0.8182\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7216 - val_loss: 0.8176\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7216 - val_loss: 0.8170\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7211 - val_loss: 0.8165\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7208 - val_loss: 0.8159\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7204 - val_loss: 0.8154\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7202 - val_loss: 0.8148\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7197 - val_loss: 0.8143\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7194 - val_loss: 0.8137\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7189 - val_loss: 0.8131\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7189 - val_loss: 0.8126\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7182 - val_loss: 0.8120\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7179 - val_loss: 0.8115\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7173 - val_loss: 0.8109\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7173 - val_loss: 0.8103\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7169 - val_loss: 0.8098\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7165 - val_loss: 0.8092\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7161 - val_loss: 0.8086\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7158 - val_loss: 0.8081\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7154 - val_loss: 0.8075\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7153 - val_loss: 0.8070\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7148 - val_loss: 0.8064\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7143 - val_loss: 0.8058\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7140 - val_loss: 0.8053\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7135 - val_loss: 0.8047\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7134 - val_loss: 0.8041\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7129 - val_loss: 0.8036\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7129 - val_loss: 0.8030\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7126 - val_loss: 0.8025\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7119 - val_loss: 0.8019\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7116 - val_loss: 0.8013\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7115 - val_loss: 0.8008\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7109 - val_loss: 0.8002\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7105 - val_loss: 0.7996\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7102 - val_loss: 0.7991\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7097 - val_loss: 0.7985\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7095 - val_loss: 0.7979\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7090 - val_loss: 0.7974\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7088 - val_loss: 0.7968\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7086 - val_loss: 0.7962\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7079 - val_loss: 0.7957\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7078 - val_loss: 0.7951\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7074 - val_loss: 0.7945\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7069 - val_loss: 0.7940\n",
      "Execution time:  28.98744297027588\n",
      "GRU:\n",
      "Mean Absolute Error: 0.7161\n",
      "Root Mean Square Error: 1.0171\n",
      "Mean Square Error: 1.0346\n",
      "\n",
      "Train RMSE: 1.017\n",
      "Train MSE: 1.035\n",
      "Train MAE: 0.716\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_5 (GRU)                  (None, 6, 55)             9570      \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_89 (TimeDis (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 6ms/step - loss: 0.7437 - val_loss: 0.7267\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.7412 - val_loss: 0.7236\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.7387 - val_loss: 0.7205\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7363 - val_loss: 0.7173\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7339 - val_loss: 0.7141\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.7313 - val_loss: 0.7108\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7288 - val_loss: 0.7075\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7261 - val_loss: 0.7042\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7238 - val_loss: 0.7008\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7214 - val_loss: 0.6974\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7189 - val_loss: 0.6939\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7165 - val_loss: 0.6904\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.7138 - val_loss: 0.6869\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.7112 - val_loss: 0.6834\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.7087 - val_loss: 0.6799\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.7062 - val_loss: 0.6763\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.7036 - val_loss: 0.6727\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.7010 - val_loss: 0.6691\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6983 - val_loss: 0.6655\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6959 - val_loss: 0.6618\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6932 - val_loss: 0.6581\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6906 - val_loss: 0.6544\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.6881 - val_loss: 0.6507\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6854 - val_loss: 0.6470\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6829 - val_loss: 0.6433\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6800 - val_loss: 0.6396\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6773 - val_loss: 0.6359\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6748 - val_loss: 0.6323\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6720 - val_loss: 0.6286\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6696 - val_loss: 0.6250\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6668 - val_loss: 0.6214\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6644 - val_loss: 0.6178\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6617 - val_loss: 0.6143\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.6590 - val_loss: 0.6107\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.6564 - val_loss: 0.6072\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.6538 - val_loss: 0.6036\n",
      "Execution time:  58.58254289627075\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6401\n",
      "Root Mean Square Error: 0.9427\n",
      "Mean Square Error: 0.8886\n",
      "\n",
      "Train RMSE: 0.943\n",
      "Train MSE: 0.889\n",
      "Train MAE: 0.640\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, 6, 40)             5160      \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_90 (TimeDis (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8835 - val_loss: 1.2923\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8835 - val_loss: 1.2921\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8834 - val_loss: 1.2920\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8834 - val_loss: 1.2918\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8831 - val_loss: 1.2917\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8831 - val_loss: 1.2915\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8830 - val_loss: 1.2914\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8830 - val_loss: 1.2912\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8828 - val_loss: 1.2910\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8828 - val_loss: 1.2909\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8826 - val_loss: 1.2907\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8825 - val_loss: 1.2905\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8825 - val_loss: 1.2904\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8823 - val_loss: 1.2902\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8822 - val_loss: 1.2900\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8821 - val_loss: 1.2899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.8819 - val_loss: 1.2897\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8819 - val_loss: 1.2895\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8818 - val_loss: 1.2894\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8817 - val_loss: 1.2892\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8816 - val_loss: 1.2890\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8815 - val_loss: 1.2888\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8814 - val_loss: 1.2887\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8812 - val_loss: 1.2885\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8811 - val_loss: 1.2883\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8811 - val_loss: 1.2881\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8809 - val_loss: 1.2880\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8810 - val_loss: 1.2878\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8807 - val_loss: 1.2876\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8806 - val_loss: 1.2874\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8806 - val_loss: 1.2872\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8804 - val_loss: 1.2871\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8803 - val_loss: 1.2869\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8802 - val_loss: 1.2867\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8801 - val_loss: 1.2865\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8800 - val_loss: 1.2863\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8799 - val_loss: 1.2862\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8798 - val_loss: 1.2860\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8796 - val_loss: 1.2858\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8796 - val_loss: 1.2856\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8794 - val_loss: 1.2854\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8794 - val_loss: 1.2852\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8792 - val_loss: 1.2851\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8791 - val_loss: 1.2849\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8789 - val_loss: 1.2847\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8789 - val_loss: 1.2845\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8787 - val_loss: 1.2843\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8787 - val_loss: 1.2841\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8786 - val_loss: 1.2839\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8784 - val_loss: 1.2838\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8782 - val_loss: 1.2836\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8783 - val_loss: 1.2834\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8782 - val_loss: 1.2832\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8779 - val_loss: 1.2830\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8779 - val_loss: 1.2828\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8778 - val_loss: 1.2826\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8776 - val_loss: 1.2824\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8775 - val_loss: 1.2823\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8774 - val_loss: 1.2821\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8772 - val_loss: 1.2819\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8771 - val_loss: 1.2817\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8770 - val_loss: 1.2815\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8769 - val_loss: 1.2813\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8768 - val_loss: 1.2811\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8767 - val_loss: 1.2809\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8766 - val_loss: 1.2807\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8765 - val_loss: 1.2805\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8763 - val_loss: 1.2803\n",
      "Execution time:  29.0505051612854\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9153\n",
      "Root Mean Square Error: 1.1092\n",
      "Mean Square Error: 1.2303\n",
      "\n",
      "Train RMSE: 1.109\n",
      "Train MSE: 1.230\n",
      "Train MAE: 0.915\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_7 (GRU)                  (None, 6, 55)             9570      \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_91 (TimeDis (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 6ms/step - loss: 0.8706 - val_loss: 1.1245\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8700 - val_loss: 1.1237\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8694 - val_loss: 1.1229\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8688 - val_loss: 1.1220\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8683 - val_loss: 1.1212\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8677 - val_loss: 1.1203\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.8671 - val_loss: 1.1194\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8664 - val_loss: 1.1184\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8658 - val_loss: 1.1175\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8652 - val_loss: 1.1165\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8645 - val_loss: 1.1156\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8639 - val_loss: 1.1146\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8632 - val_loss: 1.1136\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8626 - val_loss: 1.1126\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8619 - val_loss: 1.1115\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8612 - val_loss: 1.1105\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.8605 - val_loss: 1.1095\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8598 - val_loss: 1.1084\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8592 - val_loss: 1.1073\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8584 - val_loss: 1.1062\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8577 - val_loss: 1.1051\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8570 - val_loss: 1.1040\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8562 - val_loss: 1.1029\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8556 - val_loss: 1.1018\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8549 - val_loss: 1.1007\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8541 - val_loss: 1.0995\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8533 - val_loss: 1.0984\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.8526 - val_loss: 1.0972\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8518 - val_loss: 1.0960\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8511 - val_loss: 1.0948\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8503 - val_loss: 1.0936\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8495 - val_loss: 1.0924\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8488 - val_loss: 1.0912\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8479 - val_loss: 1.0900\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.8471 - val_loss: 1.0887\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.8463 - val_loss: 1.0875\n",
      "Execution time:  59.125470876693726\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8920\n",
      "Root Mean Square Error: 1.0852\n",
      "Mean Square Error: 1.1776\n",
      "\n",
      "Train RMSE: 1.085\n",
      "Train MSE: 1.178\n",
      "Train MAE: 0.892\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_8 (GRU)                  (None, 6, 40)             5160      \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_92 (TimeDis (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5682 - val_loss: 0.3907\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3996 - val_loss: 0.2718\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3555 - val_loss: 0.2483\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3362 - val_loss: 0.2260\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3208 - val_loss: 0.2041\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3099 - val_loss: 0.1891\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3034 - val_loss: 0.1781\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2978 - val_loss: 0.1705\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2946 - val_loss: 0.1641\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2912 - val_loss: 0.1586\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2883 - val_loss: 0.1542\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2856 - val_loss: 0.1506\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2837 - val_loss: 0.1480\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2816 - val_loss: 0.1459\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2803 - val_loss: 0.1439\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2788 - val_loss: 0.1427\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2785 - val_loss: 0.1436\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2772 - val_loss: 0.1408\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2771 - val_loss: 0.1405\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.1409\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2762 - val_loss: 0.1401\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.1387\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.1390\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2747 - val_loss: 0.1389\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2742 - val_loss: 0.1391\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2740 - val_loss: 0.1404\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2741 - val_loss: 0.1415\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2741 - val_loss: 0.1386\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2731 - val_loss: 0.1398\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2740 - val_loss: 0.1377\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2734 - val_loss: 0.1392\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2731 - val_loss: 0.1390\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2731 - val_loss: 0.1386\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2724 - val_loss: 0.1382\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2727 - val_loss: 0.1395\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2724 - val_loss: 0.1380\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2727 - val_loss: 0.1385\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2720 - val_loss: 0.1384\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.1381\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2723 - val_loss: 0.1383\n",
      "Execution time:  18.75528335571289\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1621\n",
      "Root Mean Square Error: 0.5837\n",
      "Mean Square Error: 0.3407\n",
      "\n",
      "Train RMSE: 0.584\n",
      "Train MSE: 0.341\n",
      "Train MAE: 0.162\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_9 (GRU)                  (None, 6, 55)             9570      \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_93 (TimeDis (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 6ms/step - loss: 0.4491 - val_loss: 0.2632\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.3034 - val_loss: 0.2277\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2823 - val_loss: 0.2129\n",
      "Epoch 4/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2730 - val_loss: 0.2062\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2696 - val_loss: 0.2057\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 1s 4ms/step - loss: 0.2682 - val_loss: 0.2050\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2676 - val_loss: 0.2053\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.2672 - val_loss: 0.2046\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2668 - val_loss: 0.2049\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2667 - val_loss: 0.2056\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2666 - val_loss: 0.2052\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2662 - val_loss: 0.2046\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2660 - val_loss: 0.2036\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2657 - val_loss: 0.2044\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2652 - val_loss: 0.2040\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2655 - val_loss: 0.2039\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2651 - val_loss: 0.2029\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.2647 - val_loss: 0.2026\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2647 - val_loss: 0.2025\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2644 - val_loss: 0.2015\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2643 - val_loss: 0.2018\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2639 - val_loss: 0.2008\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2637 - val_loss: 0.2002\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2636 - val_loss: 0.2003\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2634 - val_loss: 0.2004\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2632 - val_loss: 0.1989\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2628 - val_loss: 0.1985\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2627 - val_loss: 0.1973\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.2625 - val_loss: 0.1970\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2623 - val_loss: 0.1968\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2621 - val_loss: 0.1965\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2621 - val_loss: 0.1961\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2620 - val_loss: 0.1951\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2619 - val_loss: 0.1945\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2615 - val_loss: 0.1940\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.2614 - val_loss: 0.1937\n",
      "Execution time:  58.51086139678955\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1542\n",
      "Root Mean Square Error: 0.5819\n",
      "Mean Square Error: 0.3386\n",
      "\n",
      "Train RMSE: 0.582\n",
      "Train MSE: 0.339\n",
      "Train MAE: 0.154\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, 6, 40)             5160      \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 6, 40)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_94 (TimeDis (None, 6, 1)              41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 25ms/step - loss: 0.8606 - val_loss: 1.2053\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7485 - val_loss: 0.9521\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6109 - val_loss: 0.8779\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5771 - val_loss: 0.8552\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5623 - val_loss: 0.8424\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5535 - val_loss: 0.8336\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5467 - val_loss: 0.8269\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5414 - val_loss: 0.8214\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5370 - val_loss: 0.8171\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5330 - val_loss: 0.8133\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5291 - val_loss: 0.8101\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5259 - val_loss: 0.8074\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5226 - val_loss: 0.8050\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5197 - val_loss: 0.8031\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5169 - val_loss: 0.8015\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5143 - val_loss: 0.8002\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5116 - val_loss: 0.7990\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5087 - val_loss: 0.7980\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5061 - val_loss: 0.7971\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5039 - val_loss: 0.7963\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5024 - val_loss: 0.7956\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5007 - val_loss: 0.7951\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5000 - val_loss: 0.7946\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4990 - val_loss: 0.7943\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4979 - val_loss: 0.7940\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4976 - val_loss: 0.7938\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4969 - val_loss: 0.7936\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4966 - val_loss: 0.7935\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4960 - val_loss: 0.7934\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4957 - val_loss: 0.7933\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4956 - val_loss: 0.7932\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4954 - val_loss: 0.7931\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4949 - val_loss: 0.7931\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4950 - val_loss: 0.7930\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 0.7930\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4947 - val_loss: 0.7930\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4946 - val_loss: 0.7929\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4942 - val_loss: 0.7929\n",
      "Epoch 39/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4943 - val_loss: 0.7929\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4944 - val_loss: 0.7929\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4943 - val_loss: 0.7928\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4941 - val_loss: 0.7928\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.7928\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.7928\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7927\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.7927\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927\n",
      "Execution time:  29.92873191833496\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4842\n",
      "Root Mean Square Error: 0.7517\n",
      "Mean Square Error: 0.5650\n",
      "\n",
      "Train RMSE: 0.752\n",
      "Train MSE: 0.565\n",
      "Train MAE: 0.484\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_11 (GRU)                 (None, 6, 55)             9570      \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 6, 55)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_95 (TimeDis (None, 6, 1)              56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "355/355 [==============================] - 2s 6ms/step - loss: 0.6840 - val_loss: 0.7218\n",
      "Epoch 2/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5524 - val_loss: 0.6933\n",
      "Epoch 3/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5363 - val_loss: 0.6797\n",
      "Epoch 4/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5235 - val_loss: 0.6695\n",
      "Epoch 5/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5112 - val_loss: 0.6626\n",
      "Epoch 6/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5052 - val_loss: 0.6584\n",
      "Epoch 7/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.5013 - val_loss: 0.6561\n",
      "Epoch 8/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4984 - val_loss: 0.6545\n",
      "Epoch 9/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4962 - val_loss: 0.6533\n",
      "Epoch 10/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4945 - val_loss: 0.6526\n",
      "Epoch 11/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4936 - val_loss: 0.6520\n",
      "Epoch 12/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4931 - val_loss: 0.6516\n",
      "Epoch 13/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4927 - val_loss: 0.6511\n",
      "Epoch 14/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4924 - val_loss: 0.6511\n",
      "Epoch 15/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4922 - val_loss: 0.6509\n",
      "Epoch 16/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4921 - val_loss: 0.6507\n",
      "Epoch 17/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4919 - val_loss: 0.6506\n",
      "Epoch 18/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4918 - val_loss: 0.6505\n",
      "Epoch 19/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6504\n",
      "Epoch 20/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6503\n",
      "Epoch 21/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6501\n",
      "Epoch 22/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4915 - val_loss: 0.6502\n",
      "Epoch 23/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4914 - val_loss: 0.6501\n",
      "Epoch 24/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4914 - val_loss: 0.6501\n",
      "Epoch 25/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4914 - val_loss: 0.6501\n",
      "Epoch 26/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6500\n",
      "Epoch 27/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6500\n",
      "Epoch 28/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6499\n",
      "Epoch 29/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6500\n",
      "Epoch 30/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6500\n",
      "Epoch 31/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6500\n",
      "Epoch 32/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4913 - val_loss: 0.6499\n",
      "Epoch 33/36\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.4911 - val_loss: 0.6499\n",
      "Epoch 34/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6499\n",
      "Epoch 35/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6499\n",
      "Epoch 36/36\n",
      "355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6499\n",
      "Execution time:  59.047602891922\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4829\n",
      "Root Mean Square Error: 0.7516\n",
      "Mean Square Error: 0.5649\n",
      "\n",
      "Train RMSE: 0.752\n",
      "Train MSE: 0.565\n",
      "Train MAE: 0.483\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_12 (GRU)                 (None, 18, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_96 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5396 - val_loss: 0.2678\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3853 - val_loss: 0.1975\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3651 - val_loss: 0.1826\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3572 - val_loss: 0.1735\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3532 - val_loss: 0.1693\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3500 - val_loss: 0.1671\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3480 - val_loss: 0.1620\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3466 - val_loss: 0.1598\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3457 - val_loss: 0.1577\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3447 - val_loss: 0.1577\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3437 - val_loss: 0.1546\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3424 - val_loss: 0.1529\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3419 - val_loss: 0.1524\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3409 - val_loss: 0.1516\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3405 - val_loss: 0.1501\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3402 - val_loss: 0.1510\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3397 - val_loss: 0.1502\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3392 - val_loss: 0.1503\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3388 - val_loss: 0.1492\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3385 - val_loss: 0.1491\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3381 - val_loss: 0.1483\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3378 - val_loss: 0.1495\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3373 - val_loss: 0.1494\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3375 - val_loss: 0.1492\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3369 - val_loss: 0.1494\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3368 - val_loss: 0.1487\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3362 - val_loss: 0.1474\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3361 - val_loss: 0.1477\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3360 - val_loss: 0.1468\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3355 - val_loss: 0.1476\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3350 - val_loss: 0.1466\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3349 - val_loss: 0.1455\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3350 - val_loss: 0.1466\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3346 - val_loss: 0.1451\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3342 - val_loss: 0.1438\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3338 - val_loss: 0.1455\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3338 - val_loss: 0.1445\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3337 - val_loss: 0.1440\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3333 - val_loss: 0.1428\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3332 - val_loss: 0.1431\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3331 - val_loss: 0.1428\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3327 - val_loss: 0.1425\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3329 - val_loss: 0.1415\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3323 - val_loss: 0.1424\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3319 - val_loss: 0.1425\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3319 - val_loss: 0.1417\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3317 - val_loss: 0.1399\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3314 - val_loss: 0.1412\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3316 - val_loss: 0.1401\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3315 - val_loss: 0.1409\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3313 - val_loss: 0.1410\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3309 - val_loss: 0.1405\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3310 - val_loss: 0.1400\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3307 - val_loss: 0.1405\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3306 - val_loss: 0.1399\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3304 - val_loss: 0.1395\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3301 - val_loss: 0.1398\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3305 - val_loss: 0.1386\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3303 - val_loss: 0.1390\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3297 - val_loss: 0.1389\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3298 - val_loss: 0.1397\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3298 - val_loss: 0.1393\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3294 - val_loss: 0.1381\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3294 - val_loss: 0.1398\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3292 - val_loss: 0.1380\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3294 - val_loss: 0.1368\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3290 - val_loss: 0.1380\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3290 - val_loss: 0.1376\n",
      "Execution time:  53.84108328819275\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1646\n",
      "Root Mean Square Error: 0.5868\n",
      "Mean Square Error: 0.3443\n",
      "\n",
      "Train RMSE: 0.587\n",
      "Train MSE: 0.344\n",
      "Train MAE: 0.165\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, 18, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_97 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 4s 10ms/step - loss: 0.4036 - val_loss: 0.2626\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3464 - val_loss: 0.2529\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3403 - val_loss: 0.2502\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - ETA: 0s - loss: 0.337 - 3s 8ms/step - loss: 0.3373 - val_loss: 0.2502\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3353 - val_loss: 0.2499\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3346 - val_loss: 0.2505\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3339 - val_loss: 0.2513\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3331 - val_loss: 0.2505\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3327 - val_loss: 0.2513\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3319 - val_loss: 0.2509\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3317 - val_loss: 0.2504\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3312 - val_loss: 0.2506\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3309 - val_loss: 0.2508\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3305 - val_loss: 0.2507\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2508\n",
      "Execution time:  49.02683734893799\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1724\n",
      "Root Mean Square Error: 0.5859\n",
      "Mean Square Error: 0.3433\n",
      "\n",
      "Train RMSE: 0.586\n",
      "Train MSE: 0.343\n",
      "Train MAE: 0.172\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_14 (GRU)                 (None, 18, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_98 (TimeDis (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7738 - val_loss: 0.8696\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5983 - val_loss: 0.8288\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5775 - val_loss: 0.8174\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5643 - val_loss: 0.8096\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5567 - val_loss: 0.8055\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5513 - val_loss: 0.8024\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5475 - val_loss: 0.8002\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5447 - val_loss: 0.7986\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5426 - val_loss: 0.7973\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5413 - val_loss: 0.7964\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5400 - val_loss: 0.7956\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5392 - val_loss: 0.7951\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5384 - val_loss: 0.7946\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5376 - val_loss: 0.7942\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5371 - val_loss: 0.7939\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5365 - val_loss: 0.7937\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5361 - val_loss: 0.7935\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5357 - val_loss: 0.7933\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7932\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5350 - val_loss: 0.7930\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7929\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5345 - val_loss: 0.7929\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5342 - val_loss: 0.7928\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5342 - val_loss: 0.7927\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5338 - val_loss: 0.7927\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5337 - val_loss: 0.7926\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7926\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5333 - val_loss: 0.7926\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5331 - val_loss: 0.7925\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5329 - val_loss: 0.7925\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5330 - val_loss: 0.7925\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5328 - val_loss: 0.7925\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5326 - val_loss: 0.7924\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5325 - val_loss: 0.7924\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5325 - val_loss: 0.7924\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5324 - val_loss: 0.7924\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5324 - val_loss: 0.7924\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5323 - val_loss: 0.7924\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5321 - val_loss: 0.7924\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5322 - val_loss: 0.7924\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5321 - val_loss: 0.7923\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5318 - val_loss: 0.7923\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5318 - val_loss: 0.7923\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7923\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7923\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5315 - val_loss: 0.7923\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7923\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7923\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5314 - val_loss: 0.7923\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5314 - val_loss: 0.7923\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5315 - val_loss: 0.7923\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5312 - val_loss: 0.7923\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5311 - val_loss: 0.7923\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5310 - val_loss: 0.7923\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5311 - val_loss: 0.7923\n",
      "Epoch 60/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5310 - val_loss: 0.7923\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5309 - val_loss: 0.7923\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5308 - val_loss: 0.7923\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5308 - val_loss: 0.7923\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5310 - val_loss: 0.7923\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5307 - val_loss: 0.7923\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5307 - val_loss: 0.7923\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5306 - val_loss: 0.7923\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5306 - val_loss: 0.7923\n",
      "Execution time:  55.7631356716156\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4952\n",
      "Root Mean Square Error: 0.7642\n",
      "Mean Square Error: 0.5841\n",
      "\n",
      "Train RMSE: 0.764\n",
      "Train MSE: 0.584\n",
      "Train MAE: 0.495\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_15 (GRU)                 (None, 18, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_99 (TimeDis (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 4s 10ms/step - loss: 0.6307 - val_loss: 0.6957\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5556 - val_loss: 0.6808\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5446 - val_loss: 0.6747\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5390 - val_loss: 0.6715\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5366 - val_loss: 0.6698\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5349 - val_loss: 0.6686\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5341 - val_loss: 0.6680\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5335 - val_loss: 0.6674\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5330 - val_loss: 0.6672\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5328 - val_loss: 0.6669\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5325 - val_loss: 0.6666\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5323 - val_loss: 0.6665\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5321 - val_loss: 0.6663\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5320 - val_loss: 0.6662\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5317 - val_loss: 0.6659\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5318 - val_loss: 0.6661\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5317 - val_loss: 0.6658\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5315 - val_loss: 0.6658\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5315 - val_loss: 0.6659\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6659\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6658\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6659\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5311 - val_loss: 0.6658\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5311 - val_loss: 0.6658\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5309 - val_loss: 0.6657\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5307 - val_loss: 0.6657\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5304 - val_loss: 0.6658\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5304 - val_loss: 0.6657\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5304 - val_loss: 0.6655\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5301 - val_loss: 0.6656\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5302 - val_loss: 0.6655\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5299 - val_loss: 0.6654\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5300 - val_loss: 0.6654\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5299 - val_loss: 0.6655\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5296 - val_loss: 0.6653\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5296 - val_loss: 0.6653\n",
      "Execution time:  114.3955659866333\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4969\n",
      "Root Mean Square Error: 0.7676\n",
      "Mean Square Error: 0.5891\n",
      "\n",
      "Train RMSE: 0.768\n",
      "Train MSE: 0.589\n",
      "Train MAE: 0.497\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_16 (GRU)                 (None, 18, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_100 (TimeDi (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.7073 - val_loss: 0.8008\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7068 - val_loss: 0.8002\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7064 - val_loss: 0.7995\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.7061 - val_loss: 0.7988\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.7055 - val_loss: 0.7981\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7051 - val_loss: 0.7974\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7045 - val_loss: 0.7966\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7042 - val_loss: 0.7959\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7039 - val_loss: 0.7952\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7032 - val_loss: 0.7944\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7029 - val_loss: 0.7937\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7023 - val_loss: 0.7929\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7019 - val_loss: 0.7922\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7015 - val_loss: 0.7914\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7009 - val_loss: 0.7907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.7006 - val_loss: 0.7899\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6999 - val_loss: 0.7891\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6995 - val_loss: 0.7884\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6989 - val_loss: 0.7876\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6985 - val_loss: 0.7868\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6981 - val_loss: 0.7860\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6978 - val_loss: 0.7853\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6972 - val_loss: 0.7845\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6966 - val_loss: 0.7837\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6963 - val_loss: 0.7829\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6957 - val_loss: 0.7821\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6952 - val_loss: 0.7813\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6947 - val_loss: 0.7805\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6943 - val_loss: 0.7798\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6938 - val_loss: 0.7790\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6933 - val_loss: 0.7782\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6932 - val_loss: 0.7774\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6925 - val_loss: 0.7766\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6918 - val_loss: 0.7758\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6915 - val_loss: 0.7750\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6910 - val_loss: 0.7742\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6906 - val_loss: 0.7734\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6898 - val_loss: 0.7726\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6895 - val_loss: 0.7718\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6891 - val_loss: 0.7710\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6887 - val_loss: 0.7702\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6881 - val_loss: 0.7694\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6875 - val_loss: 0.7686\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6870 - val_loss: 0.7678\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6866 - val_loss: 0.7670\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6861 - val_loss: 0.7661\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6857 - val_loss: 0.7653\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6850 - val_loss: 0.7645\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6845 - val_loss: 0.7637\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6843 - val_loss: 0.7629\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6839 - val_loss: 0.7621\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6832 - val_loss: 0.7613\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6827 - val_loss: 0.7605\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6823 - val_loss: 0.7596\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6820 - val_loss: 0.7588\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6812 - val_loss: 0.7580\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6808 - val_loss: 0.7572\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6805 - val_loss: 0.7564\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6797 - val_loss: 0.7555\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6792 - val_loss: 0.7547\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6791 - val_loss: 0.7539\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6782 - val_loss: 0.7531\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6779 - val_loss: 0.7523\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6776 - val_loss: 0.7514\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6770 - val_loss: 0.7506\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6762 - val_loss: 0.7498\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6759 - val_loss: 0.7489\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6755 - val_loss: 0.7481\n",
      "Execution time:  54.31034302711487\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6786\n",
      "Root Mean Square Error: 0.9734\n",
      "Mean Square Error: 0.9476\n",
      "\n",
      "Train RMSE: 0.973\n",
      "Train MSE: 0.948\n",
      "Train MAE: 0.679\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_17 (GRU)                 (None, 18, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_101 (TimeDi (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 3s 10ms/step - loss: 0.7134 - val_loss: 0.6944\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.7111 - val_loss: 0.6913\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.7086 - val_loss: 0.6881\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.7063 - val_loss: 0.6849\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.7039 - val_loss: 0.6817\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.7015 - val_loss: 0.6784\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6992 - val_loss: 0.6751\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6969 - val_loss: 0.6717\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6944 - val_loss: 0.6682\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6920 - val_loss: 0.6648\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6895 - val_loss: 0.6613\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6871 - val_loss: 0.6577\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6848 - val_loss: 0.6541\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6821 - val_loss: 0.6505\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6796 - val_loss: 0.6468\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6771 - val_loss: 0.6431\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6746 - val_loss: 0.6394\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6721 - val_loss: 0.6357\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6697 - val_loss: 0.6319\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6670 - val_loss: 0.6282\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6645 - val_loss: 0.6244\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6618 - val_loss: 0.6207\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6593 - val_loss: 0.6169\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6566 - val_loss: 0.6131\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6542 - val_loss: 0.6094\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6514 - val_loss: 0.6056\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6489 - val_loss: 0.6018\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6463 - val_loss: 0.5981\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6436 - val_loss: 0.5943\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6409 - val_loss: 0.5904\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6383 - val_loss: 0.5866\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6355 - val_loss: 0.5827\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6328 - val_loss: 0.5788\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6299 - val_loss: 0.5748\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.6273 - val_loss: 0.5708\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.6244 - val_loss: 0.5667\n",
      "Execution time:  112.655020236969\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5996\n",
      "Root Mean Square Error: 0.9010\n",
      "Mean Square Error: 0.8118\n",
      "\n",
      "Train RMSE: 0.901\n",
      "Train MSE: 0.812\n",
      "Train MAE: 0.600\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_18 (GRU)                 (None, 18, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_102 (TimeDi (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8886 - val_loss: 1.2951\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8885 - val_loss: 1.2949\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8884 - val_loss: 1.2947\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8883 - val_loss: 1.2945\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8882 - val_loss: 1.2944\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8881 - val_loss: 1.2942\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8879 - val_loss: 1.2940\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8878 - val_loss: 1.2938\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8877 - val_loss: 1.2936\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8876 - val_loss: 1.2934\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8875 - val_loss: 1.2932\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8874 - val_loss: 1.2930\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8873 - val_loss: 1.2929\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8872 - val_loss: 1.2927\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8871 - val_loss: 1.2925\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8870 - val_loss: 1.2923\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8869 - val_loss: 1.2921\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8868 - val_loss: 1.2919\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8867 - val_loss: 1.2917\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8865 - val_loss: 1.2915\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8865 - val_loss: 1.2913\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8863 - val_loss: 1.2911\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8862 - val_loss: 1.2909\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8861 - val_loss: 1.2907\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8860 - val_loss: 1.2905\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8858 - val_loss: 1.2903\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8858 - val_loss: 1.2901\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8857 - val_loss: 1.2899\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8855 - val_loss: 1.2896\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8853 - val_loss: 1.2894\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8853 - val_loss: 1.2892\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8852 - val_loss: 1.2890\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8850 - val_loss: 1.2888\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8849 - val_loss: 1.2886\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8848 - val_loss: 1.2884\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8847 - val_loss: 1.2882\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8846 - val_loss: 1.2880\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8844 - val_loss: 1.2878\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8843 - val_loss: 1.2876\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8842 - val_loss: 1.2874\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8841 - val_loss: 1.2871\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8839 - val_loss: 1.2869\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8838 - val_loss: 1.2867\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8837 - val_loss: 1.2865\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8836 - val_loss: 1.2863\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8834 - val_loss: 1.2861\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8834 - val_loss: 1.2859\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8832 - val_loss: 1.2856\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8831 - val_loss: 1.2854\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8830 - val_loss: 1.2852\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8828 - val_loss: 1.2850\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8827 - val_loss: 1.2848\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8826 - val_loss: 1.2846\n",
      "Epoch 54/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8825 - val_loss: 1.2844\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8824 - val_loss: 1.2841\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8822 - val_loss: 1.2839\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8821 - val_loss: 1.2837\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8820 - val_loss: 1.2835\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8818 - val_loss: 1.2833\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8818 - val_loss: 1.2830\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8817 - val_loss: 1.2828\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8815 - val_loss: 1.2826\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8814 - val_loss: 1.2824\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8813 - val_loss: 1.2822\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8812 - val_loss: 1.2819\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8810 - val_loss: 1.2817\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8808 - val_loss: 1.2815\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8808 - val_loss: 1.2813\n",
      "Execution time:  55.4357807636261\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9175\n",
      "Root Mean Square Error: 1.1116\n",
      "Mean Square Error: 1.2357\n",
      "\n",
      "Train RMSE: 1.112\n",
      "Train MSE: 1.236\n",
      "Train MAE: 0.918\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_19 (GRU)                 (None, 18, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_103 (TimeDi (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 4s 10ms/step - loss: 0.8838 - val_loss: 1.1372\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8833 - val_loss: 1.1364\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8828 - val_loss: 1.1356\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8822 - val_loss: 1.1347\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8817 - val_loss: 1.1338\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8811 - val_loss: 1.1329\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8805 - val_loss: 1.1320\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8799 - val_loss: 1.1311\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8793 - val_loss: 1.1301\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8787 - val_loss: 1.1291\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8781 - val_loss: 1.1282\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8774 - val_loss: 1.1272\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8768 - val_loss: 1.1261\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8761 - val_loss: 1.1251\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8755 - val_loss: 1.1241\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8748 - val_loss: 1.1230\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8741 - val_loss: 1.1220\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8735 - val_loss: 1.1209\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8728 - val_loss: 1.1198\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8722 - val_loss: 1.1187\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8715 - val_loss: 1.1176\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8708 - val_loss: 1.1165\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8701 - val_loss: 1.1153\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8694 - val_loss: 1.1142\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8687 - val_loss: 1.1130\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8679 - val_loss: 1.1118\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8672 - val_loss: 1.1106\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8665 - val_loss: 1.1094\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8657 - val_loss: 1.1082\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8650 - val_loss: 1.1070\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8642 - val_loss: 1.1057\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8635 - val_loss: 1.1045\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8627 - val_loss: 1.1032\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8619 - val_loss: 1.1019\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8611 - val_loss: 1.1006\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.8603 - val_loss: 1.0993\n",
      "Execution time:  114.53077507019043\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9040\n",
      "Root Mean Square Error: 1.0992\n",
      "Mean Square Error: 1.2081\n",
      "\n",
      "Train RMSE: 1.099\n",
      "Train MSE: 1.208\n",
      "Train MAE: 0.904\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_20 (GRU)                 (None, 18, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_104 (TimeDi (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6381 - val_loss: 0.4800\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4388 - val_loss: 0.2479\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3912 - val_loss: 0.2251\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3778 - val_loss: 0.2079\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3696 - val_loss: 0.1983\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3639 - val_loss: 0.1900\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3600 - val_loss: 0.1835\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3571 - val_loss: 0.1798\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3546 - val_loss: 0.1762\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3533 - val_loss: 0.1737\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3512 - val_loss: 0.1700\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3495 - val_loss: 0.1678\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3485 - val_loss: 0.1664\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3473 - val_loss: 0.1645\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3466 - val_loss: 0.1626\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3458 - val_loss: 0.1618\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3450 - val_loss: 0.1610\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3443 - val_loss: 0.1598\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3439 - val_loss: 0.1593\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3435 - val_loss: 0.1584\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3430 - val_loss: 0.1575\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3425 - val_loss: 0.1568\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3421 - val_loss: 0.1574\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3422 - val_loss: 0.1568\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3413 - val_loss: 0.1552\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3411 - val_loss: 0.1551\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3407 - val_loss: 0.1554\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3403 - val_loss: 0.1546\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3400 - val_loss: 0.1541\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3398 - val_loss: 0.1539\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3391 - val_loss: 0.1535\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3392 - val_loss: 0.1531\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3389 - val_loss: 0.1542\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3391 - val_loss: 0.1532\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3384 - val_loss: 0.1520\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3379 - val_loss: 0.1525\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3382 - val_loss: 0.1522\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3382 - val_loss: 0.1521\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3375 - val_loss: 0.1529\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3377 - val_loss: 0.1525\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3375 - val_loss: 0.1523\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3374 - val_loss: 0.1524\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3372 - val_loss: 0.1522\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3371 - val_loss: 0.1523\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3367 - val_loss: 0.1526\n",
      "Execution time:  36.559377908706665\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1722\n",
      "Root Mean Square Error: 0.5872\n",
      "Mean Square Error: 0.3448\n",
      "\n",
      "Train RMSE: 0.587\n",
      "Train MSE: 0.345\n",
      "Train MAE: 0.172\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_21 (GRU)                 (None, 18, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_105 (TimeDi (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 4s 10ms/step - loss: 0.4113 - val_loss: 0.2753\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3545 - val_loss: 0.2615\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3448 - val_loss: 0.2550\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3401 - val_loss: 0.2522\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3371 - val_loss: 0.2502\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3352 - val_loss: 0.2493\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3341 - val_loss: 0.2489\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3332 - val_loss: 0.2488\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3326 - val_loss: 0.2491\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3318 - val_loss: 0.2489\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3316 - val_loss: 0.2490\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3312 - val_loss: 0.2498\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3309 - val_loss: 0.2497\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3306 - val_loss: 0.2495\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3306 - val_loss: 0.2493\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2493\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3297 - val_loss: 0.2492\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.3298 - val_loss: 0.2492\n",
      "Execution time:  58.06269812583923\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1653\n",
      "Root Mean Square Error: 0.5843\n",
      "Mean Square Error: 0.3414\n",
      "\n",
      "Train RMSE: 0.584\n",
      "Train MSE: 0.341\n",
      "Train MAE: 0.165\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_22 (GRU)                 (None, 18, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 18, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_106 (TimeDi (None, 18, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.8612 - val_loss: 1.1599\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6962 - val_loss: 0.8829\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6138 - val_loss: 0.8483\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6000 - val_loss: 0.8358\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5919 - val_loss: 0.8285\n",
      "Epoch 6/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5862 - val_loss: 0.8234\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5808 - val_loss: 0.8195\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5762 - val_loss: 0.8166\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5711 - val_loss: 0.8136\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5668 - val_loss: 0.8110\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5621 - val_loss: 0.8086\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5584 - val_loss: 0.8063\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5554 - val_loss: 0.8044\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5528 - val_loss: 0.8027\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5507 - val_loss: 0.8013\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5487 - val_loss: 0.8001\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5472 - val_loss: 0.7991\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5458 - val_loss: 0.7982\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5444 - val_loss: 0.7975\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5435 - val_loss: 0.7968\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5425 - val_loss: 0.7963\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5417 - val_loss: 0.7958\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5409 - val_loss: 0.7954\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5405 - val_loss: 0.7951\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5396 - val_loss: 0.7947\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5393 - val_loss: 0.7945\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5386 - val_loss: 0.7942\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5382 - val_loss: 0.7940\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5377 - val_loss: 0.7938\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5374 - val_loss: 0.7937\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5369 - val_loss: 0.7935\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5367 - val_loss: 0.7934\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5364 - val_loss: 0.7933\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5361 - val_loss: 0.7932\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5359 - val_loss: 0.7931\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5357 - val_loss: 0.7930\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5355 - val_loss: 0.7929\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7929\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5350 - val_loss: 0.7928\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5350 - val_loss: 0.7928\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5347 - val_loss: 0.7927\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7927\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5345 - val_loss: 0.7927\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5344 - val_loss: 0.7926\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5343 - val_loss: 0.7926\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5341 - val_loss: 0.7926\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5341 - val_loss: 0.7926\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5340 - val_loss: 0.7926\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5340 - val_loss: 0.7925\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5337 - val_loss: 0.7925\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5338 - val_loss: 0.7925\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5336 - val_loss: 0.7925\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7925\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7925\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5334 - val_loss: 0.7925\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5333 - val_loss: 0.7924\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5332 - val_loss: 0.7924\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5330 - val_loss: 0.7924\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5332 - val_loss: 0.7924\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5330 - val_loss: 0.7924\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5330 - val_loss: 0.7924\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5329 - val_loss: 0.7924\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5329 - val_loss: 0.7924\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5329 - val_loss: 0.7924\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5328 - val_loss: 0.7924\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5328 - val_loss: 0.7924\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5326 - val_loss: 0.7924\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5326 - val_loss: 0.7924\n",
      "Execution time:  54.99783754348755\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4916\n",
      "Root Mean Square Error: 0.7556\n",
      "Mean Square Error: 0.5710\n",
      "\n",
      "Train RMSE: 0.756\n",
      "Train MSE: 0.571\n",
      "Train MAE: 0.492\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_23 (GRU)                 (None, 18, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 18, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_107 (TimeDi (None, 18, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "354/354 [==============================] - 3s 10ms/step - loss: 0.6854 - val_loss: 0.7243\n",
      "Epoch 2/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5915 - val_loss: 0.7093\n",
      "Epoch 3/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5769 - val_loss: 0.6974\n",
      "Epoch 4/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5637 - val_loss: 0.6880\n",
      "Epoch 5/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5536 - val_loss: 0.6816\n",
      "Epoch 6/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5480 - val_loss: 0.6785\n",
      "Epoch 7/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5433 - val_loss: 0.6751\n",
      "Epoch 8/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5407 - val_loss: 0.6728\n",
      "Epoch 9/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5388 - val_loss: 0.6712\n",
      "Epoch 10/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5375 - val_loss: 0.6700\n",
      "Epoch 11/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5364 - val_loss: 0.6692\n",
      "Epoch 12/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5357 - val_loss: 0.6685\n",
      "Epoch 13/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5351 - val_loss: 0.6680\n",
      "Epoch 14/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5347 - val_loss: 0.6676\n",
      "Epoch 15/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5343 - val_loss: 0.6674\n",
      "Epoch 16/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5339 - val_loss: 0.6671\n",
      "Epoch 17/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5336 - val_loss: 0.6670\n",
      "Epoch 18/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5333 - val_loss: 0.6668\n",
      "Epoch 19/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5331 - val_loss: 0.6667\n",
      "Epoch 20/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5329 - val_loss: 0.6666\n",
      "Epoch 21/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5326 - val_loss: 0.6665\n",
      "Epoch 22/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5325 - val_loss: 0.6665\n",
      "Epoch 23/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5324 - val_loss: 0.6664\n",
      "Epoch 24/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5322 - val_loss: 0.6664\n",
      "Epoch 25/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5321 - val_loss: 0.6663\n",
      "Epoch 26/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5319 - val_loss: 0.6663\n",
      "Epoch 27/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5318 - val_loss: 0.6663\n",
      "Epoch 28/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5317 - val_loss: 0.6663\n",
      "Epoch 29/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5316 - val_loss: 0.6662\n",
      "Epoch 30/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5316 - val_loss: 0.6662\n",
      "Epoch 31/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5315 - val_loss: 0.6661\n",
      "Epoch 32/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6660\n",
      "Epoch 33/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5313 - val_loss: 0.6660\n",
      "Epoch 34/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5313 - val_loss: 0.6660\n",
      "Epoch 35/36\n",
      "354/354 [==============================] - 3s 8ms/step - loss: 0.5313 - val_loss: 0.6659\n",
      "Epoch 36/36\n",
      "354/354 [==============================] - 3s 9ms/step - loss: 0.5312 - val_loss: 0.6659\n",
      "Execution time:  113.31456589698792\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4932\n",
      "Root Mean Square Error: 0.7567\n",
      "Mean Square Error: 0.5726\n",
      "\n",
      "Train RMSE: 0.757\n",
      "Train MSE: 0.573\n",
      "Train MAE: 0.493\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_24 (GRU)                 (None, 36, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_108 (TimeDi (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.5819 - val_loss: 0.2680\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4452 - val_loss: 0.1949\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4257 - val_loss: 0.1879\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.4207 - val_loss: 0.1808\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4174 - val_loss: 0.1766\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4155 - val_loss: 0.1723\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4138 - val_loss: 0.1703\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4126 - val_loss: 0.1702\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4114 - val_loss: 0.1689\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4109 - val_loss: 0.1685\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4100 - val_loss: 0.1670\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4097 - val_loss: 0.1656\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4093 - val_loss: 0.1656\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4090 - val_loss: 0.1648\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4085 - val_loss: 0.1652\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4083 - val_loss: 0.1641\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4080 - val_loss: 0.1639\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4077 - val_loss: 0.1629\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4071 - val_loss: 0.1634\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4072 - val_loss: 0.1630\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4067 - val_loss: 0.1639\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4066 - val_loss: 0.1625\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4062 - val_loss: 0.1635\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4059 - val_loss: 0.1631\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4058 - val_loss: 0.1633\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4056 - val_loss: 0.1628\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4053 - val_loss: 0.1631\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4052 - val_loss: 0.1624\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4050 - val_loss: 0.1631\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4048 - val_loss: 0.1625\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4048 - val_loss: 0.1632\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4048 - val_loss: 0.1624\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4047 - val_loss: 0.1646\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4044 - val_loss: 0.1625\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4044 - val_loss: 0.1639\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4040 - val_loss: 0.1634\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4039 - val_loss: 0.1635\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4039 - val_loss: 0.1630\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4037 - val_loss: 0.1641\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4033 - val_loss: 0.1637\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4037 - val_loss: 0.1620\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4037 - val_loss: 0.1645\n",
      "Epoch 43/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4031 - val_loss: 0.1635\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4031 - val_loss: 0.1633\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4030 - val_loss: 0.1636\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4031 - val_loss: 0.1635\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4029 - val_loss: 0.1643\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4027 - val_loss: 0.1634\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4024 - val_loss: 0.1639\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4022 - val_loss: 0.1640\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4023 - val_loss: 0.1645\n",
      "Execution time:  69.33568620681763\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1818\n",
      "Root Mean Square Error: 0.5931\n",
      "Mean Square Error: 0.3518\n",
      "\n",
      "Train RMSE: 0.593\n",
      "Train MSE: 0.352\n",
      "Train MAE: 0.182\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_25 (GRU)                 (None, 36, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_109 (TimeDi (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 6s 16ms/step - loss: 0.4612 - val_loss: 0.3154\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4119 - val_loss: 0.3107\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4074 - val_loss: 0.3069\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4048 - val_loss: 0.3047\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.4028 - val_loss: 0.3035\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4015 - val_loss: 0.3039\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4009 - val_loss: 0.3042\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4002 - val_loss: 0.3042\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3998 - val_loss: 0.3044\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3987 - val_loss: 0.3044\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4005 - val_loss: 0.3063\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3978 - val_loss: 0.3053\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3986 - val_loss: 0.3079\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3963 - val_loss: 0.3060\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3995 - val_loss: 0.3063\n",
      "Execution time:  81.67205429077148\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1802\n",
      "Root Mean Square Error: 0.5921\n",
      "Mean Square Error: 0.3506\n",
      "\n",
      "Train RMSE: 0.592\n",
      "Train MSE: 0.351\n",
      "Train MAE: 0.180\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_26 (GRU)                 (None, 36, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_110 (TimeDi (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.8081 - val_loss: 0.8694\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6387 - val_loss: 0.8244\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6175 - val_loss: 0.8135\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6055 - val_loss: 0.8078\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5986 - val_loss: 0.8044\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5936 - val_loss: 0.8015\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5913 - val_loss: 0.7998\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5890 - val_loss: 0.7985\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5870 - val_loss: 0.7975\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5853 - val_loss: 0.7966\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5838 - val_loss: 0.7959\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5824 - val_loss: 0.7953\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5815 - val_loss: 0.7948\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5806 - val_loss: 0.7944\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5801 - val_loss: 0.7940\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5794 - val_loss: 0.7938\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5789 - val_loss: 0.7935\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5785 - val_loss: 0.7933\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5783 - val_loss: 0.7931\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5778 - val_loss: 0.7930\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5775 - val_loss: 0.7928\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5772 - val_loss: 0.7927\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5771 - val_loss: 0.7926\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5767 - val_loss: 0.7925\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5766 - val_loss: 0.7924\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5763 - val_loss: 0.7924\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5762 - val_loss: 0.7923\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5758 - val_loss: 0.7922\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5755 - val_loss: 0.7922\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5754 - val_loss: 0.7921\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5753 - val_loss: 0.7921\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5752 - val_loss: 0.7921\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.5748 - val_loss: 0.7920\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5747 - val_loss: 0.7920\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5745 - val_loss: 0.7920\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5743 - val_loss: 0.7919\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5742 - val_loss: 0.7919\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5739 - val_loss: 0.7919\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5739 - val_loss: 0.7919\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5736 - val_loss: 0.7919\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5735 - val_loss: 0.7918\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5734 - val_loss: 0.7918\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5731 - val_loss: 0.7918\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5731 - val_loss: 0.7918\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5729 - val_loss: 0.7918\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5729 - val_loss: 0.7918\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5728 - val_loss: 0.7918\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5725 - val_loss: 0.7918\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5726 - val_loss: 0.7917\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.7917\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5722 - val_loss: 0.7917\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5721 - val_loss: 0.7917\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5721 - val_loss: 0.7917\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5720 - val_loss: 0.7917\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5719 - val_loss: 0.7917\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5719 - val_loss: 0.7917\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5717 - val_loss: 0.7917\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5717 - val_loss: 0.7917\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5715 - val_loss: 0.7917\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5713 - val_loss: 0.7917\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5713 - val_loss: 0.7917\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5712 - val_loss: 0.7917\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5711 - val_loss: 0.7917\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5711 - val_loss: 0.7917\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5710 - val_loss: 0.7917\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5708 - val_loss: 0.7917\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5707 - val_loss: 0.7917\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5706 - val_loss: 0.7917\n",
      "Execution time:  92.3845694065094\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5047\n",
      "Root Mean Square Error: 0.7613\n",
      "Mean Square Error: 0.5796\n",
      "\n",
      "Train RMSE: 0.761\n",
      "Train MSE: 0.580\n",
      "Train MAE: 0.505\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_27 (GRU)                 (None, 36, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_111 (TimeDi (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 6s 16ms/step - loss: 0.6552 - val_loss: 0.7173\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6056 - val_loss: 0.7086\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5998 - val_loss: 0.7008\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5830 - val_loss: 0.6958\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5787 - val_loss: 0.6936\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5767 - val_loss: 0.6926\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5754 - val_loss: 0.6917\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5746 - val_loss: 0.6911\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5740 - val_loss: 0.6905\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5737 - val_loss: 0.6903\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5735 - val_loss: 0.6900\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5733 - val_loss: 0.6898\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5730 - val_loss: 0.6895\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5727 - val_loss: 0.6893\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5724 - val_loss: 0.6890\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5721 - val_loss: 0.6889\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5718 - val_loss: 0.6886\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5716 - val_loss: 0.6883\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5713 - val_loss: 0.6878\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5712 - val_loss: 0.6874\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5709 - val_loss: 0.6874\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5708 - val_loss: 0.6874\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5705 - val_loss: 0.6871\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5703 - val_loss: 0.6870\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5700 - val_loss: 0.6867\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5699 - val_loss: 0.6860\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5701 - val_loss: 0.6865\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5691 - val_loss: 0.6864\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5692 - val_loss: 0.6859\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5694 - val_loss: 0.6863\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5686 - val_loss: 0.6861\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5683 - val_loss: 0.6859\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5680 - val_loss: 0.6857\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5684 - val_loss: 0.6858\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5680 - val_loss: 0.6857\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5678 - val_loss: 0.6854\n",
      "Execution time:  192.367205619812\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5089\n",
      "Root Mean Square Error: 0.7733\n",
      "Mean Square Error: 0.5980\n",
      "\n",
      "Train RMSE: 0.773\n",
      "Train MSE: 0.598\n",
      "Train MAE: 0.509\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_28 (GRU)                 (None, 36, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_112 (TimeDi (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.6755 - val_loss: 0.7478\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6751 - val_loss: 0.7471\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6750 - val_loss: 0.7465\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6747 - val_loss: 0.7458\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6741 - val_loss: 0.7451\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6737 - val_loss: 0.7444\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6733 - val_loss: 0.7436\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6730 - val_loss: 0.7429\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6727 - val_loss: 0.7421\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6721 - val_loss: 0.7414\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6716 - val_loss: 0.7406\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6714 - val_loss: 0.7399\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6710 - val_loss: 0.7391\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6707 - val_loss: 0.7383\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6702 - val_loss: 0.7375\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6697 - val_loss: 0.7367\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6694 - val_loss: 0.7359\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6690 - val_loss: 0.7351\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6686 - val_loss: 0.7343\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6681 - val_loss: 0.7335\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6676 - val_loss: 0.7327\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6672 - val_loss: 0.7319\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6670 - val_loss: 0.7311\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6663 - val_loss: 0.7303\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6660 - val_loss: 0.7295\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6655 - val_loss: 0.7287\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6652 - val_loss: 0.7278\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6648 - val_loss: 0.7270\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6642 - val_loss: 0.7262\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6638 - val_loss: 0.7253\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6633 - val_loss: 0.7245\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6629 - val_loss: 0.7237\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6625 - val_loss: 0.7228\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6621 - val_loss: 0.7220\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6616 - val_loss: 0.7211\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6614 - val_loss: 0.7203\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6608 - val_loss: 0.7195\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6604 - val_loss: 0.7186\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6601 - val_loss: 0.7178\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6596 - val_loss: 0.7169\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6591 - val_loss: 0.7160\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6586 - val_loss: 0.7152\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.658 - 1s 16ms/step - loss: 0.6581 - val_loss: 0.7143\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6579 - val_loss: 0.7135\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6575 - val_loss: 0.7126\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6571 - val_loss: 0.7117\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6567 - val_loss: 0.7109\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6561 - val_loss: 0.7100\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6556 - val_loss: 0.7091\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6554 - val_loss: 0.7083\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6548 - val_loss: 0.7074\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6545 - val_loss: 0.7065\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6541 - val_loss: 0.7057\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6533 - val_loss: 0.7048\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6530 - val_loss: 0.7039\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6525 - val_loss: 0.7030\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6522 - val_loss: 0.7021\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6517 - val_loss: 0.7013\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6513 - val_loss: 0.7004\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6511 - val_loss: 0.6995\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6505 - val_loss: 0.6986\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6500 - val_loss: 0.6977\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6496 - val_loss: 0.6968\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6491 - val_loss: 0.6959\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6487 - val_loss: 0.6951\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6483 - val_loss: 0.6942\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6477 - val_loss: 0.6933\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6474 - val_loss: 0.6924\n",
      "Execution time:  91.26407361030579\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6350\n",
      "Root Mean Square Error: 0.9270\n",
      "Mean Square Error: 0.8594\n",
      "\n",
      "Train RMSE: 0.927\n",
      "Train MSE: 0.859\n",
      "Train MAE: 0.635\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_29 (GRU)                 (None, 36, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_113 (TimeDi (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 6s 16ms/step - loss: 0.6907 - val_loss: 0.6671\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6880 - val_loss: 0.6633\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6850 - val_loss: 0.6594\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6821 - val_loss: 0.6553\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6792 - val_loss: 0.6512\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6762 - val_loss: 0.6470\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6732 - val_loss: 0.6428\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6701 - val_loss: 0.6385\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6671 - val_loss: 0.6342\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6640 - val_loss: 0.6298\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6610 - val_loss: 0.6254\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6579 - val_loss: 0.6210\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6548 - val_loss: 0.6165\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6517 - val_loss: 0.6121\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6486 - val_loss: 0.6076\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6454 - val_loss: 0.6030\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6423 - val_loss: 0.5985\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6392 - val_loss: 0.5939\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6359 - val_loss: 0.5892\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6328 - val_loss: 0.5846\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6295 - val_loss: 0.5799\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6261 - val_loss: 0.5751\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6228 - val_loss: 0.5703\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6195 - val_loss: 0.5655\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6161 - val_loss: 0.5606\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6126 - val_loss: 0.5556\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6092 - val_loss: 0.5505\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6058 - val_loss: 0.5455\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6021 - val_loss: 0.5403\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5987 - val_loss: 0.5351\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5949 - val_loss: 0.5299\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5914 - val_loss: 0.5246\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5878 - val_loss: 0.5192\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5841 - val_loss: 0.5139\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5804 - val_loss: 0.5085\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5767 - val_loss: 0.5031\n",
      "Execution time:  190.9497594833374\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5200\n",
      "Root Mean Square Error: 0.8287\n",
      "Mean Square Error: 0.6867\n",
      "\n",
      "Train RMSE: 0.829\n",
      "Train MSE: 0.687\n",
      "Train MAE: 0.520\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_114\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_30 (GRU)                 (None, 36, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_114 (TimeDi (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.8825 - val_loss: 1.2782\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8825 - val_loss: 1.2781\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8824 - val_loss: 1.2779\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8823 - val_loss: 1.2777\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8821 - val_loss: 1.2776\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8821 - val_loss: 1.2774\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8820 - val_loss: 1.2772\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8819 - val_loss: 1.2770\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8818 - val_loss: 1.2769\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8818 - val_loss: 1.2767\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8816 - val_loss: 1.2765\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8815 - val_loss: 1.2763\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8814 - val_loss: 1.2761\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8814 - val_loss: 1.2759\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8812 - val_loss: 1.2757\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8811 - val_loss: 1.2755\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8810 - val_loss: 1.2753\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8809 - val_loss: 1.2751\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8809 - val_loss: 1.2750\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8807 - val_loss: 1.2748\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8807 - val_loss: 1.2746\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8805 - val_loss: 1.2744\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8804 - val_loss: 1.2742\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8803 - val_loss: 1.2740\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8803 - val_loss: 1.2737\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8801 - val_loss: 1.2735\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8800 - val_loss: 1.2733\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8800 - val_loss: 1.2731\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8798 - val_loss: 1.2729\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8797 - val_loss: 1.2727\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8796 - val_loss: 1.2725\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8795 - val_loss: 1.2723\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8794 - val_loss: 1.2721\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8793 - val_loss: 1.2719\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8792 - val_loss: 1.2717\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8791 - val_loss: 1.2715\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8790 - val_loss: 1.2712\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8789 - val_loss: 1.2710ETA: 0s - lo\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8787 - val_loss: 1.2708\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8786 - val_loss: 1.2706\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8785 - val_loss: 1.2704\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8785 - val_loss: 1.2702\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8783 - val_loss: 1.2700\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8782 - val_loss: 1.2697\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8781 - val_loss: 1.2695\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8780 - val_loss: 1.2693\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8779 - val_loss: 1.2691\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8778 - val_loss: 1.2689\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8777 - val_loss: 1.2686\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8775 - val_loss: 1.2684\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8774 - val_loss: 1.2682\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8773 - val_loss: 1.2680\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8772 - val_loss: 1.2678\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8771 - val_loss: 1.2675\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8770 - val_loss: 1.2673\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8769 - val_loss: 1.2671\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8768 - val_loss: 1.2669\n",
      "Epoch 58/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8766 - val_loss: 1.2666\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8765 - val_loss: 1.2664\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8764 - val_loss: 1.2662\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8763 - val_loss: 1.2660\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.8761 - val_loss: 1.2657\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8760 - val_loss: 1.2655\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8760 - val_loss: 1.2653\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8758 - val_loss: 1.2651\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8757 - val_loss: 1.2648\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8756 - val_loss: 1.2646\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.8755 - val_loss: 1.2644\n",
      "Execution time:  92.91871929168701\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9057\n",
      "Root Mean Square Error: 1.0989\n",
      "Mean Square Error: 1.2076\n",
      "\n",
      "Train RMSE: 1.099\n",
      "Train MSE: 1.208\n",
      "Train MAE: 0.906\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_115\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_31 (GRU)                 (None, 36, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_115 (TimeDi (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 6s 16ms/step - loss: 0.8822 - val_loss: 1.1311\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8817 - val_loss: 1.1303\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8812 - val_loss: 1.1294\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8806 - val_loss: 1.1285\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8800 - val_loss: 1.1276\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8794 - val_loss: 1.1267\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8788 - val_loss: 1.1257\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8782 - val_loss: 1.1247\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8775 - val_loss: 1.1237\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8769 - val_loss: 1.1227\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8763 - val_loss: 1.1217\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8756 - val_loss: 1.1207\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8750 - val_loss: 1.1196\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8743 - val_loss: 1.1186\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8736 - val_loss: 1.1175\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8730 - val_loss: 1.1164\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8723 - val_loss: 1.1153\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8716 - val_loss: 1.1142\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8709 - val_loss: 1.1131\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8702 - val_loss: 1.1120\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8696 - val_loss: 1.1109\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8689 - val_loss: 1.1097\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8682 - val_loss: 1.1086\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8674 - val_loss: 1.1074\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8667 - val_loss: 1.1062\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8660 - val_loss: 1.1051\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8653 - val_loss: 1.1039\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8646 - val_loss: 1.1027\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8638 - val_loss: 1.1014\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8631 - val_loss: 1.1002\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8623 - val_loss: 1.0990\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8615 - val_loss: 1.0977\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8608 - val_loss: 1.0965\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8600 - val_loss: 1.0952\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8592 - val_loss: 1.0939\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.8584 - val_loss: 1.0926\n",
      "Execution time:  190.12778568267822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU:\n",
      "Mean Absolute Error: 0.8958\n",
      "Root Mean Square Error: 1.0894\n",
      "Mean Square Error: 1.1868\n",
      "\n",
      "Train RMSE: 1.089\n",
      "Train MSE: 1.187\n",
      "Train MAE: 0.896\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_116\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_32 (GRU)                 (None, 36, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_116 (TimeDi (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.6181 - val_loss: 0.3863\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4670 - val_loss: 0.2303\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4419 - val_loss: 0.2191\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4339 - val_loss: 0.2114\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4286 - val_loss: 0.2051\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4252 - val_loss: 0.1990\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4225 - val_loss: 0.1948\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4200 - val_loss: 0.1928\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4183 - val_loss: 0.1895\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4169 - val_loss: 0.1875\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4157 - val_loss: 0.1848\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4145 - val_loss: 0.1831\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4137 - val_loss: 0.1823\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4128 - val_loss: 0.1801\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4122 - val_loss: 0.1799\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.4117 - val_loss: 0.1783\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4112 - val_loss: 0.1774\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4109 - val_loss: 0.1765\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4100 - val_loss: 0.1768\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4099 - val_loss: 0.1763\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4093 - val_loss: 0.1762\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4091 - val_loss: 0.1755\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4086 - val_loss: 0.1753\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4082 - val_loss: 0.1751\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4078 - val_loss: 0.1747\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4078 - val_loss: 0.1740\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4073 - val_loss: 0.1742\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4070 - val_loss: 0.1741\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4068 - val_loss: 0.1731\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4067 - val_loss: 0.1731\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4066 - val_loss: 0.1731\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4065 - val_loss: 0.1726\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4061 - val_loss: 0.1733\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4059 - val_loss: 0.1723\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4056 - val_loss: 0.1724\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4056 - val_loss: 0.1722\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4053 - val_loss: 0.1721\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4051 - val_loss: 0.1716\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4047 - val_loss: 0.1714\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4048 - val_loss: 0.1718\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.4046 - val_loss: 0.1705\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4046 - val_loss: 0.1722\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4045 - val_loss: 0.1715\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4041 - val_loss: 0.1712\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4041 - val_loss: 0.1706\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4040 - val_loss: 0.1715\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4038 - val_loss: 0.1709\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4039 - val_loss: 0.1713\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4036 - val_loss: 0.1714\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4031 - val_loss: 0.1715\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4034 - val_loss: 0.1713\n",
      "Execution time:  69.06371903419495\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1800\n",
      "Root Mean Square Error: 0.5908\n",
      "Mean Square Error: 0.3490\n",
      "\n",
      "Train RMSE: 0.591\n",
      "Train MSE: 0.349\n",
      "Train MAE: 0.180\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_117\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_33 (GRU)                 (None, 36, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_117 (TimeDi (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.4735WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "352/352 [==============================] - 6s 16ms/step - loss: 0.4735 - val_loss: 0.3242\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4169 - val_loss: 0.3156\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4097 - val_loss: 0.3106\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4061 - val_loss: 0.3085\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4036 - val_loss: 0.3067\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.4020 - val_loss: 0.3055\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.4008 - val_loss: 0.3049\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3995 - val_loss: 0.3045\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3985 - val_loss: 0.3041\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3979 - val_loss: 0.3043\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3974 - val_loss: 0.3043\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3969 - val_loss: 0.3046\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3966 - val_loss: 0.3045\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3963 - val_loss: 0.3049\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3961 - val_loss: 0.3050\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3959 - val_loss: 0.3051\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3957 - val_loss: 0.3053\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.3954 - val_loss: 0.3054\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.3952 - val_loss: 0.3055\n",
      "Execution time:  101.70372176170349\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1737\n",
      "Root Mean Square Error: 0.5875\n",
      "Mean Square Error: 0.3452\n",
      "\n",
      "Train RMSE: 0.588\n",
      "Train MSE: 0.345\n",
      "Train MAE: 0.174\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_118\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_34 (GRU)                 (None, 36, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 36, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_118 (TimeDi (None, 36, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.8559 - val_loss: 1.1152\n",
      "Epoch 2/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6865 - val_loss: 0.8572\n",
      "Epoch 3/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6333 - val_loss: 0.8358\n",
      "Epoch 4/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6230 - val_loss: 0.8266\n",
      "Epoch 5/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6165 - val_loss: 0.8210\n",
      "Epoch 6/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.6116 - val_loss: 0.8170\n",
      "Epoch 7/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6077 - val_loss: 0.8139\n",
      "Epoch 8/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6045 - val_loss: 0.8112\n",
      "Epoch 9/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.6018 - val_loss: 0.8090\n",
      "Epoch 10/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5996 - val_loss: 0.8071\n",
      "Epoch 11/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5975 - val_loss: 0.8055\n",
      "Epoch 12/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5958 - val_loss: 0.8041\n",
      "Epoch 13/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5943 - val_loss: 0.8030\n",
      "Epoch 14/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5928 - val_loss: 0.8019\n",
      "Epoch 15/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5915 - val_loss: 0.8010\n",
      "Epoch 16/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5904 - val_loss: 0.8001\n",
      "Epoch 17/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5892 - val_loss: 0.7994\n",
      "Epoch 18/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5883 - val_loss: 0.7987\n",
      "Epoch 19/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5873 - val_loss: 0.7981\n",
      "Epoch 20/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5865 - val_loss: 0.7975\n",
      "Epoch 21/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5856 - val_loss: 0.7970\n",
      "Epoch 22/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5849 - val_loss: 0.7966\n",
      "Epoch 23/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5842 - val_loss: 0.7962\n",
      "Epoch 24/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5835 - val_loss: 0.7958\n",
      "Epoch 25/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5829 - val_loss: 0.7955\n",
      "Epoch 26/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5824 - val_loss: 0.7952\n",
      "Epoch 27/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5819 - val_loss: 0.7949\n",
      "Epoch 28/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5814 - val_loss: 0.7947\n",
      "Epoch 29/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5810 - val_loss: 0.7945\n",
      "Epoch 30/68\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.5806 - val_loss: 0.7943\n",
      "Epoch 31/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5804 - val_loss: 0.7941\n",
      "Epoch 32/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5800 - val_loss: 0.7939\n",
      "Epoch 33/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5797 - val_loss: 0.7938\n",
      "Epoch 34/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5793 - val_loss: 0.7936\n",
      "Epoch 35/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5792 - val_loss: 0.7935\n",
      "Epoch 36/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5788 - val_loss: 0.7934\n",
      "Epoch 37/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5786 - val_loss: 0.7933\n",
      "Epoch 38/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5783 - val_loss: 0.7932\n",
      "Epoch 39/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5782 - val_loss: 0.7931\n",
      "Epoch 40/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5780 - val_loss: 0.7930\n",
      "Epoch 41/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5779 - val_loss: 0.7929\n",
      "Epoch 42/68\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.5777 - val_loss: 0.7928\n",
      "Epoch 43/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5775 - val_loss: 0.7928\n",
      "Epoch 44/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5773 - val_loss: 0.7927\n",
      "Epoch 45/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5772 - val_loss: 0.7927\n",
      "Epoch 46/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5770 - val_loss: 0.7926\n",
      "Epoch 47/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5770 - val_loss: 0.7926\n",
      "Epoch 48/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5768 - val_loss: 0.7925\n",
      "Epoch 49/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5766 - val_loss: 0.7925\n",
      "Epoch 50/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5766 - val_loss: 0.7924\n",
      "Epoch 51/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5765 - val_loss: 0.7924\n",
      "Epoch 52/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5763 - val_loss: 0.7923\n",
      "Epoch 53/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5763 - val_loss: 0.7923\n",
      "Epoch 54/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5762 - val_loss: 0.7923\n",
      "Epoch 55/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5761 - val_loss: 0.7923\n",
      "Epoch 56/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5760 - val_loss: 0.7922\n",
      "Epoch 57/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5759 - val_loss: 0.7922\n",
      "Epoch 58/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5758 - val_loss: 0.7922\n",
      "Epoch 59/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5758 - val_loss: 0.7922\n",
      "Epoch 60/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5756 - val_loss: 0.7921\n",
      "Epoch 61/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5757 - val_loss: 0.7921\n",
      "Epoch 62/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5756 - val_loss: 0.7921\n",
      "Epoch 63/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5756 - val_loss: 0.7921\n",
      "Epoch 64/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5755 - val_loss: 0.7921\n",
      "Epoch 65/68\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5753 - val_loss: 0.7920\n",
      "Epoch 66/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5754 - val_loss: 0.7920\n",
      "Epoch 67/68\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.5752 - val_loss: 0.7920\n",
      "Epoch 68/68\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.5752 - val_loss: 0.7920\n",
      "Execution time:  93.11560082435608\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5004\n",
      "Root Mean Square Error: 0.7609\n",
      "Mean Square Error: 0.5790\n",
      "\n",
      "Train RMSE: 0.761\n",
      "Train MSE: 0.579\n",
      "Train MAE: 0.500\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_119\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_35 (GRU)                 (None, 36, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 36, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_119 (TimeDi (None, 36, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "352/352 [==============================] - 6s 16ms/step - loss: 0.6812 - val_loss: 0.7407\n",
      "Epoch 2/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.6193 - val_loss: 0.7291\n",
      "Epoch 3/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6115 - val_loss: 0.7231\n",
      "Epoch 4/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.6044 - val_loss: 0.7175\n",
      "Epoch 5/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5982 - val_loss: 0.7140\n",
      "Epoch 6/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5953 - val_loss: 0.7110\n",
      "Epoch 7/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5926 - val_loss: 0.7079\n",
      "Epoch 8/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5890 - val_loss: 0.7050\n",
      "Epoch 9/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5862 - val_loss: 0.7025\n",
      "Epoch 10/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5844 - val_loss: 0.7003\n",
      "Epoch 11/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5828 - val_loss: 0.6983\n",
      "Epoch 12/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5813 - val_loss: 0.6969\n",
      "Epoch 13/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5800 - val_loss: 0.6956\n",
      "Epoch 14/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5784 - val_loss: 0.6948\n",
      "Epoch 15/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5774 - val_loss: 0.6941\n",
      "Epoch 16/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5766 - val_loss: 0.6935\n",
      "Epoch 17/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5761 - val_loss: 0.6931\n",
      "Epoch 18/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5756 - val_loss: 0.6926\n",
      "Epoch 19/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5753 - val_loss: 0.6922\n",
      "Epoch 20/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5750 - val_loss: 0.6919\n",
      "Epoch 21/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5747 - val_loss: 0.6916\n",
      "Epoch 22/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5745 - val_loss: 0.6913\n",
      "Epoch 23/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5743 - val_loss: 0.6912\n",
      "Epoch 24/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5741 - val_loss: 0.6910\n",
      "Epoch 25/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5739 - val_loss: 0.6909\n",
      "Epoch 26/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5738 - val_loss: 0.6907\n",
      "Epoch 27/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5737 - val_loss: 0.6906\n",
      "Epoch 28/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5735 - val_loss: 0.6905\n",
      "Epoch 29/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5735 - val_loss: 0.6904\n",
      "Epoch 30/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5734 - val_loss: 0.6903\n",
      "Epoch 31/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5733 - val_loss: 0.6903\n",
      "Epoch 32/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5731 - val_loss: 0.6902\n",
      "Epoch 33/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5731 - val_loss: 0.6902\n",
      "Epoch 34/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5730 - val_loss: 0.6901\n",
      "Epoch 35/36\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 0.5729 - val_loss: 0.6901\n",
      "Epoch 36/36\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 0.5728 - val_loss: 0.6900\n",
      "Execution time:  189.12827682495117\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4993\n",
      "Root Mean Square Error: 0.7598\n",
      "Mean Square Error: 0.5772\n",
      "\n",
      "Train RMSE: 0.760\n",
      "Train MSE: 0.577\n",
      "Train MAE: 0.499\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_120\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_36 (GRU)                 (None, 72, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_120 (TimeDi (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.5957 - val_loss: 0.2600\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5161 - val_loss: 0.1999\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5065 - val_loss: 0.1969\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5030 - val_loss: 0.1951\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5012 - val_loss: 0.1942\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4999 - val_loss: 0.1946\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4985 - val_loss: 0.1942\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4977 - val_loss: 0.1942\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4968 - val_loss: 0.1941\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4959 - val_loss: 0.1943\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4955 - val_loss: 0.1942\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.4947 - val_loss: 0.1944\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4944 - val_loss: 0.1942\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4937 - val_loss: 0.1943\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4937 - val_loss: 0.1943\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4930 - val_loss: 0.1941\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4929 - val_loss: 0.1941\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4923 - val_loss: 0.1943\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.4922 - val_loss: 0.1941\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4918 - val_loss: 0.1940\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4914 - val_loss: 0.1946\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4910 - val_loss: 0.1944\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4912 - val_loss: 0.1946\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4908 - val_loss: 0.1943\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4902 - val_loss: 0.1959\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.4899 - val_loss: 0.1956\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4894 - val_loss: 0.1961\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4892 - val_loss: 0.1964\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4885 - val_loss: 0.1964\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4880 - val_loss: 0.1968\n",
      "Execution time:  73.99497961997986\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2082\n",
      "Root Mean Square Error: 0.6079\n",
      "Mean Square Error: 0.3695\n",
      "\n",
      "Train RMSE: 0.608\n",
      "Train MSE: 0.370\n",
      "Train MAE: 0.208\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_37 (GRU)                 (None, 72, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_121 (TimeDi (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 0.5349 - val_loss: 0.3759\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4941 - val_loss: 0.3747\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4910 - val_loss: 0.3741\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4885 - val_loss: 0.3738\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4871 - val_loss: 0.3744\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4857 - val_loss: 0.3745\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4868 - val_loss: 0.3768\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4836 - val_loss: 0.3759\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4835 - val_loss: 0.3768\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4817 - val_loss: 0.3765\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4806 - val_loss: 0.3767\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4807 - val_loss: 0.3785\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4777 - val_loss: 0.3798\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4771 - val_loss: 0.3803\n",
      "Execution time:  139.44272685050964\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2152\n",
      "Root Mean Square Error: 0.6169\n",
      "Mean Square Error: 0.3805\n",
      "\n",
      "Train RMSE: 0.617\n",
      "Train MSE: 0.381\n",
      "Train MAE: 0.215\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_122\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_38 (GRU)                 (None, 72, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_122 (TimeDi (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.8141 - val_loss: 0.8641\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 0.6860 - val_loss: 0.8193\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6728 - val_loss: 0.8093\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6682 - val_loss: 0.8057\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6655 - val_loss: 0.8033\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6628 - val_loss: 0.8021\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6582 - val_loss: 0.8004\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6618 - val_loss: 0.7995\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6616 - val_loss: 0.7987\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6628 - val_loss: 0.7982\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6622 - val_loss: 0.7977\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6615 - val_loss: 0.7974\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6608 - val_loss: 0.7972\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6582 - val_loss: 0.7968\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6589 - val_loss: 0.7965\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6587 - val_loss: 0.7962\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6582 - val_loss: 0.7960\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6576 - val_loss: 0.7958\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6569 - val_loss: 0.7957\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6547 - val_loss: 0.7955\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6540 - val_loss: 0.7954\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6535 - val_loss: 0.7950\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6551 - val_loss: 0.7950\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6527 - val_loss: 0.7949\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6518 - val_loss: 0.7948\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6514 - val_loss: 0.7948\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6506 - val_loss: 0.7949\n",
      "Epoch 28/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6444 - val_loss: 0.7949\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.6429 - val_loss: 0.7947\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6436 - val_loss: 0.7947\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6411 - val_loss: 0.7947\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6398 - val_loss: 0.7947\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6390 - val_loss: 0.7946\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6384 - val_loss: 0.7946\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6372 - val_loss: 0.7945\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6368 - val_loss: 0.7944\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6415 - val_loss: 0.7942\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6521 - val_loss: 0.7942\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6501 - val_loss: 0.7941\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6498 - val_loss: 0.7941\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6495 - val_loss: 0.7941\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6445 - val_loss: 0.7941\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6359 - val_loss: 0.7940\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6356 - val_loss: 0.7940\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6352 - val_loss: 0.7940\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6350 - val_loss: 0.7939\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6340 - val_loss: 0.7939\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6338 - val_loss: 0.7938\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 0.6335 - val_loss: 0.7937\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6333 - val_loss: 0.7937\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6332 - val_loss: 0.7936\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6335 - val_loss: 0.7936\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6331 - val_loss: 0.7935\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.632 - 2s 29ms/step - loss: 0.6325 - val_loss: 0.7934\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6337 - val_loss: 0.7934\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6325 - val_loss: 0.7934\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6321 - val_loss: 0.7933\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6326 - val_loss: 0.7932\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6326 - val_loss: 0.7932\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6322 - val_loss: 0.7932\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6323 - val_loss: 0.7931\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6319 - val_loss: 0.7931\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6312 - val_loss: 0.7930\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6318 - val_loss: 0.7930\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6316 - val_loss: 0.7929\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6317 - val_loss: 0.7929\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6308 - val_loss: 0.7928\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6307 - val_loss: 0.7928\n",
      "Execution time:  165.44869828224182\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6108\n",
      "Root Mean Square Error: 0.9075\n",
      "Mean Square Error: 0.8236\n",
      "\n",
      "Train RMSE: 0.908\n",
      "Train MSE: 0.824\n",
      "Train MAE: 0.611\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_39 (GRU)                 (None, 72, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_123 (TimeDi (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 0.7047 - val_loss: 0.7305\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6636 - val_loss: 0.7330\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6517 - val_loss: 0.7269\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6351 - val_loss: 0.7265\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6317 - val_loss: 0.7234\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6308 - val_loss: 0.7226\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6301 - val_loss: 0.7221\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6294 - val_loss: 0.7218\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6292 - val_loss: 0.7213\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6278 - val_loss: 0.7197\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6269 - val_loss: 0.7197\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6274 - val_loss: 0.7176\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6263 - val_loss: 0.7148\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6241 - val_loss: 0.7098\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6241 - val_loss: 0.7087\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6177 - val_loss: 0.7038\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6143 - val_loss: 0.7027\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6097 - val_loss: 0.7013\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6257 - val_loss: 0.7006\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6105 - val_loss: 0.7012\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6045 - val_loss: 0.7011\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6078 - val_loss: 0.7009\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6065 - val_loss: 0.7014\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6023 - val_loss: 0.7011\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.5998 - val_loss: 0.7000\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6004 - val_loss: 0.6994\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.5991 - val_loss: 0.7003\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.5977 - val_loss: 0.7015\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.5970 - val_loss: 0.7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.5966 - val_loss: 0.7019\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.5963 - val_loss: 0.7014\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.5961 - val_loss: 0.7017\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.5955 - val_loss: 0.7019\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.5940 - val_loss: 0.6998\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.5960 - val_loss: 0.6976\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.5956 - val_loss: 0.7002\n",
      "Execution time:  352.48098826408386\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5834\n",
      "Root Mean Square Error: 0.8860\n",
      "Mean Square Error: 0.7850\n",
      "\n",
      "Train RMSE: 0.886\n",
      "Train MSE: 0.785\n",
      "Train MAE: 0.583\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_124\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_40 (GRU)                 (None, 72, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_124 (TimeDi (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.7000 - val_loss: 0.7982\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6998 - val_loss: 0.7976\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6995 - val_loss: 0.7970\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6993 - val_loss: 0.7963\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6990 - val_loss: 0.7957\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6987 - val_loss: 0.7950\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6984 - val_loss: 0.7943\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6980 - val_loss: 0.7936\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6978 - val_loss: 0.7929\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6975 - val_loss: 0.7922\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6973 - val_loss: 0.7914\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6968 - val_loss: 0.7907\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6965 - val_loss: 0.7899\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6962 - val_loss: 0.7892\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6959 - val_loss: 0.7884\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6956 - val_loss: 0.7876\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6952 - val_loss: 0.7869\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6949 - val_loss: 0.7861\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6946 - val_loss: 0.7853\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6942 - val_loss: 0.7845\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6940 - val_loss: 0.7837\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6936 - val_loss: 0.7829\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6932 - val_loss: 0.7822\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6929 - val_loss: 0.7814\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6924 - val_loss: 0.7806\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6921 - val_loss: 0.7798\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6919 - val_loss: 0.7790\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6916 - val_loss: 0.7782\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6912 - val_loss: 0.7773\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6909 - val_loss: 0.7765\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6905 - val_loss: 0.7757\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6902 - val_loss: 0.7749\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6899 - val_loss: 0.7741\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6895 - val_loss: 0.7733\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6891 - val_loss: 0.7725\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6888 - val_loss: 0.7716\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6884 - val_loss: 0.7708\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6881 - val_loss: 0.7700\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6878 - val_loss: 0.7692\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6875 - val_loss: 0.7683\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6871 - val_loss: 0.7675\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6869 - val_loss: 0.7667\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6863 - val_loss: 0.7658\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6860 - val_loss: 0.7650\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6857 - val_loss: 0.7642\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6853 - val_loss: 0.7633\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6850 - val_loss: 0.7625\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6847 - val_loss: 0.7616\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6842 - val_loss: 0.7608\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6839 - val_loss: 0.7600\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6835 - val_loss: 0.7591\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6832 - val_loss: 0.7583\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6828 - val_loss: 0.7574\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6826 - val_loss: 0.7566\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6823 - val_loss: 0.7557\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6818 - val_loss: 0.7549\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6815 - val_loss: 0.7540\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6811 - val_loss: 0.7532\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6807 - val_loss: 0.7523\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6805 - val_loss: 0.7515\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6801 - val_loss: 0.7506\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6798 - val_loss: 0.7498\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6794 - val_loss: 0.7489\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6791 - val_loss: 0.7481\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6786 - val_loss: 0.7472\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6783 - val_loss: 0.7464\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6780 - val_loss: 0.7455\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6776 - val_loss: 0.7446\n",
      "Execution time:  164.49325609207153\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6788\n",
      "Root Mean Square Error: 0.9745\n",
      "Mean Square Error: 0.9497\n",
      "\n",
      "Train RMSE: 0.975\n",
      "Train MSE: 0.950\n",
      "Train MAE: 0.679\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_41 (GRU)                 (None, 72, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_125 (TimeDi (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 0.7241 - val_loss: 0.7141\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.7226 - val_loss: 0.7119\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7210 - val_loss: 0.7095\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7194 - val_loss: 0.7071\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.7177 - val_loss: 0.7046\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7159 - val_loss: 0.7021\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.7141 - val_loss: 0.6995\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7124 - val_loss: 0.6968\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7106 - val_loss: 0.6942\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7089 - val_loss: 0.6915\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7071 - val_loss: 0.6888\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.7053 - val_loss: 0.6860\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7035 - val_loss: 0.6832\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.7016 - val_loss: 0.6804\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6999 - val_loss: 0.6775\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6980 - val_loss: 0.6746\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6962 - val_loss: 0.6717\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6943 - val_loss: 0.6688\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6925 - val_loss: 0.6658\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6906 - val_loss: 0.6628\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6887 - val_loss: 0.6597\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6868 - val_loss: 0.6567\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6849 - val_loss: 0.6537\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6830 - val_loss: 0.6507\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6812 - val_loss: 0.6477\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6794 - val_loss: 0.6448\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6776 - val_loss: 0.6419\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6758 - val_loss: 0.6390\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6740 - val_loss: 0.6362\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6722 - val_loss: 0.6333\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6704 - val_loss: 0.6304\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6686 - val_loss: 0.6275\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6668 - val_loss: 0.6246\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6649 - val_loss: 0.6217\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6631 - val_loss: 0.6188\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6612 - val_loss: 0.6158\n",
      "Execution time:  350.4732291698456\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6327\n",
      "Root Mean Square Error: 0.9383\n",
      "Mean Square Error: 0.8804\n",
      "\n",
      "Train RMSE: 0.938\n",
      "Train MSE: 0.880\n",
      "Train MAE: 0.633\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_126\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_42 (GRU)                 (None, 72, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_126 (TimeDi (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.8948 - val_loss: 1.2981\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8947 - val_loss: 1.2980\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 0.8947 - val_loss: 1.2978\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8946 - val_loss: 1.2976\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8945 - val_loss: 1.2974\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8944 - val_loss: 1.2972\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8943 - val_loss: 1.2970\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8942 - val_loss: 1.2969\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8942 - val_loss: 1.2967\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8941 - val_loss: 1.2965\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8940 - val_loss: 1.2963\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8939 - val_loss: 1.2960\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8939 - val_loss: 1.2958\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8937 - val_loss: 1.2956\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8937 - val_loss: 1.2954\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8936 - val_loss: 1.2952\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8934 - val_loss: 1.2950\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8934 - val_loss: 1.2948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8933 - val_loss: 1.2946\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8932 - val_loss: 1.2944\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8931 - val_loss: 1.2941\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8930 - val_loss: 1.2939\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8929 - val_loss: 1.2937\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8928 - val_loss: 1.2935\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8927 - val_loss: 1.2933\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8926 - val_loss: 1.2931\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8925 - val_loss: 1.2928\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8924 - val_loss: 1.2926\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8923 - val_loss: 1.2924\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.8922 - val_loss: 1.2922\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8921 - val_loss: 1.2919\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8920 - val_loss: 1.2917\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8919 - val_loss: 1.2915\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8918 - val_loss: 1.2913\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8917 - val_loss: 1.2910\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8916 - val_loss: 1.2908\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8915 - val_loss: 1.2906\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8914 - val_loss: 1.2904\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8913 - val_loss: 1.2901\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8912 - val_loss: 1.2899\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8911 - val_loss: 1.2897\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8910 - val_loss: 1.2895\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8909 - val_loss: 1.2892\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8908 - val_loss: 1.2890\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8907 - val_loss: 1.2888\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8906 - val_loss: 1.2885\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8905 - val_loss: 1.2883\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8904 - val_loss: 1.2881\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8903 - val_loss: 1.2878\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8902 - val_loss: 1.2876\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8901 - val_loss: 1.2874\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8900 - val_loss: 1.2872\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8899 - val_loss: 1.2869\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8898 - val_loss: 1.2867\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8897 - val_loss: 1.2865\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8896 - val_loss: 1.2862\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8895 - val_loss: 1.2860\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8894 - val_loss: 1.2857\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8892 - val_loss: 1.2855\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8892 - val_loss: 1.2853\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8891 - val_loss: 1.2850\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8890 - val_loss: 1.2848\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.8889 - val_loss: 1.2846\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.8888 - val_loss: 1.2843\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8886 - val_loss: 1.2841\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8885 - val_loss: 1.2839\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8884 - val_loss: 1.2836\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.8883 - val_loss: 1.2834\n",
      "Execution time:  164.11080074310303\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9233\n",
      "Root Mean Square Error: 1.1187\n",
      "Mean Square Error: 1.2514\n",
      "\n",
      "Train RMSE: 1.119\n",
      "Train MSE: 1.251\n",
      "Train MAE: 0.923\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_127\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_43 (GRU)                 (None, 72, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_127 (TimeDi (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 0.8820 - val_loss: 1.1299\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8816 - val_loss: 1.1292\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8812 - val_loss: 1.1285\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8807 - val_loss: 1.1276\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8802 - val_loss: 1.1268\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8797 - val_loss: 1.1259\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8792 - val_loss: 1.1250\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8786 - val_loss: 1.1240\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8780 - val_loss: 1.1231\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8774 - val_loss: 1.1221\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8769 - val_loss: 1.1211\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8763 - val_loss: 1.1201\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8757 - val_loss: 1.1190\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8750 - val_loss: 1.1179\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8744 - val_loss: 1.1169\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8738 - val_loss: 1.1158\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8732 - val_loss: 1.1146\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8725 - val_loss: 1.1135\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8718 - val_loss: 1.1124\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8711 - val_loss: 1.1112\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.8705 - val_loss: 1.1100\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8698 - val_loss: 1.1088\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.8691 - val_loss: 1.1076\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8684 - val_loss: 1.1063\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8676 - val_loss: 1.1051\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.8669 - val_loss: 1.1038\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8661 - val_loss: 1.1025\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.8654 - val_loss: 1.1012\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8646 - val_loss: 1.0998\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8638 - val_loss: 1.0985\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8631 - val_loss: 1.0971\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8623 - val_loss: 1.0957\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.8615 - val_loss: 1.0943\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8606 - val_loss: 1.0928\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.8598 - val_loss: 1.0914\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.8590 - val_loss: 1.0899\n",
      "Execution time:  351.1460087299347\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8933\n",
      "Root Mean Square Error: 1.0897\n",
      "Mean Square Error: 1.1876\n",
      "\n",
      "Train RMSE: 1.090\n",
      "Train MSE: 1.188\n",
      "Train MAE: 0.893\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_128\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_44 (GRU)                 (None, 72, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_128 (TimeDi (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.6398 - val_loss: 0.4376\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5306 - val_loss: 0.2128\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5148 - val_loss: 0.2122\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5080 - val_loss: 0.2102\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.5044 - val_loss: 0.2080\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.5024 - val_loss: 0.2065\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.5009 - val_loss: 0.2056\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4999 - val_loss: 0.2046\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4988 - val_loss: 0.2037\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4981 - val_loss: 0.2028\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4976 - val_loss: 0.2022\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4968 - val_loss: 0.2018\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4964 - val_loss: 0.2013\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.4959 - val_loss: 0.2007\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4956 - val_loss: 0.2010\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4948 - val_loss: 0.2006\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4947 - val_loss: 0.2004\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4942 - val_loss: 0.2004\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4939 - val_loss: 0.2003\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4936 - val_loss: 0.1997\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.4932 - val_loss: 0.2001\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4928 - val_loss: 0.1999\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4929 - val_loss: 0.1995\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4925 - val_loss: 0.1997\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4922 - val_loss: 0.2000\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4920 - val_loss: 0.1998\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.4916 - val_loss: 0.2000\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4915 - val_loss: 0.2000\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4913 - val_loss: 0.1994\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4911 - val_loss: 0.1997\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4910 - val_loss: 0.1998\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4908 - val_loss: 0.1998\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4906 - val_loss: 0.1999\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.4903 - val_loss: 0.2000\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4901 - val_loss: 0.1997\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.4902 - val_loss: 0.2003\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4899 - val_loss: 0.1997\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4897 - val_loss: 0.2000\n",
      "Epoch 39/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.4895 - val_loss: 0.2001\n",
      "Execution time:  96.05110597610474\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1998\n",
      "Root Mean Square Error: 0.6027\n",
      "Mean Square Error: 0.3632\n",
      "\n",
      "Train RMSE: 0.603\n",
      "Train MSE: 0.363\n",
      "Train MAE: 0.200\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_129\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_45 (GRU)                 (None, 72, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_129 (TimeDi (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 0.5401 - val_loss: 0.3813\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4946 - val_loss: 0.3798\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4903 - val_loss: 0.3786\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4879 - val_loss: 0.3786\n",
      "Epoch 5/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4865 - val_loss: 0.3787\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.4852 - val_loss: 0.3791\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4843 - val_loss: 0.3785\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.4835 - val_loss: 0.3783\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4826 - val_loss: 0.3778\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4819 - val_loss: 0.3775\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.4813 - val_loss: 0.3773\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4808 - val_loss: 0.3773\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.4805 - val_loss: 0.3773\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4802 - val_loss: 0.3772\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4799 - val_loss: 0.3770\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4796 - val_loss: 0.3773\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4794 - val_loss: 0.3770\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4793 - val_loss: 0.3769\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4790 - val_loss: 0.3771\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4784 - val_loss: 0.3771\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.4780 - val_loss: 0.3772\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4775 - val_loss: 0.3772\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4768 - val_loss: 0.3771\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4764 - val_loss: 0.3765\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4761 - val_loss: 0.3756\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4754 - val_loss: 0.3751\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4746 - val_loss: 0.3748\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4741 - val_loss: 0.3746\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4734 - val_loss: 0.3745\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4729 - val_loss: 0.3738\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4722 - val_loss: 0.3736\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4715 - val_loss: 0.3731\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.4704 - val_loss: 0.3727\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4694 - val_loss: 0.3719\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.4685 - val_loss: 0.3718\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.4676 - val_loss: 0.3715\n",
      "Execution time:  349.9762351512909\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2497\n",
      "Root Mean Square Error: 0.6709\n",
      "Mean Square Error: 0.4501\n",
      "\n",
      "Train RMSE: 0.671\n",
      "Train MSE: 0.450\n",
      "Train MAE: 0.250\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_130\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_46 (GRU)                 (None, 72, 40)            5160      \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 72, 40)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_130 (TimeDi (None, 72, 1)             41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.8776 - val_loss: 1.1863\n",
      "Epoch 2/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.7551 - val_loss: 0.8824\n",
      "Epoch 3/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6846 - val_loss: 0.8439\n",
      "Epoch 4/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6704 - val_loss: 0.8307\n",
      "Epoch 5/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6607 - val_loss: 0.8223\n",
      "Epoch 6/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6550 - val_loss: 0.8163\n",
      "Epoch 7/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6514 - val_loss: 0.8123\n",
      "Epoch 8/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6488 - val_loss: 0.8093\n",
      "Epoch 9/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6468 - val_loss: 0.8070\n",
      "Epoch 10/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6451 - val_loss: 0.8051\n",
      "Epoch 11/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6438 - val_loss: 0.8036\n",
      "Epoch 12/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6426 - val_loss: 0.8023\n",
      "Epoch 13/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6417 - val_loss: 0.8013\n",
      "Epoch 14/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6409 - val_loss: 0.8005\n",
      "Epoch 15/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6400 - val_loss: 0.7997\n",
      "Epoch 16/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6394 - val_loss: 0.7991\n",
      "Epoch 17/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6388 - val_loss: 0.7985\n",
      "Epoch 18/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6381 - val_loss: 0.7980\n",
      "Epoch 19/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6377 - val_loss: 0.7976\n",
      "Epoch 20/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6372 - val_loss: 0.7972\n",
      "Epoch 21/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6368 - val_loss: 0.7968\n",
      "Epoch 22/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6364 - val_loss: 0.7965\n",
      "Epoch 23/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6361 - val_loss: 0.7963\n",
      "Epoch 24/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6356 - val_loss: 0.7960\n",
      "Epoch 25/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6353 - val_loss: 0.7958\n",
      "Epoch 26/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6350 - val_loss: 0.7955\n",
      "Epoch 27/68\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 0.6348 - val_loss: 0.7953\n",
      "Epoch 28/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6345 - val_loss: 0.7952\n",
      "Epoch 29/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6341 - val_loss: 0.7950\n",
      "Epoch 30/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6339 - val_loss: 0.7948\n",
      "Epoch 31/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6337 - val_loss: 0.7947\n",
      "Epoch 32/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6335 - val_loss: 0.7946\n",
      "Epoch 33/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6334 - val_loss: 0.7944\n",
      "Epoch 34/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6331 - val_loss: 0.7943\n",
      "Epoch 35/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6329 - val_loss: 0.7942s - l\n",
      "Epoch 36/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6328 - val_loss: 0.7941\n",
      "Epoch 37/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6325 - val_loss: 0.7940\n",
      "Epoch 38/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6324 - val_loss: 0.7939\n",
      "Epoch 39/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6323 - val_loss: 0.7938\n",
      "Epoch 40/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6321 - val_loss: 0.7937\n",
      "Epoch 41/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6319 - val_loss: 0.7937\n",
      "Epoch 42/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6318 - val_loss: 0.7936\n",
      "Epoch 43/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6316 - val_loss: 0.7935\n",
      "Epoch 44/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6316 - val_loss: 0.7934\n",
      "Epoch 45/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6314 - val_loss: 0.7934\n",
      "Epoch 46/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6313 - val_loss: 0.7933\n",
      "Epoch 47/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6312 - val_loss: 0.7933\n",
      "Epoch 48/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6310 - val_loss: 0.7932\n",
      "Epoch 49/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6308 - val_loss: 0.7932\n",
      "Epoch 50/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6308 - val_loss: 0.7931\n",
      "Epoch 51/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6307 - val_loss: 0.7931\n",
      "Epoch 52/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6306 - val_loss: 0.7930\n",
      "Epoch 53/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6305 - val_loss: 0.7930\n",
      "Epoch 54/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6304 - val_loss: 0.7930\n",
      "Epoch 55/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6302 - val_loss: 0.7929\n",
      "Epoch 56/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6302 - val_loss: 0.7929\n",
      "Epoch 57/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6300 - val_loss: 0.7928\n",
      "Epoch 58/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6300 - val_loss: 0.7928\n",
      "Epoch 59/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6299 - val_loss: 0.7928\n",
      "Epoch 60/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6298 - val_loss: 0.7927\n",
      "Epoch 61/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6298 - val_loss: 0.7927\n",
      "Epoch 62/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6296 - val_loss: 0.7927\n",
      "Epoch 63/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6295 - val_loss: 0.7927\n",
      "Epoch 64/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6294 - val_loss: 0.7926\n",
      "Epoch 65/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6293 - val_loss: 0.7926\n",
      "Epoch 66/68\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.6293 - val_loss: 0.7926\n",
      "Epoch 67/68\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.6292 - val_loss: 0.7926\n",
      "Epoch 68/68\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.6291 - val_loss: 0.7925\n",
      "Execution time:  164.49348998069763\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5264\n",
      "Root Mean Square Error: 0.7791\n",
      "Mean Square Error: 0.6070\n",
      "\n",
      "Train RMSE: 0.779\n",
      "Train MSE: 0.607\n",
      "Train MAE: 0.526\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_131\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_47 (GRU)                 (None, 72, 55)            9570      \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 72, 55)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_131 (TimeDi (None, 72, 1)             56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 0.7229 - val_loss: 0.7491\n",
      "Epoch 2/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6741 - val_loss: 0.7435\n",
      "Epoch 3/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6714 - val_loss: 0.7417\n",
      "Epoch 4/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6672 - val_loss: 0.7402\n",
      "Epoch 5/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6493 - val_loss: 0.7393\n",
      "Epoch 6/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6385 - val_loss: 0.7365\n",
      "Epoch 7/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6368 - val_loss: 0.7342\n",
      "Epoch 8/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6356 - val_loss: 0.7323\n",
      "Epoch 9/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6345 - val_loss: 0.7306\n",
      "Epoch 10/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6336 - val_loss: 0.7289\n",
      "Epoch 11/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6326 - val_loss: 0.7273\n",
      "Epoch 12/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6320 - val_loss: 0.7259\n",
      "Epoch 13/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6314 - val_loss: 0.7247\n",
      "Epoch 14/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6307 - val_loss: 0.7239\n",
      "Epoch 15/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6298 - val_loss: 0.7233\n",
      "Epoch 16/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6290 - val_loss: 0.7230\n",
      "Epoch 17/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6284 - val_loss: 0.7228\n",
      "Epoch 18/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6278 - val_loss: 0.7226\n",
      "Epoch 19/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6272 - val_loss: 0.7223\n",
      "Epoch 20/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6267 - val_loss: 0.7222\n",
      "Epoch 21/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6262 - val_loss: 0.7218\n",
      "Epoch 22/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6257 - val_loss: 0.7212\n",
      "Epoch 23/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6253 - val_loss: 0.7205\n",
      "Epoch 24/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6249 - val_loss: 0.7195\n",
      "Epoch 25/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6245 - val_loss: 0.7186\n",
      "Epoch 26/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6241 - val_loss: 0.7175\n",
      "Epoch 27/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6237 - val_loss: 0.7164\n",
      "Epoch 28/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6233 - val_loss: 0.7153\n",
      "Epoch 29/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6228 - val_loss: 0.7140\n",
      "Epoch 30/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6223 - val_loss: 0.7131\n",
      "Epoch 31/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6219 - val_loss: 0.7119\n",
      "Epoch 32/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6212 - val_loss: 0.7111\n",
      "Epoch 33/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6204 - val_loss: 0.7103\n",
      "Epoch 34/36\n",
      "349/349 [==============================] - 9s 27ms/step - loss: 0.6194 - val_loss: 0.7096\n",
      "Epoch 35/36\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.6184 - val_loss: 0.7088\n",
      "Epoch 36/36\n",
      "349/349 [==============================] - 10s 27ms/step - loss: 0.6172 - val_loss: 0.7082\n",
      "Execution time:  349.2170240879059\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5517\n",
      "Root Mean Square Error: 0.8189\n",
      "Mean Square Error: 0.6707\n",
      "\n",
      "Train RMSE: 0.819\n",
      "Train MSE: 0.671\n",
      "Train MAE: 0.552\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_132\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_48 (GRU)                 (None, 144, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_132 (TimeDi (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 4s 58ms/step - loss: 0.6494 - val_loss: 0.2595\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5901 - val_loss: 0.2032\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5851 - val_loss: 0.2118\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5818 - val_loss: 0.2142\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5794 - val_loss: 0.2150\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5779 - val_loss: 0.2157\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5763 - val_loss: 0.2155\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5749 - val_loss: 0.2155\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5733 - val_loss: 0.2156\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5718 - val_loss: 0.2158\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5699 - val_loss: 0.2164\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5676 - val_loss: 0.2166\n",
      "Execution time:  53.778528690338135\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2877\n",
      "Root Mean Square Error: 0.6909\n",
      "Mean Square Error: 0.4773\n",
      "\n",
      "Train RMSE: 0.691\n",
      "Train MSE: 0.477\n",
      "Train MAE: 0.288\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_133\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_49 (GRU)                 (None, 144, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_133 (TimeDi (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 19s 55ms/step - loss: 0.5987 - val_loss: 0.4014\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5785 - val_loss: 0.4043\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5744 - val_loss: 0.4074\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5741 - val_loss: 0.4099\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5678 - val_loss: 0.4141\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5685 - val_loss: 0.4086\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5628 - val_loss: 0.4089\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5615 - val_loss: 0.4114\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5616 - val_loss: 0.4114\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5607 - val_loss: 0.4110\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5599 - val_loss: 0.4085\n",
      "Execution time:  202.80041694641113\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2784\n",
      "Root Mean Square Error: 0.6900\n",
      "Mean Square Error: 0.4762\n",
      "\n",
      "Train RMSE: 0.690\n",
      "Train MSE: 0.476\n",
      "Train MAE: 0.278\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_50 (GRU)                 (None, 144, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_134 (TimeDi (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 60ms/step - loss: 0.8522 - val_loss: 0.8824\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.7174 - val_loss: 0.8257\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6989 - val_loss: 0.8105\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6923 - val_loss: 0.8044\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6890 - val_loss: 0.8011\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6871 - val_loss: 0.7994\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6853 - val_loss: 0.7985\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6833 - val_loss: 0.7977\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6813 - val_loss: 0.7971\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6793 - val_loss: 0.7965\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6775 - val_loss: 0.7959\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6760 - val_loss: 0.7955\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6744 - val_loss: 0.7948\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6736 - val_loss: 0.7947\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6714 - val_loss: 0.7940\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6709 - val_loss: 0.7939\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6693 - val_loss: 0.7935\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6687 - val_loss: 0.7934\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6671 - val_loss: 0.7929\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6658 - val_loss: 0.7925\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6672 - val_loss: 0.7926\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6636 - val_loss: 0.7923\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6656 - val_loss: 0.7923\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6639 - val_loss: 0.7921\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6625 - val_loss: 0.7921\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6717 - val_loss: 0.7931\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6628 - val_loss: 0.7918\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6596 - val_loss: 0.7918\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6568 - val_loss: 0.7918\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6553 - val_loss: 0.7917\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6546 - val_loss: 0.7916\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6536 - val_loss: 0.7916\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6530 - val_loss: 0.7916\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6523 - val_loss: 0.7915\n",
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6534 - val_loss: 0.7914\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6528 - val_loss: 0.7914\n",
      "Epoch 37/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6536 - val_loss: 0.7915\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6485 - val_loss: 0.7915\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6471 - val_loss: 0.7914\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6446 - val_loss: 0.7914\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6434 - val_loss: 0.7914\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6491 - val_loss: 0.7915\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6440 - val_loss: 0.7914\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6424 - val_loss: 0.7914\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6438 - val_loss: 0.7914\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6408 - val_loss: 0.7913\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6400 - val_loss: 0.7913\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6392 - val_loss: 0.7913\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6382 - val_loss: 0.7912\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6374 - val_loss: 0.7912\n",
      "Epoch 51/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6365 - val_loss: 0.7912\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6356 - val_loss: 0.7911\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6347 - val_loss: 0.7911\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6351 - val_loss: 0.7911\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6332 - val_loss: 0.7911\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6329 - val_loss: 0.7910\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6309 - val_loss: 0.7910\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6296 - val_loss: 0.7910\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6309 - val_loss: 0.7911\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6340 - val_loss: 0.7910\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6302 - val_loss: 0.7909\n",
      "Epoch 62/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6290 - val_loss: 0.7908\n",
      "Epoch 63/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6290 - val_loss: 0.7908\n",
      "Epoch 64/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6287 - val_loss: 0.7908\n",
      "Epoch 65/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6270 - val_loss: 0.7908\n",
      "Epoch 66/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6278 - val_loss: 0.7908\n",
      "Epoch 67/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6268 - val_loss: 0.7908\n",
      "Epoch 68/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6279 - val_loss: 0.7907\n",
      "Execution time:  283.308287858963\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6338\n",
      "Root Mean Square Error: 0.9317\n",
      "Mean Square Error: 0.8681\n",
      "\n",
      "Train RMSE: 0.932\n",
      "Train MSE: 0.868\n",
      "Train MAE: 0.634\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_51 (GRU)                 (None, 144, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_135 (TimeDi (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 19s 55ms/step - loss: 0.7280 - val_loss: 0.7161\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6865 - val_loss: 0.7164\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6942 - val_loss: 0.7147\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6929 - val_loss: 0.7129\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6912 - val_loss: 0.7111\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6899 - val_loss: 0.7095\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6924 - val_loss: 0.7086\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6898 - val_loss: 0.7081\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6830 - val_loss: 0.7073\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6830 - val_loss: 0.7061\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6840 - val_loss: 0.7056\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6814 - val_loss: 0.7048\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6729 - val_loss: 0.7050\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6759 - val_loss: 0.7093\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6744 - val_loss: 0.7064\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6717 - val_loss: 0.7054\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6693 - val_loss: 0.7050\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6723 - val_loss: 0.7053\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6769 - val_loss: 0.7043\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6654 - val_loss: 0.7039\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6659 - val_loss: 0.7036\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6653 - val_loss: 0.7034\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6646 - val_loss: 0.7040\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6670 - val_loss: 0.7035\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6669 - val_loss: 0.7036\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6647 - val_loss: 0.7038\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6613 - val_loss: 0.7028\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6617 - val_loss: 0.7026\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6603 - val_loss: 0.7025\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6603 - val_loss: 0.7024\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6604 - val_loss: 0.7023\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6613 - val_loss: 0.7023\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6622 - val_loss: 0.7019\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6634 - val_loss: 0.7020\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6619 - val_loss: 0.7019\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.6615 - val_loss: 0.7019\n",
      "Execution time:  662.8875730037689\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6897\n",
      "Root Mean Square Error: 0.9872\n",
      "Mean Square Error: 0.9745\n",
      "\n",
      "Train RMSE: 0.987\n",
      "Train MSE: 0.975\n",
      "Train MAE: 0.690\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_52 (GRU)                 (None, 144, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_136 (TimeDi (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 60ms/step - loss: 0.7071 - val_loss: 0.8241\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7069 - val_loss: 0.8236\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7067 - val_loss: 0.8230\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7065 - val_loss: 0.8225\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7063 - val_loss: 0.8219\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7061 - val_loss: 0.8213\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7058 - val_loss: 0.8206\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7056 - val_loss: 0.8200\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7054 - val_loss: 0.8194\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7052 - val_loss: 0.8187\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7050 - val_loss: 0.8181\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7047 - val_loss: 0.8174\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7045 - val_loss: 0.8168\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.7042 - val_loss: 0.8161\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7041 - val_loss: 0.8154\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.7037 - val_loss: 0.8147\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7036 - val_loss: 0.8141\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7034 - val_loss: 0.8134\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7030 - val_loss: 0.8127\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.7028 - val_loss: 0.8120\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7027 - val_loss: 0.8113\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7023 - val_loss: 0.8106\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7021 - val_loss: 0.8100\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.7018 - val_loss: 0.8093\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7015 - val_loss: 0.8086\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7013 - val_loss: 0.8079\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7011 - val_loss: 0.8072\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.7008 - val_loss: 0.8065\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7005 - val_loss: 0.8058\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7003 - val_loss: 0.8051\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.7001 - val_loss: 0.8044\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6999 - val_loss: 0.8037\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6996 - val_loss: 0.8030\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6994 - val_loss: 0.8023\n",
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6991 - val_loss: 0.8016\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6989 - val_loss: 0.8009\n",
      "Epoch 37/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6986 - val_loss: 0.8002\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6984 - val_loss: 0.7995\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6980 - val_loss: 0.7988\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6981 - val_loss: 0.7981\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6977 - val_loss: 0.7974\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6974 - val_loss: 0.7967\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6972 - val_loss: 0.7960\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6969 - val_loss: 0.7953\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6967 - val_loss: 0.7946\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6964 - val_loss: 0.7939\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6962 - val_loss: 0.7932\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6959 - val_loss: 0.7925\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6956 - val_loss: 0.7918\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6954 - val_loss: 0.7911\n",
      "Epoch 51/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6952 - val_loss: 0.7904\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6950 - val_loss: 0.7897\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6947 - val_loss: 0.7890\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6944 - val_loss: 0.7883\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6942 - val_loss: 0.7875\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6940 - val_loss: 0.7868\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6937 - val_loss: 0.7861\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6935 - val_loss: 0.7854\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6932 - val_loss: 0.7847\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6930 - val_loss: 0.7840\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6929 - val_loss: 0.7833\n",
      "Epoch 62/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6925 - val_loss: 0.7826\n",
      "Epoch 63/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6923 - val_loss: 0.7819\n",
      "Epoch 64/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6920 - val_loss: 0.7812\n",
      "Epoch 65/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6917 - val_loss: 0.7805\n",
      "Epoch 66/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6915 - val_loss: 0.7798\n",
      "Epoch 67/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6913 - val_loss: 0.7791\n",
      "Epoch 68/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6910 - val_loss: 0.7784\n",
      "Execution time:  283.5206711292267\n",
      "GRU:\n",
      "Mean Absolute Error: 0.7071\n",
      "Root Mean Square Error: 1.0158\n",
      "Mean Square Error: 1.0318\n",
      "\n",
      "Train RMSE: 1.016\n",
      "Train MSE: 1.032\n",
      "Train MAE: 0.707\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_137\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_53 (GRU)                 (None, 144, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_137 (TimeDi (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 18s 53ms/step - loss: 0.7182 - val_loss: 0.7184\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7170 - val_loss: 0.7162\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7157 - val_loss: 0.7137\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 18s 51ms/step - loss: 0.7142 - val_loss: 0.7111\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7128 - val_loss: 0.7084\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7112 - val_loss: 0.7056\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7097 - val_loss: 0.7028\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7081 - val_loss: 0.6999\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7066 - val_loss: 0.6970\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7050 - val_loss: 0.6940\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.7034 - val_loss: 0.6910\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7018 - val_loss: 0.6880\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7002 - val_loss: 0.6849\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6985 - val_loss: 0.6819\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6969 - val_loss: 0.6787\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6952 - val_loss: 0.6756\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6935 - val_loss: 0.6724\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6919 - val_loss: 0.6691\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6902 - val_loss: 0.6659\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6885 - val_loss: 0.6627\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6869 - val_loss: 0.6595\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6854 - val_loss: 0.6564\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6838 - val_loss: 0.6535\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6824 - val_loss: 0.6506\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6809 - val_loss: 0.6477\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6795 - val_loss: 0.6449\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6780 - val_loss: 0.6421\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6766 - val_loss: 0.6393\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6753 - val_loss: 0.6365\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6738 - val_loss: 0.6337\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6724 - val_loss: 0.6308\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6710 - val_loss: 0.6279\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6695 - val_loss: 0.6250\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6681 - val_loss: 0.6220\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6666 - val_loss: 0.6191\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6652 - val_loss: 0.6161\n",
      "Execution time:  648.0484073162079\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6327\n",
      "Root Mean Square Error: 0.9534\n",
      "Mean Square Error: 0.9090\n",
      "\n",
      "Train RMSE: 0.953\n",
      "Train MSE: 0.909\n",
      "Train MAE: 0.633\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_138\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_54 (GRU)                 (None, 144, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_138 (TimeDi (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 60ms/step - loss: 0.9025 - val_loss: 1.2862\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.9024 - val_loss: 1.2861\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9024 - val_loss: 1.2860\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9023 - val_loss: 1.2859\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9023 - val_loss: 1.2858\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.9022 - val_loss: 1.2856\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9022 - val_loss: 1.2855\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9021 - val_loss: 1.2854\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9021 - val_loss: 1.2852\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.9020 - val_loss: 1.2851\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9020 - val_loss: 1.2849\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9019 - val_loss: 1.2848\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9018 - val_loss: 1.2846\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.9018 - val_loss: 1.2845\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9017 - val_loss: 1.2843\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9016 - val_loss: 1.2842\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9016 - val_loss: 1.2840\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.9015 - val_loss: 1.2839\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9015 - val_loss: 1.2837\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9014 - val_loss: 1.2836\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9013 - val_loss: 1.2834\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.9013 - val_loss: 1.2832\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9012 - val_loss: 1.2831\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9012 - val_loss: 1.2829\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9011 - val_loss: 1.2828\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9010 - val_loss: 1.2826\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9009 - val_loss: 1.2824\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9009 - val_loss: 1.2823\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.9008 - val_loss: 1.2821\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9007 - val_loss: 1.2819\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9007 - val_loss: 1.2818\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9006 - val_loss: 1.2816\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.9006 - val_loss: 1.2814\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9005 - val_loss: 1.2813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.9004 - val_loss: 1.2811\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.9003 - val_loss: 1.2809\n",
      "Epoch 37/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.9003 - val_loss: 1.2808\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.9002 - val_loss: 1.2806\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.9001 - val_loss: 1.2804\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.9000 - val_loss: 1.2802\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8999 - val_loss: 1.2801\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.8999 - val_loss: 1.2799\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8998 - val_loss: 1.2797\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.8997 - val_loss: 1.2795\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.8997 - val_loss: 1.2794\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8996 - val_loss: 1.2792\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8995 - val_loss: 1.2790\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.8994 - val_loss: 1.2788\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.8994 - val_loss: 1.2787\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8993 - val_loss: 1.2785\n",
      "Epoch 51/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8992 - val_loss: 1.2783\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8992 - val_loss: 1.2781\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8991 - val_loss: 1.2780\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8990 - val_loss: 1.2778\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2776\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2774\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.8988 - val_loss: 1.2772\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8987 - val_loss: 1.2771\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8986 - val_loss: 1.2769\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8985 - val_loss: 1.2767\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8985 - val_loss: 1.2765\n",
      "Epoch 62/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8984 - val_loss: 1.2763\n",
      "Epoch 63/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8983 - val_loss: 1.2762\n",
      "Epoch 64/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8983 - val_loss: 1.2760\n",
      "Epoch 65/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.8982 - val_loss: 1.2758\n",
      "Epoch 66/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8981 - val_loss: 1.2756\n",
      "Epoch 67/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8981 - val_loss: 1.2754\n",
      "Epoch 68/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.8980 - val_loss: 1.2753\n",
      "Execution time:  283.69112038612366\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9187\n",
      "Root Mean Square Error: 1.1143\n",
      "Mean Square Error: 1.2417\n",
      "\n",
      "Train RMSE: 1.114\n",
      "Train MSE: 1.242\n",
      "Train MAE: 0.919\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_139\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_55 (GRU)                 (None, 144, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_139 (TimeDi (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 19s 54ms/step - loss: 0.8976 - val_loss: 1.1373\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8973 - val_loss: 1.1367\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8970 - val_loss: 1.1361\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8967 - val_loss: 1.1355\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8963 - val_loss: 1.1348\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8959 - val_loss: 1.1341\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8955 - val_loss: 1.1333\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8951 - val_loss: 1.1326\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8947 - val_loss: 1.1318\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8942 - val_loss: 1.1310\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8938 - val_loss: 1.1302\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8933 - val_loss: 1.1293\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8928 - val_loss: 1.1285\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8924 - val_loss: 1.1276\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8919 - val_loss: 1.1268\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8914 - val_loss: 1.1259\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 19s 54ms/step - loss: 0.8909 - val_loss: 1.1250\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8904 - val_loss: 1.1241\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8899 - val_loss: 1.1231\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8894 - val_loss: 1.1222\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8889 - val_loss: 1.1212\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8884 - val_loss: 1.1203\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8878 - val_loss: 1.1193\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8873 - val_loss: 1.1183\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8867 - val_loss: 1.1173\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8862 - val_loss: 1.1163\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.8856 - val_loss: 1.1153\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8851 - val_loss: 1.1142\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8845 - val_loss: 1.1132\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8839 - val_loss: 1.1121\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8833 - val_loss: 1.1110\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8827 - val_loss: 1.1099\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8822 - val_loss: 1.1088\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 18s 54ms/step - loss: 0.8815 - val_loss: 1.1077\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.8809 - val_loss: 1.1066\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.8803 - val_loss: 1.1054\n",
      "Execution time:  659.218207359314\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9098\n",
      "Root Mean Square Error: 1.1077\n",
      "Mean Square Error: 1.2270\n",
      "\n",
      "Train RMSE: 1.108\n",
      "Train MSE: 1.227\n",
      "Train MAE: 0.910\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_140\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_56 (GRU)                 (None, 144, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_140 (TimeDi (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 60ms/step - loss: 0.6632 - val_loss: 0.5478\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5915 - val_loss: 0.2371\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5816 - val_loss: 0.2284\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5793 - val_loss: 0.2266\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5777 - val_loss: 0.2249\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5764 - val_loss: 0.2237\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5752 - val_loss: 0.2227\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5745 - val_loss: 0.2225\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.5738 - val_loss: 0.2221\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5734 - val_loss: 0.2221\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5725 - val_loss: 0.2221\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5719 - val_loss: 0.2221\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5711 - val_loss: 0.2225\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5705 - val_loss: 0.2222\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5702 - val_loss: 0.2230\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5696 - val_loss: 0.2237\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5690 - val_loss: 0.2233\n",
      "Epoch 18/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.5683 - val_loss: 0.2242\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.5678 - val_loss: 0.2246\n",
      "Execution time:  81.1926691532135\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2799\n",
      "Root Mean Square Error: 0.6787\n",
      "Mean Square Error: 0.4607\n",
      "\n",
      "Train RMSE: 0.679\n",
      "Train MSE: 0.461\n",
      "Train MAE: 0.280\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_141\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_57 (GRU)                 (None, 144, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_141 (TimeDi (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 19s 55ms/step - loss: 0.6136 - val_loss: 0.4010\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5783 - val_loss: 0.4061\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5738 - val_loss: 0.4088\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5711 - val_loss: 0.4100\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5692 - val_loss: 0.4102\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5675 - val_loss: 0.4102\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5655 - val_loss: 0.4108\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5646 - val_loss: 0.4105\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5628 - val_loss: 0.4109\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5618 - val_loss: 0.4097\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.5600 - val_loss: 0.4089\n",
      "Execution time:  204.0517075061798\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2568\n",
      "Root Mean Square Error: 0.6574\n",
      "Mean Square Error: 0.4322\n",
      "\n",
      "Train RMSE: 0.657\n",
      "Train MSE: 0.432\n",
      "Train MAE: 0.257\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_142\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_58 (GRU)                 (None, 144, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 144, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_142 (TimeDi (None, 144, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "77/77 [==============================] - 5s 63ms/step - loss: 0.8919 - val_loss: 1.2114\n",
      "Epoch 2/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.8036 - val_loss: 0.8898\n",
      "Epoch 3/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7302 - val_loss: 0.8500\n",
      "Epoch 4/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7150 - val_loss: 0.8341\n",
      "Epoch 5/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.7066 - val_loss: 0.8243\n",
      "Epoch 6/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.7010 - val_loss: 0.8171\n",
      "Epoch 7/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6968 - val_loss: 0.8120\n",
      "Epoch 8/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6939 - val_loss: 0.8085\n",
      "Epoch 9/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6916 - val_loss: 0.8059\n",
      "Epoch 10/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6899 - val_loss: 0.8039\n",
      "Epoch 11/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6884 - val_loss: 0.8025\n",
      "Epoch 12/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6871 - val_loss: 0.8013\n",
      "Epoch 13/68\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.6860 - val_loss: 0.8003\n",
      "Epoch 14/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6849 - val_loss: 0.7996\n",
      "Epoch 15/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6840 - val_loss: 0.7989\n",
      "Epoch 16/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6831 - val_loss: 0.7984\n",
      "Epoch 17/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6822 - val_loss: 0.7979\n",
      "Epoch 18/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6813 - val_loss: 0.7974\n",
      "Epoch 19/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6805 - val_loss: 0.7970\n",
      "Epoch 20/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6798 - val_loss: 0.7967\n",
      "Epoch 21/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6789 - val_loss: 0.7963\n",
      "Epoch 22/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6782 - val_loss: 0.7960\n",
      "Epoch 23/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6775 - val_loss: 0.7957\n",
      "Epoch 24/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6768 - val_loss: 0.7954\n",
      "Epoch 25/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6761 - val_loss: 0.7952\n",
      "Epoch 26/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6755 - val_loss: 0.7950\n",
      "Epoch 27/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6748 - val_loss: 0.7948\n",
      "Epoch 28/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6742 - val_loss: 0.7946\n",
      "Epoch 29/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6736 - val_loss: 0.7944\n",
      "Epoch 30/68\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.6730 - val_loss: 0.7942\n",
      "Epoch 31/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6724 - val_loss: 0.7941\n",
      "Epoch 32/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6719 - val_loss: 0.7939\n",
      "Epoch 33/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6712 - val_loss: 0.7938\n",
      "Epoch 34/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6706 - val_loss: 0.7936\n",
      "Epoch 35/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6701 - val_loss: 0.7935\n",
      "Epoch 36/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6695 - val_loss: 0.7934\n",
      "Epoch 37/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6691 - val_loss: 0.7933\n",
      "Epoch 38/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6685 - val_loss: 0.7932\n",
      "Epoch 39/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6680 - val_loss: 0.7931\n",
      "Epoch 40/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6675 - val_loss: 0.7930\n",
      "Epoch 41/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6669 - val_loss: 0.7929\n",
      "Epoch 42/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6664 - val_loss: 0.7928\n",
      "Epoch 43/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6660 - val_loss: 0.7928\n",
      "Epoch 44/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6655 - val_loss: 0.7927\n",
      "Epoch 45/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6649 - val_loss: 0.7926\n",
      "Epoch 46/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6646 - val_loss: 0.7925\n",
      "Epoch 47/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6641 - val_loss: 0.7924\n",
      "Epoch 48/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6635 - val_loss: 0.7923\n",
      "Epoch 49/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6630 - val_loss: 0.7923\n",
      "Epoch 50/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6625 - val_loss: 0.7922\n",
      "Epoch 51/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6620 - val_loss: 0.7921\n",
      "Epoch 52/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6614 - val_loss: 0.7920\n",
      "Epoch 53/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6609 - val_loss: 0.7920\n",
      "Epoch 54/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6603 - val_loss: 0.7919\n",
      "Epoch 55/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6595 - val_loss: 0.7919\n",
      "Epoch 56/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6589 - val_loss: 0.7918\n",
      "Epoch 57/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6582 - val_loss: 0.7917\n",
      "Epoch 58/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6574 - val_loss: 0.7917\n",
      "Epoch 59/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6567 - val_loss: 0.7917\n",
      "Epoch 60/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6560 - val_loss: 0.7916\n",
      "Epoch 61/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6552 - val_loss: 0.7916\n",
      "Epoch 62/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6540 - val_loss: 0.7916\n",
      "Epoch 63/68\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.6505 - val_loss: 0.7916\n",
      "Epoch 64/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6501 - val_loss: 0.7916\n",
      "Epoch 65/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6499 - val_loss: 0.7916\n",
      "Epoch 66/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6489 - val_loss: 0.7916\n",
      "Epoch 67/68\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.6476 - val_loss: 0.7916\n",
      "Epoch 68/68\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.6474 - val_loss: 0.7916\n",
      "Execution time:  283.4575936794281\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6195\n",
      "Root Mean Square Error: 0.9082\n",
      "Mean Square Error: 0.8248\n",
      "\n",
      "Train RMSE: 0.908\n",
      "Train MSE: 0.825\n",
      "Train MAE: 0.619\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_143\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_59 (GRU)                 (None, 144, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 144, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_143 (TimeDi (None, 144, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "342/342 [==============================] - 19s 54ms/step - loss: 0.7670 - val_loss: 0.7488\n",
      "Epoch 2/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.7253 - val_loss: 0.7443\n",
      "Epoch 3/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.7198 - val_loss: 0.7423\n",
      "Epoch 4/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6994 - val_loss: 0.7455\n",
      "Epoch 5/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6853 - val_loss: 0.7429\n",
      "Epoch 6/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6837 - val_loss: 0.7403\n",
      "Epoch 7/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6823 - val_loss: 0.7377\n",
      "Epoch 8/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6803 - val_loss: 0.7351\n",
      "Epoch 9/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6780 - val_loss: 0.7323\n",
      "Epoch 10/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6767 - val_loss: 0.7301\n",
      "Epoch 11/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6755 - val_loss: 0.7283\n",
      "Epoch 12/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6740 - val_loss: 0.7266\n",
      "Epoch 13/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6725 - val_loss: 0.7250\n",
      "Epoch 14/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6710 - val_loss: 0.7243\n",
      "Epoch 15/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6701 - val_loss: 0.7224\n",
      "Epoch 16/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6684 - val_loss: 0.7218\n",
      "Epoch 17/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6678 - val_loss: 0.7199\n",
      "Epoch 18/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6668 - val_loss: 0.7195\n",
      "Epoch 19/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6662 - val_loss: 0.7178\n",
      "Epoch 20/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6649 - val_loss: 0.7177\n",
      "Epoch 21/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6634 - val_loss: 0.7168\n",
      "Epoch 22/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6630 - val_loss: 0.7157\n",
      "Epoch 23/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6619 - val_loss: 0.7154\n",
      "Epoch 24/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6608 - val_loss: 0.7144\n",
      "Epoch 25/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6600 - val_loss: 0.7137\n",
      "Epoch 26/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6590 - val_loss: 0.7132\n",
      "Epoch 27/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6599 - val_loss: 0.7126\n",
      "Epoch 28/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6576 - val_loss: 0.7121\n",
      "Epoch 29/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6550 - val_loss: 0.7117\n",
      "Epoch 30/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6552 - val_loss: 0.7112\n",
      "Epoch 31/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6542 - val_loss: 0.7104\n",
      "Epoch 32/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6542 - val_loss: 0.7103\n",
      "Epoch 33/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6518 - val_loss: 0.7096\n",
      "Epoch 34/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6529 - val_loss: 0.7089\n",
      "Epoch 35/36\n",
      "342/342 [==============================] - 18s 53ms/step - loss: 0.6538 - val_loss: 0.7093\n",
      "Epoch 36/36\n",
      "342/342 [==============================] - 18s 52ms/step - loss: 0.6499 - val_loss: 0.7085\n",
      "Execution time:  651.64852643013\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6304\n",
      "Root Mean Square Error: 0.9240\n",
      "Mean Square Error: 0.8537\n",
      "\n",
      "Train RMSE: 0.924\n",
      "Train MSE: 0.854\n",
      "Train MAE: 0.630\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_144\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_60 (GRU)                 (None, 432, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_144 (TimeDi (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 12s 168ms/step - loss: 0.6512 - val_loss: 0.5451\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6206 - val_loss: 0.3505\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6209 - val_loss: 0.3727\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6154 - val_loss: 0.3703\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6133 - val_loss: 0.3711\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6117 - val_loss: 0.3726\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 12s 174ms/step - loss: 0.6101 - val_loss: 0.3737\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6087 - val_loss: 0.3747\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6074 - val_loss: 0.3775\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6059 - val_loss: 0.3813\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 12s 170ms/step - loss: 0.6043 - val_loss: 0.3887\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6014 - val_loss: 0.4051\n",
      "Execution time:  153.90134501457214\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5390\n",
      "Root Mean Square Error: 0.9866\n",
      "Mean Square Error: 0.9734\n",
      "\n",
      "Train RMSE: 0.987\n",
      "Train MSE: 0.973\n",
      "Train MAE: 0.539\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_145\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_61 (GRU)                 (None, 432, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_145 (TimeDi (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 46s 145ms/step - loss: 0.6645 - val_loss: 0.3214\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6557 - val_loss: 0.3085\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6437 - val_loss: 0.3834\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6358 - val_loss: 0.3631\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6322 - val_loss: 0.3620\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6296 - val_loss: 0.3304\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6270 - val_loss: 0.3210\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6406 - val_loss: 0.3264\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6259 - val_loss: 0.3212\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6299 - val_loss: 0.3629\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6248 - val_loss: 0.3792\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6221 - val_loss: 0.3719\n",
      "Execution time:  547.9326717853546\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5462\n",
      "Root Mean Square Error: 0.9696\n",
      "Mean Square Error: 0.9401\n",
      "\n",
      "Train RMSE: 0.970\n",
      "Train MSE: 0.940\n",
      "Train MAE: 0.546\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_146\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_62 (GRU)                 (None, 432, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_146 (TimeDi (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.8856 - val_loss: 1.1049\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.7238 - val_loss: 0.8069\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6774 - val_loss: 0.7934\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6745 - val_loss: 0.7889\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6732 - val_loss: 0.7866\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6724 - val_loss: 0.7851\n",
      "Epoch 7/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6719 - val_loss: 0.7841\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6716 - val_loss: 0.7835\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6713 - val_loss: 0.7829\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6711 - val_loss: 0.7825\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6709 - val_loss: 0.7822\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6708 - val_loss: 0.7820\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6707 - val_loss: 0.7818\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6706 - val_loss: 0.7816\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6705 - val_loss: 0.7814\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6704 - val_loss: 0.7813\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6704 - val_loss: 0.7812\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7811\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7810\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6702 - val_loss: 0.7809\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6702 - val_loss: 0.7808\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6702 - val_loss: 0.7808\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6701 - val_loss: 0.7806\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6701 - val_loss: 0.7806\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6700 - val_loss: 0.7804\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6700 - val_loss: 0.7804\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6699 - val_loss: 0.7804\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7802\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 13s 178ms/step - loss: 0.6697 - val_loss: 0.7800\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6696 - val_loss: 0.7797\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6696 - val_loss: 0.7797\n",
      "Execution time:  908.5840327739716\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6862\n",
      "Root Mean Square Error: 0.9968\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_147\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_63 (GRU)                 (None, 432, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_147 (TimeDi (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.7748 - val_loss: 0.6152\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.7075 - val_loss: 0.6102\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.6997 - val_loss: 0.6092\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6995 - val_loss: 0.6089\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 51s 160ms/step - loss: 0.6994 - val_loss: 0.6086\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 49s 156ms/step - loss: 0.6993 - val_loss: 0.6085\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6992 - val_loss: 0.6084\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6992 - val_loss: 0.6083\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 45s 141ms/step - loss: 0.6991 - val_loss: 0.6082\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6991 - val_loss: 0.6081\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6990 - val_loss: 0.6080\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6990 - val_loss: 0.6080\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6990 - val_loss: 0.6079\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6990 - val_loss: 0.6079\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 45s 141ms/step - loss: 0.6990 - val_loss: 0.6078\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6078\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6075\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Execution time:  1643.392021894455\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6862\n",
      "Root Mean Square Error: 0.9968\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_148\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_64 (GRU)                 (None, 432, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_148 (TimeDi (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6778 - val_loss: 0.8169\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6778 - val_loss: 0.8166\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6776 - val_loss: 0.8163\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6775 - val_loss: 0.8160\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6775 - val_loss: 0.8157\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6773 - val_loss: 0.8153\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6773 - val_loss: 0.8150\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6771 - val_loss: 0.8146\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6770 - val_loss: 0.8143\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 187ms/step - loss: 0.6769 - val_loss: 0.8139\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6768 - val_loss: 0.8135\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6767 - val_loss: 0.8131\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6766 - val_loss: 0.8128\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6765 - val_loss: 0.8124\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6764 - val_loss: 0.8120\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6762 - val_loss: 0.8116\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6761 - val_loss: 0.8112\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6759 - val_loss: 0.8108\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6758 - val_loss: 0.8104\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6757 - val_loss: 0.8100\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6756 - val_loss: 0.8096\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6755 - val_loss: 0.8092\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6753 - val_loss: 0.8088\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6753 - val_loss: 0.8084\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6751 - val_loss: 0.8080\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6750 - val_loss: 0.8076\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6748 - val_loss: 0.8072\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6747 - val_loss: 0.8068\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6746 - val_loss: 0.8063\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6744 - val_loss: 0.8059\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6744 - val_loss: 0.8055\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6742 - val_loss: 0.8051\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6740 - val_loss: 0.8047\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6740 - val_loss: 0.8042\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6738 - val_loss: 0.8038\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6737 - val_loss: 0.8034\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6735 - val_loss: 0.8030\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6735 - val_loss: 0.8025\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6733 - val_loss: 0.8021\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6731 - val_loss: 0.8017\n",
      "Epoch 41/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6730 - val_loss: 0.8012\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6729 - val_loss: 0.8008\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.6727 - val_loss: 0.8004\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6726 - val_loss: 0.7999\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6724 - val_loss: 0.7995\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6723 - val_loss: 0.7991\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6722 - val_loss: 0.7986\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6720 - val_loss: 0.7982\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6719 - val_loss: 0.7978\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6718 - val_loss: 0.7973\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6717 - val_loss: 0.7969\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6715 - val_loss: 0.7964\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6714 - val_loss: 0.7960\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6712 - val_loss: 0.7956\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 14s 188ms/step - loss: 0.6711 - val_loss: 0.7951\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6710 - val_loss: 0.7947\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.6708 - val_loss: 0.7942\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6707 - val_loss: 0.7938\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6706 - val_loss: 0.7933\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6704 - val_loss: 0.7929\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6703 - val_loss: 0.7925\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 13s 187ms/step - loss: 0.6701 - val_loss: 0.7920\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6701 - val_loss: 0.7916\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6699 - val_loss: 0.7911\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6698 - val_loss: 0.7907\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 13s 186ms/step - loss: 0.6696 - val_loss: 0.7902\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6694 - val_loss: 0.7898\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6693 - val_loss: 0.7893\n",
      "Execution time:  911.485524892807\n",
      "GRU:\n",
      "Mean Absolute Error: 0.7125\n",
      "Root Mean Square Error: 1.0362\n",
      "Mean Square Error: 1.0736\n",
      "\n",
      "Train RMSE: 1.036\n",
      "Train MSE: 1.074\n",
      "Train MAE: 0.713\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_149\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_65 (GRU)                 (None, 432, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_149 (TimeDi (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6995 - val_loss: 0.6094\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6991 - val_loss: 0.6081\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6987 - val_loss: 0.6066\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6982 - val_loss: 0.6049\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6977 - val_loss: 0.6033\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6971 - val_loss: 0.6015\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6966 - val_loss: 0.5997\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6960 - val_loss: 0.5978\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6954 - val_loss: 0.5959\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6948 - val_loss: 0.5939\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6942 - val_loss: 0.5919\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6935 - val_loss: 0.5898\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6929 - val_loss: 0.5877\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6923 - val_loss: 0.5856\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6916 - val_loss: 0.5835\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6909 - val_loss: 0.5813\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6903 - val_loss: 0.5791\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6895 - val_loss: 0.5769\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6889 - val_loss: 0.5747\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6882 - val_loss: 0.5725\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6875 - val_loss: 0.5703\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6869 - val_loss: 0.5681\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6863 - val_loss: 0.5659\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6856 - val_loss: 0.5639\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6850 - val_loss: 0.5620\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6844 - val_loss: 0.5601\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6839 - val_loss: 0.5583\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6834 - val_loss: 0.5565\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6829 - val_loss: 0.5547\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6824 - val_loss: 0.5530\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6819 - val_loss: 0.5513\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6814 - val_loss: 0.5495\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6809 - val_loss: 0.5478\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6805 - val_loss: 0.5461\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6800 - val_loss: 0.5444\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6795 - val_loss: 0.5427\n",
      "Execution time:  1631.1546838283539\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6410\n",
      "Root Mean Square Error: 0.9688\n",
      "Mean Square Error: 0.9385\n",
      "\n",
      "Train RMSE: 0.969\n",
      "Train MSE: 0.938\n",
      "Train MAE: 0.641\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_150\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_66 (GRU)                 (None, 432, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_150 (TimeDi (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 12s 173ms/step - loss: 0.9007 - val_loss: 1.2721\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.9006 - val_loss: 1.2720\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.9006 - val_loss: 1.2719\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.9006 - val_loss: 1.2718\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.9005 - val_loss: 1.2718\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.9005 - val_loss: 1.2717\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.9004 - val_loss: 1.2716\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.9004 - val_loss: 1.2715\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.9004 - val_loss: 1.2714\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 178ms/step - loss: 0.9003 - val_loss: 1.2712\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.9003 - val_loss: 1.2711\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.9002 - val_loss: 1.2710\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.9002 - val_loss: 1.2709\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.9001 - val_loss: 1.2708\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.9000 - val_loss: 1.2707\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.9000 - val_loss: 1.2706\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.9000 - val_loss: 1.2705\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8999 - val_loss: 1.2703\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8998 - val_loss: 1.2702\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8998 - val_loss: 1.2701\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8997 - val_loss: 1.2700\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8997 - val_loss: 1.2699\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8996 - val_loss: 1.2697\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8996 - val_loss: 1.2696\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8995 - val_loss: 1.2695\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8994 - val_loss: 1.2693\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8994 - val_loss: 1.2692\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 13s 178ms/step - loss: 0.8993 - val_loss: 1.2691\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.8993 - val_loss: 1.2690\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8992 - val_loss: 1.2688\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8992 - val_loss: 1.2687\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8991 - val_loss: 1.2686\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8990 - val_loss: 1.2684\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8990 - val_loss: 1.2683\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8989 - val_loss: 1.2682\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8988 - val_loss: 1.2680\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.8988 - val_loss: 1.2679\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8987 - val_loss: 1.2678\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8987 - val_loss: 1.2676\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8986 - val_loss: 1.2675\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8985 - val_loss: 1.2673\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8985 - val_loss: 1.2672\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8984 - val_loss: 1.2671\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8983 - val_loss: 1.2669\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.8983 - val_loss: 1.2668\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.8982 - val_loss: 1.2666\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8981 - val_loss: 1.2665\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8980 - val_loss: 1.2664\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8980 - val_loss: 1.2662\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8979 - val_loss: 1.2661\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8979 - val_loss: 1.2659\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8978 - val_loss: 1.2658\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8977 - val_loss: 1.2656\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8977 - val_loss: 1.2655\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8976 - val_loss: 1.2653\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8975 - val_loss: 1.2652\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8974 - val_loss: 1.2651\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8974 - val_loss: 1.2649\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8973 - val_loss: 1.2648\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.8972 - val_loss: 1.2646\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8972 - val_loss: 1.2645\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8971 - val_loss: 1.2643\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8970 - val_loss: 1.2642\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 0.8970 - val_loss: 1.2640\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.8969 - val_loss: 1.2639\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.8968 - val_loss: 1.2637\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.8968 - val_loss: 1.2636\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 0.8967 - val_loss: 1.2634\n",
      "Execution time:  905.6367120742798\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9031\n",
      "Root Mean Square Error: 1.1018\n",
      "Mean Square Error: 1.2140\n",
      "\n",
      "Train RMSE: 1.102\n",
      "Train MSE: 1.214\n",
      "Train MAE: 0.903\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_151\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_67 (GRU)                 (None, 432, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_151 (TimeDi (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 48s 152ms/step - loss: 0.9021 - val_loss: 1.0917\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.9020 - val_loss: 1.0913\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.9018 - val_loss: 1.0908\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 45s 144ms/step - loss: 0.9015 - val_loss: 1.0903\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.9013 - val_loss: 1.0898\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.9010 - val_loss: 1.0892\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.9008 - val_loss: 1.0886\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.9005 - val_loss: 1.0880\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.9002 - val_loss: 1.0873\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8999 - val_loss: 1.0867\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8996 - val_loss: 1.0860\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8992 - val_loss: 1.0853\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8989 - val_loss: 1.0845\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8985 - val_loss: 1.0838\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8982 - val_loss: 1.0831\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8978 - val_loss: 1.0823\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8975 - val_loss: 1.0815\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8971 - val_loss: 1.0807\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8967 - val_loss: 1.0799\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8963 - val_loss: 1.0790\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8959 - val_loss: 1.0782\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8955 - val_loss: 1.0773\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8951 - val_loss: 1.0765\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8946 - val_loss: 1.0756\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8942 - val_loss: 1.0747\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8938 - val_loss: 1.0737\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8933 - val_loss: 1.0728\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8929 - val_loss: 1.0718\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8924 - val_loss: 1.0709\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8919 - val_loss: 1.0699\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8915 - val_loss: 1.0689\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8910 - val_loss: 1.0678\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8905 - val_loss: 1.0668\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8900 - val_loss: 1.0657\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.8895 - val_loss: 1.0647\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 46s 144ms/step - loss: 0.8889 - val_loss: 1.0636\n",
      "Execution time:  1647.4934480190277\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9025\n",
      "Root Mean Square Error: 1.1050\n",
      "Mean Square Error: 1.2209\n",
      "\n",
      "Train RMSE: 1.105\n",
      "Train MSE: 1.221\n",
      "Train MAE: 0.903\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_152\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_68 (GRU)                 (None, 432, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_152 (TimeDi (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6662 - val_loss: 0.7085\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 0.6291 - val_loss: 0.5535\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6074 - val_loss: 0.4236\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6085 - val_loss: 0.4031\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 184ms/step - loss: 0.6089 - val_loss: 0.4006\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 0.6085 - val_loss: 0.4003\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6080 - val_loss: 0.4003\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6076 - val_loss: 0.4002\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6073 - val_loss: 0.3999\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6069 - val_loss: 0.3996\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6066 - val_loss: 0.3993\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6062 - val_loss: 0.3988\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6059 - val_loss: 0.3982\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6056 - val_loss: 0.3975\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6052 - val_loss: 0.3966\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6049 - val_loss: 0.3959\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6047 - val_loss: 0.3952\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6044 - val_loss: 0.3946\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6041 - val_loss: 0.3938\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6039 - val_loss: 0.3930\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6034 - val_loss: 0.3920\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 0.6032 - val_loss: 0.3913\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6027 - val_loss: 0.3904\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6025 - val_loss: 0.3897\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6022 - val_loss: 0.3888\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6019 - val_loss: 0.3880\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6016 - val_loss: 0.3874\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 12s 171ms/step - loss: 0.6013 - val_loss: 0.3870\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 12s 170ms/step - loss: 0.6011 - val_loss: 0.3867\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6008 - val_loss: 0.3867\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6006 - val_loss: 0.3865\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6004 - val_loss: 0.3868\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6000 - val_loss: 0.3871\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.5998 - val_loss: 0.3877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.5995 - val_loss: 0.3887\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.5992 - val_loss: 0.3899\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.5989 - val_loss: 0.3918\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.5985 - val_loss: 0.3940\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.5981 - val_loss: 0.3969\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.5975 - val_loss: 0.4005\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.5968 - val_loss: 0.4052\n",
      "Execution time:  529.8658058643341\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5398\n",
      "Root Mean Square Error: 0.9892\n",
      "Mean Square Error: 0.9786\n",
      "\n",
      "Train RMSE: 0.989\n",
      "Train MSE: 0.979\n",
      "Train MAE: 0.540\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_153\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_69 (GRU)                 (None, 432, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_153 (Dropout)        (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_153 (TimeDi (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 46s 145ms/step - loss: 0.6673 - val_loss: 0.2950\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6537 - val_loss: 0.2861\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6516 - val_loss: 0.2802\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 46s 145ms/step - loss: 0.6520 - val_loss: 0.2769\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6521 - val_loss: 0.2754\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6516 - val_loss: 0.2755\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6506 - val_loss: 0.2861\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6428 - val_loss: 0.3439\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6326 - val_loss: 0.3557\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6296 - val_loss: 0.3591\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6286 - val_loss: 0.3600\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6282 - val_loss: 0.3607\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6272 - val_loss: 0.3633\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6262 - val_loss: 0.3654\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6268 - val_loss: 0.3650\n",
      "Execution time:  684.8749639987946\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5534\n",
      "Root Mean Square Error: 0.9898\n",
      "Mean Square Error: 0.9797\n",
      "\n",
      "Train RMSE: 0.990\n",
      "Train MSE: 0.980\n",
      "Train MAE: 0.553\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_154\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_70 (GRU)                 (None, 432, 40)           5160      \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 432, 40)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_154 (TimeDi (None, 432, 1)            41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "72/72 [==============================] - 12s 166ms/step - loss: 0.8926 - val_loss: 1.2355\n",
      "Epoch 2/68\n",
      "72/72 [==============================] - 12s 171ms/step - loss: 0.8258 - val_loss: 0.9347\n",
      "Epoch 3/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6957 - val_loss: 0.8156\n",
      "Epoch 4/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6811 - val_loss: 0.8014\n",
      "Epoch 5/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6776 - val_loss: 0.7954\n",
      "Epoch 6/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6758 - val_loss: 0.7920\n",
      "Epoch 7/68\n",
      "72/72 [==============================] - 12s 171ms/step - loss: 0.6746 - val_loss: 0.7898\n",
      "Epoch 8/68\n",
      "72/72 [==============================] - 12s 174ms/step - loss: 0.6738 - val_loss: 0.7882\n",
      "Epoch 9/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6733 - val_loss: 0.7870\n",
      "Epoch 10/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6728 - val_loss: 0.7861\n",
      "Epoch 11/68\n",
      "72/72 [==============================] - 12s 170ms/step - loss: 0.6724 - val_loss: 0.7854\n",
      "Epoch 12/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6721 - val_loss: 0.7848\n",
      "Epoch 13/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6719 - val_loss: 0.7842\n",
      "Epoch 14/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6717 - val_loss: 0.7838\n",
      "Epoch 15/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6715 - val_loss: 0.7835\n",
      "Epoch 16/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6713 - val_loss: 0.7831\n",
      "Epoch 17/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6712 - val_loss: 0.7829\n",
      "Epoch 18/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6711 - val_loss: 0.7826\n",
      "Epoch 19/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6710 - val_loss: 0.7824\n",
      "Epoch 20/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6709 - val_loss: 0.7822\n",
      "Epoch 21/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6708 - val_loss: 0.7820\n",
      "Epoch 22/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6707 - val_loss: 0.7818\n",
      "Epoch 23/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6706 - val_loss: 0.7817\n",
      "Epoch 24/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6706 - val_loss: 0.7816\n",
      "Epoch 25/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6705 - val_loss: 0.7814\n",
      "Epoch 26/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6704 - val_loss: 0.7813\n",
      "Epoch 27/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6704 - val_loss: 0.7812\n",
      "Epoch 28/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6703 - val_loss: 0.7811\n",
      "Epoch 29/68\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 0.6703 - val_loss: 0.7810\n",
      "Epoch 30/68\n",
      "72/72 [==============================] - 12s 174ms/step - loss: 0.6703 - val_loss: 0.7810\n",
      "Epoch 31/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6702 - val_loss: 0.7809\n",
      "Epoch 32/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6702 - val_loss: 0.7808\n",
      "Epoch 33/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 34/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6701 - val_loss: 0.7807\n",
      "Epoch 35/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6701 - val_loss: 0.7806\n",
      "Epoch 36/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6700 - val_loss: 0.7806\n",
      "Epoch 37/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 38/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6700 - val_loss: 0.7805\n",
      "Epoch 39/68\n",
      "72/72 [==============================] - 12s 171ms/step - loss: 0.6700 - val_loss: 0.7804\n",
      "Epoch 40/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7804\n",
      "Epoch 41/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 42/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7803\n",
      "Epoch 43/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 44/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 45/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7802\n",
      "Epoch 46/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 47/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 48/68\n",
      "72/72 [==============================] - 12s 171ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 49/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6698 - val_loss: 0.7801\n",
      "Epoch 50/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 51/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 52/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7800\n",
      "Epoch 53/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 54/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 55/68\n",
      "72/72 [==============================] - 12s 173ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 56/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 57/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6697 - val_loss: 0.7799\n",
      "Epoch 58/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 59/68\n",
      "72/72 [==============================] - 13s 174ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 60/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 61/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 62/68\n",
      "72/72 [==============================] - 12s 171ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 63/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 64/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6697 - val_loss: 0.7798\n",
      "Epoch 65/68\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.6696 - val_loss: 0.7797\n",
      "Epoch 66/68\n",
      "72/72 [==============================] - 13s 175ms/step - loss: 0.6696 - val_loss: 0.7797\n",
      "Epoch 67/68\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 0.6696 - val_loss: 0.7797\n",
      "Epoch 68/68\n",
      "72/72 [==============================] - 12s 172ms/step - loss: 0.6696 - val_loss: 0.7797\n",
      "Execution time:  865.2246866226196\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6862\n",
      "Root Mean Square Error: 0.9968\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_155\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_71 (GRU)                 (None, 432, 55)           9570      \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 432, 55)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_155 (TimeDi (None, 432, 1)            56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "317/317 [==============================] - 46s 145ms/step - loss: 0.8161 - val_loss: 0.6198\n",
      "Epoch 2/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.7021 - val_loss: 0.6129\n",
      "Epoch 3/36\n",
      "317/317 [==============================] - 45s 141ms/step - loss: 0.7006 - val_loss: 0.6110\n",
      "Epoch 4/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.7000 - val_loss: 0.6101\n",
      "Epoch 5/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6997 - val_loss: 0.6095\n",
      "Epoch 6/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6995 - val_loss: 0.6091\n",
      "Epoch 7/36\n",
      "317/317 [==============================] - 45s 141ms/step - loss: 0.6994 - val_loss: 0.6088\n",
      "Epoch 8/36\n",
      "317/317 [==============================] - 45s 141ms/step - loss: 0.6993 - val_loss: 0.6086\n",
      "Epoch 9/36\n",
      "317/317 [==============================] - 47s 148ms/step - loss: 0.6992 - val_loss: 0.6084\n",
      "Epoch 10/36\n",
      "317/317 [==============================] - 46s 145ms/step - loss: 0.6991 - val_loss: 0.6082\n",
      "Epoch 11/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6991 - val_loss: 0.6081\n",
      "Epoch 12/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6990 - val_loss: 0.6080\n",
      "Epoch 13/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6990 - val_loss: 0.6079\n",
      "Epoch 14/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6078\n",
      "Epoch 15/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 16/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6077\n",
      "Epoch 17/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 18/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 19/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 20/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076\n",
      "Epoch 21/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6075\n",
      "Epoch 22/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 23/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 24/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 25/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 26/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 27/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 28/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 29/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 30/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 31/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 32/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 33/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 34/36\n",
      "317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 35/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Epoch 36/36\n",
      "317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075\n",
      "Execution time:  1635.3623416423798\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6862\n",
      "Root Mean Square Error: 0.9968\n",
      "Mean Square Error: 0.9937\n",
      "\n",
      "Train RMSE: 0.997\n",
      "Train MSE: 0.994\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_72 (GRU)                 (None, 1008, 40)          5160      \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_156 (TimeDi (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 0.6499 - val_loss: 0.6928\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 23s 384ms/step - loss: 0.5896 - val_loss: 0.5294\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.5761 - val_loss: 0.4939\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5686 - val_loss: 0.4713\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5656 - val_loss: 0.4615\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5634 - val_loss: 0.4542\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5619 - val_loss: 0.4489\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5607 - val_loss: 0.4441\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5597 - val_loss: 0.4407\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5589 - val_loss: 0.4381\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5582 - val_loss: 0.4362\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.5573 - val_loss: 0.4344\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.5567 - val_loss: 0.4333\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5560 - val_loss: 0.4331\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.5552 - val_loss: 0.4334\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.5544 - val_loss: 0.4343\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5531 - val_loss: 0.4360\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 25s 422ms/step - loss: 0.5515 - val_loss: 0.4401\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 25s 415ms/step - loss: 0.5492 - val_loss: 0.4536\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.5453 - val_loss: 0.4695\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.5421 - val_loss: 0.4977\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5393 - val_loss: 0.5125\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5377 - val_loss: 0.5218\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5365 - val_loss: 0.5250\n",
      "Execution time:  593.1951131820679\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6514\n",
      "Root Mean Square Error: 1.1407\n",
      "Mean Square Error: 1.3011\n",
      "\n",
      "Train RMSE: 1.141\n",
      "Train MSE: 1.301\n",
      "Train MAE: 0.651\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_157\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_73 (GRU)                 (None, 1008, 55)          9570      \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_157 (TimeDi (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 91s 343ms/step - loss: 0.6325 - val_loss: 0.3807\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.6060 - val_loss: 0.3240\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6010 - val_loss: 0.3170\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5978 - val_loss: 0.3119\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5950 - val_loss: 0.3215\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.5934 - val_loss: 0.3213\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.5907 - val_loss: 0.3320\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5882 - val_loss: 0.3482\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.5855 - val_loss: 0.3600\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 91s 340ms/step - loss: 0.5818 - val_loss: 0.3669\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.5740 - val_loss: 0.3955\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.5712 - val_loss: 0.4132\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5692 - val_loss: 0.4284\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5683 - val_loss: 0.4398\n",
      "Execution time:  1273.9686002731323\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6702\n",
      "Root Mean Square Error: 1.1418\n",
      "Mean Square Error: 1.3037\n",
      "\n",
      "Train RMSE: 1.142\n",
      "Train MSE: 1.304\n",
      "Train MAE: 0.670\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_158\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_74 (GRU)                 (None, 1008, 40)          5160      \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_158 (TimeDi (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 23s 388ms/step - loss: 0.9570 - val_loss: 1.2026\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 23s 385ms/step - loss: 0.7150 - val_loss: 0.8351\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 23s 387ms/step - loss: 0.6790 - val_loss: 0.8281\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 23s 386ms/step - loss: 0.6766 - val_loss: 0.8256\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 23s 386ms/step - loss: 0.6754 - val_loss: 0.8242\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.6748 - val_loss: 0.8234\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 23s 385ms/step - loss: 0.6743 - val_loss: 0.8229\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.6740 - val_loss: 0.8225\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 23s 386ms/step - loss: 0.6738 - val_loss: 0.8222\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 23s 385ms/step - loss: 0.6737 - val_loss: 0.8220\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 0.6735 - val_loss: 0.8218\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 23s 383ms/step - loss: 0.6734 - val_loss: 0.8217\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6733 - val_loss: 0.8216\n",
      "Epoch 14/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6733 - val_loss: 0.8215\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6732 - val_loss: 0.8214\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6731 - val_loss: 0.8213\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6731 - val_loss: 0.8213\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6730 - val_loss: 0.8212\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6730 - val_loss: 0.8212\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6730 - val_loss: 0.8211\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6730 - val_loss: 0.8211\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6729 - val_loss: 0.8211\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6729 - val_loss: 0.8210\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6729 - val_loss: 0.8210\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8210\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6728 - val_loss: 0.8208\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8207\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8207\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6726 - val_loss: 0.8207\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6726 - val_loss: 0.8207\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8207\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8207\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8205\n",
      "Execution time:  1661.805815935135\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6893\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0465\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.047\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_159\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_75 (GRU)                 (None, 1008, 55)          9570      \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_159 (TimeDi (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.7771 - val_loss: 0.6587\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6964 - val_loss: 0.6571\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6959 - val_loss: 0.6566\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6957 - val_loss: 0.6564\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6956 - val_loss: 0.6562\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.6955 - val_loss: 0.6561\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.6955 - val_loss: 0.6561\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6955 - val_loss: 0.6560\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6954 - val_loss: 0.6558\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 91s 340ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 32/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Execution time:  3257.5765478610992\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6893\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0465\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.046\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_160\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_76 (GRU)                 (None, 1008, 40)          5160      \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_160 (TimeDi (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 23s 381ms/step - loss: 0.6722 - val_loss: 0.8264\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 23s 384ms/step - loss: 0.6721 - val_loss: 0.8263\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 0.6719 - val_loss: 0.8261\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 0.6719 - val_loss: 0.8259\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 0.6718 - val_loss: 0.8257\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 23s 384ms/step - loss: 0.6717 - val_loss: 0.8255\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 0.6715 - val_loss: 0.8253\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6714 - val_loss: 0.8251\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6713 - val_loss: 0.8249\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6712 - val_loss: 0.8247\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6711 - val_loss: 0.8245\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6709 - val_loss: 0.8243\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6708 - val_loss: 0.8240\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6707 - val_loss: 0.8238\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6705 - val_loss: 0.8236\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6704 - val_loss: 0.8233\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6703 - val_loss: 0.8231\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6702 - val_loss: 0.8229\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6700 - val_loss: 0.8226\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6699 - val_loss: 0.8224\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6697 - val_loss: 0.8221\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6696 - val_loss: 0.8218\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6694 - val_loss: 0.8216\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6693 - val_loss: 0.8213\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6692 - val_loss: 0.8211\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6690 - val_loss: 0.8208\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6689 - val_loss: 0.8205\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6687 - val_loss: 0.8203\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6686 - val_loss: 0.8200\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6684 - val_loss: 0.8197\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6683 - val_loss: 0.8195\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6681 - val_loss: 0.8192\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6680 - val_loss: 0.8189\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6678 - val_loss: 0.8186\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6677 - val_loss: 0.8183\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6675 - val_loss: 0.8181\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6673 - val_loss: 0.8178\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 25s 409ms/step - loss: 0.6672 - val_loss: 0.8175\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6670 - val_loss: 0.8172\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6669 - val_loss: 0.8169\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6668 - val_loss: 0.8166\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 0.6666 - val_loss: 0.8163\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6664 - val_loss: 0.8160\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6663 - val_loss: 0.8158\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6661 - val_loss: 0.8155\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 0.6659 - val_loss: 0.8152\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6658 - val_loss: 0.8149\n",
      "Epoch 48/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6656 - val_loss: 0.8146\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6654 - val_loss: 0.8143\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6653 - val_loss: 0.8140\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.6651 - val_loss: 0.8137\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6650 - val_loss: 0.8134\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6648 - val_loss: 0.8131\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6647 - val_loss: 0.8128\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.6645 - val_loss: 0.8125\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6643 - val_loss: 0.8122\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6642 - val_loss: 0.8119\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6640 - val_loss: 0.8115\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.6638 - val_loss: 0.8112\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6637 - val_loss: 0.8109\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 0.6635 - val_loss: 0.8106\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6633 - val_loss: 0.8103\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6632 - val_loss: 0.8100\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6630 - val_loss: 0.8097\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6629 - val_loss: 0.8094\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.6627 - val_loss: 0.8091\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 25s 410ms/step - loss: 0.6625 - val_loss: 0.8088\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.6623 - val_loss: 0.8084\n",
      "Execution time:  1678.5151374340057\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6937\n",
      "Root Mean Square Error: 1.0382\n",
      "Mean Square Error: 1.0780\n",
      "\n",
      "Train RMSE: 1.038\n",
      "Train MSE: 1.078\n",
      "Train MAE: 0.694\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_161\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_77 (GRU)                 (None, 1008, 55)          9570      \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_161 (TimeDi (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 92s 346ms/step - loss: 0.6945 - val_loss: 0.6588\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 89s 335ms/step - loss: 0.6940 - val_loss: 0.6582\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6934 - val_loss: 0.6575\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6927 - val_loss: 0.6568\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6920 - val_loss: 0.6559\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6913 - val_loss: 0.6550\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6905 - val_loss: 0.6540\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6897 - val_loss: 0.6529\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6888 - val_loss: 0.6518\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6880 - val_loss: 0.6506\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6871 - val_loss: 0.6493\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6862 - val_loss: 0.6480\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6853 - val_loss: 0.6467\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6843 - val_loss: 0.6452\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6834 - val_loss: 0.6438\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6824 - val_loss: 0.6423\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 93s 349ms/step - loss: 0.6814 - val_loss: 0.6407\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6805 - val_loss: 0.6391\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6795 - val_loss: 0.6375\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6785 - val_loss: 0.6358\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 90s 336ms/step - loss: 0.6775 - val_loss: 0.6341\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6765 - val_loss: 0.6324\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6755 - val_loss: 0.6306\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6744 - val_loss: 0.6287\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6734 - val_loss: 0.6269\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6723 - val_loss: 0.6250\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6713 - val_loss: 0.6231\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6702 - val_loss: 0.6211\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6691 - val_loss: 0.6192\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6681 - val_loss: 0.6172\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6670 - val_loss: 0.6152\n",
      "Epoch 32/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6659 - val_loss: 0.6132\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6648 - val_loss: 0.6112\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6637 - val_loss: 0.6092\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6626 - val_loss: 0.6072\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6616 - val_loss: 0.6052\n",
      "Execution time:  3252.168182373047\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6944\n",
      "Root Mean Square Error: 1.0576\n",
      "Mean Square Error: 1.1186\n",
      "\n",
      "Train RMSE: 1.058\n",
      "Train MSE: 1.119\n",
      "Train MAE: 0.694\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_162\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_78 (GRU)                 (None, 1008, 40)          5160      \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_162 (TimeDi (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 25s 415ms/step - loss: 0.9759 - val_loss: 1.3136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/68\n",
      "60/60 [==============================] - 25s 419ms/step - loss: 0.9758 - val_loss: 1.3136\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 0.9758 - val_loss: 1.3135\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 25s 419ms/step - loss: 0.9758 - val_loss: 1.3134\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 25s 417ms/step - loss: 0.9757 - val_loss: 1.3134\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 25s 422ms/step - loss: 0.9757 - val_loss: 1.3133\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9756 - val_loss: 1.3132\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 27s 449ms/step - loss: 0.9756 - val_loss: 1.3132\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 27s 444ms/step - loss: 0.9755 - val_loss: 1.3131\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 27s 444ms/step - loss: 0.9754 - val_loss: 1.3130\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 27s 443ms/step - loss: 0.9754 - val_loss: 1.3129\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 0.9753 - val_loss: 1.3129\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 27s 443ms/step - loss: 0.9753 - val_loss: 1.3128\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9752 - val_loss: 1.3127\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 27s 448ms/step - loss: 0.9751 - val_loss: 1.3126\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9751 - val_loss: 1.3125\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 26s 442ms/step - loss: 0.9750 - val_loss: 1.3124\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 0.9749 - val_loss: 1.3124\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9749 - val_loss: 1.3123\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 26s 436ms/step - loss: 0.9748 - val_loss: 1.3122\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 0.9748 - val_loss: 1.3121\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 27s 444ms/step - loss: 0.9747 - val_loss: 1.3120\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 26s 435ms/step - loss: 0.9746 - val_loss: 1.3119\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 0.9746 - val_loss: 1.3118\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 0.9745 - val_loss: 1.3117\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9744 - val_loss: 1.3116\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 0.9744 - val_loss: 1.3115\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9743 - val_loss: 1.3114\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9742 - val_loss: 1.3113\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9741 - val_loss: 1.3112\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 0.9741 - val_loss: 1.3111\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 26s 435ms/step - loss: 0.9740 - val_loss: 1.3110\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 0.9739 - val_loss: 1.3109\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 27s 447ms/step - loss: 0.9739 - val_loss: 1.3108\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 27s 445ms/step - loss: 0.9738 - val_loss: 1.3107\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 0.9737 - val_loss: 1.3106\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 0.9736 - val_loss: 1.3105\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9736 - val_loss: 1.3104\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 0.9735 - val_loss: 1.3103\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 0.9734 - val_loss: 1.3102\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 26s 436ms/step - loss: 0.9733 - val_loss: 1.3101\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9733 - val_loss: 1.3100\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 0.9732 - val_loss: 1.3099\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 0.9731 - val_loss: 1.3098\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 26s 436ms/step - loss: 0.9730 - val_loss: 1.3097\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 0.9730 - val_loss: 1.3095\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 26s 436ms/step - loss: 0.9729 - val_loss: 1.3094\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 0.9728 - val_loss: 1.3093\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 26s 434ms/step - loss: 0.9727 - val_loss: 1.3092\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9727 - val_loss: 1.3091\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 26s 438ms/step - loss: 0.9726 - val_loss: 1.3090\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 0.9725 - val_loss: 1.3089\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9724 - val_loss: 1.3088\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 26s 434ms/step - loss: 0.9723 - val_loss: 1.3087\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 0.9723 - val_loss: 1.3085\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 26s 438ms/step - loss: 0.9722 - val_loss: 1.3084\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 26s 434ms/step - loss: 0.9721 - val_loss: 1.3083\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 26s 434ms/step - loss: 0.9720 - val_loss: 1.3082\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 26s 432ms/step - loss: 0.9720 - val_loss: 1.3081\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 0.9719 - val_loss: 1.3080\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9718 - val_loss: 1.3078\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 26s 436ms/step - loss: 0.9717 - val_loss: 1.3077\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 0.9716 - val_loss: 1.3076\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 26s 437ms/step - loss: 0.9715 - val_loss: 1.3075\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 0.9714 - val_loss: 1.3074\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 27s 445ms/step - loss: 0.9714 - val_loss: 1.3073\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 26s 436ms/step - loss: 0.9713 - val_loss: 1.3071\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 0.9712 - val_loss: 1.3070\n",
      "Execution time:  1814.725310087204\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9029\n",
      "Root Mean Square Error: 1.1217\n",
      "Mean Square Error: 1.2581\n",
      "\n",
      "Train RMSE: 1.122\n",
      "Train MSE: 1.258\n",
      "Train MAE: 0.903\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_163\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_79 (GRU)                 (None, 1008, 55)          9570      \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_163 (TimeDi (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 92s 344ms/step - loss: 0.9761 - val_loss: 1.1464\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.9759 - val_loss: 1.1461\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9756 - val_loss: 1.1458\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 89s 335ms/step - loss: 0.9753 - val_loss: 1.1454\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9750 - val_loss: 1.1449\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.9746 - val_loss: 1.1445\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9742 - val_loss: 1.1440\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9738 - val_loss: 1.1434\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9734 - val_loss: 1.1429\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9730 - val_loss: 1.1423\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9725 - val_loss: 1.1417\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9721 - val_loss: 1.1411\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9716 - val_loss: 1.1404\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9711 - val_loss: 1.1397\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9706 - val_loss: 1.1390\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9701 - val_loss: 1.1383\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.9695 - val_loss: 1.1376\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9690 - val_loss: 1.1368\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9684 - val_loss: 1.1360\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9679 - val_loss: 1.1352\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9673 - val_loss: 1.1343\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9666 - val_loss: 1.1335\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9660 - val_loss: 1.1326\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9654 - val_loss: 1.1316\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9647 - val_loss: 1.1307\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9640 - val_loss: 1.1297\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9633 - val_loss: 1.1287\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9626 - val_loss: 1.1276\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9618 - val_loss: 1.1265\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.9611 - val_loss: 1.1254\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.9603 - val_loss: 1.1243\n",
      "Epoch 32/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.9595 - val_loss: 1.1231\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.9587 - val_loss: 1.1219\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.9578 - val_loss: 1.1206\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.9569 - val_loss: 1.1193\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.9560 - val_loss: 1.1180\n",
      "Execution time:  3256.1397848129272\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9021\n",
      "Root Mean Square Error: 1.1249\n",
      "Mean Square Error: 1.2654\n",
      "\n",
      "Train RMSE: 1.125\n",
      "Train MSE: 1.265\n",
      "Train MAE: 0.902\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_164\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_80 (GRU)                 (None, 1008, 40)          5160      \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_164 (TimeDi (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 23s 377ms/step - loss: 0.6520 - val_loss: 0.7382\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 0.5899 - val_loss: 0.5312\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 0.5653 - val_loss: 0.4690\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 0.5641 - val_loss: 0.4613\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 23s 376ms/step - loss: 0.5627 - val_loss: 0.4556\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 0.5617 - val_loss: 0.4508\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5609 - val_loss: 0.4470\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.5602 - val_loss: 0.4431\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.5596 - val_loss: 0.4404\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.5590 - val_loss: 0.4379\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5586 - val_loss: 0.4356\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.5581 - val_loss: 0.4333\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5577 - val_loss: 0.4313\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.5572 - val_loss: 0.4296\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5568 - val_loss: 0.4280\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.5565 - val_loss: 0.4264\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5562 - val_loss: 0.4252\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.5558 - val_loss: 0.4239\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5555 - val_loss: 0.4229\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5552 - val_loss: 0.4217\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5549 - val_loss: 0.4206\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 0.5546 - val_loss: 0.4198\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.5543 - val_loss: 0.4190\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 0.5540 - val_loss: 0.4182\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.5538 - val_loss: 0.4175\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 0.5535 - val_loss: 0.4170\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.5532 - val_loss: 0.4166\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 24s 407ms/step - loss: 0.5530 - val_loss: 0.4163\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5527 - val_loss: 0.4161\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5524 - val_loss: 0.4159\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5522 - val_loss: 0.4159\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5519 - val_loss: 0.4160\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.5516 - val_loss: 0.4162\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.5514 - val_loss: 0.4167\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.5510 - val_loss: 0.4174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.5507 - val_loss: 0.4180\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.5504 - val_loss: 0.4188\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.5500 - val_loss: 0.4198\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.5497 - val_loss: 0.4208\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.5493 - val_loss: 0.4221\n",
      "Execution time:  968.2555317878723\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6504\n",
      "Root Mean Square Error: 1.1419\n",
      "Mean Square Error: 1.3039\n",
      "\n",
      "Train RMSE: 1.142\n",
      "Train MSE: 1.304\n",
      "Train MAE: 0.650\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_165\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_81 (GRU)                 (None, 1008, 55)          9570      \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_165 (TimeDi (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 91s 343ms/step - loss: 0.6483 - val_loss: 0.3268\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.5990 - val_loss: 0.3076\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.5956 - val_loss: 0.3015\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.5946 - val_loss: 0.3086\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.5937 - val_loss: 0.3147\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.5924 - val_loss: 0.3189\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.5914 - val_loss: 0.3197\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5903 - val_loss: 0.3176\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.5896 - val_loss: 0.3184\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.5895 - val_loss: 0.3173\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.5885 - val_loss: 0.3160\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.5879 - val_loss: 0.3176\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.5875 - val_loss: 0.3156\n",
      "Execution time:  1178.3893194198608\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6568\n",
      "Root Mean Square Error: 1.1633\n",
      "Mean Square Error: 1.3533\n",
      "\n",
      "Train RMSE: 1.163\n",
      "Train MSE: 1.353\n",
      "Train MAE: 0.657\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  40\n",
      "dropout1:  0.40519643149940265\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 68\n",
      "batchsize: 45\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_166\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_82 (GRU)                 (None, 1008, 40)          5160      \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 1008, 40)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_166 (TimeDi (None, 1008, 1)           41        \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/68\n",
      "60/60 [==============================] - 22s 374ms/step - loss: 0.9662 - val_loss: 1.2776\n",
      "Epoch 2/68\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 0.9005 - val_loss: 1.0340\n",
      "Epoch 3/68\n",
      "60/60 [==============================] - 23s 385ms/step - loss: 0.7193 - val_loss: 0.8563\n",
      "Epoch 4/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.6882 - val_loss: 0.8404\n",
      "Epoch 5/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.6828 - val_loss: 0.8343\n",
      "Epoch 6/68\n",
      "60/60 [==============================] - 23s 392ms/step - loss: 0.6802 - val_loss: 0.8310\n",
      "Epoch 7/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.6786 - val_loss: 0.8289\n",
      "Epoch 8/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6776 - val_loss: 0.8274\n",
      "Epoch 9/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6768 - val_loss: 0.8264\n",
      "Epoch 10/68\n",
      "60/60 [==============================] - 23s 388ms/step - loss: 0.6763 - val_loss: 0.8256\n",
      "Epoch 11/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6758 - val_loss: 0.8249\n",
      "Epoch 12/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6754 - val_loss: 0.8244\n",
      "Epoch 13/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.6751 - val_loss: 0.8240\n",
      "Epoch 14/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.6749 - val_loss: 0.8236\n",
      "Epoch 15/68\n",
      "60/60 [==============================] - 23s 385ms/step - loss: 0.6747 - val_loss: 0.8233\n",
      "Epoch 16/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6745 - val_loss: 0.8231\n",
      "Epoch 17/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.6743 - val_loss: 0.8229\n",
      "Epoch 18/68\n",
      "60/60 [==============================] - 23s 389ms/step - loss: 0.6742 - val_loss: 0.8227\n",
      "Epoch 19/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6741 - val_loss: 0.8225\n",
      "Epoch 20/68\n",
      "60/60 [==============================] - 23s 387ms/step - loss: 0.6740 - val_loss: 0.8223\n",
      "Epoch 21/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6738 - val_loss: 0.8222\n",
      "Epoch 22/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6738 - val_loss: 0.8221\n",
      "Epoch 23/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6737 - val_loss: 0.8220\n",
      "Epoch 24/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6736 - val_loss: 0.8219\n",
      "Epoch 25/68\n",
      "60/60 [==============================] - 23s 386ms/step - loss: 0.6735 - val_loss: 0.8218\n",
      "Epoch 26/68\n",
      "60/60 [==============================] - 23s 392ms/step - loss: 0.6735 - val_loss: 0.8217\n",
      "Epoch 27/68\n",
      "60/60 [==============================] - 23s 392ms/step - loss: 0.6734 - val_loss: 0.8216\n",
      "Epoch 28/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.6734 - val_loss: 0.8215\n",
      "Epoch 29/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6733 - val_loss: 0.8215\n",
      "Epoch 30/68\n",
      "60/60 [==============================] - 27s 454ms/step - loss: 0.6733 - val_loss: 0.8214\n",
      "Epoch 31/68\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 0.6732 - val_loss: 0.8214\n",
      "Epoch 32/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6732 - val_loss: 0.8213\n",
      "Epoch 33/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.6731 - val_loss: 0.8213\n",
      "Epoch 34/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6731 - val_loss: 0.8212\n",
      "Epoch 35/68\n",
      "60/60 [==============================] - 24s 392ms/step - loss: 0.6731 - val_loss: 0.8212\n",
      "Epoch 36/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6731 - val_loss: 0.8211\n",
      "Epoch 37/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.6730 - val_loss: 0.8211\n",
      "Epoch 38/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6730 - val_loss: 0.8211\n",
      "Epoch 39/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6730 - val_loss: 0.8210\n",
      "Epoch 40/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.6729 - val_loss: 0.8210\n",
      "Epoch 41/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6729 - val_loss: 0.8210\n",
      "Epoch 42/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.6729 - val_loss: 0.8210\n",
      "Epoch 43/68\n",
      "60/60 [==============================] - 23s 390ms/step - loss: 0.6729 - val_loss: 0.8209\n",
      "Epoch 44/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6729 - val_loss: 0.8209\n",
      "Epoch 45/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6729 - val_loss: 0.8209\n",
      "Epoch 46/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.6728 - val_loss: 0.8209\n",
      "Epoch 47/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6728 - val_loss: 0.8208\n",
      "Epoch 48/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6728 - val_loss: 0.8208\n",
      "Epoch 49/68\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8208\n",
      "Epoch 50/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6728 - val_loss: 0.8208\n",
      "Epoch 51/68\n",
      "60/60 [==============================] - 23s 391ms/step - loss: 0.6728 - val_loss: 0.8208\n",
      "Epoch 52/68\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 53/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6727 - val_loss: 0.8208\n",
      "Epoch 54/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 55/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 56/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 57/68\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 58/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 59/68\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 60/68\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 61/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 62/68\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 0.6727 - val_loss: 0.8207\n",
      "Epoch 63/68\n",
      "60/60 [==============================] - 24s 400ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 64/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 65/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 66/68\n",
      "60/60 [==============================] - 24s 395ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 67/68\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Epoch 68/68\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 0.6726 - val_loss: 0.8206\n",
      "Execution time:  1643.7634372711182\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6893\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0465\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.046\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  55\n",
      "dropout1:  0.11814836227952394\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 36\n",
      "batchsize: 9\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_167\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_83 (GRU)                 (None, 1008, 55)          9570      \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 1008, 55)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_167 (TimeDi (None, 1008, 1)           56        \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/36\n",
      "266/266 [==============================] - 91s 341ms/step - loss: 0.8342 - val_loss: 0.6673\n",
      "Epoch 2/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6990 - val_loss: 0.6602\n",
      "Epoch 3/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6971 - val_loss: 0.6584\n",
      "Epoch 4/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6965 - val_loss: 0.6576\n",
      "Epoch 5/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6961 - val_loss: 0.6571\n",
      "Epoch 6/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6959 - val_loss: 0.6568\n",
      "Epoch 7/36\n",
      "266/266 [==============================] - 89s 336ms/step - loss: 0.6958 - val_loss: 0.6565\n",
      "Epoch 8/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6957 - val_loss: 0.6564\n",
      "Epoch 9/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6956 - val_loss: 0.6562\n",
      "Epoch 10/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6956 - val_loss: 0.6561\n",
      "Epoch 11/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6955 - val_loss: 0.6561\n",
      "Epoch 12/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6955 - val_loss: 0.6560\n",
      "Epoch 13/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 14/36\n",
      "266/266 [==============================] - 90s 340ms/step - loss: 0.6954 - val_loss: 0.6559\n",
      "Epoch 15/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6954 - val_loss: 0.6558\n",
      "Epoch 16/36\n",
      "266/266 [==============================] - 91s 340ms/step - loss: 0.6953 - val_loss: 0.6558\n",
      "Epoch 17/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 18/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 19/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 20/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 21/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557\n",
      "Epoch 22/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 23/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 24/36\n",
      "266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 25/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 26/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 27/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 28/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 29/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 30/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 31/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 32/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 33/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 34/36\n",
      "266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 35/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Epoch 36/36\n",
      "266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556\n",
      "Execution time:  3255.470086336136\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6893\n",
      "Root Mean Square Error: 1.0230\n",
      "Mean Square Error: 1.0465\n",
      "\n",
      "Train RMSE: 1.023\n",
      "Train MSE: 1.046\n",
      "Train MAE: 0.689\n"
     ]
    }
   ],
   "source": [
    "def _model(cmodel,units,activationDense,dropout1,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(cmodel(units = units, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(rate=dropout1))\n",
    "    model.add(TimeDistributed(Dense(1,kernel_initializer='normal',activation=activationDense)))\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "models = [LSTM,GRU]\n",
    "nmodels = [\"LSTM\",\"GRU\"]\n",
    "sequences = [\"1h\",\"3h\",\"6h\",\"12h\",\"1d\", \"3d\", \"7d\"]\n",
    "X_trains = [X_train1h,X_train3h, X_train6h, X_train12h,X_train1d,X_train3d, X_train7d]\n",
    "y_trains= [y_train1h,y_train3h, y_train6h,y_train12h,y_train1d,y_train3d, y_train7d]\n",
    "activations = ['relu']\n",
    "activationsDense = ['tanh','sigmoid']\n",
    "optimizers = ['adam','adadelta','adamax']\n",
    "list_validationSplit = [0.1,0.2]\n",
    "list_dropout1 =  np.random.uniform(0.1,0.8,5)   \n",
    "list_units = np.random.randint(6,high=100, size=5) \n",
    "list_epochs = np.random.randint(5,high=100, size=5)   \n",
    "list_batchsize = np.random.randint(6,high=64, size=5)     \n",
    "\n",
    "list_results = pd.DataFrame()\n",
    "\n",
    "for cmodel,nmodel in zip(models,nmodels):\n",
    "    for X_train, y_train,sequence in zip(X_trains,y_trains,sequences):\n",
    "        for optimizer in optimizers:\n",
    "            #for activation in activations:\n",
    "                for activationDense in activationsDense:\n",
    "                    for units,epochs,batchsize,dropout1,validationsplit in zip(list_units,list_epochs,list_batchsize,list_dropout1,list_validationSplit): \n",
    "                        start = time()\n",
    "                        print(\"###########################\\n\")\n",
    "                        print(\"MODEL: \", nmodel)\n",
    "                        print('sequence: ',sequence)\n",
    "                        print('units: ',units)\n",
    "                        print('dropout1: ',dropout1)\n",
    "                        print('optimizer:',optimizer)\n",
    "                        #print('activation:',activation)\n",
    "                        print('activationDense:',activationDense)\n",
    "                        print('epochs:',epochs)\n",
    "                        print('batchsize:',batchsize)\n",
    "                        print('validation_split:',validationsplit)\n",
    "                    \n",
    "                        model = _model(cmodel,units,activationDense,dropout1,optimizer)\n",
    "                        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batchsize, validation_split=validationsplit,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min')], shuffle=False)\n",
    "                        end = time()\n",
    "                        totalTime = end-start\n",
    "                        print ('Execution time: ',totalTime)\n",
    "\n",
    "                        X_train_pred = model.predict(X_train, verbose=0)\n",
    "                        \n",
    "                        mae,rmse,mse = evaluate_prediction(X_train_pred, X_train,nmodel)\n",
    "                        \n",
    "                        print('Train RMSE: %.3f' % rmse);\n",
    "                        print('Train MSE: %.3f' % mse);\n",
    "                        print('Train MAE: %.3f' % mae);\n",
    "                        \n",
    "                        result = pd.DataFrame({\n",
    "                                               #'activation':[activation],\n",
    "                                               'model':[nmodel],\n",
    "                                               'sequence':[sequence],\n",
    "                                               'activationDense':[activationDense],\n",
    "                                               'optimizer':[optimizer],\n",
    "                                               'dropout1':[dropout1],\n",
    "                                               'units':[units],\n",
    "                                               'epochs':[epochs],\n",
    "                                               'batchsize':[batchsize],\n",
    "                                               'validation_split':[validationsplit],\n",
    "                                               \n",
    "                                               'RMSE':[rmse],\n",
    "                                               'MSE':[mse],\n",
    "                                               'MAE':[mae],                            \n",
    "                                               'Time':[totalTime]})\n",
    "                        list_results = list_results.append(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_results.to_csv(\"resultats-cerca-optim-lstm-una-capa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sequence</th>\n",
       "      <th>activationDense</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout1</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.581774</td>\n",
       "      <td>0.338461</td>\n",
       "      <td>0.159277</td>\n",
       "      <td>27.412817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.581916</td>\n",
       "      <td>0.338626</td>\n",
       "      <td>0.151122</td>\n",
       "      <td>51.123055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.752527</td>\n",
       "      <td>0.566297</td>\n",
       "      <td>0.483684</td>\n",
       "      <td>28.099232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.758053</td>\n",
       "      <td>0.574645</td>\n",
       "      <td>0.483228</td>\n",
       "      <td>35.801286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.958890</td>\n",
       "      <td>0.919471</td>\n",
       "      <td>0.669135</td>\n",
       "      <td>27.464905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.932710</td>\n",
       "      <td>0.869947</td>\n",
       "      <td>0.639812</td>\n",
       "      <td>51.920173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.118657</td>\n",
       "      <td>1.251392</td>\n",
       "      <td>0.923693</td>\n",
       "      <td>29.166857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.103161</td>\n",
       "      <td>1.216963</td>\n",
       "      <td>0.908986</td>\n",
       "      <td>52.628776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.584879</td>\n",
       "      <td>0.342083</td>\n",
       "      <td>0.161601</td>\n",
       "      <td>27.472851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.582802</td>\n",
       "      <td>0.339658</td>\n",
       "      <td>0.153627</td>\n",
       "      <td>51.269109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.752643</td>\n",
       "      <td>0.566471</td>\n",
       "      <td>0.484742</td>\n",
       "      <td>28.038204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.751879</td>\n",
       "      <td>0.565323</td>\n",
       "      <td>0.482665</td>\n",
       "      <td>52.554424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.586655</td>\n",
       "      <td>0.344164</td>\n",
       "      <td>0.171474</td>\n",
       "      <td>34.469355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.585573</td>\n",
       "      <td>0.342896</td>\n",
       "      <td>0.165658</td>\n",
       "      <td>103.103569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.757094</td>\n",
       "      <td>0.573191</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>50.677530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.586757</td>\n",
       "      <td>0.496910</td>\n",
       "      <td>104.571928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.966205</td>\n",
       "      <td>0.933551</td>\n",
       "      <td>0.670099</td>\n",
       "      <td>50.471102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.894303</td>\n",
       "      <td>0.799777</td>\n",
       "      <td>0.593011</td>\n",
       "      <td>103.204283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.119369</td>\n",
       "      <td>1.252988</td>\n",
       "      <td>0.924787</td>\n",
       "      <td>51.198771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.093491</td>\n",
       "      <td>1.195723</td>\n",
       "      <td>0.899046</td>\n",
       "      <td>102.731944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model sequence activationDense optimizer  dropout1  units  epochs  \\\n",
       "0  LSTM       1h            tanh      adam  0.405196     40      68   \n",
       "0  LSTM       1h            tanh      adam  0.118148     55      36   \n",
       "0  LSTM       1h         sigmoid      adam  0.405196     40      68   \n",
       "0  LSTM       1h         sigmoid      adam  0.118148     55      36   \n",
       "0  LSTM       1h            tanh  adadelta  0.405196     40      68   \n",
       "0  LSTM       1h            tanh  adadelta  0.118148     55      36   \n",
       "0  LSTM       1h         sigmoid  adadelta  0.405196     40      68   \n",
       "0  LSTM       1h         sigmoid  adadelta  0.118148     55      36   \n",
       "0  LSTM       1h            tanh    adamax  0.405196     40      68   \n",
       "0  LSTM       1h            tanh    adamax  0.118148     55      36   \n",
       "0  LSTM       1h         sigmoid    adamax  0.405196     40      68   \n",
       "0  LSTM       1h         sigmoid    adamax  0.118148     55      36   \n",
       "0  LSTM       3h            tanh      adam  0.405196     40      68   \n",
       "0  LSTM       3h            tanh      adam  0.118148     55      36   \n",
       "0  LSTM       3h         sigmoid      adam  0.405196     40      68   \n",
       "0  LSTM       3h         sigmoid      adam  0.118148     55      36   \n",
       "0  LSTM       3h            tanh  adadelta  0.405196     40      68   \n",
       "0  LSTM       3h            tanh  adadelta  0.118148     55      36   \n",
       "0  LSTM       3h         sigmoid  adadelta  0.405196     40      68   \n",
       "0  LSTM       3h         sigmoid  adadelta  0.118148     55      36   \n",
       "\n",
       "   batchsize  validation_split      RMSE       MSE       MAE        Time  \n",
       "0         45               0.1  0.581774  0.338461  0.159277   27.412817  \n",
       "0          9               0.2  0.581916  0.338626  0.151122   51.123055  \n",
       "0         45               0.1  0.752527  0.566297  0.483684   28.099232  \n",
       "0          9               0.2  0.758053  0.574645  0.483228   35.801286  \n",
       "0         45               0.1  0.958890  0.919471  0.669135   27.464905  \n",
       "0          9               0.2  0.932710  0.869947  0.639812   51.920173  \n",
       "0         45               0.1  1.118657  1.251392  0.923693   29.166857  \n",
       "0          9               0.2  1.103161  1.216963  0.908986   52.628776  \n",
       "0         45               0.1  0.584879  0.342083  0.161601   27.472851  \n",
       "0          9               0.2  0.582802  0.339658  0.153627   51.269109  \n",
       "0         45               0.1  0.752643  0.566471  0.484742   28.038204  \n",
       "0          9               0.2  0.751879  0.565323  0.482665   52.554424  \n",
       "0         45               0.1  0.586655  0.344164  0.171474   34.469355  \n",
       "0          9               0.2  0.585573  0.342896  0.165658  103.103569  \n",
       "0         45               0.1  0.757094  0.573191  0.495606   50.677530  \n",
       "0          9               0.2  0.766000  0.586757  0.496910  104.571928  \n",
       "0         45               0.1  0.966205  0.933551  0.670099   50.471102  \n",
       "0          9               0.2  0.894303  0.799777  0.593011  103.204283  \n",
       "0         45               0.1  1.119369  1.252988  0.924787   51.198771  \n",
       "0          9               0.2  1.093491  1.195723  0.899046  102.731944  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sequence</th>\n",
       "      <th>activationDense</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout1</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.581650</td>\n",
       "      <td>0.338317</td>\n",
       "      <td>0.154136</td>\n",
       "      <td>58.817262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.581774</td>\n",
       "      <td>0.338461</td>\n",
       "      <td>0.159277</td>\n",
       "      <td>27.412817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.581867</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>0.154247</td>\n",
       "      <td>58.510861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.581916</td>\n",
       "      <td>0.338626</td>\n",
       "      <td>0.151122</td>\n",
       "      <td>51.123055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.582345</td>\n",
       "      <td>0.339125</td>\n",
       "      <td>0.160290</td>\n",
       "      <td>30.322881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.146864</td>\n",
       "      <td>1.315297</td>\n",
       "      <td>0.643848</td>\n",
       "      <td>1682.198702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.154867</td>\n",
       "      <td>1.333718</td>\n",
       "      <td>0.664537</td>\n",
       "      <td>1262.185469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.158497</td>\n",
       "      <td>1.342116</td>\n",
       "      <td>0.648923</td>\n",
       "      <td>2014.745057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.160135</td>\n",
       "      <td>1.345913</td>\n",
       "      <td>0.653456</td>\n",
       "      <td>1424.883327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.163293</td>\n",
       "      <td>1.353250</td>\n",
       "      <td>0.656830</td>\n",
       "      <td>1178.389319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model sequence activationDense optimizer  dropout1  units  epochs  \\\n",
       "0    GRU       1h            tanh      adam  0.118148     55      36   \n",
       "0   LSTM       1h            tanh      adam  0.405196     40      68   \n",
       "0    GRU       1h            tanh    adamax  0.118148     55      36   \n",
       "0   LSTM       1h            tanh      adam  0.118148     55      36   \n",
       "0    GRU       1h            tanh      adam  0.405196     40      68   \n",
       "..   ...      ...             ...       ...       ...    ...     ...   \n",
       "0   LSTM       7d            tanh      adam  0.405196     40      68   \n",
       "0   LSTM       7d            tanh    adamax  0.405196     40      68   \n",
       "0   LSTM       7d            tanh      adam  0.118148     55      36   \n",
       "0   LSTM       7d            tanh    adamax  0.118148     55      36   \n",
       "0    GRU       7d            tanh    adamax  0.118148     55      36   \n",
       "\n",
       "    batchsize  validation_split      RMSE       MSE       MAE         Time  \n",
       "0           9               0.2  0.581650  0.338317  0.154136    58.817262  \n",
       "0          45               0.1  0.581774  0.338461  0.159277    27.412817  \n",
       "0           9               0.2  0.581867  0.338569  0.154247    58.510861  \n",
       "0           9               0.2  0.581916  0.338626  0.151122    51.123055  \n",
       "0          45               0.1  0.582345  0.339125  0.160290    30.322881  \n",
       "..        ...               ...       ...       ...       ...          ...  \n",
       "0          45               0.1  1.146864  1.315297  0.643848  1682.198702  \n",
       "0          45               0.1  1.154867  1.333718  0.664537  1262.185469  \n",
       "0           9               0.2  1.158497  1.342116  0.648923  2014.745057  \n",
       "0           9               0.2  1.160135  1.345913  0.653456  1424.883327  \n",
       "0           9               0.2  1.163293  1.353250  0.656830  1178.389319  \n",
       "\n",
       "[168 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results.sort_values(by=['RMSE', 'sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PAC3_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
