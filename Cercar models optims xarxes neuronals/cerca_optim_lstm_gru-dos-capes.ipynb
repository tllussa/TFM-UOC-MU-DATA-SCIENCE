{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cercador model òptim de LSTM i GRU de dues capes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lMCuRV3SJK6w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dj_kr\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from time import time\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d9z8yxsJK6x"
   },
   "source": [
    "## Càrrega de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "seq5Q3gjJK6y",
    "outputId": "72e82bf7-9c19-4d0d-da4a-42c4aff349b3"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/SentDATA.csv')\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df = df.set_index('Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VLv_DJhJK6y"
   },
   "source": [
    "## Transformació de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NUx3mg0RJK6y"
   },
   "outputs": [],
   "source": [
    "columns = ['PM1','PM25','PM10','PM1ATM','PM25ATM','PM10ATM']\n",
    "\n",
    "df1 = df.copy();\n",
    "\n",
    "df1 = df1.rename(columns={\"PM 1\":\"PM1\",\"PM 2.5\":\"PM25\",\"PM 10\":\"PM10\",\"PM 1 ATM\":\"PM1ATM\",\"PM 2.5 ATM\":\"PM25ATM\",\"PM 10 ATM\":\"PM10ATM\"})\n",
    "\n",
    "df1['PM1'] = df['PM 1'].astype(np.float32)\n",
    "df1['PM25'] = df['PM 2.5'].astype(np.float32)\n",
    "df1['PM10'] = df['PM 10'].astype(np.float32)\n",
    "df1['PM1ATM'] = df['PM 1 ATM'].astype(np.float32)\n",
    "df1['PM25ATM'] = df['PM 2.5 ATM'].astype(np.float32)\n",
    "df1['PM10ATM'] = df['PM 10 ATM'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eh0Xz9k-IHtg"
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLJVxlmlJK6z"
   },
   "source": [
    "## Crear dades d'entrenament i de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1Dwm-E9JK6z",
    "outputId": "620f3cbe-6354-40b1-88c3-a80206b571f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3991, 7), (998, 7))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(df2) * 0.8)\n",
    "test_size = len(df2) - train_size\n",
    "train, test = df2.iloc[0:train_size], df2.iloc[train_size:len(df2)]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuMi9G1LJK6z"
   },
   "source": [
    "## Normalitzar les dades d'entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqcC5lWvJK6z",
    "outputId": "ca0f0c98-e1f3-4e86-dc5f-65dcb992a209"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-14-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-14-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-14-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-14-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-14-ad7c79e4e223>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Standardize the data\n",
    "for col in columns:\n",
    "    scaler = StandardScaler()\n",
    "    train[col] = scaler.fit_transform(train[[col]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LsNLCHSJK6z"
   },
   "source": [
    "## Crear finestra de temps PM 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEH4EmxcJK6z",
    "outputId": "b8c5f2c4-adca-47ef-fb6b-c734f0160dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1h shape: (3847, 144, 1)\n",
      "X_train3d shape: (3973, 18, 1)\n",
      "X_train6h shape: (3955, 36, 1)\n",
      "X_train12h shape: (3919, 72, 1)\n",
      "X_train1d shape: (3847, 144, 1)\n",
      "X_train3d shape: (3559, 432, 1)\n",
      "X_train7d shape: (2983, 1008, 1)\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS=144 #6 registres hora x 24h x 3 --> equival a una finestra d'un dia\n",
    "\n",
    "def create_sequences(X, y, time_steps=TIME_STEPS):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        Xs.append(X.iloc[i:(i+time_steps)].values)\n",
    "        ys.append(y.iloc[i+time_steps])\n",
    "    \n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train1h, y_train1h = create_sequences(train[[columns[1]]], train[columns[1]], 6) #1 hour\n",
    "\n",
    "X_train3h, y_train3h = create_sequences(train[[columns[1]]], train[columns[1]], 18) #3 hours\n",
    "\n",
    "X_train6h, y_train6h = create_sequences(train[[columns[1]]], train[columns[1]], 36) #6 hours\n",
    "\n",
    "X_train12h, y_train12h = create_sequences(train[[columns[1]]], train[columns[1]], 72) #12 hours\n",
    "\n",
    "X_train1d, y_train1d = create_sequences(train[[columns[1]]], train[columns[1]], 144) #1 day\n",
    "\n",
    "X_train3d, y_train3d = create_sequences(train[[columns[1]]], train[columns[1]], 432) #3 days\n",
    "\n",
    "X_train7d, y_train7d = create_sequences(train[[columns[1]]], train[columns[1]], 1008) #7 days\n",
    "#X_test, y_test = create_sequences(test[[columns[1]]], test[columns[1]])\n",
    "\n",
    "print(f'X_train1h shape: {X_train1d.shape}')\n",
    "print(f'X_train3d shape: {X_train3h.shape}')\n",
    "print(f'X_train6h shape: {X_train6h.shape}')\n",
    "print(f'X_train12h shape: {X_train12h.shape}')\n",
    "print(f'X_train1d shape: {X_train1d.shape}')\n",
    "print(f'X_train3d shape: {X_train3d.shape}')\n",
    "print(f'X_train7d shape: {X_train7d.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "\n",
    "    print(model_name + ':')\n",
    "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
    "    print('Root Mean Square Error: {:.4f}'.format(rmse))\n",
    "    print('Mean Square Error: {:.4f}'.format(mse))\n",
    "    print('')\n",
    "    return mae,rmse,mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9caadjyJK6z"
   },
   "source": [
    "## Cerca dels models òptims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7znr_i72OjVb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 6, 43)             7740      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 6, 43)             14964     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4317 - val_loss: 0.3462\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.3034 - val_loss: 0.2480\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2837 - val_loss: 0.2247\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2820 - val_loss: 0.2119\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2802 - val_loss: 0.2129\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2804 - val_loss: 0.2104\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2792 - val_loss: 0.2011\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2789 - val_loss: 0.2045\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2775 - val_loss: 0.1950\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2766 - val_loss: 0.1824\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2772 - val_loss: 0.1871\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2760 - val_loss: 0.1829\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2754 - val_loss: 0.1664\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2737 - val_loss: 0.1608\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2731 - val_loss: 0.1508\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2716 - val_loss: 0.1385\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2710 - val_loss: 0.1337\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2704 - val_loss: 0.1261\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2689 - val_loss: 0.1201\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2685 - val_loss: 0.1205\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2676 - val_loss: 0.1225\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2665 - val_loss: 0.1159\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2669 - val_loss: 0.1121\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2658 - val_loss: 0.1120\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2666 - val_loss: 0.1069\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2661 - val_loss: 0.1087\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2655 - val_loss: 0.1038\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2657 - val_loss: 0.1040\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2657 - val_loss: 0.1043\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2651 - val_loss: 0.1003\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2647 - val_loss: 0.1007\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2645 - val_loss: 0.0992\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2633 - val_loss: 0.1007\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.0990\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2637 - val_loss: 0.0954\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2630 - val_loss: 0.0987\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2629 - val_loss: 0.0982\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2629 - val_loss: 0.0954\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2630 - val_loss: 0.0967\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2629 - val_loss: 0.0977\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.1005\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2633 - val_loss: 0.0968\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2626 - val_loss: 0.0937\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2629 - val_loss: 0.0953\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2630 - val_loss: 0.0895\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2631 - val_loss: 0.0935\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2623 - val_loss: 0.0926\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2616 - val_loss: 0.0977\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2615 - val_loss: 0.0878\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2616 - val_loss: 0.0944\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2617 - val_loss: 0.0920\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2614 - val_loss: 0.0931\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2613 - val_loss: 0.0927\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2613 - val_loss: 0.0905\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2609 - val_loss: 0.0960\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2614 - val_loss: 0.0906\n",
      "Execution time:  92.2193295955658\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1655\n",
      "Root Mean Square Error: 0.5747\n",
      "Mean Square Error: 0.3303\n",
      "\n",
      "Train RMSE: 0.575\n",
      "Train MSE: 0.330\n",
      "Train MAE: 0.165\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 6, 45)             8460      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 6, 45)             16380     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 2s 16ms/step - loss: 0.5278 - val_loss: 0.3699\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.3832 - val_loss: 0.3112\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.3322 - val_loss: 0.2638\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2965 - val_loss: 0.2365\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2806 - val_loss: 0.2270\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2749 - val_loss: 0.2234\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2741 - val_loss: 0.2225\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2723 - val_loss: 0.2210\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2724 - val_loss: 0.2208\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2712 - val_loss: 0.2180\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2714 - val_loss: 0.2181\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2714 - val_loss: 0.2195\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2705 - val_loss: 0.2168\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2706 - val_loss: 0.2163\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2696 - val_loss: 0.2140\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2695 - val_loss: 0.2140\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2685 - val_loss: 0.2105\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2686 - val_loss: 0.2069\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2677 - val_loss: 0.2038\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2677 - val_loss: 0.2034\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2664 - val_loss: 0.1996\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2667 - val_loss: 0.1969\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2667 - val_loss: 0.1970\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2656 - val_loss: 0.1941\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2653 - val_loss: 0.1905\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2646 - val_loss: 0.1919\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2649 - val_loss: 0.1902\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2648 - val_loss: 0.1889\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.1886\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.1878\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2625 - val_loss: 0.1878\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2627 - val_loss: 0.1858\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2625 - val_loss: 0.1826\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2626 - val_loss: 0.1832\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2620 - val_loss: 0.1823\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2627 - val_loss: 0.1816\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2619 - val_loss: 0.1821\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2614 - val_loss: 0.1826\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2614 - val_loss: 0.1812\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.1802\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2600 - val_loss: 0.1814\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2601 - val_loss: 0.1800\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2607 - val_loss: 0.1804\n",
      "Execution time:  40.12517833709717\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1552\n",
      "Root Mean Square Error: 0.5734\n",
      "Mean Square Error: 0.3288\n",
      "\n",
      "Train RMSE: 0.573\n",
      "Train MSE: 0.329\n",
      "Train MAE: 0.155\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 6, 43)             7740      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 6, 43)             14964     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6393 - val_loss: 0.8293\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5356 - val_loss: 0.8101\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5193 - val_loss: 0.8073\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5036 - val_loss: 0.8065\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5018 - val_loss: 0.8063\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5010 - val_loss: 0.8061\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5009 - val_loss: 0.8061\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5002 - val_loss: 0.8060\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.8060\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5000 - val_loss: 0.8060\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4998 - val_loss: 0.8060\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4996 - val_loss: 0.8060\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4996 - val_loss: 0.8060\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4991 - val_loss: 0.8059\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4993 - val_loss: 0.8059\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4988 - val_loss: 0.8059\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4986 - val_loss: 0.8059\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4988 - val_loss: 0.8059\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4987 - val_loss: 0.8059\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4985 - val_loss: 0.8059\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4985 - val_loss: 0.8059\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4984 - val_loss: 0.8059\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4983 - val_loss: 0.8059\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4982 - val_loss: 0.8059\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4986 - val_loss: 0.8059\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4981 - val_loss: 0.8059\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4984 - val_loss: 0.8059\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4983 - val_loss: 0.8059\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4980 - val_loss: 0.8059\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4981 - val_loss: 0.8059\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4981 - val_loss: 0.8059\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4978 - val_loss: 0.8059\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4978 - val_loss: 0.8059\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4975 - val_loss: 0.8059\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4978 - val_loss: 0.8059\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4978 - val_loss: 0.8059\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4977 - val_loss: 0.8059\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4972 - val_loss: 0.8059\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4973 - val_loss: 0.8059\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4972 - val_loss: 0.8059\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4973 - val_loss: 0.8059\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4970 - val_loss: 0.8059\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4973 - val_loss: 0.8059\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4971 - val_loss: 0.8059\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4970 - val_loss: 0.8059\n",
      "Execution time:  89.09990072250366\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4907\n",
      "Root Mean Square Error: 0.7503\n",
      "Mean Square Error: 0.5629\n",
      "\n",
      "Train RMSE: 0.750\n",
      "Train MSE: 0.563\n",
      "Train MAE: 0.491\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 6, 45)             8460      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 6, 45)             16380     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.7585 - val_loss: 0.7674\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5761 - val_loss: 0.7114\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5480 - val_loss: 0.6899\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5353 - val_loss: 0.6780\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5275 - val_loss: 0.6707\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5188 - val_loss: 0.6681\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5035 - val_loss: 0.6648\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5001 - val_loss: 0.6639\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4991 - val_loss: 0.6634\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4983 - val_loss: 0.6630\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4979 - val_loss: 0.6631\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4974 - val_loss: 0.6631\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4970 - val_loss: 0.6629\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4967 - val_loss: 0.6628\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4966 - val_loss: 0.6627\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4966 - val_loss: 0.6627\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4962 - val_loss: 0.6628\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4958 - val_loss: 0.6628\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4958 - val_loss: 0.6624\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4956 - val_loss: 0.6626\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4954 - val_loss: 0.6627\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4952 - val_loss: 0.6625\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4958 - val_loss: 0.6625\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4952 - val_loss: 0.6627\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4952 - val_loss: 0.6623\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4952 - val_loss: 0.6624\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4949 - val_loss: 0.6627\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4950 - val_loss: 0.6629\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4951 - val_loss: 0.6624\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4947 - val_loss: 0.6623\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4949 - val_loss: 0.6623\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4947 - val_loss: 0.6627\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4948 - val_loss: 0.6624\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4946 - val_loss: 0.6624\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4946 - val_loss: 0.6627\n",
      "Epoch 36/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4947 - val_loss: 0.6624\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4945 - val_loss: 0.6627\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4948 - val_loss: 0.6625\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4945 - val_loss: 0.6620\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4943 - val_loss: 0.6625\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4943 - val_loss: 0.6622\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4942 - val_loss: 0.6622\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4945 - val_loss: 0.6622\n",
      "Execution time:  41.00082015991211\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4926\n",
      "Root Mean Square Error: 0.7513\n",
      "Mean Square Error: 0.5645\n",
      "\n",
      "Train RMSE: 0.751\n",
      "Train MSE: 0.564\n",
      "Train MAE: 0.493\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 6, 43)             7740      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 6, 43)             14964     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7058 - val_loss: 0.8053\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7052 - val_loss: 0.8043\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7046 - val_loss: 0.8033\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7041 - val_loss: 0.8021\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.7035 - val_loss: 0.8010\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7029 - val_loss: 0.7998\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7023 - val_loss: 0.7986\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7017 - val_loss: 0.7973\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7011 - val_loss: 0.7960\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7004 - val_loss: 0.7947\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6998 - val_loss: 0.7934\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6990 - val_loss: 0.7920\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6983 - val_loss: 0.7906\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6976 - val_loss: 0.7892\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.6971 - val_loss: 0.7877\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.6962 - val_loss: 0.7863\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6955 - val_loss: 0.7848\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6948 - val_loss: 0.7833\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6940 - val_loss: 0.7817\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6932 - val_loss: 0.7802\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6924 - val_loss: 0.7786\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6917 - val_loss: 0.7770\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6909 - val_loss: 0.7753\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6901 - val_loss: 0.7736\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6893 - val_loss: 0.7720\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.6885 - val_loss: 0.7702\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.6876 - val_loss: 0.7685\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6867 - val_loss: 0.7667\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6857 - val_loss: 0.7649\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6849 - val_loss: 0.7631\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6840 - val_loss: 0.7612\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6831 - val_loss: 0.7593\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6820 - val_loss: 0.7574\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.6812 - val_loss: 0.7555\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6802 - val_loss: 0.7535\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6792 - val_loss: 0.7515\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.6782 - val_loss: 0.7495\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.6773 - val_loss: 0.7475\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6762 - val_loss: 0.7454\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6752 - val_loss: 0.7433\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6742 - val_loss: 0.7412\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6733 - val_loss: 0.7391\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6721 - val_loss: 0.7370\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6711 - val_loss: 0.7349\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6700 - val_loss: 0.7327\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6688 - val_loss: 0.7305\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6677 - val_loss: 0.7283\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.6667 - val_loss: 0.7261\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.6657 - val_loss: 0.7239\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6645 - val_loss: 0.7216\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6633 - val_loss: 0.7194\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6621 - val_loss: 0.7171\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6611 - val_loss: 0.7148\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6602 - val_loss: 0.7125\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6588 - val_loss: 0.7101\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6576 - val_loss: 0.7077\n",
      "Execution time:  88.28747320175171\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6603\n",
      "Root Mean Square Error: 0.9612\n",
      "Mean Square Error: 0.9238\n",
      "\n",
      "Train RMSE: 0.961\n",
      "Train MSE: 0.924\n",
      "Train MAE: 0.660\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 6, 45)             8460      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 6, 45)             16380     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 20ms/step - loss: 0.7196 - val_loss: 0.7078\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7195 - val_loss: 0.7075\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7192 - val_loss: 0.7072\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7189 - val_loss: 0.7068\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7187 - val_loss: 0.7064\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7183 - val_loss: 0.7061\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7182 - val_loss: 0.7057\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7179 - val_loss: 0.7054\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7176 - val_loss: 0.7050\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7174 - val_loss: 0.7046\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7171 - val_loss: 0.7042\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7168 - val_loss: 0.7039\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7165 - val_loss: 0.7035\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.7163 - val_loss: 0.7031\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.7160 - val_loss: 0.7027\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.7157 - val_loss: 0.7023\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7155 - val_loss: 0.7019\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7152 - val_loss: 0.7015\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7149 - val_loss: 0.7011\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7146 - val_loss: 0.7008\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7144 - val_loss: 0.7004\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7141 - val_loss: 0.7000\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7138 - val_loss: 0.6996\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7136 - val_loss: 0.6992\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7132 - val_loss: 0.6988\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7130 - val_loss: 0.6983\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7127 - val_loss: 0.6979\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7125 - val_loss: 0.6975\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7122 - val_loss: 0.6971\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7119 - val_loss: 0.6967\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7116 - val_loss: 0.6963\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7114 - val_loss: 0.6959\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7110 - val_loss: 0.6955\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.7107 - val_loss: 0.6951\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.7105 - val_loss: 0.6946\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.7102 - val_loss: 0.6942\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7099 - val_loss: 0.6938\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7096 - val_loss: 0.6934\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7094 - val_loss: 0.6930\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7091 - val_loss: 0.6925\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7088 - val_loss: 0.6921\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.7085 - val_loss: 0.6917\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.7082 - val_loss: 0.6913\n",
      "Execution time:  41.589165687561035\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.7041\n",
      "Root Mean Square Error: 0.9905\n",
      "Mean Square Error: 0.9810\n",
      "\n",
      "Train RMSE: 0.990\n",
      "Train MSE: 0.981\n",
      "Train MAE: 0.704\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 6, 43)             7740      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 6, 43)             14964     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8905 - val_loss: 1.3057\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8904 - val_loss: 1.3054\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8902 - val_loss: 1.3051\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8900 - val_loss: 1.3048\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8898 - val_loss: 1.3044\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8896 - val_loss: 1.3040\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8894 - val_loss: 1.3037\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8893 - val_loss: 1.3033\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8891 - val_loss: 1.3029\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8889 - val_loss: 1.3025\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8886 - val_loss: 1.3021\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8884 - val_loss: 1.3017\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8882 - val_loss: 1.3013\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8880 - val_loss: 1.3009\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8878 - val_loss: 1.3004\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8875 - val_loss: 1.3000\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8873 - val_loss: 1.2996\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8871 - val_loss: 1.2991\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8869 - val_loss: 1.2987\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8866 - val_loss: 1.2983\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8864 - val_loss: 1.2978\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8862 - val_loss: 1.2973\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8859 - val_loss: 1.2969\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8857 - val_loss: 1.2964\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8854 - val_loss: 1.2960\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8852 - val_loss: 1.2955\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8850 - val_loss: 1.2950\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8847 - val_loss: 1.2945\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8845 - val_loss: 1.2940\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8842 - val_loss: 1.2935\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8839 - val_loss: 1.2930\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8837 - val_loss: 1.2925\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8834 - val_loss: 1.2920\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8832 - val_loss: 1.2915\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8829 - val_loss: 1.2910\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8826 - val_loss: 1.2905\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8824 - val_loss: 1.2900\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8821 - val_loss: 1.2894\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8818 - val_loss: 1.2889\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8815 - val_loss: 1.2884\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8813 - val_loss: 1.2878\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8809 - val_loss: 1.2873\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8807 - val_loss: 1.2867\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8804 - val_loss: 1.2862\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8801 - val_loss: 1.2856\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8798 - val_loss: 1.2850\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8795 - val_loss: 1.2844\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8792 - val_loss: 1.2839\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8789 - val_loss: 1.2833\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8786 - val_loss: 1.2827\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8783 - val_loss: 1.2821\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8779 - val_loss: 1.2815\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8777 - val_loss: 1.2809\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8773 - val_loss: 1.2803\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.8770 - val_loss: 1.2796\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.8766 - val_loss: 1.2790\n",
      "Execution time:  90.68845510482788\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9154\n",
      "Root Mean Square Error: 1.1038\n",
      "Mean Square Error: 1.2183\n",
      "\n",
      "Train RMSE: 1.104\n",
      "Train MSE: 1.218\n",
      "Train MAE: 0.915\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 6, 45)             8460      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 6, 45)             16380     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.8784 - val_loss: 1.1381\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8783 - val_loss: 1.1380\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8782 - val_loss: 1.1380\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8782 - val_loss: 1.1379\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8781 - val_loss: 1.1378\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8781 - val_loss: 1.1377\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8780 - val_loss: 1.1376\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8780 - val_loss: 1.1375\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8779 - val_loss: 1.1374\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8779 - val_loss: 1.1373\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8778 - val_loss: 1.1372\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8778 - val_loss: 1.1371\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8777 - val_loss: 1.1370\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8776 - val_loss: 1.1370\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8776 - val_loss: 1.1369\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8775 - val_loss: 1.1368\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8775 - val_loss: 1.1366\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8774 - val_loss: 1.1365\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8773 - val_loss: 1.1364\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8773 - val_loss: 1.1363\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8772 - val_loss: 1.1362\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8772 - val_loss: 1.1361\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8771 - val_loss: 1.1360\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8770 - val_loss: 1.1359\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8770 - val_loss: 1.1358\n",
      "Epoch 26/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8769 - val_loss: 1.1357\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8768 - val_loss: 1.1356\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8768 - val_loss: 1.1355\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8767 - val_loss: 1.1354\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8766 - val_loss: 1.1353\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8766 - val_loss: 1.1352\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8765 - val_loss: 1.1350\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8765 - val_loss: 1.1349\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8764 - val_loss: 1.1348\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8763 - val_loss: 1.1347\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8763 - val_loss: 1.1346\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8762 - val_loss: 1.1345\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8761 - val_loss: 1.1344\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8761 - val_loss: 1.1342\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8760 - val_loss: 1.1341\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8759 - val_loss: 1.1340\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.8758 - val_loss: 1.1339\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8758 - val_loss: 1.1338\n",
      "Execution time:  41.50987648963928\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9261\n",
      "Root Mean Square Error: 1.1132\n",
      "Mean Square Error: 1.2392\n",
      "\n",
      "Train RMSE: 1.113\n",
      "Train MSE: 1.239\n",
      "Train MAE: 0.926\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 6, 43)             7740      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 6, 43)             14964     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "316/326 [============================>.] - ETA: 0s - loss: 0.5068WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4995 - val_loss: 0.3910\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.3673 - val_loss: 0.3296\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.3179 - val_loss: 0.2528\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2976 - val_loss: 0.2183\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2872 - val_loss: 0.2027\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2824 - val_loss: 0.1945\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2796 - val_loss: 0.1887\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2784 - val_loss: 0.1862\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2772 - val_loss: 0.1860\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2768 - val_loss: 0.1803\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2769 - val_loss: 0.1778\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2767 - val_loss: 0.1791\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2760 - val_loss: 0.1796\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2752 - val_loss: 0.1764\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2751 - val_loss: 0.1720\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2747 - val_loss: 0.1726\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2748 - val_loss: 0.1729\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2745 - val_loss: 0.1706\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2743 - val_loss: 0.1711\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2737 - val_loss: 0.1672\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2729 - val_loss: 0.1709\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2729 - val_loss: 0.1665\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2732 - val_loss: 0.1663\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2726 - val_loss: 0.1668\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2724 - val_loss: 0.1609\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2726 - val_loss: 0.1647\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2718 - val_loss: 0.1609\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2723 - val_loss: 0.1577\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2722 - val_loss: 0.1599\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2719 - val_loss: 0.1590\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2715 - val_loss: 0.1546\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2707 - val_loss: 0.1535\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2703 - val_loss: 0.1531\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2706 - val_loss: 0.1499\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2702 - val_loss: 0.1448\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2703 - val_loss: 0.1472\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2696 - val_loss: 0.1424\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2698 - val_loss: 0.1403\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2694 - val_loss: 0.1400\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2696 - val_loss: 0.1374\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2688 - val_loss: 0.1348\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2685 - val_loss: 0.1377\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2681 - val_loss: 0.1319\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2679 - val_loss: 0.1277\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2678 - val_loss: 0.1287\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2678 - val_loss: 0.1270\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2673 - val_loss: 0.1242\n",
      "Epoch 48/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2670 - val_loss: 0.1256\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2664 - val_loss: 0.1215\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2664 - val_loss: 0.1204\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2655 - val_loss: 0.1191\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2658 - val_loss: 0.1186\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2659 - val_loss: 0.1149\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2652 - val_loss: 0.1169\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2650 - val_loss: 0.1155\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2648 - val_loss: 0.1142\n",
      "Execution time:  91.40200185775757\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1623\n",
      "Root Mean Square Error: 0.5744\n",
      "Mean Square Error: 0.3299\n",
      "\n",
      "Train RMSE: 0.574\n",
      "Train MSE: 0.330\n",
      "Train MAE: 0.162\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 6, 45)             8460      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 6, 45)             16380     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.6122 - val_loss: 0.4047\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4330 - val_loss: 0.3774\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.3977 - val_loss: 0.3483\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.3723 - val_loss: 0.3157\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.3472 - val_loss: 0.2860\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.3248 - val_loss: 0.2635\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.3082 - val_loss: 0.2505\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2981 - val_loss: 0.2411\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2905 - val_loss: 0.2339\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.2277\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2795 - val_loss: 0.2230\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2759 - val_loss: 0.2199\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2738 - val_loss: 0.2176\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2731 - val_loss: 0.2163\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2719 - val_loss: 0.2155\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2715 - val_loss: 0.2151\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2710 - val_loss: 0.2144\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2705 - val_loss: 0.2140\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2703 - val_loss: 0.2133\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2708 - val_loss: 0.2127\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2702 - val_loss: 0.2127\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2701 - val_loss: 0.2122\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2704 - val_loss: 0.2123\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2699 - val_loss: 0.2120\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2700 - val_loss: 0.2114\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2695 - val_loss: 0.2110\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2695 - val_loss: 0.2111\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2694 - val_loss: 0.2111\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2694 - val_loss: 0.2107\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2690 - val_loss: 0.2106\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2690 - val_loss: 0.2099\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2689 - val_loss: 0.2097\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2688 - val_loss: 0.2096\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2681 - val_loss: 0.2088\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2681 - val_loss: 0.2090\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2687 - val_loss: 0.2087\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2679 - val_loss: 0.2083\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2681 - val_loss: 0.2078\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.2076\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2679 - val_loss: 0.2074\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2675 - val_loss: 0.2067\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2674 - val_loss: 0.2070\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.2671 - val_loss: 0.2068\n",
      "Execution time:  41.7000367641449\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1639\n",
      "Root Mean Square Error: 0.5757\n",
      "Mean Square Error: 0.3314\n",
      "\n",
      "Train RMSE: 0.576\n",
      "Train MSE: 0.331\n",
      "Train MAE: 0.164\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 6, 43)             7740      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 6, 43)             14964     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7347 - val_loss: 0.8898\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5789 - val_loss: 0.8460\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5517 - val_loss: 0.8259\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5392 - val_loss: 0.8158\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5320 - val_loss: 0.8107\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5280 - val_loss: 0.8083\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5257 - val_loss: 0.8073\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5244 - val_loss: 0.8067\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5235 - val_loss: 0.8065\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.8063\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5227 - val_loss: 0.8062\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5226 - val_loss: 0.8061\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5224 - val_loss: 0.8061\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5222 - val_loss: 0.8060\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5222 - val_loss: 0.8060\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5221 - val_loss: 0.8060\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5215 - val_loss: 0.8060\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5209 - val_loss: 0.8060\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5203 - val_loss: 0.8060\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5190 - val_loss: 0.8060\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5176 - val_loss: 0.8060\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5159 - val_loss: 0.8060\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5124 - val_loss: 0.8060\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5093 - val_loss: 0.8060\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5076 - val_loss: 0.8060\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5058 - val_loss: 0.8060\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5044 - val_loss: 0.8060\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5031 - val_loss: 0.8060\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5020 - val_loss: 0.8060\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5015 - val_loss: 0.8060\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5012 - val_loss: 0.8060\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5008 - val_loss: 0.8060\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5006 - val_loss: 0.8060\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5003 - val_loss: 0.8060\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5002 - val_loss: 0.8060\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5002 - val_loss: 0.8060\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5001 - val_loss: 0.8060\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5000 - val_loss: 0.8060\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4995 - val_loss: 0.8060\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4998 - val_loss: 0.8060\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4997 - val_loss: 0.8060\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5000 - val_loss: 0.8060\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4996 - val_loss: 0.8060\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4996 - val_loss: 0.8060\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4993 - val_loss: 0.8060\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4995 - val_loss: 0.8060\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4993 - val_loss: 0.8060\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4993 - val_loss: 0.8060\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4992 - val_loss: 0.8060\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4992 - val_loss: 0.8060\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4988 - val_loss: 0.8060\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4991 - val_loss: 0.8060\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4990 - val_loss: 0.8059\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4992 - val_loss: 0.8059\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4991 - val_loss: 0.8059\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4984 - val_loss: 0.8059\n",
      "Execution time:  90.45861434936523\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4900\n",
      "Root Mean Square Error: 0.7484\n",
      "Mean Square Error: 0.5601\n",
      "\n",
      "Train RMSE: 0.748\n",
      "Train MSE: 0.560\n",
      "Train MAE: 0.490\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 6, 45)             8460      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 6, 45)             16380     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 20ms/step - loss: 0.8385 - val_loss: 0.9584\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6544 - val_loss: 0.7762\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5960 - val_loss: 0.7405\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5746 - val_loss: 0.7223\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5613 - val_loss: 0.7098\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5522 - val_loss: 0.7003\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5454 - val_loss: 0.6927\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5401 - val_loss: 0.6867\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5357 - val_loss: 0.6819\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5322 - val_loss: 0.6783\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5296 - val_loss: 0.6757\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5278 - val_loss: 0.6738\n",
      "Epoch 13/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5263 - val_loss: 0.6723\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5251 - val_loss: 0.6712\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5243 - val_loss: 0.6703\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5235 - val_loss: 0.6696\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5229 - val_loss: 0.6690\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5224 - val_loss: 0.6685\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5219 - val_loss: 0.6679\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5212 - val_loss: 0.6673\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5203 - val_loss: 0.6668\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5195 - val_loss: 0.6663\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5186 - val_loss: 0.6658\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5170 - val_loss: 0.6653\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5153 - val_loss: 0.6647\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.5127 - val_loss: 0.6639\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5090 - val_loss: 0.6632\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5052 - val_loss: 0.6622\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5022 - val_loss: 0.6623\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5003 - val_loss: 0.6623\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4993 - val_loss: 0.6624\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4988 - val_loss: 0.6624\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4982 - val_loss: 0.6624\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4980 - val_loss: 0.6623\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4982 - val_loss: 0.6623\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4978 - val_loss: 0.6623\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4974 - val_loss: 0.6623\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4973 - val_loss: 0.6624\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4971 - val_loss: 0.6624\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4971 - val_loss: 0.6622\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 0.4969 - val_loss: 0.6624\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4968 - val_loss: 0.6623\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4970 - val_loss: 0.6623\n",
      "Execution time:  41.39910626411438\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4909\n",
      "Root Mean Square Error: 0.7490\n",
      "Mean Square Error: 0.5610\n",
      "\n",
      "Train RMSE: 0.749\n",
      "Train MSE: 0.561\n",
      "Train MAE: 0.491\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 18, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 18, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 5s 15ms/step - loss: 0.4619 - val_loss: 0.3679\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3831 - val_loss: 0.2967\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3597 - val_loss: 0.2615\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3509 - val_loss: 0.2423\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3467 - val_loss: 0.2354\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3458 - val_loss: 0.2344\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3443 - val_loss: 0.2287\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3437 - val_loss: 0.2251\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3432 - val_loss: 0.2285\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3424 - val_loss: 0.2216\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3419 - val_loss: 0.2218\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3411 - val_loss: 0.2209\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3408 - val_loss: 0.2170\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3406 - val_loss: 0.2138\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3401 - val_loss: 0.2157\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3394 - val_loss: 0.2134\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3394 - val_loss: 0.2092\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3392 - val_loss: 0.2093\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3382 - val_loss: 0.2080\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3381 - val_loss: 0.2060\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3376 - val_loss: 0.2033\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3371 - val_loss: 0.2020\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3369 - val_loss: 0.2004\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3367 - val_loss: 0.2012\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3362 - val_loss: 0.1980\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3362 - val_loss: 0.1982\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3361 - val_loss: 0.1957\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3359 - val_loss: 0.1937\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3356 - val_loss: 0.1908\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3351 - val_loss: 0.1898\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3348 - val_loss: 0.1871\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3341 - val_loss: 0.1870\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3340 - val_loss: 0.1821\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3340 - val_loss: 0.1828\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3342 - val_loss: 0.1804\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3338 - val_loss: 0.1775\n",
      "Epoch 37/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3332 - val_loss: 0.1771\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3335 - val_loss: 0.1725\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3331 - val_loss: 0.1706\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3325 - val_loss: 0.1653\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3323 - val_loss: 0.1657\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3326 - val_loss: 0.1653\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3318 - val_loss: 0.1607\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3319 - val_loss: 0.1581\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3315 - val_loss: 0.1585\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3312 - val_loss: 0.1568\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3311 - val_loss: 0.1558\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3319 - val_loss: 0.1539\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3304 - val_loss: 0.1510\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3308 - val_loss: 0.1498\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3305 - val_loss: 0.1462\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3300 - val_loss: 0.1466\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3297 - val_loss: 0.1458\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3299 - val_loss: 0.1450\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3302 - val_loss: 0.1407\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3296 - val_loss: 0.1400\n",
      "Execution time:  188.61861181259155\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1867\n",
      "Root Mean Square Error: 0.6067\n",
      "Mean Square Error: 0.3681\n",
      "\n",
      "Train RMSE: 0.607\n",
      "Train MSE: 0.368\n",
      "Train MAE: 0.187\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 18, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 18, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5234 - val_loss: 0.3605\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.4143 - val_loss: 0.3246\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3827 - val_loss: 0.3031\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3670 - val_loss: 0.2908\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3581 - val_loss: 0.2822\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3508 - val_loss: 0.2765\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3463 - val_loss: 0.2722\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3427 - val_loss: 0.2687\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3392 - val_loss: 0.2663\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3372 - val_loss: 0.2651\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3357 - val_loss: 0.2646\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3350 - val_loss: 0.2636\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3343 - val_loss: 0.2622\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3335 - val_loss: 0.2622\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3334 - val_loss: 0.2617\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3331 - val_loss: 0.2615\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3333 - val_loss: 0.2610\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3329 - val_loss: 0.2613\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3324 - val_loss: 0.2613\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3322 - val_loss: 0.2603\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3322 - val_loss: 0.2606\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3323 - val_loss: 0.2605\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3320 - val_loss: 0.2599\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3320 - val_loss: 0.2603\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3314 - val_loss: 0.2605A: 0s - loss: 0\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3318 - val_loss: 0.2598\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.3314 - val_loss: 0.2599\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.3315 - val_loss: 0.2599\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3311 - val_loss: 0.2592\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3314 - val_loss: 0.2585\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3310 - val_loss: 0.2588\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3307 - val_loss: 0.2586\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3306 - val_loss: 0.2578\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3302 - val_loss: 0.2579\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3303 - val_loss: 0.2581\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3303 - val_loss: 0.2574: \n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3300 - val_loss: 0.2572\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3295 - val_loss: 0.2567\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3296 - val_loss: 0.2571\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3292 - val_loss: 0.2571\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3289 - val_loss: 0.2565\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3288 - val_loss: 0.2554\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3288 - val_loss: 0.2557: 0s - los\n",
      "Execution time:  79.69973134994507\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1808\n",
      "Root Mean Square Error: 0.5989\n",
      "Mean Square Error: 0.3587\n",
      "\n",
      "Train RMSE: 0.599\n",
      "Train MSE: 0.359\n",
      "Train MAE: 0.181\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 18, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 18, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 13ms/step - loss: 0.6606 - val_loss: 0.8320\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5871 - val_loss: 0.8173\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5587 - val_loss: 0.8104\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5498 - val_loss: 0.8077\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5449 - val_loss: 0.8066\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5425 - val_loss: 0.8061\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5412 - val_loss: 0.8059\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5404 - val_loss: 0.8058\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5401 - val_loss: 0.8057\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5399 - val_loss: 0.8057\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5396 - val_loss: 0.8057\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5392 - val_loss: 0.8057\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5393 - val_loss: 0.8056\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5390 - val_loss: 0.8056\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5389 - val_loss: 0.8056\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5386 - val_loss: 0.8056\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5386 - val_loss: 0.8056\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5382 - val_loss: 0.8056\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5380 - val_loss: 0.8056\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5381 - val_loss: 0.8056\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5375 - val_loss: 0.8056\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5372 - val_loss: 0.8056\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5365 - val_loss: 0.8056\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5365 - val_loss: 0.8056\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5363 - val_loss: 0.8056\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5358 - val_loss: 0.8056\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5353 - val_loss: 0.8056\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5355 - val_loss: 0.8056\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5357 - val_loss: 0.8056\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5353 - val_loss: 0.8056\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5354 - val_loss: 0.8056\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5352 - val_loss: 0.8056\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5350 - val_loss: 0.8056\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5349 - val_loss: 0.8056\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5352 - val_loss: 0.8056\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5347 - val_loss: 0.8056\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5348 - val_loss: 0.8056\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5344 - val_loss: 0.8056\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5343 - val_loss: 0.8056\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5342 - val_loss: 0.8056\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5343 - val_loss: 0.8056\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5340 - val_loss: 0.8056\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5337 - val_loss: 0.8056\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5336 - val_loss: 0.8056\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5337 - val_loss: 0.8056\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5332 - val_loss: 0.8056\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5335 - val_loss: 0.8056\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5335 - val_loss: 0.8056\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5330 - val_loss: 0.8056\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5332 - val_loss: 0.8056\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5327 - val_loss: 0.8056\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5327 - val_loss: 0.8056\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5324 - val_loss: 0.8056\n",
      "Execution time:  185.33522510528564\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5096\n",
      "Root Mean Square Error: 0.7691\n",
      "Mean Square Error: 0.5915\n",
      "\n",
      "Train RMSE: 0.769\n",
      "Train MSE: 0.592\n",
      "Train MAE: 0.510\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 18, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 18, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.7444 - val_loss: 0.7594\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6187 - val_loss: 0.7285\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5943 - val_loss: 0.7137\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5771 - val_loss: 0.7042\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5655 - val_loss: 0.6979\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5564 - val_loss: 0.6941\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5518 - val_loss: 0.6909\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5483 - val_loss: 0.6882\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5453 - val_loss: 0.6861\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5430 - val_loss: 0.6846\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5412 - val_loss: 0.6833\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5397 - val_loss: 0.6826\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5391 - val_loss: 0.6815\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5383 - val_loss: 0.6808\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5378 - val_loss: 0.6805\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5374 - val_loss: 0.6799s: 0\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5372 - val_loss: 0.6797\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5369 - val_loss: 0.6793\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5367 - val_loss: 0.6794\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5363 - val_loss: 0.6793\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5359 - val_loss: 0.6792\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5358 - val_loss: 0.6792\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5357 - val_loss: 0.6789\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5355 - val_loss: 0.6791\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5354 - val_loss: 0.6789\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5352 - val_loss: 0.6788\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5348 - val_loss: 0.6793\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5349 - val_loss: 0.6789\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5347 - val_loss: 0.6788\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5345 - val_loss: 0.6789\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5345 - val_loss: 0.6788\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5338 - val_loss: 0.6788\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5341 - val_loss: 0.6787\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5336 - val_loss: 0.6788\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5336 - val_loss: 0.6790\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5333 - val_loss: 0.6784\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5333 - val_loss: 0.6785\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5330 - val_loss: 0.6787\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5329 - val_loss: 0.6784\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5331 - val_loss: 0.6784\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5327 - val_loss: 0.6791\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5325 - val_loss: 0.6789\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5326 - val_loss: 0.6782\n",
      "Execution time:  80.90086579322815\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5068\n",
      "Root Mean Square Error: 0.7639\n",
      "Mean Square Error: 0.5836\n",
      "\n",
      "Train RMSE: 0.764\n",
      "Train MSE: 0.584\n",
      "Train MAE: 0.507\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 18, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 18, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 13ms/step - loss: 0.7128 - val_loss: 0.8092\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.7110 - val_loss: 0.8062\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.7094 - val_loss: 0.8031\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.7077 - val_loss: 0.7999\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 11ms/step - loss: 0.7060 - val_loss: 0.7967\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.7042 - val_loss: 0.7934\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.7024 - val_loss: 0.7900\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.7006 - val_loss: 0.7865\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6988 - val_loss: 0.7830\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6968 - val_loss: 0.7794\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6950 - val_loss: 0.7756\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6930 - val_loss: 0.7718\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6911 - val_loss: 0.7679\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6891 - val_loss: 0.7639\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6869 - val_loss: 0.7598\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6848 - val_loss: 0.7556\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6827 - val_loss: 0.7513\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6805 - val_loss: 0.7469\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6784 - val_loss: 0.7424\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6760 - val_loss: 0.7378\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6738 - val_loss: 0.7331\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6714 - val_loss: 0.7284\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6690 - val_loss: 0.7235\n",
      "Epoch 24/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6666 - val_loss: 0.7185\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6641 - val_loss: 0.7134\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6615 - val_loss: 0.7081\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6587 - val_loss: 0.7027\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6563 - val_loss: 0.6972\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6534 - val_loss: 0.6915\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6507 - val_loss: 0.6857\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6480 - val_loss: 0.6797\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6448 - val_loss: 0.6735\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6417 - val_loss: 0.6672\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6387 - val_loss: 0.6607\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6354 - val_loss: 0.6540\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6323 - val_loss: 0.6472\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6288 - val_loss: 0.6402\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6254 - val_loss: 0.6331\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6215 - val_loss: 0.6257\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6183 - val_loss: 0.6183\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6144 - val_loss: 0.6107\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6105 - val_loss: 0.6030\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6069 - val_loss: 0.5952\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6032 - val_loss: 0.5874\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5997 - val_loss: 0.5796\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5958 - val_loss: 0.5717\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5918 - val_loss: 0.5639\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5882 - val_loss: 0.5562\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5843 - val_loss: 0.5486\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5808 - val_loss: 0.5410\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5773 - val_loss: 0.5335\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5739 - val_loss: 0.5261\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5703 - val_loss: 0.5189\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5670 - val_loss: 0.5117\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5637 - val_loss: 0.5046\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5607 - val_loss: 0.4977\n",
      "Execution time:  187.65354013442993\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5354\n",
      "Root Mean Square Error: 0.8736\n",
      "Mean Square Error: 0.7632\n",
      "\n",
      "Train RMSE: 0.874\n",
      "Train MSE: 0.763\n",
      "Train MAE: 0.535\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 18, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 18, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7050 - val_loss: 0.6922\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7040 - val_loss: 0.6911\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7032 - val_loss: 0.6899\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7023 - val_loss: 0.6888\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7013 - val_loss: 0.6875\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7004 - val_loss: 0.6863\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6994 - val_loss: 0.6851\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6985 - val_loss: 0.6838\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6975 - val_loss: 0.6826\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6964 - val_loss: 0.6813\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6954 - val_loss: 0.6800\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6944 - val_loss: 0.6787\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6934 - val_loss: 0.6774\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6924 - val_loss: 0.6761\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6914 - val_loss: 0.6747\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6904 - val_loss: 0.6734\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6895 - val_loss: 0.6720\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6883 - val_loss: 0.6707\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6873 - val_loss: 0.6693\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6862 - val_loss: 0.6679\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6851 - val_loss: 0.6665\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6840 - val_loss: 0.6651\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6831 - val_loss: 0.6637\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6819 - val_loss: 0.6623\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6809 - val_loss: 0.6609\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6797 - val_loss: 0.6594\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6786 - val_loss: 0.6580\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6776 - val_loss: 0.6565\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6764 - val_loss: 0.6550\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6753 - val_loss: 0.6535\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6741 - val_loss: 0.6520\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6730 - val_loss: 0.6505\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6719 - val_loss: 0.6490\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6707 - val_loss: 0.6475\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6694 - val_loss: 0.6460\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6684 - val_loss: 0.6445\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6672 - val_loss: 0.6429\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6662 - val_loss: 0.6414\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6648 - val_loss: 0.6398\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6637 - val_loss: 0.6382\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6624 - val_loss: 0.6366\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6613 - val_loss: 0.6350\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6599 - val_loss: 0.6334\n",
      "Execution time:  81.96568703651428\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6471\n",
      "Root Mean Square Error: 0.9355\n",
      "Mean Square Error: 0.8752\n",
      "\n",
      "Train RMSE: 0.936\n",
      "Train MSE: 0.875\n",
      "Train MAE: 0.647\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_36 (LSTM)               (None, 18, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 18, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 12ms/step - loss: 0.8954 - val_loss: 1.3100\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8950 - val_loss: 1.3091\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8945 - val_loss: 1.3083\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8940 - val_loss: 1.3074\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8935 - val_loss: 1.3065\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8930 - val_loss: 1.3055\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8926 - val_loss: 1.3046\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8920 - val_loss: 1.3036\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8915 - val_loss: 1.3027\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8909 - val_loss: 1.3017\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8904 - val_loss: 1.3007\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8898 - val_loss: 1.2996\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8892 - val_loss: 1.2986\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8887 - val_loss: 1.2975\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8881 - val_loss: 1.2965\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8875 - val_loss: 1.2954\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8870 - val_loss: 1.2943\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8864 - val_loss: 1.2932\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8858 - val_loss: 1.2921\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8851 - val_loss: 1.2909\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8845 - val_loss: 1.2898\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8839 - val_loss: 1.2886\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8833 - val_loss: 1.2874\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8826 - val_loss: 1.2862\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8819 - val_loss: 1.2850\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8812 - val_loss: 1.2837\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8806 - val_loss: 1.2824\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8799 - val_loss: 1.2811\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8792 - val_loss: 1.2798\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8784 - val_loss: 1.2785\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8777 - val_loss: 1.2771\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8769 - val_loss: 1.2757\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8761 - val_loss: 1.2743\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8754 - val_loss: 1.2729\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8745 - val_loss: 1.2714\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8737 - val_loss: 1.2699\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8729 - val_loss: 1.2683\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8721 - val_loss: 1.2668\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8712 - val_loss: 1.2652\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8703 - val_loss: 1.2636\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8694 - val_loss: 1.2619\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8684 - val_loss: 1.2602\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8675 - val_loss: 1.2585\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8666 - val_loss: 1.2567\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8656 - val_loss: 1.2549\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8645 - val_loss: 1.2530\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8634 - val_loss: 1.2512\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8623 - val_loss: 1.2493\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8613 - val_loss: 1.2473\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8602 - val_loss: 1.2453\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8590 - val_loss: 1.2433\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.8579 - val_loss: 1.2413\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8567 - val_loss: 1.2392\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8555 - val_loss: 1.2370\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8543 - val_loss: 1.2349\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8530 - val_loss: 1.2327\n",
      "Execution time:  179.52207684516907\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8846\n",
      "Root Mean Square Error: 1.0744\n",
      "Mean Square Error: 1.1544\n",
      "\n",
      "Train RMSE: 1.074\n",
      "Train MSE: 1.154\n",
      "Train MAE: 0.885\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_38 (LSTM)               (None, 18, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 18, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8843 - val_loss: 1.1420\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8842 - val_loss: 1.1419\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8842 - val_loss: 1.1418\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8841 - val_loss: 1.1416\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8840 - val_loss: 1.1415\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8839 - val_loss: 1.1414\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8838 - val_loss: 1.1412\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8838 - val_loss: 1.1411\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8836 - val_loss: 1.1409\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8836 - val_loss: 1.1408\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8835 - val_loss: 1.1406\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8834 - val_loss: 1.1405\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8833 - val_loss: 1.1403\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8832 - val_loss: 1.1402\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8831 - val_loss: 1.1400\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8830 - val_loss: 1.1399\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8829 - val_loss: 1.1397\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8828 - val_loss: 1.1396\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8827 - val_loss: 1.1394\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8826 - val_loss: 1.1392\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8825 - val_loss: 1.1391\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8824 - val_loss: 1.1389\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8823 - val_loss: 1.1387\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8822 - val_loss: 1.1386\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8821 - val_loss: 1.1384\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8820 - val_loss: 1.1382\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8819 - val_loss: 1.1381\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8818 - val_loss: 1.1379\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8817 - val_loss: 1.1377\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8816 - val_loss: 1.1376\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8815 - val_loss: 1.1374\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8813 - val_loss: 1.1372\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8812 - val_loss: 1.1370\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8811 - val_loss: 1.1369\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8810 - val_loss: 1.1367\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8809 - val_loss: 1.1365\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8808 - val_loss: 1.1363\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8807 - val_loss: 1.1361\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8806 - val_loss: 1.1360\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8805 - val_loss: 1.1358\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8804 - val_loss: 1.1356 0s - loss: \n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8803 - val_loss: 1.1354\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8801 - val_loss: 1.1352\n",
      "Execution time:  79.24483799934387\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9282\n",
      "Root Mean Square Error: 1.1156\n",
      "Mean Square Error: 1.2445\n",
      "\n",
      "Train RMSE: 1.116\n",
      "Train MSE: 1.245\n",
      "Train MAE: 0.928\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_40 (LSTM)               (None, 18, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 18, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 12ms/step - loss: 0.4931 - val_loss: 0.3888\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.4058 - val_loss: 0.3173\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3832 - val_loss: 0.2797\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3696 - val_loss: 0.2562\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3612 - val_loss: 0.2408\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3557 - val_loss: 0.2331\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3517 - val_loss: 0.2244\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3490 - val_loss: 0.2164\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3467 - val_loss: 0.2140\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3451 - val_loss: 0.2104\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3429 - val_loss: 0.2072\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3421 - val_loss: 0.2073\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3411 - val_loss: 0.2036\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3406 - val_loss: 0.2012\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3402 - val_loss: 0.1988\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3389 - val_loss: 0.1991\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3391 - val_loss: 0.1963\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3385 - val_loss: 0.1959\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3380 - val_loss: 0.1971\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3380 - val_loss: 0.1944\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3376 - val_loss: 0.1951\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3371 - val_loss: 0.1935\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3367 - val_loss: 0.1925\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3371 - val_loss: 0.1910\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3365 - val_loss: 0.1907\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3366 - val_loss: 0.1914\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3364 - val_loss: 0.1913\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3359 - val_loss: 0.1888\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3362 - val_loss: 0.1892\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3359 - val_loss: 0.1874\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3362 - val_loss: 0.1855\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3353 - val_loss: 0.1875\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3353 - val_loss: 0.1848\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3355 - val_loss: 0.1874\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3356 - val_loss: 0.1852\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3351 - val_loss: 0.1862\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3354 - val_loss: 0.1861\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3354 - val_loss: 0.1841\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3349 - val_loss: 0.1845\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3346 - val_loss: 0.1809\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3346 - val_loss: 0.1817\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3347 - val_loss: 0.1819\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3343 - val_loss: 0.1813\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3344 - val_loss: 0.1806\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3340 - val_loss: 0.1795\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3342 - val_loss: 0.1808\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3342 - val_loss: 0.1791\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3340 - val_loss: 0.1787\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3335 - val_loss: 0.1777\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3338 - val_loss: 0.1769\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3340 - val_loss: 0.1752\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3332 - val_loss: 0.1752\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3333 - val_loss: 0.1749\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3329 - val_loss: 0.1753\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3328 - val_loss: 0.1733\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.3330 - val_loss: 0.1740\n",
      "Execution time:  183.25079369544983\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1804\n",
      "Root Mean Square Error: 0.5810\n",
      "Mean Square Error: 0.3376\n",
      "\n",
      "Train RMSE: 0.581\n",
      "Train MSE: 0.338\n",
      "Train MAE: 0.180\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 18, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 18, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5410 - val_loss: 0.3844\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.4389 - val_loss: 0.3493\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.4133 - val_loss: 0.3279\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3990 - val_loss: 0.3156\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3877 - val_loss: 0.3053\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3782 - val_loss: 0.2974\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3709 - val_loss: 0.2913\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3649 - val_loss: 0.2871\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3599 - val_loss: 0.2834\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3563 - val_loss: 0.2803\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3530 - val_loss: 0.2778\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3501 - val_loss: 0.2755 l\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3478 - val_loss: 0.2733\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3454 - val_loss: 0.2712\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3438 - val_loss: 0.2693\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3422 - val_loss: 0.2678\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3406 - val_loss: 0.2668\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3392 - val_loss: 0.2653\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3381 - val_loss: 0.2646\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3369 - val_loss: 0.2636\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3362 - val_loss: 0.2629\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3352 - val_loss: 0.2621\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3342 - val_loss: 0.2616\n",
      "Epoch 24/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3341 - val_loss: 0.2611\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3332 - val_loss: 0.2604\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3330 - val_loss: 0.2599\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3323 - val_loss: 0.2593\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3322 - val_loss: 0.2586\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3314 - val_loss: 0.2584\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3314 - val_loss: 0.2581\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3311 - val_loss: 0.2578\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.3308 - val_loss: 0.2574\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3304 - val_loss: 0.2572\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3302 - val_loss: 0.2572\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3303 - val_loss: 0.2566\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3303 - val_loss: 0.2566\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3298 - val_loss: 0.2564\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3297 - val_loss: 0.2564\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3300 - val_loss: 0.2563\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3295 - val_loss: 0.2562\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3291 - val_loss: 0.2560\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3290 - val_loss: 0.2558\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3290 - val_loss: 0.2559\n",
      "Execution time:  80.3897271156311\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.1800\n",
      "Root Mean Square Error: 0.5803\n",
      "Mean Square Error: 0.3367\n",
      "\n",
      "Train RMSE: 0.580\n",
      "Train MSE: 0.337\n",
      "Train MAE: 0.180\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_44 (LSTM)               (None, 18, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 18, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 13ms/step - loss: 0.7259 - val_loss: 0.8687\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6192 - val_loss: 0.8445\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.6043 - val_loss: 0.8344\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5911 - val_loss: 0.8273\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5784 - val_loss: 0.8225\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5707 - val_loss: 0.8188\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5641 - val_loss: 0.8159\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5590 - val_loss: 0.8135\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5551 - val_loss: 0.8117\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5526 - val_loss: 0.8103\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5499 - val_loss: 0.8092\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5487 - val_loss: 0.8084\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5472 - val_loss: 0.8078\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5456 - val_loss: 0.8073\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5446 - val_loss: 0.8069\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5442 - val_loss: 0.8067\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5430 - val_loss: 0.8064\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5425 - val_loss: 0.8063\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5415 - val_loss: 0.8061\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5410 - val_loss: 0.8060\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5410 - val_loss: 0.8060\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5406 - val_loss: 0.8059\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5401 - val_loss: 0.8058\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5398 - val_loss: 0.8058\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5396 - val_loss: 0.8058\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5396 - val_loss: 0.8058\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5392 - val_loss: 0.8057\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5390 - val_loss: 0.8057\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5388 - val_loss: 0.8057\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5388 - val_loss: 0.8057\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5384 - val_loss: 0.8057\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5385 - val_loss: 0.8057\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5386 - val_loss: 0.8057\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5385 - val_loss: 0.8057\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5382 - val_loss: 0.8057\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5382 - val_loss: 0.8056\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5383 - val_loss: 0.8056\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5384 - val_loss: 0.8056\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5382 - val_loss: 0.8056\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5382 - val_loss: 0.8056\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5380 - val_loss: 0.8056\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5376 - val_loss: 0.8056\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5378 - val_loss: 0.8056\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5378 - val_loss: 0.8056\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5377 - val_loss: 0.8056\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5378 - val_loss: 0.8056\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5377 - val_loss: 0.8056\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5375 - val_loss: 0.8056\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5376 - val_loss: 0.8056\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5374 - val_loss: 0.8056\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5375 - val_loss: 0.8056\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.5374 - val_loss: 0.8056\n",
      "Execution time:  184.46612906455994\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5021\n",
      "Root Mean Square Error: 0.7550\n",
      "Mean Square Error: 0.5701\n",
      "\n",
      "Train RMSE: 0.755\n",
      "Train MSE: 0.570\n",
      "Train MAE: 0.502\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_46 (LSTM)               (None, 18, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 18, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.8043 - val_loss: 0.8641\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6547 - val_loss: 0.7682\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6288 - val_loss: 0.7484\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6181 - val_loss: 0.7382\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6102 - val_loss: 0.7309\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6031 - val_loss: 0.7249\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5951 - val_loss: 0.7191\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5876 - val_loss: 0.7143\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5826 - val_loss: 0.7106\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5767 - val_loss: 0.7071\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5708 - val_loss: 0.7042\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5667 - val_loss: 0.7013\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5634 - val_loss: 0.6989\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5605 - val_loss: 0.6966\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5580 - val_loss: 0.6948\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.5559 - val_loss: 0.6932\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5541 - val_loss: 0.6918\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5524 - val_loss: 0.6904\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5508 - val_loss: 0.6894\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5494 - val_loss: 0.6883\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5483 - val_loss: 0.6874\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5470 - val_loss: 0.6864\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5460 - val_loss: 0.6856\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5452 - val_loss: 0.6851\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5446 - val_loss: 0.6844\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5434 - val_loss: 0.6839\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5432 - val_loss: 0.6833\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5426 - val_loss: 0.6829\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5418 - val_loss: 0.6826\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5414 - val_loss: 0.6821\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5409 - val_loss: 0.6818\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5406 - val_loss: 0.6815\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5401 - val_loss: 0.6813\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5397 - val_loss: 0.6811\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5394 - val_loss: 0.6809\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5392 - val_loss: 0.6806\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5389 - val_loss: 0.6806\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5389 - val_loss: 0.6803\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5386 - val_loss: 0.6801\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5383 - val_loss: 0.6801\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5378 - val_loss: 0.6800\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5380 - val_loss: 0.6799\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5377 - val_loss: 0.6799\n",
      "Execution time:  79.56351470947266\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5041\n",
      "Root Mean Square Error: 0.7550\n",
      "Mean Square Error: 0.5701\n",
      "\n",
      "Train RMSE: 0.755\n",
      "Train MSE: 0.570\n",
      "Train MAE: 0.504\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_48 (LSTM)               (None, 36, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 36, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5046 - val_loss: 0.3819\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.4459 - val_loss: 0.3214\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4292 - val_loss: 0.2924\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.4216 - val_loss: 0.2774\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4174 - val_loss: 0.2698\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4150 - val_loss: 0.2635\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4134 - val_loss: 0.2591\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4123 - val_loss: 0.2584\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4113 - val_loss: 0.2556\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4116 - val_loss: 0.2541\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4114 - val_loss: 0.2548\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4109 - val_loss: 0.2544\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4106 - val_loss: 0.2552\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4103 - val_loss: 0.2540\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4097 - val_loss: 0.2549\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4103 - val_loss: 0.2498\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4087 - val_loss: 0.2479\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4094 - val_loss: 0.2499\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4083 - val_loss: 0.2470\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4079 - val_loss: 0.2473\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4079 - val_loss: 0.2465\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4074 - val_loss: 0.2455\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4073 - val_loss: 0.2469\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4070 - val_loss: 0.2452\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4063 - val_loss: 0.2452\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4058 - val_loss: 0.2441\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4052 - val_loss: 0.2428\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4051 - val_loss: 0.2449\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4048 - val_loss: 0.2459\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4047 - val_loss: 0.2426\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4037 - val_loss: 0.2426\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4038 - val_loss: 0.2418\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4038 - val_loss: 0.2428\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4034 - val_loss: 0.2399\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4029 - val_loss: 0.2403\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4022 - val_loss: 0.2403\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4024 - val_loss: 0.2404\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4013 - val_loss: 0.2383\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4026 - val_loss: 0.2399\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4007 - val_loss: 0.2377\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4008 - val_loss: 0.2358\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4012 - val_loss: 0.2368\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4010 - val_loss: 0.2391\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4007 - val_loss: 0.2379\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3989 - val_loss: 0.2394\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3991 - val_loss: 0.2369\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3987 - val_loss: 0.2344\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3987 - val_loss: 0.2365\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3986 - val_loss: 0.2365\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3983 - val_loss: 0.2386\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3975 - val_loss: 0.2339\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3976 - val_loss: 0.2359\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3972 - val_loss: 0.2373\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3970 - val_loss: 0.2366\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3971 - val_loss: 0.2365\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.3968 - val_loss: 0.2355\n",
      "Execution time:  442.22993898391724\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2268\n",
      "Root Mean Square Error: 0.6344\n",
      "Mean Square Error: 0.4025\n",
      "\n",
      "Train RMSE: 0.634\n",
      "Train MSE: 0.402\n",
      "Train MAE: 0.227\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_50 (LSTM)               (None, 36, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_51 (LSTM)               (None, 36, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 0.5463 - val_loss: 0.3999\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4744 - val_loss: 0.3748\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.4501 - val_loss: 0.3602\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4366 - val_loss: 0.3511\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4280 - val_loss: 0.3452\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4223 - val_loss: 0.3411\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4185 - val_loss: 0.3383\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.4151 - val_loss: 0.3354\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4122 - val_loss: 0.3336\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4102 - val_loss: 0.3316\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4082 - val_loss: 0.3297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4063 - val_loss: 0.3275\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4044 - val_loss: 0.3264\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4028 - val_loss: 0.3258\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4018 - val_loss: 0.3250\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4009 - val_loss: 0.3244\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4003 - val_loss: 0.3237\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3997 - val_loss: 0.3237\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.3989 - val_loss: 0.3231\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3984 - val_loss: 0.3231\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3982 - val_loss: 0.3228\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3977 - val_loss: 0.3226\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3975 - val_loss: 0.3226\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.3973 - val_loss: 0.3221\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3968 - val_loss: 0.3224\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3965 - val_loss: 0.3218\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3968 - val_loss: 0.3224\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3964 - val_loss: 0.3220\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.3963 - val_loss: 0.3222\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.3959 - val_loss: 0.3223\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3958 - val_loss: 0.3228\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3951 - val_loss: 0.3219\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3955 - val_loss: 0.3227\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.3947 - val_loss: 0.3227\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.3942 - val_loss: 0.3231\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3945 - val_loss: 0.3228\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3945 - val_loss: 0.3230\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3939 - val_loss: 0.3231\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3936 - val_loss: 0.3235\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.3932 - val_loss: 0.3239\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.3929 - val_loss: 0.3243\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3928 - val_loss: 0.3244\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3922 - val_loss: 0.3249\n",
      "Execution time:  134.29966068267822\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2109\n",
      "Root Mean Square Error: 0.6137\n",
      "Mean Square Error: 0.3766\n",
      "\n",
      "Train RMSE: 0.614\n",
      "Train MSE: 0.377\n",
      "Train MAE: 0.211\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_52 (LSTM)               (None, 36, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_53 (LSTM)               (None, 36, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 8s 26ms/step - loss: 0.6805 - val_loss: 0.8273\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6181 - val_loss: 0.8170\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5999 - val_loss: 0.8119\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5936 - val_loss: 0.8091\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5890 - val_loss: 0.8074\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5865 - val_loss: 0.8064\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5842 - val_loss: 0.8058\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5830 - val_loss: 0.8055\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5822 - val_loss: 0.8053\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5815 - val_loss: 0.8052\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5812 - val_loss: 0.8051\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5806 - val_loss: 0.8051\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5803 - val_loss: 0.8050\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5801 - val_loss: 0.8050\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5799 - val_loss: 0.8050\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5793 - val_loss: 0.8050\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5789 - val_loss: 0.8050\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5787 - val_loss: 0.8050\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 8s 25ms/step - loss: 0.5782 - val_loss: 0.8049\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5778 - val_loss: 0.8049\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5776 - val_loss: 0.8049\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5769 - val_loss: 0.8049\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 8s 25ms/step - loss: 0.5761 - val_loss: 0.8049\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5761 - val_loss: 0.8049\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5755 - val_loss: 0.8049\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5747 - val_loss: 0.8049\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5754 - val_loss: 0.8049\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5755 - val_loss: 0.8049\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5737 - val_loss: 0.8049\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5736 - val_loss: 0.8049\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5732 - val_loss: 0.8049\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5726 - val_loss: 0.8049\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5717 - val_loss: 0.8049\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5713 - val_loss: 0.8049\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5715 - val_loss: 0.8049\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5708 - val_loss: 0.8049\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5713 - val_loss: 0.8049\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5705 - val_loss: 0.8049\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5700 - val_loss: 0.8049\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5698 - val_loss: 0.8049\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5688 - val_loss: 0.8049\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5693 - val_loss: 0.8049\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5683 - val_loss: 0.8049\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5685 - val_loss: 0.8049\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5671 - val_loss: 0.8049\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5668 - val_loss: 0.8049\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5675 - val_loss: 0.8049\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5662 - val_loss: 0.8049\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5671 - val_loss: 0.8049\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5677 - val_loss: 0.8049\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5662 - val_loss: 0.8049\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5661 - val_loss: 0.8049\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5653 - val_loss: 0.8049\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5651 - val_loss: 0.8049\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 8s 25ms/step - loss: 0.5655 - val_loss: 0.8049\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 8s 25ms/step - loss: 0.5645 - val_loss: 0.8049\n",
      "Execution time:  440.38399744033813\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5417\n",
      "Root Mean Square Error: 0.8082\n",
      "Mean Square Error: 0.6531\n",
      "\n",
      "Train RMSE: 0.808\n",
      "Train MSE: 0.653\n",
      "Train MAE: 0.542\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_54 (LSTM)               (None, 36, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 36, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 39ms/step - loss: 0.7355 - val_loss: 0.7710\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.6397 - val_loss: 0.7466\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.6254 - val_loss: 0.7375\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.6170 - val_loss: 0.7292\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.6048 - val_loss: 0.7220\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5973 - val_loss: 0.7181\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.5934 - val_loss: 0.7155\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5899 - val_loss: 0.7139\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5873 - val_loss: 0.7127\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5853 - val_loss: 0.7115\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5835 - val_loss: 0.7103\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.5822 - val_loss: 0.7098\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5811 - val_loss: 0.7089\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5801 - val_loss: 0.7084\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5795 - val_loss: 0.7079\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5785 - val_loss: 0.7072\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5779 - val_loss: 0.7070\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5773 - val_loss: 0.7070\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5768 - val_loss: 0.7064\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5766 - val_loss: 0.7065\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5764 - val_loss: 0.7063\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5761 - val_loss: 0.7062\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.5755 - val_loss: 0.7062\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5753 - val_loss: 0.7055\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5751 - val_loss: 0.7056\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5749 - val_loss: 0.7054\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5747 - val_loss: 0.7055\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.5742 - val_loss: 0.7055\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5741 - val_loss: 0.7057\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5740 - val_loss: 0.7055\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5737 - val_loss: 0.7062\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5733 - val_loss: 0.7057\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.5732 - val_loss: 0.7066\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.5726 - val_loss: 0.7061\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5726 - val_loss: 0.7057\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5725 - val_loss: 0.7066\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5716 - val_loss: 0.7066\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5712 - val_loss: 0.7069\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.5713 - val_loss: 0.7045\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5715 - val_loss: 0.7050\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5710 - val_loss: 0.7027\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5710 - val_loss: 0.7040\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5703 - val_loss: 0.7049\n",
      "Execution time:  135.72730016708374\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5231\n",
      "Root Mean Square Error: 0.7820\n",
      "Mean Square Error: 0.6115\n",
      "\n",
      "Train RMSE: 0.782\n",
      "Train MSE: 0.612\n",
      "Train MAE: 0.523\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_56 (LSTM)               (None, 36, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 36, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 8s 26ms/step - loss: 0.7094 - val_loss: 0.8032\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.7072 - val_loss: 0.7989\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.7051 - val_loss: 0.7945\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.7027 - val_loss: 0.7899\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.7004 - val_loss: 0.7851\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6980 - val_loss: 0.7802\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6955 - val_loss: 0.7752\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6930 - val_loss: 0.7700\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6903 - val_loss: 0.7646\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6876 - val_loss: 0.7591\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6848 - val_loss: 0.7533\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6821 - val_loss: 0.7473\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6790 - val_loss: 0.7411\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6760 - val_loss: 0.7347\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6730 - val_loss: 0.7280\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6696 - val_loss: 0.7211\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6663 - val_loss: 0.7140\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6628 - val_loss: 0.7066\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6593 - val_loss: 0.6989\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6555 - val_loss: 0.6910\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6518 - val_loss: 0.6827\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6478 - val_loss: 0.6741\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6439 - val_loss: 0.6652\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6395 - val_loss: 0.6558\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6351 - val_loss: 0.6461\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6303 - val_loss: 0.6360\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6255 - val_loss: 0.6256\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6205 - val_loss: 0.6147\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6152 - val_loss: 0.6036\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6102 - val_loss: 0.5921\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.6047 - val_loss: 0.5804\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5992 - val_loss: 0.5685\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5939 - val_loss: 0.5566\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5885 - val_loss: 0.5449\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5835 - val_loss: 0.5334\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5782 - val_loss: 0.5221\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5734 - val_loss: 0.5111\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5688 - val_loss: 0.5004\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5643 - val_loss: 0.4900\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5600 - val_loss: 0.4800\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5563 - val_loss: 0.4703\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5525 - val_loss: 0.4609\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5491 - val_loss: 0.4517\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5454 - val_loss: 0.4428\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5423 - val_loss: 0.4342\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5391 - val_loss: 0.4259\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5364 - val_loss: 0.4178\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5335 - val_loss: 0.4101\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5309 - val_loss: 0.4027\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5283 - val_loss: 0.3959\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5263 - val_loss: 0.3896\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5248 - val_loss: 0.3839\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5225 - val_loss: 0.3787\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5208 - val_loss: 0.3741\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.5198 - val_loss: 0.3699\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 8s 23ms/step - loss: 0.5181 - val_loss: 0.3661\n",
      "Execution time:  441.71819138526917\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4465\n",
      "Root Mean Square Error: 0.8062\n",
      "Mean Square Error: 0.6500\n",
      "\n",
      "Train RMSE: 0.806\n",
      "Train MSE: 0.650\n",
      "Train MAE: 0.447\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_58 (LSTM)               (None, 36, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 36, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 0.7267 - val_loss: 0.7137\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7264 - val_loss: 0.7132\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7259 - val_loss: 0.7126\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7255 - val_loss: 0.7121\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7251 - val_loss: 0.7115\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7246 - val_loss: 0.7109\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7242 - val_loss: 0.7103\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7237 - val_loss: 0.7097\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7233 - val_loss: 0.7091\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7228 - val_loss: 0.7085\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7224 - val_loss: 0.7079\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7219 - val_loss: 0.7073\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7215 - val_loss: 0.7067\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7210 - val_loss: 0.7060\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7205 - val_loss: 0.7054\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7201 - val_loss: 0.7048\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7197 - val_loss: 0.7042\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7192 - val_loss: 0.7036\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7187 - val_loss: 0.7029\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7183 - val_loss: 0.7023\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7178 - val_loss: 0.7017\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7174 - val_loss: 0.7011\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7170 - val_loss: 0.7004\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7165 - val_loss: 0.6998\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7160 - val_loss: 0.6992\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7156 - val_loss: 0.6985\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7151 - val_loss: 0.6979\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7147 - val_loss: 0.6972\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7142 - val_loss: 0.6966\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7138 - val_loss: 0.6959\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7133 - val_loss: 0.6953\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7128 - val_loss: 0.6946\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7124 - val_loss: 0.6940\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7119 - val_loss: 0.6933\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7114 - val_loss: 0.6927\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7109 - val_loss: 0.6920\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7105 - val_loss: 0.6913\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7100 - val_loss: 0.6906\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7095 - val_loss: 0.6900\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7090 - val_loss: 0.6893\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7086 - val_loss: 0.6886\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7080 - val_loss: 0.6879\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7076 - val_loss: 0.6872\n",
      "Execution time:  132.74413561820984\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6987\n",
      "Root Mean Square Error: 0.9866\n",
      "Mean Square Error: 0.9733\n",
      "\n",
      "Train RMSE: 0.987\n",
      "Train MSE: 0.973\n",
      "Train MAE: 0.699\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_60 (LSTM)               (None, 36, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 36, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.8979 - val_loss: 1.3064\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8975 - val_loss: 1.3056\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8971 - val_loss: 1.3047\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8967 - val_loss: 1.3038\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8962 - val_loss: 1.3029\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8958 - val_loss: 1.3020\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8952 - val_loss: 1.3010\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8948 - val_loss: 1.3000\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8943 - val_loss: 1.2990\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8938 - val_loss: 1.2979\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8932 - val_loss: 1.2969\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8927 - val_loss: 1.2958\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8922 - val_loss: 1.2947\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8916 - val_loss: 1.2935\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8910 - val_loss: 1.2924\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8905 - val_loss: 1.2912\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8898 - val_loss: 1.2900\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8892 - val_loss: 1.2887\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8886 - val_loss: 1.2875\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8880 - val_loss: 1.2862\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8873 - val_loss: 1.2849\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8866 - val_loss: 1.2835\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8860 - val_loss: 1.2822\n",
      "Epoch 24/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8852 - val_loss: 1.2808\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8846 - val_loss: 1.2793\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8838 - val_loss: 1.2778\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8830 - val_loss: 1.2763\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8823 - val_loss: 1.2748\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8815 - val_loss: 1.2732\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8806 - val_loss: 1.2716\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8798 - val_loss: 1.2699\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8789 - val_loss: 1.2682\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8780 - val_loss: 1.2665\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8771 - val_loss: 1.2647\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8763 - val_loss: 1.2628\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8752 - val_loss: 1.2609\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8743 - val_loss: 1.2590\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8732 - val_loss: 1.2570\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8722 - val_loss: 1.2550\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8710 - val_loss: 1.2529\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8700 - val_loss: 1.2508\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8688 - val_loss: 1.2486\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8676 - val_loss: 1.2464\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8664 - val_loss: 1.2441\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8652 - val_loss: 1.2418\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8639 - val_loss: 1.2394\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8627 - val_loss: 1.2369\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8613 - val_loss: 1.2344\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8600 - val_loss: 1.2319\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8585 - val_loss: 1.2293\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8571 - val_loss: 1.2266\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8557 - val_loss: 1.2239\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8542 - val_loss: 1.2212\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8525 - val_loss: 1.2183\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8510 - val_loss: 1.2154\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.8494 - val_loss: 1.2125\n",
      "Execution time:  443.73863768577576\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8735\n",
      "Root Mean Square Error: 1.0650\n",
      "Mean Square Error: 1.1343\n",
      "\n",
      "Train RMSE: 1.065\n",
      "Train MSE: 1.134\n",
      "Train MAE: 0.873\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_62 (LSTM)               (None, 36, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 36, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 0.8899 - val_loss: 1.1449\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8898 - val_loss: 1.1448\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8897 - val_loss: 1.1446\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8896 - val_loss: 1.1444\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8894 - val_loss: 1.1442\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8893 - val_loss: 1.1440\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8892 - val_loss: 1.1438\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8890 - val_loss: 1.1435\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8889 - val_loss: 1.1433\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8888 - val_loss: 1.1431\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8886 - val_loss: 1.1429\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8885 - val_loss: 1.1427\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8883 - val_loss: 1.1424\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8882 - val_loss: 1.1422\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8881 - val_loss: 1.1420\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8879 - val_loss: 1.1418\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8878 - val_loss: 1.1415\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8876 - val_loss: 1.1413\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8875 - val_loss: 1.1411\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8873 - val_loss: 1.1408\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8872 - val_loss: 1.1406\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8870 - val_loss: 1.1403\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8869 - val_loss: 1.1401\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8867 - val_loss: 1.1399\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8866 - val_loss: 1.1396\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.8864 - val_loss: 1.1394\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 4s 38ms/step - loss: 0.8863 - val_loss: 1.1391\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8861 - val_loss: 1.1389\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8860 - val_loss: 1.1386\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.8858 - val_loss: 1.1384\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8857 - val_loss: 1.1381\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8855 - val_loss: 1.1379\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8853 - val_loss: 1.1376\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8852 - val_loss: 1.1373\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8850 - val_loss: 1.1371\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8849 - val_loss: 1.1368\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8847 - val_loss: 1.1365\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8845 - val_loss: 1.1363\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8844 - val_loss: 1.1360\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8842 - val_loss: 1.1357\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8840 - val_loss: 1.1355\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8839 - val_loss: 1.1352\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8837 - val_loss: 1.1349\n",
      "Execution time:  137.9662766456604\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9291\n",
      "Root Mean Square Error: 1.1166\n",
      "Mean Square Error: 1.2469\n",
      "\n",
      "Train RMSE: 1.117\n",
      "Train MSE: 1.247\n",
      "Train MAE: 0.929\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_64 (LSTM)               (None, 36, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 36, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 8s 25ms/step - loss: 0.5148 - val_loss: 0.3561\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4569 - val_loss: 0.3214\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4420 - val_loss: 0.2968\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4327 - val_loss: 0.2815\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4277 - val_loss: 0.2739\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4242 - val_loss: 0.2670\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4208 - val_loss: 0.2635\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4187 - val_loss: 0.2581\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4163 - val_loss: 0.2540\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4152 - val_loss: 0.2509\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4138 - val_loss: 0.2495\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4125 - val_loss: 0.2466\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4111 - val_loss: 0.2467\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4102 - val_loss: 0.2417\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4095 - val_loss: 0.2420\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4089 - val_loss: 0.2385\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4080 - val_loss: 0.2375\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4075 - val_loss: 0.2378\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4069 - val_loss: 0.2362\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4067 - val_loss: 0.2333\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4064 - val_loss: 0.2336\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4058 - val_loss: 0.2315\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4060 - val_loss: 0.2329\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4058 - val_loss: 0.2311\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4049 - val_loss: 0.2310\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4048 - val_loss: 0.2300\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4045 - val_loss: 0.2284\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4046 - val_loss: 0.2294\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4045 - val_loss: 0.2291\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4046 - val_loss: 0.2276\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4039 - val_loss: 0.2279\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4039 - val_loss: 0.2271\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 8s 24ms/step - loss: 0.4040 - val_loss: 0.2269\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 8s 25ms/step - loss: 0.4037 - val_loss: 0.2272\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4035 - val_loss: 0.2269\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4032 - val_loss: 0.2260\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4034 - val_loss: 0.2262\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4033 - val_loss: 0.2247\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4032 - val_loss: 0.2260\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4030 - val_loss: 0.2252\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4027 - val_loss: 0.2241\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4030 - val_loss: 0.2235\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4029 - val_loss: 0.2244\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4028 - val_loss: 0.2228\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4023 - val_loss: 0.2218\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4022 - val_loss: 0.2226\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4024 - val_loss: 0.2228\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4019 - val_loss: 0.2233\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4020 - val_loss: 0.2210\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4015 - val_loss: 0.2218\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4016 - val_loss: 0.2212\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4013 - val_loss: 0.2214\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4011 - val_loss: 0.2199\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4013 - val_loss: 0.2208\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.4010 - val_loss: 0.2204\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.4006 - val_loss: 0.2183\n",
      "Execution time:  415.0670449733734\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2028\n",
      "Root Mean Square Error: 0.6027\n",
      "Mean Square Error: 0.3632\n",
      "\n",
      "Train RMSE: 0.603\n",
      "Train MSE: 0.363\n",
      "Train MAE: 0.203\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_66 (LSTM)               (None, 36, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 36, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.5650 - val_loss: 0.4089\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4800 - val_loss: 0.3886\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4642 - val_loss: 0.3735\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4525 - val_loss: 0.3637\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4440 - val_loss: 0.3566\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4372 - val_loss: 0.3508\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4324 - val_loss: 0.3464\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4274 - val_loss: 0.3431\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4242 - val_loss: 0.3406\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4216 - val_loss: 0.3385\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4190 - val_loss: 0.3367\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4168 - val_loss: 0.3352\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4152 - val_loss: 0.3336\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4135 - val_loss: 0.3326\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4122 - val_loss: 0.3314\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4107 - val_loss: 0.3304\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4096 - val_loss: 0.3295\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4086 - val_loss: 0.3287\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4075 - val_loss: 0.3280\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4064 - val_loss: 0.3268\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4058 - val_loss: 0.3261\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4047 - val_loss: 0.3254\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4040 - val_loss: 0.3247\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4034 - val_loss: 0.3240\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4023 - val_loss: 0.3235\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4021 - val_loss: 0.3230\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4016 - val_loss: 0.3226\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4010 - val_loss: 0.3221\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4004 - val_loss: 0.3217\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.4000 - val_loss: 0.3213\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3996 - val_loss: 0.3210\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3990 - val_loss: 0.3206\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3986 - val_loss: 0.3203\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3981 - val_loss: 0.3200\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3979 - val_loss: 0.3198\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3973 - val_loss: 0.3195\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3970 - val_loss: 0.3194\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3968 - val_loss: 0.3192\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3964 - val_loss: 0.3191\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3961 - val_loss: 0.3190\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3961 - val_loss: 0.3187\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3959 - val_loss: 0.3187\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3953 - val_loss: 0.3185\n",
      "Execution time:  127.3875629901886\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.2071\n",
      "Root Mean Square Error: 0.5907\n",
      "Mean Square Error: 0.3490\n",
      "\n",
      "Train RMSE: 0.591\n",
      "Train MSE: 0.349\n",
      "Train MAE: 0.207\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_68 (LSTM)               (None, 36, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 36, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 8s 25ms/step - loss: 0.7140 - val_loss: 0.8485\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.6380 - val_loss: 0.8307\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.6243 - val_loss: 0.8233\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.6142 - val_loss: 0.8191\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.6059 - val_loss: 0.8162\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.6002 - val_loss: 0.8139\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5968 - val_loss: 0.8123\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5943 - val_loss: 0.8111\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5924 - val_loss: 0.8100\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5907 - val_loss: 0.8092\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5895 - val_loss: 0.8085\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5884 - val_loss: 0.8079\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5870 - val_loss: 0.8074\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5860 - val_loss: 0.8070\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5853 - val_loss: 0.8067\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5847 - val_loss: 0.8064\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5839 - val_loss: 0.8062\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5832 - val_loss: 0.8060\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5827 - val_loss: 0.8058\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5821 - val_loss: 0.8057\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5818 - val_loss: 0.8056\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5814 - val_loss: 0.8055\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5810 - val_loss: 0.8054\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5809 - val_loss: 0.8054\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5804 - val_loss: 0.8053\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5804 - val_loss: 0.8053\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5801 - val_loss: 0.8053\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5797 - val_loss: 0.8052\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5795 - val_loss: 0.8052\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5792 - val_loss: 0.8052\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5792 - val_loss: 0.8051\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5790 - val_loss: 0.8051\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5791 - val_loss: 0.8051\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5785 - val_loss: 0.8051\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5788 - val_loss: 0.8051\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5785 - val_loss: 0.8051\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5787 - val_loss: 0.8051\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5783 - val_loss: 0.8051\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5785 - val_loss: 0.8051\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5779 - val_loss: 0.8050\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5782 - val_loss: 0.8050\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5780 - val_loss: 0.8050\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5779 - val_loss: 0.8050\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5777 - val_loss: 0.8050\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5776 - val_loss: 0.8050\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5775 - val_loss: 0.8050\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5774 - val_loss: 0.8050\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5775 - val_loss: 0.8050\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5774 - val_loss: 0.8050\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5775 - val_loss: 0.8050\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5772 - val_loss: 0.8050\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5770 - val_loss: 0.8050\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5769 - val_loss: 0.8050\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5771 - val_loss: 0.8050\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 7s 23ms/step - loss: 0.5766 - val_loss: 0.8050\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 7s 22ms/step - loss: 0.5771 - val_loss: 0.8050\n",
      "Execution time:  414.16626811027527\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5115\n",
      "Root Mean Square Error: 0.7575\n",
      "Mean Square Error: 0.5738\n",
      "\n",
      "Train RMSE: 0.757\n",
      "Train MSE: 0.574\n",
      "Train MAE: 0.511\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_70 (LSTM)               (None, 36, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 36, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.7862 - val_loss: 0.8329\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.6590 - val_loss: 0.7710\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.6406 - val_loss: 0.7551\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.6303 - val_loss: 0.7461\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.6231 - val_loss: 0.7396\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.6174 - val_loss: 0.7342\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.6127 - val_loss: 0.7302\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.6086 - val_loss: 0.7268\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.6047 - val_loss: 0.7242\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.6018 - val_loss: 0.7226\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5994 - val_loss: 0.7214\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5972 - val_loss: 0.7201\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5955 - val_loss: 0.7191\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5940 - val_loss: 0.7180\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5927 - val_loss: 0.7171\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5916 - val_loss: 0.7162\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5905 - val_loss: 0.7153\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5894 - val_loss: 0.7145\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5884 - val_loss: 0.7138\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5876 - val_loss: 0.7132\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5867 - val_loss: 0.7125\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5859 - val_loss: 0.7120\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5850 - val_loss: 0.7115\n",
      "Epoch 24/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5843 - val_loss: 0.7112\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5835 - val_loss: 0.7107\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5831 - val_loss: 0.7104\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5825 - val_loss: 0.7101\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5820 - val_loss: 0.7099\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5813 - val_loss: 0.7096\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5808 - val_loss: 0.7095\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5804 - val_loss: 0.7092\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5801 - val_loss: 0.7089\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5797 - val_loss: 0.7088\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.5794 - val_loss: 0.7086\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5790 - val_loss: 0.7085\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5788 - val_loss: 0.7085\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5784 - val_loss: 0.7084\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5781 - val_loss: 0.7083\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5778 - val_loss: 0.7081\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.5778 - val_loss: 0.7082\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5773 - val_loss: 0.7081\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5772 - val_loss: 0.7081\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.5768 - val_loss: 0.7082\n",
      "Execution time:  125.61718106269836\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5183\n",
      "Root Mean Square Error: 0.7637\n",
      "Mean Square Error: 0.5832\n",
      "\n",
      "Train RMSE: 0.764\n",
      "Train MSE: 0.583\n",
      "Train MAE: 0.518\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_72 (LSTM)               (None, 72, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_73 (LSTM)               (None, 72, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.5693 - val_loss: 0.4496\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 13s 42ms/step - loss: 0.5296 - val_loss: 0.4230\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.5202 - val_loss: 0.4180\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.5142 - val_loss: 0.3974\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.5106 - val_loss: 0.3823\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.5073 - val_loss: 0.3790\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.5040 - val_loss: 0.3645\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.5029 - val_loss: 0.3697\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.5013 - val_loss: 0.3606\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4996 - val_loss: 0.3532\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4982 - val_loss: 0.3593\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.4953 - val_loss: 0.3480\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.4939 - val_loss: 0.3409\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.4925 - val_loss: 0.3344\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 13s 42ms/step - loss: 0.4851 - val_loss: 0.3296\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.4830 - val_loss: 0.3098\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 15s 47ms/step - loss: 0.4783 - val_loss: 0.3115\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 15s 47ms/step - loss: 0.4775 - val_loss: 0.3188\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.4808 - val_loss: 0.3091\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4752 - val_loss: 0.2994\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4730 - val_loss: 0.3182\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4730 - val_loss: 0.3009\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4714 - val_loss: 0.3139\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4676 - val_loss: 0.3065\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4707 - val_loss: 0.3129\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4672 - val_loss: 0.3158\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4697 - val_loss: 0.3111\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4654 - val_loss: 0.3096\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4654 - val_loss: 0.3089\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4656 - val_loss: 0.3051\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4636 - val_loss: 0.3137\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4618 - val_loss: 0.3170\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4667 - val_loss: 0.3018\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4590 - val_loss: 0.3116\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4585 - val_loss: 0.3003\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.4586 - val_loss: 0.3009\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4591 - val_loss: 0.2975\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.4576 - val_loss: 0.2971\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4546 - val_loss: 0.2969\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4577 - val_loss: 0.2987\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4540 - val_loss: 0.2972\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4545 - val_loss: 0.2979\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4501 - val_loss: 0.3020\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4535 - val_loss: 0.2990\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.4591 - val_loss: 0.3042\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4496 - val_loss: 0.3002\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4542 - val_loss: 0.2999\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4506 - val_loss: 0.2947\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4490 - val_loss: 0.2946\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4459 - val_loss: 0.2995\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4440 - val_loss: 0.2970\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4469 - val_loss: 0.2975\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4449 - val_loss: 0.2930\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 15s 45ms/step - loss: 0.4386 - val_loss: 0.2952\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 15s 46ms/step - loss: 0.4418 - val_loss: 0.2940\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4375 - val_loss: 0.2947\n",
      "Execution time:  794.231693983078\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3578\n",
      "Root Mean Square Error: 0.8350\n",
      "Mean Square Error: 0.6972\n",
      "\n",
      "Train RMSE: 0.835\n",
      "Train MSE: 0.697\n",
      "Train MAE: 0.358\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_74 (LSTM)               (None, 72, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 72, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_37 (TimeDis (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5885 - val_loss: 0.4277\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.5383 - val_loss: 0.4212\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.5256 - val_loss: 0.4156\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.5191 - val_loss: 0.4117\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.5139 - val_loss: 0.4092\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.5094 - val_loss: 0.4080\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.5054 - val_loss: 0.4075\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.5020 - val_loss: 0.4067\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4975 - val_loss: 0.3967\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4921 - val_loss: 0.3939\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4965 - val_loss: 0.3974\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4861 - val_loss: 0.3959\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4821 - val_loss: 0.3939\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4795 - val_loss: 0.3956\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4751 - val_loss: 0.3913\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4936 - val_loss: 0.4167\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4882 - val_loss: 0.4048\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4779 - val_loss: 0.3914\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4853 - val_loss: 0.4115\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4893 - val_loss: 0.4117\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4859 - val_loss: 0.4133\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4825 - val_loss: 0.4060\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4794 - val_loss: 0.3957\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4748 - val_loss: 0.3950\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4928 - val_loss: 0.4124\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4763 - val_loss: 0.3965\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4715 - val_loss: 0.3937\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4645 - val_loss: 0.3938\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4604 - val_loss: 0.3939\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4596 - val_loss: 0.3957\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4702 - val_loss: 0.3961\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4580 - val_loss: 0.3930\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4584 - val_loss: 0.3952\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.4782 - val_loss: 0.3939\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4537 - val_loss: 0.3967\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4509 - val_loss: 0.3983\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4574 - val_loss: 0.3980\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4534 - val_loss: 0.3992\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4504 - val_loss: 0.4007\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 5s 50ms/step - loss: 0.4483 - val_loss: 0.3969\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4469 - val_loss: 0.3986\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.4448 - val_loss: 0.3990\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.4440 - val_loss: 0.3998\n",
      "Execution time:  231.65903115272522\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3556\n",
      "Root Mean Square Error: 0.8537\n",
      "Mean Square Error: 0.7288\n",
      "\n",
      "Train RMSE: 0.854\n",
      "Train MSE: 0.729\n",
      "Train MAE: 0.356\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_76 (LSTM)               (None, 72, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 72, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_38 (TimeDis (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 14s 45ms/step - loss: 0.7113 - val_loss: 0.8222\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 14s 42ms/step - loss: 0.6571 - val_loss: 0.8126\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6457 - val_loss: 0.8092\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6419 - val_loss: 0.8073\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6385 - val_loss: 0.8062\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6357 - val_loss: 0.8054\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6341 - val_loss: 0.8049\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 16s 48ms/step - loss: 0.6327 - val_loss: 0.8046\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 15s 46ms/step - loss: 0.6314 - val_loss: 0.8044\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6313 - val_loss: 0.8043\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6301 - val_loss: 0.8042\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 15s 46ms/step - loss: 0.6293 - val_loss: 0.8042\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 15s 47ms/step - loss: 0.6295 - val_loss: 0.8041\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6285 - val_loss: 0.8041\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6278 - val_loss: 0.8041\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6294 - val_loss: 0.8041\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6281 - val_loss: 0.8041\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6277 - val_loss: 0.8041\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6250 - val_loss: 0.8040\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6244 - val_loss: 0.8041\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6237 - val_loss: 0.8040\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6234 - val_loss: 0.8040\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6221 - val_loss: 0.8040\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6207 - val_loss: 0.8039\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6211 - val_loss: 0.8039\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6198 - val_loss: 0.8039\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6191 - val_loss: 0.8039\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6159 - val_loss: 0.8040\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6145 - val_loss: 0.8039\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6132 - val_loss: 0.8039\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6126 - val_loss: 0.8039\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6108 - val_loss: 0.8039\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6081 - val_loss: 0.8039\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6130 - val_loss: 0.8039\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6042 - val_loss: 0.8040\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6022 - val_loss: 0.8039\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6024 - val_loss: 0.8039\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5992 - val_loss: 0.8039\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6004 - val_loss: 0.8039\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5997 - val_loss: 0.8040\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6002 - val_loss: 0.8039\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6026 - val_loss: 0.8039\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5985 - val_loss: 0.8039\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5937 - val_loss: 0.8040\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5970 - val_loss: 0.8040\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5941 - val_loss: 0.8039\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5921 - val_loss: 0.8039\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5959 - val_loss: 0.8039\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5904 - val_loss: 0.8039\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5884 - val_loss: 0.8039\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5955 - val_loss: 0.8041\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5926 - val_loss: 0.8040\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5923 - val_loss: 0.8041\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5871 - val_loss: 0.8040\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5837 - val_loss: 0.8040\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5867 - val_loss: 0.8039\n",
      "Execution time:  796.5217626094818\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5903\n",
      "Root Mean Square Error: 0.8702\n",
      "Mean Square Error: 0.7572\n",
      "\n",
      "Train RMSE: 0.870\n",
      "Train MSE: 0.757\n",
      "Train MAE: 0.590\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_78 (LSTM)               (None, 72, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_79 (LSTM)               (None, 72, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_39 (TimeDis (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 7s 69ms/step - loss: 0.7470 - val_loss: 0.7683\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6739 - val_loss: 0.7506\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6659 - val_loss: 0.7513\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6521 - val_loss: 0.7459\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6449 - val_loss: 0.7437\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6411 - val_loss: 0.7404\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6381 - val_loss: 0.7390\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6361 - val_loss: 0.7379\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6349 - val_loss: 0.7371\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6340 - val_loss: 0.7363\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6333 - val_loss: 0.7350\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6320 - val_loss: 0.7346\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.6319 - val_loss: 0.7315\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6291 - val_loss: 0.7299\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6273 - val_loss: 0.7286\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6260 - val_loss: 0.7278\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6257 - val_loss: 0.7285\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6240 - val_loss: 0.7283\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6272 - val_loss: 0.7268\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6281 - val_loss: 0.7289\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6217 - val_loss: 0.7266\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6264 - val_loss: 0.7276\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6209 - val_loss: 0.7266\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6253 - val_loss: 0.7277\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6195 - val_loss: 0.7268\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6207 - val_loss: 0.7262\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6222 - val_loss: 0.7258\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6177 - val_loss: 0.7259\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6154 - val_loss: 0.7257\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6164 - val_loss: 0.7260\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6138 - val_loss: 0.7249\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6220 - val_loss: 0.7237\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6142 - val_loss: 0.7248\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6115 - val_loss: 0.7230\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6115 - val_loss: 0.7239\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6114 - val_loss: 0.7230\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6094 - val_loss: 0.7225\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6089 - val_loss: 0.7267\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6139 - val_loss: 0.7236\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6076 - val_loss: 0.7206\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.6069 - val_loss: 0.7216\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6064 - val_loss: 0.7202\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6044 - val_loss: 0.7201\n",
      "Execution time:  243.15848755836487\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5676\n",
      "Root Mean Square Error: 0.8397\n",
      "Mean Square Error: 0.7052\n",
      "\n",
      "Train RMSE: 0.840\n",
      "Train MSE: 0.705\n",
      "Train MAE: 0.568\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_80 (LSTM)               (None, 72, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_81 (LSTM)               (None, 72, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_40 (TimeDis (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 15s 47ms/step - loss: 0.6846 - val_loss: 0.7633\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6826 - val_loss: 0.7579\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6802 - val_loss: 0.7520\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6777 - val_loss: 0.7457\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6750 - val_loss: 0.7392\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6723 - val_loss: 0.7323\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6696 - val_loss: 0.7252\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6667 - val_loss: 0.7179\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6638 - val_loss: 0.7102\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6607 - val_loss: 0.7023\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6577 - val_loss: 0.6940\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6544 - val_loss: 0.6854\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6511 - val_loss: 0.6765\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6477 - val_loss: 0.6672\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6441 - val_loss: 0.6575\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6405 - val_loss: 0.6475\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6365 - val_loss: 0.6370\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6327 - val_loss: 0.6263\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6287 - val_loss: 0.6152\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6244 - val_loss: 0.6038\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6203 - val_loss: 0.5923\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6161 - val_loss: 0.5806\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6120 - val_loss: 0.5688\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6079 - val_loss: 0.5571\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6036 - val_loss: 0.5455\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5998 - val_loss: 0.5342\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5957 - val_loss: 0.5232\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5922 - val_loss: 0.5124\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5887 - val_loss: 0.5020\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5855 - val_loss: 0.4920\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5823 - val_loss: 0.4824\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5794 - val_loss: 0.4731\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5767 - val_loss: 0.4641\n",
      "Epoch 34/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5740 - val_loss: 0.4554\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5717 - val_loss: 0.4470\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5693 - val_loss: 0.4389\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5670 - val_loss: 0.4311\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5653 - val_loss: 0.4235\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5632 - val_loss: 0.4162\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5613 - val_loss: 0.4091\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5598 - val_loss: 0.4022\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5581 - val_loss: 0.3957\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5564 - val_loss: 0.3893\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5550 - val_loss: 0.3833\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5536 - val_loss: 0.3774\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5523 - val_loss: 0.3719\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5514 - val_loss: 0.3667\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5502 - val_loss: 0.3618\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5494 - val_loss: 0.3571\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5483 - val_loss: 0.3528\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5478 - val_loss: 0.3488\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5470 - val_loss: 0.3450\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5467 - val_loss: 0.3417\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5462 - val_loss: 0.3388\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5456 - val_loss: 0.3362\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5448 - val_loss: 0.3337\n",
      "Execution time:  799.7367260456085\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4307\n",
      "Root Mean Square Error: 0.7951\n",
      "Mean Square Error: 0.6322\n",
      "\n",
      "Train RMSE: 0.795\n",
      "Train MSE: 0.632\n",
      "Train MAE: 0.431\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_82 (LSTM)               (None, 72, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_83 (LSTM)               (None, 72, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_41 (TimeDis (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.7171 - val_loss: 0.7112\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7165 - val_loss: 0.7104\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7159 - val_loss: 0.7096\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.7153 - val_loss: 0.7087\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7146 - val_loss: 0.7078\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7139 - val_loss: 0.7068\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7133 - val_loss: 0.7059\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.7126 - val_loss: 0.7049\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7119 - val_loss: 0.7039\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7111 - val_loss: 0.7029\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7103 - val_loss: 0.7019\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7096 - val_loss: 0.7008\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7089 - val_loss: 0.6998\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7081 - val_loss: 0.6987\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.7074 - val_loss: 0.6977\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7066 - val_loss: 0.6966\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7058 - val_loss: 0.6955\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7050 - val_loss: 0.6944\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7043 - val_loss: 0.6933\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7035 - val_loss: 0.6921\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7027 - val_loss: 0.6910\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7019 - val_loss: 0.6898\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.7010 - val_loss: 0.6887\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.7002 - val_loss: 0.6875\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6994 - val_loss: 0.6863\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6986 - val_loss: 0.6850\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6977 - val_loss: 0.6838\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6968 - val_loss: 0.6826\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6959 - val_loss: 0.6813\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6951 - val_loss: 0.6800\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6942 - val_loss: 0.6787\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6933 - val_loss: 0.6774\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6924 - val_loss: 0.6761\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6913 - val_loss: 0.6747\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6904 - val_loss: 0.6734\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6895 - val_loss: 0.6720\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6886 - val_loss: 0.6706\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.6876 - val_loss: 0.6692\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6866 - val_loss: 0.6678\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6857 - val_loss: 0.6663\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.6846 - val_loss: 0.6649\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6836 - val_loss: 0.6634\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6827 - val_loss: 0.6620\n",
      "Execution time:  245.54612040519714\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6699\n",
      "Root Mean Square Error: 0.9645\n",
      "Mean Square Error: 0.9303\n",
      "\n",
      "Train RMSE: 0.965\n",
      "Train MSE: 0.930\n",
      "Train MAE: 0.670\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_84 (LSTM)               (None, 72, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_85 (LSTM)               (None, 72, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_42 (TimeDis (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 15s 48ms/step - loss: 0.8981 - val_loss: 1.3030\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.8978 - val_loss: 1.3023\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8975 - val_loss: 1.3015\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8972 - val_loss: 1.3006\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8968 - val_loss: 1.2998\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8964 - val_loss: 1.2988\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8960 - val_loss: 1.2979\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8956 - val_loss: 1.2969\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8952 - val_loss: 1.2959\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8947 - val_loss: 1.2948\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8942 - val_loss: 1.2937\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8938 - val_loss: 1.2925\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8932 - val_loss: 1.2914\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8927 - val_loss: 1.2901\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8922 - val_loss: 1.2889\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8917 - val_loss: 1.2876\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8910 - val_loss: 1.2862\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8905 - val_loss: 1.2848\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8899 - val_loss: 1.2834\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8892 - val_loss: 1.2819\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8885 - val_loss: 1.2803\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8878 - val_loss: 1.2787\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8871 - val_loss: 1.2771\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8864 - val_loss: 1.2753\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8856 - val_loss: 1.2736\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8848 - val_loss: 1.2717\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8840 - val_loss: 1.2698\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8832 - val_loss: 1.2678\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8823 - val_loss: 1.2658\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8813 - val_loss: 1.2636\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8804 - val_loss: 1.2614\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8794 - val_loss: 1.2591\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8784 - val_loss: 1.2567\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8772 - val_loss: 1.2543\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8761 - val_loss: 1.2517\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8750 - val_loss: 1.2491\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8737 - val_loss: 1.2463\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8725 - val_loss: 1.2435\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8712 - val_loss: 1.2406\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8698 - val_loss: 1.2376\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8685 - val_loss: 1.2345\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8670 - val_loss: 1.2312\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8655 - val_loss: 1.2280\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8639 - val_loss: 1.2246\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8623 - val_loss: 1.2211\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8606 - val_loss: 1.2175\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8590 - val_loss: 1.2139\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8572 - val_loss: 1.2102\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8553 - val_loss: 1.2064\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8535 - val_loss: 1.2026\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8516 - val_loss: 1.1987\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.8497 - val_loss: 1.1947\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8477 - val_loss: 1.1907\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8456 - val_loss: 1.1867\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8436 - val_loss: 1.1826\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8416 - val_loss: 1.1785\n",
      "Execution time:  797.0529072284698\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8564\n",
      "Root Mean Square Error: 1.0514\n",
      "Mean Square Error: 1.1054\n",
      "\n",
      "Train RMSE: 1.051\n",
      "Train MSE: 1.105\n",
      "Train MAE: 0.856\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_86 (LSTM)               (None, 72, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_87 (LSTM)               (None, 72, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_43 (TimeDis (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8886 - val_loss: 1.1423\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8885 - val_loss: 1.1420\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8884 - val_loss: 1.1418\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.8882 - val_loss: 1.1416\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8881 - val_loss: 1.1413\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8879 - val_loss: 1.1410\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8877 - val_loss: 1.1407\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8876 - val_loss: 1.1405\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8874 - val_loss: 1.1402\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.8872 - val_loss: 1.1399\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8870 - val_loss: 1.1396\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8868 - val_loss: 1.1393\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8867 - val_loss: 1.1390\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.8865 - val_loss: 1.1387\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.8863 - val_loss: 1.1384\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8861 - val_loss: 1.1381\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8859 - val_loss: 1.1377\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.8857 - val_loss: 1.1374\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8855 - val_loss: 1.1371\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.8853 - val_loss: 1.1368\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8851 - val_loss: 1.1364\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.8849 - val_loss: 1.1361\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.8847 - val_loss: 1.1358\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8845 - val_loss: 1.1354\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8843 - val_loss: 1.1351\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8841 - val_loss: 1.1347\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8839 - val_loss: 1.1344\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8837 - val_loss: 1.1340\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8834 - val_loss: 1.1336\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8832 - val_loss: 1.1333\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8830 - val_loss: 1.1329\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8828 - val_loss: 1.1325\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8825 - val_loss: 1.1322\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8823 - val_loss: 1.1318\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8821 - val_loss: 1.1314\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8819 - val_loss: 1.1310\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8816 - val_loss: 1.1306\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8814 - val_loss: 1.1302\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8812 - val_loss: 1.1299\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8809 - val_loss: 1.1295\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8807 - val_loss: 1.1290\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.8804 - val_loss: 1.1286\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.8802 - val_loss: 1.1282\n",
      "Execution time:  244.50229263305664\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9229\n",
      "Root Mean Square Error: 1.1109\n",
      "Mean Square Error: 1.2341\n",
      "\n",
      "Train RMSE: 1.111\n",
      "Train MSE: 1.234\n",
      "Train MAE: 0.923\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_88 (LSTM)               (None, 72, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_89 (LSTM)               (None, 72, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_44 (TimeDis (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 15s 47ms/step - loss: 0.5649 - val_loss: 0.4087\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.5301 - val_loss: 0.3877\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 15s 45ms/step - loss: 0.5221 - val_loss: 0.3750\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5170 - val_loss: 0.3650\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5117 - val_loss: 0.3480\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5056 - val_loss: 0.3259\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4999 - val_loss: 0.2993\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4955 - val_loss: 0.2917\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4921 - val_loss: 0.2906\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4889 - val_loss: 0.2811\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4867 - val_loss: 0.2694\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.4834 - val_loss: 0.2694\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.4824 - val_loss: 0.2673\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4804 - val_loss: 0.2701\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4801 - val_loss: 0.2637\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4785 - val_loss: 0.2637\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4780 - val_loss: 0.2626\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4763 - val_loss: 0.2656\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4760 - val_loss: 0.2615\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4750 - val_loss: 0.2630\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4744 - val_loss: 0.2641\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4741 - val_loss: 0.2669\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4736 - val_loss: 0.2611\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4709 - val_loss: 0.2587\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4702 - val_loss: 0.2602\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4712 - val_loss: 0.2595\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4696 - val_loss: 0.2605\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4694 - val_loss: 0.2564\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4680 - val_loss: 0.2574\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4676 - val_loss: 0.2545\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4660 - val_loss: 0.2524\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4657 - val_loss: 0.2553\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4653 - val_loss: 0.2545\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4654 - val_loss: 0.2559\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4647 - val_loss: 0.2557\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4629 - val_loss: 0.2550\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 15s 46ms/step - loss: 0.4619 - val_loss: 0.2541\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4607 - val_loss: 0.2542\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4598 - val_loss: 0.2528\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4575 - val_loss: 0.2559\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4584 - val_loss: 0.2549\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4569 - val_loss: 0.2493\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4528 - val_loss: 0.2511\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4528 - val_loss: 0.2523\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4520 - val_loss: 0.2507\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4511 - val_loss: 0.2496\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4491 - val_loss: 0.2487\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4485 - val_loss: 0.2515\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4485 - val_loss: 0.2446\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4469 - val_loss: 0.2486\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4463 - val_loss: 0.2487\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4440 - val_loss: 0.2503\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4461 - val_loss: 0.2493\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.4438 - val_loss: 0.2494\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4434 - val_loss: 0.2486\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4417 - val_loss: 0.2452\n",
      "Execution time:  803.3064453601837\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3466\n",
      "Root Mean Square Error: 0.8337\n",
      "Mean Square Error: 0.6950\n",
      "\n",
      "Train RMSE: 0.834\n",
      "Train MSE: 0.695\n",
      "Train MAE: 0.347\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_90 (LSTM)               (None, 72, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_91 (LSTM)               (None, 72, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_45 (TimeDis (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.5907 - val_loss: 0.4297\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.5322 - val_loss: 0.4223\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.5243 - val_loss: 0.4158\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.5193 - val_loss: 0.4109\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.5155 - val_loss: 0.4074\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.5122 - val_loss: 0.4045\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.5097 - val_loss: 0.4020\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.5075 - val_loss: 0.3998\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.5049 - val_loss: 0.3978\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.5026 - val_loss: 0.3959\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.5005 - val_loss: 0.3942\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4985 - val_loss: 0.3923\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4969 - val_loss: 0.3907\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4952 - val_loss: 0.3893\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4938 - val_loss: 0.3878\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.4927 - val_loss: 0.3860\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4916 - val_loss: 0.3846\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4902 - val_loss: 0.3834\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4887 - val_loss: 0.3820\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4875 - val_loss: 0.3809\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4853 - val_loss: 0.3796\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4839 - val_loss: 0.3787\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.4813 - val_loss: 0.3776\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.4819 - val_loss: 0.3774\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4759 - val_loss: 0.3759\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4772 - val_loss: 0.3754\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4739 - val_loss: 0.3747\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4722 - val_loss: 0.3741\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4710 - val_loss: 0.3739\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4703 - val_loss: 0.3737\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.4693 - val_loss: 0.3736\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4680 - val_loss: 0.3737\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.4669 - val_loss: 0.3739\n",
      "Epoch 34/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4658 - val_loss: 0.3742\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4646 - val_loss: 0.3746\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4632 - val_loss: 0.3750\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4617 - val_loss: 0.3754\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4603 - val_loss: 0.3757\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4587 - val_loss: 0.3761\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4583 - val_loss: 0.3765\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4590 - val_loss: 0.3769\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.4563 - val_loss: 0.3771\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.4542 - val_loss: 0.3774\n",
      "Execution time:  246.59963583946228\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.3159\n",
      "Root Mean Square Error: 0.7866\n",
      "Mean Square Error: 0.6188\n",
      "\n",
      "Train RMSE: 0.787\n",
      "Train MSE: 0.619\n",
      "Train MAE: 0.316\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_92 (LSTM)               (None, 72, 43)            7740      \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "lstm_93 (LSTM)               (None, 72, 43)            14964     \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_46 (TimeDis (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 15s 47ms/step - loss: 0.7319 - val_loss: 0.8397\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6669 - val_loss: 0.8247\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6545 - val_loss: 0.8184\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6492 - val_loss: 0.8150\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6455 - val_loss: 0.8129\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6419 - val_loss: 0.8114\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6400 - val_loss: 0.8104\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6385 - val_loss: 0.8096\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6373 - val_loss: 0.8090\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6363 - val_loss: 0.8084\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6355 - val_loss: 0.8079\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6350 - val_loss: 0.8075\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6340 - val_loss: 0.8072\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6333 - val_loss: 0.8068\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6329 - val_loss: 0.8066\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6321 - val_loss: 0.8063\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6315 - val_loss: 0.8061\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6307 - val_loss: 0.8059\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6298 - val_loss: 0.8057\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6280 - val_loss: 0.8056\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6277 - val_loss: 0.8055\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6266 - val_loss: 0.8053\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6258 - val_loss: 0.8053\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6257 - val_loss: 0.8052\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6248 - val_loss: 0.8051\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 15s 45ms/step - loss: 0.6243 - val_loss: 0.8050\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6231 - val_loss: 0.8049\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6230 - val_loss: 0.8049\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6222 - val_loss: 0.8048\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6220 - val_loss: 0.8048\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6212 - val_loss: 0.8047\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6208 - val_loss: 0.8047\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6203 - val_loss: 0.8046\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6196 - val_loss: 0.8046\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6191 - val_loss: 0.8045\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6185 - val_loss: 0.8045\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6182 - val_loss: 0.8045\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6176 - val_loss: 0.8045\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6162 - val_loss: 0.8044\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6160 - val_loss: 0.8044\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6153 - val_loss: 0.8044\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6147 - val_loss: 0.8043\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6136 - val_loss: 0.8043\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6126 - val_loss: 0.8043\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6125 - val_loss: 0.8043\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6114 - val_loss: 0.8043\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6105 - val_loss: 0.8043\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6096 - val_loss: 0.8043\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6089 - val_loss: 0.8042\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6091 - val_loss: 0.8042\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6082 - val_loss: 0.8042\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6083 - val_loss: 0.8042\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6068 - val_loss: 0.8042\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6058 - val_loss: 0.8042\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 15s 46ms/step - loss: 0.6048 - val_loss: 0.8042\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6048 - val_loss: 0.8042\n",
      "Execution time:  809.4667508602142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "Mean Absolute Error: 0.5721\n",
      "Root Mean Square Error: 0.8523\n",
      "Mean Square Error: 0.7263\n",
      "\n",
      "Train RMSE: 0.852\n",
      "Train MSE: 0.726\n",
      "Train MAE: 0.572\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_94 (LSTM)               (None, 72, 45)            8460      \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "lstm_95 (LSTM)               (None, 72, 45)            16380     \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_47 (TimeDis (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 6s 62ms/step - loss: 0.8041 - val_loss: 0.8534\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6852 - val_loss: 0.7840\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6667 - val_loss: 0.7691\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6586 - val_loss: 0.7611\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6532 - val_loss: 0.7567\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6503 - val_loss: 0.7534\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6479 - val_loss: 0.7510\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6464 - val_loss: 0.7491\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6446 - val_loss: 0.7477\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6435 - val_loss: 0.7461\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6420 - val_loss: 0.7449\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6406 - val_loss: 0.7439\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6394 - val_loss: 0.7429\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6384 - val_loss: 0.7420\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6377 - val_loss: 0.7411\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6369 - val_loss: 0.7401\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6361 - val_loss: 0.7392\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6352 - val_loss: 0.7384\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6345 - val_loss: 0.7377\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6338 - val_loss: 0.7370\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6331 - val_loss: 0.7363\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6326 - val_loss: 0.7357\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6320 - val_loss: 0.7352\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6313 - val_loss: 0.7345\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6309 - val_loss: 0.7344\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6304 - val_loss: 0.7337\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6298 - val_loss: 0.7335\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6294 - val_loss: 0.7335\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6287 - val_loss: 0.7337\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6283 - val_loss: 0.7330\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6278 - val_loss: 0.7334\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6276 - val_loss: 0.7325\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6269 - val_loss: 0.7330\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6273 - val_loss: 0.7321\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6270 - val_loss: 0.7332\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6268 - val_loss: 0.7323\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6259 - val_loss: 0.7330\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6263 - val_loss: 0.7324\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6257 - val_loss: 0.7321\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6252 - val_loss: 0.7330\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.6260 - val_loss: 0.7320\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.6253 - val_loss: 0.7322\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.6247 - val_loss: 0.7320\n",
      "Execution time:  247.14954662322998\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5647\n",
      "Root Mean Square Error: 0.8230\n",
      "Mean Square Error: 0.6773\n",
      "\n",
      "Train RMSE: 0.823\n",
      "Train MSE: 0.677\n",
      "Train MAE: 0.565\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_96 (LSTM)               (None, 144, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_97 (LSTM)               (None, 144, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_48 (TimeDis (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 28s 90ms/step - loss: 0.6175 - val_loss: 0.4644\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5975 - val_loss: 0.4487\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5882 - val_loss: 0.4361\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5985 - val_loss: 0.4235\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5821 - val_loss: 0.3948\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5793 - val_loss: 0.3825\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5732 - val_loss: 0.4160\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5773 - val_loss: 0.4190\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5728 - val_loss: 0.3640\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5841 - val_loss: 0.4148\n",
      "Epoch 11/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5731 - val_loss: 0.3939\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5688 - val_loss: 0.3527\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5402 - val_loss: 0.3425\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5320 - val_loss: 0.3290\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5389 - val_loss: 0.3277\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5318 - val_loss: 0.3471\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5671 - val_loss: 0.4612\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5472 - val_loss: 0.3446\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5305 - val_loss: 0.3279\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5243 - val_loss: 0.3248\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5270 - val_loss: 0.3310\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5372 - val_loss: 0.3255\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5306 - val_loss: 0.3149\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5217 - val_loss: 0.3135\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5202 - val_loss: 0.3136\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5206 - val_loss: 0.3097\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5200 - val_loss: 0.3122\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5198 - val_loss: 0.3135\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5189 - val_loss: 0.3125TA: 0s - los\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5207 - val_loss: 0.3425\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5505 - val_loss: 0.3206\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5202 - val_loss: 0.3018\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5167 - val_loss: 0.2983\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5300 - val_loss: 0.3115\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5205 - val_loss: 0.3020\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5169 - val_loss: 0.3037\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5152 - val_loss: 0.3026\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5144 - val_loss: 0.3037\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5158 - val_loss: 0.3230\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5154 - val_loss: 0.3408\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5300 - val_loss: 0.3061\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5139 - val_loss: 0.3274\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5271 - val_loss: 0.3216\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5138 - val_loss: 0.3074\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5141 - val_loss: 0.2966\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5096 - val_loss: 0.3007\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5085 - val_loss: 0.3065\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5121 - val_loss: 0.4534\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5205 - val_loss: 0.3025\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5072 - val_loss: 0.3013\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5169 - val_loss: 0.3106\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5100 - val_loss: 0.3009\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5045 - val_loss: 0.3011\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5036 - val_loss: 0.2967\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5072 - val_loss: 0.2894\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5041 - val_loss: 0.2938\n",
      "Execution time:  1539.680047750473\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4032\n",
      "Root Mean Square Error: 0.8836\n",
      "Mean Square Error: 0.7808\n",
      "\n",
      "Train RMSE: 0.884\n",
      "Train MSE: 0.781\n",
      "Train MAE: 0.403\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_98 (LSTM)               (None, 144, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_99 (LSTM)               (None, 144, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_49 (TimeDis (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 12s 119ms/step - loss: 0.6229 - val_loss: 0.4201\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6059 - val_loss: 0.4252\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5935 - val_loss: 0.4267\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5887 - val_loss: 0.4271\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5832 - val_loss: 0.4254\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5807 - val_loss: 0.4240\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5854 - val_loss: 0.4209\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5749 - val_loss: 0.4175\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5771 - val_loss: 0.4071\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5906 - val_loss: 0.4235\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5669 - val_loss: 0.4199\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5556 - val_loss: 0.4841\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5688 - val_loss: 0.4404\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5450 - val_loss: 0.4102\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5582 - val_loss: 0.4046\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5485 - val_loss: 0.3990\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5422 - val_loss: 0.4017\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5396 - val_loss: 0.3969\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5516 - val_loss: 0.3973\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5361 - val_loss: 0.3975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5275 - val_loss: 0.4071\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5280 - val_loss: 0.3954\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5374 - val_loss: 0.3948\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5346 - val_loss: 0.3959\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5347 - val_loss: 0.3957\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5209 - val_loss: 0.3943\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5435 - val_loss: 0.3938\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5187 - val_loss: 0.3961\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5277 - val_loss: 0.3967\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5336 - val_loss: 0.3970\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5169 - val_loss: 0.3966\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5208 - val_loss: 0.3993\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5402 - val_loss: 0.4019\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5157 - val_loss: 0.4003\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5184 - val_loss: 0.4016\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5209 - val_loss: 0.4006\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5323 - val_loss: 0.4044\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5146 - val_loss: 0.4008\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5132 - val_loss: 0.3999\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5113 - val_loss: 0.4070\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5138 - val_loss: 0.4001\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5117 - val_loss: 0.4029\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5093 - val_loss: 0.4046\n",
      "Execution time:  461.9719235897064\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4015\n",
      "Root Mean Square Error: 0.8768\n",
      "Mean Square Error: 0.7688\n",
      "\n",
      "Train RMSE: 0.877\n",
      "Train MSE: 0.769\n",
      "Train MAE: 0.402\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_100 (LSTM)              (None, 144, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_101 (LSTM)              (None, 144, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_50 (TimeDis (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 29s 91ms/step - loss: 0.7486 - val_loss: 0.8179\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7232 - val_loss: 0.8102\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.7203 - val_loss: 0.8077\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7159 - val_loss: 0.8062\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.7135 - val_loss: 0.8051\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7116 - val_loss: 0.8044\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7108 - val_loss: 0.8038\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7103 - val_loss: 0.8053\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.7104 - val_loss: 0.8048\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7083 - val_loss: 0.8045\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7078 - val_loss: 0.8033\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7119 - val_loss: 0.8047\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7101 - val_loss: 0.8044\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.7081 - val_loss: 0.8034\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7055 - val_loss: 0.8033\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.7051 - val_loss: 0.8031\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.7047 - val_loss: 0.8030\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.7043 - val_loss: 0.8029\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7041 - val_loss: 0.8028\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7039 - val_loss: 0.8028\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7035 - val_loss: 0.8021\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7035 - val_loss: 0.8020\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7036 - val_loss: 0.8020\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7034 - val_loss: 0.8020\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7043 - val_loss: 0.8025\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7031 - val_loss: 0.8025\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7018 - val_loss: 0.8013\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7055 - val_loss: 0.8013\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7052 - val_loss: 0.8013\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7050 - val_loss: 0.8014\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7045 - val_loss: 0.8014\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7041 - val_loss: 0.8014\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7032 - val_loss: 0.8014\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7009 - val_loss: 0.8015\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7048 - val_loss: 0.8014\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7065 - val_loss: 0.8014\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7020 - val_loss: 0.8013\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7006 - val_loss: 0.8014\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7003 - val_loss: 0.8014\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7001 - val_loss: 0.8014\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6999 - val_loss: 0.8014\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6999 - val_loss: 0.8014\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6997 - val_loss: 0.8013\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6998 - val_loss: 0.8013\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6997 - val_loss: 0.8014\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6997 - val_loss: 0.8013\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6998 - val_loss: 0.8014\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6996 - val_loss: 0.8014\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6999 - val_loss: 0.8013\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6998 - val_loss: 0.8014\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6997 - val_loss: 0.8013\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7003 - val_loss: 0.8013\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7003 - val_loss: 0.8013\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.7015 - val_loss: 0.8013\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7008 - val_loss: 0.8013\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.7006 - val_loss: 0.8013\n",
      "Execution time:  1539.5536215305328\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.7134\n",
      "Root Mean Square Error: 1.0049\n",
      "Mean Square Error: 1.0098\n",
      "\n",
      "Train RMSE: 1.005\n",
      "Train MSE: 1.010\n",
      "Train MAE: 0.713\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_102 (LSTM)              (None, 144, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_103 (LSTM)              (None, 144, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_51 (TimeDis (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.7662 - val_loss: 0.7593\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7073 - val_loss: 0.7394\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7178 - val_loss: 0.7360\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7095 - val_loss: 0.7336\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.7088 - val_loss: 0.7322\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7083 - val_loss: 0.7311\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7079 - val_loss: 0.7303\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.7074 - val_loss: 0.7297\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7062 - val_loss: 0.7290\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7049 - val_loss: 0.7284\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7005 - val_loss: 0.7291\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.7202 - val_loss: 0.7551\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7269 - val_loss: 0.7523\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7236 - val_loss: 0.7510\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7225 - val_loss: 0.7493\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7212 - val_loss: 0.7479\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.7202 - val_loss: 0.7465\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7191 - val_loss: 0.7445\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7178 - val_loss: 0.7428\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7168 - val_loss: 0.7410\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7158 - val_loss: 0.7391\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7148 - val_loss: 0.7373\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7139 - val_loss: 0.7357\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.7132 - val_loss: 0.7345\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7128 - val_loss: 0.7334\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7124 - val_loss: 0.7323\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7118 - val_loss: 0.7314\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7115 - val_loss: 0.7306\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7113 - val_loss: 0.7300\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7112 - val_loss: 0.7292\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7108 - val_loss: 0.7284\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7104 - val_loss: 0.7271\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7098 - val_loss: 0.7242\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7089 - val_loss: 0.7200\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.7017 - val_loss: 0.7196\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7148 - val_loss: 0.7199\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6993 - val_loss: 0.7207\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6955 - val_loss: 0.7218\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6931 - val_loss: 0.7219\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6961 - val_loss: 0.7217\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6913 - val_loss: 0.7222\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7077 - val_loss: 0.7491\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.7239 - val_loss: 0.7453\n",
      "Execution time:  457.72136425971985\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6864\n",
      "Root Mean Square Error: 0.9548\n",
      "Mean Square Error: 0.9117\n",
      "\n",
      "Train RMSE: 0.955\n",
      "Train MSE: 0.912\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_104 (LSTM)              (None, 144, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_105 (LSTM)              (None, 144, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_52 (TimeDis (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 29s 91ms/step - loss: 0.6954 - val_loss: 0.7855\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6945 - val_loss: 0.7824\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6934 - val_loss: 0.7790\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.6922 - val_loss: 0.7755\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6910 - val_loss: 0.7717\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6897 - val_loss: 0.7678\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6883 - val_loss: 0.7637\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6869 - val_loss: 0.7594\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6855 - val_loss: 0.7550\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6840 - val_loss: 0.7504\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6824 - val_loss: 0.7456\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6808 - val_loss: 0.7407\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6791 - val_loss: 0.7357\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.6776 - val_loss: 0.7305\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.6758 - val_loss: 0.7252\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6742 - val_loss: 0.7197\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6724 - val_loss: 0.7141\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6707 - val_loss: 0.7083\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6689 - val_loss: 0.7023\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6671 - val_loss: 0.6961\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6651 - val_loss: 0.6897\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6632 - val_loss: 0.6831\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.6612 - val_loss: 0.6762\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6590 - val_loss: 0.6691\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6569 - val_loss: 0.6617\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6546 - val_loss: 0.6540\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6523 - val_loss: 0.6461\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6499 - val_loss: 0.6378\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6475 - val_loss: 0.6294\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6450 - val_loss: 0.6207\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6424 - val_loss: 0.6118\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6398 - val_loss: 0.6027\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6370 - val_loss: 0.5934\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6344 - val_loss: 0.5840\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6316 - val_loss: 0.5745\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6291 - val_loss: 0.5649\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6263 - val_loss: 0.5553\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6238 - val_loss: 0.5457\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6212 - val_loss: 0.5362\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6186 - val_loss: 0.5268\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.6162 - val_loss: 0.5177\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6139 - val_loss: 0.5088\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6117 - val_loss: 0.5001\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6096 - val_loss: 0.4918\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6078 - val_loss: 0.4836\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6059 - val_loss: 0.4758\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6042 - val_loss: 0.4683\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6025 - val_loss: 0.4610\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6012 - val_loss: 0.4539\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5996 - val_loss: 0.4472\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5985 - val_loss: 0.4406\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5972 - val_loss: 0.4343\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5960 - val_loss: 0.4282\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5949 - val_loss: 0.4223\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5939 - val_loss: 0.4166\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5930 - val_loss: 0.4111\n",
      "Execution time:  1547.029044866562\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4910\n",
      "Root Mean Square Error: 0.8566\n",
      "Mean Square Error: 0.7337\n",
      "\n",
      "Train RMSE: 0.857\n",
      "Train MSE: 0.734\n",
      "Train MAE: 0.491\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_106 (LSTM)              (None, 144, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_107 (LSTM)              (None, 144, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_53 (TimeDis (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 0.7097 - val_loss: 0.7075\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7093 - val_loss: 0.7069\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.7089 - val_loss: 0.7062\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.7085 - val_loss: 0.7055\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7081 - val_loss: 0.7048\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7076 - val_loss: 0.7040\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7072 - val_loss: 0.7033\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7067 - val_loss: 0.7025\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7063 - val_loss: 0.7017\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 0.7059 - val_loss: 0.7009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7053 - val_loss: 0.7001\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7049 - val_loss: 0.6993\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7044 - val_loss: 0.6984\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7038 - val_loss: 0.6976\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.7034 - val_loss: 0.6967\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7029 - val_loss: 0.6958\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7024 - val_loss: 0.6950\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.7019 - val_loss: 0.6941\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7014 - val_loss: 0.6932\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7009 - val_loss: 0.6923\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.7003 - val_loss: 0.6913\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6998 - val_loss: 0.6904\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6993 - val_loss: 0.6895\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6987 - val_loss: 0.6885\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6982 - val_loss: 0.6876\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.6976 - val_loss: 0.6866\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 0.6971 - val_loss: 0.6856\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6966 - val_loss: 0.6846\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6960 - val_loss: 0.6836\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6954 - val_loss: 0.6826\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6949 - val_loss: 0.6816\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6943 - val_loss: 0.6806\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 0.6938 - val_loss: 0.6795\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6931 - val_loss: 0.6785\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6925 - val_loss: 0.6775\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6920 - val_loss: 0.6764\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6914 - val_loss: 0.6753\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6908 - val_loss: 0.6743\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6902 - val_loss: 0.6732\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6896 - val_loss: 0.6721\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6890 - val_loss: 0.6710\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6884 - val_loss: 0.6699\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6879 - val_loss: 0.6689\n",
      "Execution time:  460.52639174461365\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6743\n",
      "Root Mean Square Error: 0.9756\n",
      "Mean Square Error: 0.9519\n",
      "\n",
      "Train RMSE: 0.976\n",
      "Train MSE: 0.952\n",
      "Train MAE: 0.674\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_108 (LSTM)              (None, 144, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_109 (LSTM)              (None, 144, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_54 (TimeDis (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 28s 90ms/step - loss: 0.9077 - val_loss: 1.2945\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9074 - val_loss: 1.2939\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9072 - val_loss: 1.2932\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.9069 - val_loss: 1.2925\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.9066 - val_loss: 1.2917\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9063 - val_loss: 1.2909\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9059 - val_loss: 1.2901\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9056 - val_loss: 1.2892\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9053 - val_loss: 1.2883\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9049 - val_loss: 1.2874\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9045 - val_loss: 1.2864\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9041 - val_loss: 1.2854\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9037 - val_loss: 1.2844\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9032 - val_loss: 1.2833\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9028 - val_loss: 1.2822\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9023 - val_loss: 1.2810\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9018 - val_loss: 1.2799\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9013 - val_loss: 1.2787\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.9008 - val_loss: 1.2774\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.9003 - val_loss: 1.2761\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8997 - val_loss: 1.2748\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8991 - val_loss: 1.2734\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8985 - val_loss: 1.2720\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8979 - val_loss: 1.2705\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8973 - val_loss: 1.2690\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8967 - val_loss: 1.2674\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8960 - val_loss: 1.2658\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8953 - val_loss: 1.2641\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8946 - val_loss: 1.2624\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8938 - val_loss: 1.2607\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8931 - val_loss: 1.2588\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8923 - val_loss: 1.2570\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8915 - val_loss: 1.2550\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.8906 - val_loss: 1.2531\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8898 - val_loss: 1.2510\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8889 - val_loss: 1.2489\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8880 - val_loss: 1.2467\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8870 - val_loss: 1.2445\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8860 - val_loss: 1.2422\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8850 - val_loss: 1.2398\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8840 - val_loss: 1.2374\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.8829 - val_loss: 1.2349\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8818 - val_loss: 1.2324\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.8807 - val_loss: 1.2298\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8795 - val_loss: 1.2271\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.8783 - val_loss: 1.2244\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8771 - val_loss: 1.2216\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8758 - val_loss: 1.2187\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8746 - val_loss: 1.2158\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.8733 - val_loss: 1.2128\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8719 - val_loss: 1.2097\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8705 - val_loss: 1.2066\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8692 - val_loss: 1.2034\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8677 - val_loss: 1.2002\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8663 - val_loss: 1.1969\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.8647 - val_loss: 1.1935\n",
      "Execution time:  1541.8392975330353\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8662\n",
      "Root Mean Square Error: 1.0608\n",
      "Mean Square Error: 1.1252\n",
      "\n",
      "Train RMSE: 1.061\n",
      "Train MSE: 1.125\n",
      "Train MAE: 0.866\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_110 (LSTM)              (None, 144, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_111 (LSTM)              (None, 144, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_55 (TimeDis (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.8993 - val_loss: 1.1415\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8992 - val_loss: 1.1413\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8991 - val_loss: 1.1411\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8990 - val_loss: 1.1409\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8989 - val_loss: 1.1407\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.8987 - val_loss: 1.1404\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8986 - val_loss: 1.1402\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8985 - val_loss: 1.1399\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8983 - val_loss: 1.1397\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8982 - val_loss: 1.1394\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8980 - val_loss: 1.1392\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8979 - val_loss: 1.1389\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8978 - val_loss: 1.1386\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8976 - val_loss: 1.1383\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8974 - val_loss: 1.1381\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8973 - val_loss: 1.1378\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.8971 - val_loss: 1.1375\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8970 - val_loss: 1.1372\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8968 - val_loss: 1.1369\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8967 - val_loss: 1.1366\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8965 - val_loss: 1.1363\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8963 - val_loss: 1.1360\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8962 - val_loss: 1.1357\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8960 - val_loss: 1.1354\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.8958 - val_loss: 1.1351\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8956 - val_loss: 1.1348\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8955 - val_loss: 1.1344\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8953 - val_loss: 1.1341\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8951 - val_loss: 1.1338\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8949 - val_loss: 1.1334\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8947 - val_loss: 1.1331\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8946 - val_loss: 1.1328\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8944 - val_loss: 1.1324\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8942 - val_loss: 1.1321\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8940 - val_loss: 1.1317\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8938 - val_loss: 1.1314\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8936 - val_loss: 1.1310\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8934 - val_loss: 1.1307\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8932 - val_loss: 1.1303\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.8930 - val_loss: 1.1299\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.8928 - val_loss: 1.1296\n",
      "Epoch 42/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 10s 101ms/step - loss: 0.8926 - val_loss: 1.1292\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.8924 - val_loss: 1.1288\n",
      "Execution time:  460.13913679122925\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9210\n",
      "Root Mean Square Error: 1.1098\n",
      "Mean Square Error: 1.2316\n",
      "\n",
      "Train RMSE: 1.110\n",
      "Train MSE: 1.232\n",
      "Train MAE: 0.921\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_112 (LSTM)              (None, 144, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_113 (LSTM)              (None, 144, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_56 (TimeDis (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 28s 90ms/step - loss: 0.6131 - val_loss: 0.4485\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5945 - val_loss: 0.4233\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5902 - val_loss: 0.3996\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5858 - val_loss: 0.3735\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5816 - val_loss: 0.3393\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5718 - val_loss: 0.3219\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5486 - val_loss: 0.3066\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5404 - val_loss: 0.2958\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5382 - val_loss: 0.2847\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5318 - val_loss: 0.2847\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5301 - val_loss: 0.2852\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5304 - val_loss: 0.2862\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5305 - val_loss: 0.2853\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5317 - val_loss: 0.2847\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5303 - val_loss: 0.2873\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5280 - val_loss: 0.2732\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5220 - val_loss: 0.2720\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5212 - val_loss: 0.2726\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5214 - val_loss: 0.2709\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5231 - val_loss: 0.2708\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5204 - val_loss: 0.2726\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5229 - val_loss: 0.2678\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5192 - val_loss: 0.2685\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5192 - val_loss: 0.2700\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5201 - val_loss: 0.2705\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5198 - val_loss: 0.2683\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5205 - val_loss: 0.2649\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5178 - val_loss: 0.2661\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5173 - val_loss: 0.2678\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5185 - val_loss: 0.2686\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5173 - val_loss: 0.2674\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5168 - val_loss: 0.2608\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5127 - val_loss: 0.2592\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5118 - val_loss: 0.2595\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5118 - val_loss: 0.2589\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5124 - val_loss: 0.2589\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5122 - val_loss: 0.2603\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5132 - val_loss: 0.2605\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5124 - val_loss: 0.2612\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.5135 - val_loss: 0.2624\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5134 - val_loss: 0.2629\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 28s 89ms/step - loss: 0.5132 - val_loss: 0.2623\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5134 - val_loss: 0.2652\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5143 - val_loss: 0.2633\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5121 - val_loss: 0.2628\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5124 - val_loss: 0.2599\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 28s 87ms/step - loss: 0.5113 - val_loss: 0.2599\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.5117 - val_loss: 0.2614\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5103 - val_loss: 0.2622\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5125 - val_loss: 0.2618\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5114 - val_loss: 0.2657\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5127 - val_loss: 0.2684\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5135 - val_loss: 0.2616\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5107 - val_loss: 0.2652\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5122 - val_loss: 0.2620\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.5105 - val_loss: 0.2678\n",
      "Execution time:  1547.7490448951721\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4062\n",
      "Root Mean Square Error: 0.8761\n",
      "Mean Square Error: 0.7676\n",
      "\n",
      "Train RMSE: 0.876\n",
      "Train MSE: 0.768\n",
      "Train MAE: 0.406\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_114 (LSTM)              (None, 144, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_115 (LSTM)              (None, 144, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_57 (TimeDis (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 11s 106ms/step - loss: 0.6192 - val_loss: 0.4303\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5957 - val_loss: 0.4323\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5898 - val_loss: 0.4331\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5849 - val_loss: 0.4329\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5809 - val_loss: 0.4331\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5777 - val_loss: 0.4336\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5730 - val_loss: 0.4344\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 0.5685 - val_loss: 0.4324\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5639 - val_loss: 0.4301\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5536 - val_loss: 0.4224\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5512 - val_loss: 0.4187\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5486 - val_loss: 0.4141\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5438 - val_loss: 0.4085\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5447 - val_loss: 0.4071\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5386 - val_loss: 0.4038\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5419 - val_loss: 0.4032\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5349 - val_loss: 0.4016\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5349 - val_loss: 0.4003\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5346 - val_loss: 0.3991\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5347 - val_loss: 0.3984\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5349 - val_loss: 0.3981\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5358 - val_loss: 0.3983\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5324 - val_loss: 0.3976\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5330 - val_loss: 0.3971\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5336 - val_loss: 0.3970\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5328 - val_loss: 0.3967\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5322 - val_loss: 0.3969\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5330 - val_loss: 0.3973\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5297 - val_loss: 0.3963\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5302 - val_loss: 0.3962\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5305 - val_loss: 0.3962\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5294 - val_loss: 0.3966\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5275 - val_loss: 0.3960\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5281 - val_loss: 0.3963\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5282 - val_loss: 0.3961\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5270 - val_loss: 0.3966\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.5255 - val_loss: 0.3953\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5232 - val_loss: 0.3958\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5225 - val_loss: 0.3958\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5228 - val_loss: 0.3971\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5209 - val_loss: 0.4003\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.5149 - val_loss: 0.3963\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.5161 - val_loss: 0.3960\n",
      "Execution time:  460.0937395095825\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.4009\n",
      "Root Mean Square Error: 0.8676\n",
      "Mean Square Error: 0.7528\n",
      "\n",
      "Train RMSE: 0.868\n",
      "Train MSE: 0.753\n",
      "Train MAE: 0.401\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_116 (LSTM)              (None, 144, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_117 (LSTM)              (None, 144, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_58 (TimeDis (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 28s 90ms/step - loss: 0.7418 - val_loss: 0.8301\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6948 - val_loss: 0.8182\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6911 - val_loss: 0.8143\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6896 - val_loss: 0.8123\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6891 - val_loss: 0.8111\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6883 - val_loss: 0.8101\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6879 - val_loss: 0.8093\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6870 - val_loss: 0.8088\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6865 - val_loss: 0.8083\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6915 - val_loss: 0.8080\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 27s 85ms/step - loss: 0.6873 - val_loss: 0.8078\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6852 - val_loss: 0.8075\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6849 - val_loss: 0.8072\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6851 - val_loss: 0.8070\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6853 - val_loss: 0.8067\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6853 - val_loss: 0.8065\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6849 - val_loss: 0.8063\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6845 - val_loss: 0.8061\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6846 - val_loss: 0.8059\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6851 - val_loss: 0.8057\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6851 - val_loss: 0.8056\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6835 - val_loss: 0.8054\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6826 - val_loss: 0.8052\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6814 - val_loss: 0.8051\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6807 - val_loss: 0.8050\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6799 - val_loss: 0.8049\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6798 - val_loss: 0.8048\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 27s 85ms/step - loss: 0.6799 - val_loss: 0.8047\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6796 - val_loss: 0.8046\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6795 - val_loss: 0.8046\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6794 - val_loss: 0.8045\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6790 - val_loss: 0.8044\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6819 - val_loss: 0.8044\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 27s 87ms/step - loss: 0.6765 - val_loss: 0.8044\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6761 - val_loss: 0.8043\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6758 - val_loss: 0.8042\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6754 - val_loss: 0.8042\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6752 - val_loss: 0.8041\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6751 - val_loss: 0.8041\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6752 - val_loss: 0.8040\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 27s 85ms/step - loss: 0.6751 - val_loss: 0.8039\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6750 - val_loss: 0.8039\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6779 - val_loss: 0.8039\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6718 - val_loss: 0.8038\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6759 - val_loss: 0.8038\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6765 - val_loss: 0.8038\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6722 - val_loss: 0.8037\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6773 - val_loss: 0.8037\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6701 - val_loss: 0.8036\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6700 - val_loss: 0.8036\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6715 - val_loss: 0.8036\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6765 - val_loss: 0.8035\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 28s 88ms/step - loss: 0.6705 - val_loss: 0.8035\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6786 - val_loss: 0.8035\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6760 - val_loss: 0.8035\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 27s 86ms/step - loss: 0.6751 - val_loss: 0.8034\n",
      "Execution time:  1528.9490535259247\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6673\n",
      "Root Mean Square Error: 0.9594\n",
      "Mean Square Error: 0.9205\n",
      "\n",
      "Train RMSE: 0.959\n",
      "Train MSE: 0.921\n",
      "Train MAE: 0.667\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_118 (LSTM)              (None, 144, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_119 (LSTM)              (None, 144, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.8274 - val_loss: 0.8520\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.7189 - val_loss: 0.7815\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.7023 - val_loss: 0.7692\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6947 - val_loss: 0.7633\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6903 - val_loss: 0.7595\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6884 - val_loss: 0.7569\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6869 - val_loss: 0.7550\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6856 - val_loss: 0.7535\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6845 - val_loss: 0.7521\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6833 - val_loss: 0.7509\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6821 - val_loss: 0.7496\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.6806 - val_loss: 0.7484\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6790 - val_loss: 0.7472\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6773 - val_loss: 0.7457\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6757 - val_loss: 0.7439\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6738 - val_loss: 0.7416\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6717 - val_loss: 0.7385\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6690 - val_loss: 0.7336\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6695 - val_loss: 0.7311\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6680 - val_loss: 0.7297\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6674 - val_loss: 0.7285\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6665 - val_loss: 0.7271\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6655 - val_loss: 0.7259\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6642 - val_loss: 0.7244\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6630 - val_loss: 0.7236\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6607 - val_loss: 0.7222\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6577 - val_loss: 0.7219\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6543 - val_loss: 0.7211\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6525 - val_loss: 0.7198\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6538 - val_loss: 0.7193\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6541 - val_loss: 0.7195\n",
      "Epoch 32/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6503 - val_loss: 0.7191\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6499 - val_loss: 0.7187\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6499 - val_loss: 0.7185\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.6495 - val_loss: 0.7184\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6495 - val_loss: 0.7182\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6491 - val_loss: 0.7180\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6495 - val_loss: 0.7177\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6490 - val_loss: 0.7178\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6483 - val_loss: 0.7177\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.6483 - val_loss: 0.7175\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.6488 - val_loss: 0.7173\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.6516 - val_loss: 0.7173\n",
      "Execution time:  459.6406066417694\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6252\n",
      "Root Mean Square Error: 0.9181\n",
      "Mean Square Error: 0.8428\n",
      "\n",
      "Train RMSE: 0.918\n",
      "Train MSE: 0.843\n",
      "Train MAE: 0.625\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_120 (LSTM)              (None, 432, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_121 (LSTM)              (None, 432, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_60 (TimeDis (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6448 - val_loss: 0.4795\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6592 - val_loss: 0.3918\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6601 - val_loss: 0.4769\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6436 - val_loss: 0.3876\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6300 - val_loss: 0.4009\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6254 - val_loss: 0.4102\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6225 - val_loss: 0.4165\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6207 - val_loss: 0.4200\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6193 - val_loss: 0.4218\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6184 - val_loss: 0.4233\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6178 - val_loss: 0.4246\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6171 - val_loss: 0.4251\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6166 - val_loss: 0.4254\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6160 - val_loss: 0.4260\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6156 - val_loss: 0.4271\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6152 - val_loss: 0.4277\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6151 - val_loss: 0.4323\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6145 - val_loss: 0.4401\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6131 - val_loss: 0.4514\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6105 - val_loss: 0.4611\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6082 - val_loss: 0.4649\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6072 - val_loss: 0.4677\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6067 - val_loss: 0.4692\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6064 - val_loss: 0.4701\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6063 - val_loss: 0.4712\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6061 - val_loss: 0.4715\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6061 - val_loss: 0.4719\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4714\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6061 - val_loss: 0.4718\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4719\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4719\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6060 - val_loss: 0.4719\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6059 - val_loss: 0.4707\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4712\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6060 - val_loss: 0.4716\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6059 - val_loss: 0.4706\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6060 - val_loss: 0.4710\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6060 - val_loss: 0.4715\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6060 - val_loss: 0.4719\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6059 - val_loss: 0.4713\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4717\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4722\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6059 - val_loss: 0.4714\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4719\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4720\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4721\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6059 - val_loss: 0.4713\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6060 - val_loss: 0.4718\n",
      "Execution time:  4198.491082668304\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6161\n",
      "Root Mean Square Error: 1.0443\n",
      "Mean Square Error: 1.0905\n",
      "\n",
      "Train RMSE: 1.044\n",
      "Train MSE: 1.091\n",
      "Train MAE: 0.616\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_122 (LSTM)              (None, 432, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_123 (LSTM)              (None, 432, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_61 (TimeDis (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6934 - val_loss: 0.3609\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6768 - val_loss: 0.3008\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6743 - val_loss: 0.3198\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6687 - val_loss: 0.3328\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6637 - val_loss: 0.3347\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6618 - val_loss: 0.3346\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6606 - val_loss: 0.3345\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 29s 300ms/step - loss: 0.6590 - val_loss: 0.3354\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6576 - val_loss: 0.3360\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 28s 300ms/step - loss: 0.6566 - val_loss: 0.3362\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6560 - val_loss: 0.3356\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6550 - val_loss: 0.3350\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6543 - val_loss: 0.3353\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 28s 300ms/step - loss: 0.6528 - val_loss: 0.3328\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6522 - val_loss: 0.3355\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6506 - val_loss: 0.3368\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6454 - val_loss: 0.3249\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6402 - val_loss: 0.3639\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6396 - val_loss: 0.3600\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6376 - val_loss: 0.3689\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6339 - val_loss: 0.3789\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6316 - val_loss: 0.3799\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6305 - val_loss: 0.3800\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6302 - val_loss: 0.3786\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6310 - val_loss: 0.3730\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6317 - val_loss: 0.3799\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6300 - val_loss: 0.3806\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6295 - val_loss: 0.3808\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6298 - val_loss: 0.3786\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6303 - val_loss: 0.3788\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 28s 299ms/step - loss: 0.6307 - val_loss: 0.3680\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6306 - val_loss: 0.3781\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6291 - val_loss: 0.3792\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6287 - val_loss: 0.3799\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6285 - val_loss: 0.3796\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 28s 299ms/step - loss: 0.6293 - val_loss: 0.3743\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 29s 300ms/step - loss: 0.6294 - val_loss: 0.3691\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6293 - val_loss: 0.3754\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6282 - val_loss: 0.3757\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6411 - val_loss: 0.3614\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6349 - val_loss: 0.3690\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6303 - val_loss: 0.3446\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6314 - val_loss: 0.3763\n",
      "Execution time:  1254.0413279533386\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5610\n",
      "Root Mean Square Error: 1.0127\n",
      "Mean Square Error: 1.0257\n",
      "\n",
      "Train RMSE: 1.013\n",
      "Train MSE: 1.026\n",
      "Train MAE: 0.561\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_124 (LSTM)              (None, 432, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_125 (LSTM)              (None, 432, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_62 (TimeDis (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 74s 254ms/step - loss: 0.7419 - val_loss: 0.8070\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 74s 254ms/step - loss: 0.6795 - val_loss: 0.7981\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6777 - val_loss: 0.7962\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6771 - val_loss: 0.7953\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6768 - val_loss: 0.7947\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6766 - val_loss: 0.7943\n",
      "Epoch 7/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6764 - val_loss: 0.7941\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6763 - val_loss: 0.7938\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6762 - val_loss: 0.7936\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6761 - val_loss: 0.7935\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6761 - val_loss: 0.7934\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6760 - val_loss: 0.7933\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6760 - val_loss: 0.7931\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6759 - val_loss: 0.7931\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6759 - val_loss: 0.7930\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7929\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6758 - val_loss: 0.7929\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 75s 259ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 75s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Execution time:  4246.793601036072\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6930\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9904\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_126 (LSTM)              (None, 432, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_127 (LSTM)              (None, 432, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_63 (TimeDis (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.8742 - val_loss: 0.7188\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.7187 - val_loss: 0.6367\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7102 - val_loss: 0.6293\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7084 - val_loss: 0.6262\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7075 - val_loss: 0.6244\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7069 - val_loss: 0.6233\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7066 - val_loss: 0.6226\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7063 - val_loss: 0.6220\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7061 - val_loss: 0.6216\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7060 - val_loss: 0.6212\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.7058 - val_loss: 0.6209\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7057 - val_loss: 0.6207\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.7056 - val_loss: 0.6205\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7056 - val_loss: 0.6203\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7055 - val_loss: 0.6201\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7054 - val_loss: 0.6200\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7054 - val_loss: 0.6198\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7053 - val_loss: 0.6197\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7053 - val_loss: 0.6196\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7052 - val_loss: 0.6195\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.7052 - val_loss: 0.6194\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7052 - val_loss: 0.6193\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.7051 - val_loss: 0.6192\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7051 - val_loss: 0.6192\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7051 - val_loss: 0.6191\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7051 - val_loss: 0.6190\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7050 - val_loss: 0.6190\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7050 - val_loss: 0.6189\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7050 - val_loss: 0.6189\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7050 - val_loss: 0.6188\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.7049 - val_loss: 0.6188\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.7049 - val_loss: 0.6185\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.7048 - val_loss: 0.6185\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7048 - val_loss: 0.6185\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.7048 - val_loss: 0.6185\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Execution time:  1267.4651937484741\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6930\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9904\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_128 (LSTM)              (None, 432, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_129 (LSTM)              (None, 432, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_64 (TimeDis (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6770 - val_loss: 0.7994\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6761 - val_loss: 0.7959\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6752 - val_loss: 0.7923\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6741 - val_loss: 0.7885\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6731 - val_loss: 0.7846\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6719 - val_loss: 0.7805\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6707 - val_loss: 0.7764\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6695 - val_loss: 0.7720\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6683 - val_loss: 0.7675\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6670 - val_loss: 0.7628\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6656 - val_loss: 0.7580\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6642 - val_loss: 0.7531\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6628 - val_loss: 0.7480\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6614 - val_loss: 0.7430\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6601 - val_loss: 0.7379\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6587 - val_loss: 0.7328\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6574 - val_loss: 0.7277\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6561 - val_loss: 0.7226\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6549 - val_loss: 0.7174\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6536 - val_loss: 0.7121\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6524 - val_loss: 0.7068\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6511 - val_loss: 0.7014\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6498 - val_loss: 0.6958\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6485 - val_loss: 0.6901\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6471 - val_loss: 0.6842\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6458 - val_loss: 0.6782\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6444 - val_loss: 0.6721\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6430 - val_loss: 0.6658\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6415 - val_loss: 0.6595\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6401 - val_loss: 0.6530\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6387 - val_loss: 0.6465\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6372 - val_loss: 0.6399\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6358 - val_loss: 0.6333\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6344 - val_loss: 0.6267\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6331 - val_loss: 0.6202\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6317 - val_loss: 0.6136\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6304 - val_loss: 0.6071\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6291 - val_loss: 0.6006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6277 - val_loss: 0.5941\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6265 - val_loss: 0.5877\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6253 - val_loss: 0.5814\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6241 - val_loss: 0.5752\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6230 - val_loss: 0.5690\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6220 - val_loss: 0.5631\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6209 - val_loss: 0.5572\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6200 - val_loss: 0.5516\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6191 - val_loss: 0.5461\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6182 - val_loss: 0.5409\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6174 - val_loss: 0.5359\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6166 - val_loss: 0.5311\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6159 - val_loss: 0.5266\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.6153 - val_loss: 0.5223\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6148 - val_loss: 0.5181\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6142 - val_loss: 0.5142\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6137 - val_loss: 0.5105\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6131 - val_loss: 0.5069\n",
      "Execution time:  4229.341509580612\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5534\n",
      "Root Mean Square Error: 0.9417\n",
      "Mean Square Error: 0.8867\n",
      "\n",
      "Train RMSE: 0.942\n",
      "Train MSE: 0.887\n",
      "Train MAE: 0.553\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_130 (LSTM)              (None, 432, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_131 (LSTM)              (None, 432, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_65 (TimeDis (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.7009 - val_loss: 0.6011\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7008 - val_loss: 0.6008\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.7007 - val_loss: 0.6004\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.7006 - val_loss: 0.6001\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7005 - val_loss: 0.5997\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7004 - val_loss: 0.5993\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7002 - val_loss: 0.5989\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7001 - val_loss: 0.5985\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7000 - val_loss: 0.5981\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6999 - val_loss: 0.5977\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6998 - val_loss: 0.5972\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6996 - val_loss: 0.5968\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6995 - val_loss: 0.5964\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6994 - val_loss: 0.5959\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6992 - val_loss: 0.5955\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.6991 - val_loss: 0.5950\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6990 - val_loss: 0.5945\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6988 - val_loss: 0.5941\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 29s 309ms/step - loss: 0.6987 - val_loss: 0.5936\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6985 - val_loss: 0.5931\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6984 - val_loss: 0.5926\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6983 - val_loss: 0.5921\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6981 - val_loss: 0.5916\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6980 - val_loss: 0.5911\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6978 - val_loss: 0.5906\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6977 - val_loss: 0.5901\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6975 - val_loss: 0.5896\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6974 - val_loss: 0.5891\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6972 - val_loss: 0.5886\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6971 - val_loss: 0.5881\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6969 - val_loss: 0.5876\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6968 - val_loss: 0.5870\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6966 - val_loss: 0.5865\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6965 - val_loss: 0.5860\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6963 - val_loss: 0.5855\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6962 - val_loss: 0.5849\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6960 - val_loss: 0.5844\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6959 - val_loss: 0.5839\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 29s 309ms/step - loss: 0.6957 - val_loss: 0.5833\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6956 - val_loss: 0.5828\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6954 - val_loss: 0.5822\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6953 - val_loss: 0.5817\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.6951 - val_loss: 0.5812\n",
      "Execution time:  1289.9682788848877\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6476\n",
      "Root Mean Square Error: 0.9533\n",
      "Mean Square Error: 0.9089\n",
      "\n",
      "Train RMSE: 0.953\n",
      "Train MSE: 0.909\n",
      "Train MAE: 0.648\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_132 (LSTM)              (None, 432, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_133 (LSTM)              (None, 432, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_66 (TimeDis (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9076 - val_loss: 1.2918\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9074 - val_loss: 1.2913\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9071 - val_loss: 1.2908\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9069 - val_loss: 1.2902\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9066 - val_loss: 1.2896\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9063 - val_loss: 1.2889\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9060 - val_loss: 1.2882\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9057 - val_loss: 1.2875\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9054 - val_loss: 1.2868\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9050 - val_loss: 1.2860\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 76s 259ms/step - loss: 0.9047 - val_loss: 1.2852\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9043 - val_loss: 1.2844\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.9039 - val_loss: 1.2835\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.9035 - val_loss: 1.2826\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.9031 - val_loss: 1.2817\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.9026 - val_loss: 1.2807\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.9022 - val_loss: 1.2798\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.9017 - val_loss: 1.2787\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.9012 - val_loss: 1.2777\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.9007 - val_loss: 1.2766\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.9002 - val_loss: 1.2755\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8997 - val_loss: 1.2744\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8991 - val_loss: 1.2732\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8986 - val_loss: 1.2719\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8980 - val_loss: 1.2707\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8974 - val_loss: 1.2693\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8967 - val_loss: 1.2680\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.8961 - val_loss: 1.2665\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8954 - val_loss: 1.2651\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8947 - val_loss: 1.2635\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8940 - val_loss: 1.2619\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.8932 - val_loss: 1.2603\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.8924 - val_loss: 1.2585\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8916 - val_loss: 1.2567\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.8907 - val_loss: 1.2548\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8898 - val_loss: 1.2529\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8889 - val_loss: 1.2508\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.8879 - val_loss: 1.2487\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8868 - val_loss: 1.2465\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.8858 - val_loss: 1.2441\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8847 - val_loss: 1.2417\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8835 - val_loss: 1.2391\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8823 - val_loss: 1.2365\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8810 - val_loss: 1.2337\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8796 - val_loss: 1.2308\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8782 - val_loss: 1.2277\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8768 - val_loss: 1.2245\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.8753 - val_loss: 1.2212\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8736 - val_loss: 1.2177\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8720 - val_loss: 1.2140\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.8702 - val_loss: 1.2102\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8684 - val_loss: 1.2061\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8664 - val_loss: 1.2019\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.8644 - val_loss: 1.1975\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8623 - val_loss: 1.1929\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.8600 - val_loss: 1.1881\n",
      "Execution time:  4233.551719427109\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8593\n",
      "Root Mean Square Error: 1.0605\n",
      "Mean Square Error: 1.1246\n",
      "\n",
      "Train RMSE: 1.060\n",
      "Train MSE: 1.125\n",
      "Train MAE: 0.859\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_134 (LSTM)              (None, 432, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_135 (LSTM)              (None, 432, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_67 (TimeDis (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.9068 - val_loss: 1.0968\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.9067 - val_loss: 1.0967\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.9067 - val_loss: 1.0966\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.9067 - val_loss: 1.0965\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.9066 - val_loss: 1.0964\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.9066 - val_loss: 1.0963\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.9065 - val_loss: 1.0962\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9065 - val_loss: 1.0961\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9064 - val_loss: 1.0959\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.9064 - val_loss: 1.0958\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9063 - val_loss: 1.0957\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9063 - val_loss: 1.0956\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.9062 - val_loss: 1.0955\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.9061 - val_loss: 1.0953\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9061 - val_loss: 1.0952\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9060 - val_loss: 1.0951\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9060 - val_loss: 1.0949\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9059 - val_loss: 1.0948\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.9058 - val_loss: 1.0947\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.9058 - val_loss: 1.0945\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.9057 - val_loss: 1.0944\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9057 - val_loss: 1.0943\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9056 - val_loss: 1.0941\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9055 - val_loss: 1.0940\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.9055 - val_loss: 1.0938\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.9054 - val_loss: 1.0937\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9053 - val_loss: 1.0935\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9053 - val_loss: 1.0934\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9052 - val_loss: 1.0932\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9051 - val_loss: 1.0931\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9050 - val_loss: 1.0929\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.9050 - val_loss: 1.0928\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.9049 - val_loss: 1.0926\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.9048 - val_loss: 1.0925\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9048 - val_loss: 1.0923\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9047 - val_loss: 1.0922\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.9046 - val_loss: 1.0920\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9045 - val_loss: 1.0918\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9045 - val_loss: 1.0917\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.9044 - val_loss: 1.0915\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.9043 - val_loss: 1.0913\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.9042 - val_loss: 1.0912\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.9042 - val_loss: 1.0910\n",
      "Execution time:  1260.0782957077026\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9163\n",
      "Root Mean Square Error: 1.1086\n",
      "Mean Square Error: 1.2290\n",
      "\n",
      "Train RMSE: 1.109\n",
      "Train MSE: 1.229\n",
      "Train MAE: 0.916\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_136 (LSTM)              (None, 432, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_137 (LSTM)              (None, 432, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_68 (TimeDis (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6342 - val_loss: 0.4774\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6232 - val_loss: 0.3874\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6203 - val_loss: 0.3613\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6194 - val_loss: 0.3549\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6184 - val_loss: 0.3581\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6174 - val_loss: 0.3636\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 76s 260ms/step - loss: 0.6162 - val_loss: 0.3682\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6153 - val_loss: 0.3718\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6145 - val_loss: 0.3720\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 75s 258ms/step - loss: 0.6137 - val_loss: 0.3740\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6128 - val_loss: 0.3762\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6121 - val_loss: 0.3761\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6116 - val_loss: 0.3770\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6110 - val_loss: 0.3726\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6105 - val_loss: 0.3730\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6097 - val_loss: 0.3719\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6099 - val_loss: 0.3627\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6051 - val_loss: 0.3869\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6001 - val_loss: 0.3940\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6038 - val_loss: 0.3829\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.5983 - val_loss: 0.4378\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5931 - val_loss: 0.4536\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5921 - val_loss: 0.4591\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.5913 - val_loss: 0.4615\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5911 - val_loss: 0.4623\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5907 - val_loss: 0.4626\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.5905 - val_loss: 0.4633\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5896 - val_loss: 0.4625\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.5893 - val_loss: 0.4642\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5890 - val_loss: 0.4644\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5880 - val_loss: 0.4646\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5880 - val_loss: 0.4627\n",
      "Epoch 33/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5880 - val_loss: 0.4651\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5882 - val_loss: 0.4649\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5875 - val_loss: 0.4658\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.5870 - val_loss: 0.4614\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.5851 - val_loss: 0.4615\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5849 - val_loss: 0.4617\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5801 - val_loss: 0.4647\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5784 - val_loss: 0.4658\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5754 - val_loss: 0.4668\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5819 - val_loss: 0.4682\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.5810 - val_loss: 0.4701\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5800 - val_loss: 0.4656\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5763 - val_loss: 0.4668\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5728 - val_loss: 0.4669\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5665 - val_loss: 0.4675\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5627 - val_loss: 0.4660\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5622 - val_loss: 0.4662\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5636 - val_loss: 0.4640\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5762 - val_loss: 0.4623\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5509 - val_loss: 0.4640\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 79s 271ms/step - loss: 0.5584 - val_loss: 0.4637\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5435 - val_loss: 0.4631\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5365 - val_loss: 0.4612\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.5264 - val_loss: 0.4592\n",
      "Execution time:  4212.094009399414\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5534\n",
      "Root Mean Square Error: 1.0070\n",
      "Mean Square Error: 1.0141\n",
      "\n",
      "Train RMSE: 1.007\n",
      "Train MSE: 1.014\n",
      "Train MAE: 0.553\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_138 (LSTM)              (None, 432, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_139 (LSTM)              (None, 432, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.6908 - val_loss: 0.4069\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.6616 - val_loss: 0.3135\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6696 - val_loss: 0.3270\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 28s 300ms/step - loss: 0.6639 - val_loss: 0.3286\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6616 - val_loss: 0.3290\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6601 - val_loss: 0.3299\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6587 - val_loss: 0.3306\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 28s 298ms/step - loss: 0.6578 - val_loss: 0.3311\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 28s 298ms/step - loss: 0.6569 - val_loss: 0.3320\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6560 - val_loss: 0.3326\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.6553 - val_loss: 0.3333\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.6546 - val_loss: 0.3337\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6541 - val_loss: 0.3342\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6536 - val_loss: 0.3345\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6530 - val_loss: 0.3349\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6525 - val_loss: 0.3352\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6519 - val_loss: 0.3353\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6516 - val_loss: 0.3354\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6510 - val_loss: 0.3354\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6508 - val_loss: 0.3351\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6501 - val_loss: 0.3353\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.6496 - val_loss: 0.3348\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.6490 - val_loss: 0.3346\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6484 - val_loss: 0.3340\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6476 - val_loss: 0.3334\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6467 - val_loss: 0.3300\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6426 - val_loss: 0.3385\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.6337 - val_loss: 0.3554\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6300 - val_loss: 0.3612\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6296 - val_loss: 0.3705\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6280 - val_loss: 0.3695\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.6259 - val_loss: 0.3771\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.6244 - val_loss: 0.3775\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.6245 - val_loss: 0.3796\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6241 - val_loss: 0.3798\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6240 - val_loss: 0.3804\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6239 - val_loss: 0.3805\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6238 - val_loss: 0.3808\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.6237 - val_loss: 0.3808\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.6237 - val_loss: 0.3813\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6235 - val_loss: 0.3811\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.6235 - val_loss: 0.3816\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.6233 - val_loss: 0.3815\n",
      "Execution time:  1258.6436741352081\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.5478\n",
      "Root Mean Square Error: 0.9963\n",
      "Mean Square Error: 0.9926\n",
      "\n",
      "Train RMSE: 0.996\n",
      "Train MSE: 0.993\n",
      "Train MAE: 0.548\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_140 (LSTM)              (None, 432, 43)           7740      \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "lstm_141 (LSTM)              (None, 432, 43)           14964     \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_70 (TimeDis (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.7984 - val_loss: 0.8214\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 74s 253ms/step - loss: 0.6836 - val_loss: 0.8033\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 74s 253ms/step - loss: 0.6797 - val_loss: 0.7995\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6784 - val_loss: 0.7976\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 74s 255ms/step - loss: 0.6777 - val_loss: 0.7965\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6773 - val_loss: 0.7958\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6770 - val_loss: 0.7952\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6768 - val_loss: 0.7948\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6766 - val_loss: 0.7944\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6764 - val_loss: 0.7942\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6763 - val_loss: 0.7939\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6762 - val_loss: 0.7937\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6761 - val_loss: 0.7935\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6761 - val_loss: 0.7934\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6760 - val_loss: 0.7933\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6759 - val_loss: 0.7931\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6759 - val_loss: 0.7931\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6759 - val_loss: 0.7930\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6758 - val_loss: 0.7929\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 75s 259ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 75s 257ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 75s 255ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 75s 256ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Execution time:  4201.5671553611755\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6930\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9904\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_142 (LSTM)              (None, 432, 45)           8460      \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "lstm_143 (LSTM)              (None, 432, 45)           16380     \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_71 (TimeDis (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 31s 326ms/step - loss: 0.8892 - val_loss: 0.9085\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7595 - val_loss: 0.6766\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7190 - val_loss: 0.6468\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7135 - val_loss: 0.6381\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7112 - val_loss: 0.6335\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7099 - val_loss: 0.6307\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7091 - val_loss: 0.6288\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7085 - val_loss: 0.6273\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 29s 309ms/step - loss: 0.7080 - val_loss: 0.6262\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7076 - val_loss: 0.6253\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7073 - val_loss: 0.6246\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7071 - val_loss: 0.6240\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.7068 - val_loss: 0.6235\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7067 - val_loss: 0.6230\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7065 - val_loss: 0.6226\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7064 - val_loss: 0.6223\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7062 - val_loss: 0.6220\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7061 - val_loss: 0.6217\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.7060 - val_loss: 0.6214\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.7059 - val_loss: 0.6212\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7058 - val_loss: 0.6210\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7058 - val_loss: 0.6208\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7057 - val_loss: 0.6206\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7056 - val_loss: 0.6205\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7056 - val_loss: 0.6203\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.7055 - val_loss: 0.6202\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7054 - val_loss: 0.6200\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7054 - val_loss: 0.6199\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.7053 - val_loss: 0.6198\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7053 - val_loss: 0.6197\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7053 - val_loss: 0.6196\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7052 - val_loss: 0.6195\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7052 - val_loss: 0.6194\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7052 - val_loss: 0.6193\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7051 - val_loss: 0.6192\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7051 - val_loss: 0.6192\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.7051 - val_loss: 0.6191\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.7050 - val_loss: 0.6190\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 29s 309ms/step - loss: 0.7050 - val_loss: 0.6190\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.7050 - val_loss: 0.6189\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.7050 - val_loss: 0.6188\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.7049 - val_loss: 0.6188\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Execution time:  1265.280571937561\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6932\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9905\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_144 (LSTM)              (None, 1008, 43)          7740      \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "lstm_145 (LSTM)              (None, 1008, 43)          14964     \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_72 (TimeDis (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5957 - val_loss: 0.2276\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5751 - val_loss: 0.4579\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 116s 474ms/step - loss: 0.5607 - val_loss: 0.4157\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 116s 476ms/step - loss: 0.5674 - val_loss: 0.5167\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 116s 474ms/step - loss: 0.5651 - val_loss: 0.4912\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5655 - val_loss: 0.3717\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 116s 476ms/step - loss: 0.5633 - val_loss: 0.4060\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 117s 478ms/step - loss: 0.5623 - val_loss: 0.4283\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 116s 477ms/step - loss: 0.5666 - val_loss: 0.4018\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 116s 476ms/step - loss: 0.5675 - val_loss: 0.3887\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5625 - val_loss: 0.3851\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5613 - val_loss: 0.3887\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5607 - val_loss: 0.3923\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 116s 474ms/step - loss: 0.5588 - val_loss: 0.3909\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5573 - val_loss: 0.3896\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 116s 476ms/step - loss: 0.5561 - val_loss: 0.3874\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 117s 478ms/step - loss: 0.5562 - val_loss: 0.3921\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 117s 478ms/step - loss: 0.5538 - val_loss: 0.4256\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 116s 477ms/step - loss: 0.5506 - val_loss: 0.4503\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 117s 478ms/step - loss: 0.5478 - val_loss: 0.4735\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 116s 477ms/step - loss: 0.5453 - val_loss: 0.4885\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5483 - val_loss: 0.4876\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5519 - val_loss: 0.4565\n",
      "Epoch 24/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 116s 474ms/step - loss: 0.5548 - val_loss: 0.4324\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 116s 475ms/step - loss: 0.5536 - val_loss: 0.4191\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 116s 476ms/step - loss: 0.5518 - val_loss: 0.4189\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 116s 477ms/step - loss: 0.5478 - val_loss: 0.4448\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5457 - val_loss: 0.4700\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 117s 478ms/step - loss: 0.5436 - val_loss: 0.4859\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5439 - val_loss: 0.4954\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5423 - val_loss: 0.5016\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5389 - val_loss: 0.5069\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5402 - val_loss: 0.5090\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5398 - val_loss: 0.5111\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5399 - val_loss: 0.5133\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5409 - val_loss: 0.5147\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5401 - val_loss: 0.5167\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5388 - val_loss: 0.5182\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5395 - val_loss: 0.5199\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5424 - val_loss: 0.5169\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5392 - val_loss: 0.5188\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5388 - val_loss: 0.5214\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5395 - val_loss: 0.5218\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5447 - val_loss: 0.5137\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5435 - val_loss: 0.5081\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5431 - val_loss: 0.5036\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5416 - val_loss: 0.5023\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5407 - val_loss: 0.5019\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5404 - val_loss: 0.5047\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5384 - val_loss: 0.5075\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5403 - val_loss: 0.5082\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5346 - val_loss: 0.5141\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5394 - val_loss: 0.5159\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 116s 477ms/step - loss: 0.5398 - val_loss: 0.5176\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5388 - val_loss: 0.5188\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5421 - val_loss: 0.5160\n",
      "Execution time:  6566.312698364258\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6330\n",
      "Root Mean Square Error: 1.1150\n",
      "Mean Square Error: 1.2431\n",
      "\n",
      "Train RMSE: 1.115\n",
      "Train MSE: 1.243\n",
      "Train MAE: 0.633\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_146 (LSTM)              (None, 1008, 45)          8460      \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "lstm_147 (LSTM)              (None, 1008, 45)          16380     \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_73 (TimeDis (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 0.6789 - val_loss: 0.5106\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 62s 772ms/step - loss: 0.6402 - val_loss: 0.5063\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 62s 778ms/step - loss: 0.6216 - val_loss: 0.4433\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 62s 778ms/step - loss: 0.6105 - val_loss: 0.4128\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.6063 - val_loss: 0.3903\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.6026 - val_loss: 0.3895\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 62s 778ms/step - loss: 0.6018 - val_loss: 0.3771\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5985 - val_loss: 0.3843\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5984 - val_loss: 0.3817\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5956 - val_loss: 0.3607\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 62s 778ms/step - loss: 0.5940 - val_loss: 0.3822\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 63s 783ms/step - loss: 0.5933 - val_loss: 0.3775\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5934 - val_loss: 0.3823\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.5904 - val_loss: 0.3285\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.6004 - val_loss: 0.3473\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5849 - val_loss: 0.3486\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.5821 - val_loss: 0.3581\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5786 - val_loss: 0.3673\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 62s 778ms/step - loss: 0.5761 - val_loss: 0.3756\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5945 - val_loss: 0.3434\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.5806 - val_loss: 0.3734\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.5937 - val_loss: 0.3916\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5908 - val_loss: 0.3902\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.5903 - val_loss: 0.3902\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5895 - val_loss: 0.3903\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5890 - val_loss: 0.3904\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 63s 783ms/step - loss: 0.5885 - val_loss: 0.3905\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.5881 - val_loss: 0.3908\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5877 - val_loss: 0.3910\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.5874 - val_loss: 0.3913\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5872 - val_loss: 0.3916\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5870 - val_loss: 0.3920\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5867 - val_loss: 0.3925\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5865 - val_loss: 0.3929\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.5863 - val_loss: 0.3934\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5861 - val_loss: 0.3939\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5859 - val_loss: 0.3944\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.5857 - val_loss: 0.3950\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5855 - val_loss: 0.3959\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.5853 - val_loss: 0.3967\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 62s 781ms/step - loss: 0.5852 - val_loss: 0.3975\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5852 - val_loss: 0.3982\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 62s 778ms/step - loss: 0.5850 - val_loss: 0.3992\n",
      "Execution time:  2708.8113062381744\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6724\n",
      "Root Mean Square Error: 1.1562\n",
      "Mean Square Error: 1.3368\n",
      "\n",
      "Train RMSE: 1.156\n",
      "Train MSE: 1.337\n",
      "Train MAE: 0.672\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_148 (LSTM)              (None, 1008, 43)          7740      \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "lstm_149 (LSTM)              (None, 1008, 43)          14964     \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_74 (TimeDis (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 119s 488ms/step - loss: 0.7568 - val_loss: 0.8411\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6814 - val_loss: 0.8373\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 117s 482ms/step - loss: 0.6801 - val_loss: 0.8362\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6796 - val_loss: 0.8357\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6793 - val_loss: 0.8353\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6791 - val_loss: 0.8351\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6790 - val_loss: 0.8349\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6789 - val_loss: 0.8348\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6788 - val_loss: 0.8347\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6788 - val_loss: 0.8346\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6787 - val_loss: 0.8345\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6787 - val_loss: 0.8345\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6786 - val_loss: 0.8344\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6786 - val_loss: 0.8344\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8343\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 117s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 55/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Execution time:  6620.891446590424\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6954\n",
      "Root Mean Square Error: 1.0192\n",
      "Mean Square Error: 1.0387\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.695\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_150 (LSTM)              (None, 1008, 45)          8460      \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "lstm_151 (LSTM)              (None, 1008, 45)          16380     \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_75 (TimeDis (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 60s 753ms/step - loss: 0.9045 - val_loss: 0.7174\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 60s 752ms/step - loss: 0.7109 - val_loss: 0.6773\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 60s 754ms/step - loss: 0.7052 - val_loss: 0.6735\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 60s 748ms/step - loss: 0.7039 - val_loss: 0.6718\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7032 - val_loss: 0.6708\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.7027 - val_loss: 0.6702\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.7024 - val_loss: 0.6698\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.7022 - val_loss: 0.6694\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 60s 754ms/step - loss: 0.7021 - val_loss: 0.6692\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7019 - val_loss: 0.6690\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 60s 753ms/step - loss: 0.7018 - val_loss: 0.6688\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7017 - val_loss: 0.6687\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.7017 - val_loss: 0.6685\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7016 - val_loss: 0.6684\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7016 - val_loss: 0.6683\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.7015 - val_loss: 0.6683\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7015 - val_loss: 0.6682\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 61s 761ms/step - loss: 0.7014 - val_loss: 0.6681\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7014 - val_loss: 0.6680\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7013 - val_loss: 0.6680\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.7013 - val_loss: 0.6679\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7013 - val_loss: 0.6679\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7013 - val_loss: 0.6679\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7012 - val_loss: 0.6678\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.7012 - val_loss: 0.6678\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7012 - val_loss: 0.6677\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.7012 - val_loss: 0.6677\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7012 - val_loss: 0.6677\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.7011 - val_loss: 0.6676\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7011 - val_loss: 0.6676\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7011 - val_loss: 0.6676\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 60s 754ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Execution time:  2639.207366466522\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6956\n",
      "Root Mean Square Error: 1.0192\n",
      "Mean Square Error: 1.0389\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.696\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_152 (LSTM)              (None, 1008, 43)          7740      \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "lstm_153 (LSTM)              (None, 1008, 43)          14964     \n",
      "_________________________________________________________________\n",
      "dropout_153 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_76 (TimeDis (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 118s 485ms/step - loss: 0.6767 - val_loss: 0.8466\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6761 - val_loss: 0.8453\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6754 - val_loss: 0.8439\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6747 - val_loss: 0.8425\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6739 - val_loss: 0.8409\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6730 - val_loss: 0.8392\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6721 - val_loss: 0.8374\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6711 - val_loss: 0.8355\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6702 - val_loss: 0.8335\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6691 - val_loss: 0.8314\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6680 - val_loss: 0.8292\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6669 - val_loss: 0.8268\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6657 - val_loss: 0.8244\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6644 - val_loss: 0.8218\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6631 - val_loss: 0.8191\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6617 - val_loss: 0.8163\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6603 - val_loss: 0.8132\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6587 - val_loss: 0.8100\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6572 - val_loss: 0.8067\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6554 - val_loss: 0.8031\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6537 - val_loss: 0.7993\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6518 - val_loss: 0.7953\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6499 - val_loss: 0.7911\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6479 - val_loss: 0.7867\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 117s 478ms/step - loss: 0.6459 - val_loss: 0.7822\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6437 - val_loss: 0.7774\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6415 - val_loss: 0.7725\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6393 - val_loss: 0.7674\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6370 - val_loss: 0.7620\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6346 - val_loss: 0.7562\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6320 - val_loss: 0.7502\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6293 - val_loss: 0.7436\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6265 - val_loss: 0.7367\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6234 - val_loss: 0.7291\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6200 - val_loss: 0.7209\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 119s 487ms/step - loss: 0.6166 - val_loss: 0.7120\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6128 - val_loss: 0.7024\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 121s 495ms/step - loss: 0.6088 - val_loss: 0.6920\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 119s 489ms/step - loss: 0.6047 - val_loss: 0.6808\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6003 - val_loss: 0.6691\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5959 - val_loss: 0.6569\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5915 - val_loss: 0.6441\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5871 - val_loss: 0.6309\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.5827 - val_loss: 0.6172\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5783 - val_loss: 0.6031\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.5742 - val_loss: 0.5889\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5703 - val_loss: 0.5753\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5668 - val_loss: 0.5625\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5641 - val_loss: 0.5510\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5618 - val_loss: 0.5407\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5600 - val_loss: 0.5315\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5585 - val_loss: 0.5234\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5572 - val_loss: 0.5162\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5561 - val_loss: 0.5096\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5553 - val_loss: 0.5035\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.5544 - val_loss: 0.4980\n",
      "Execution time:  6600.593079566956\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6403\n",
      "Root Mean Square Error: 1.1032\n",
      "Mean Square Error: 1.2171\n",
      "\n",
      "Train RMSE: 1.103\n",
      "Train MSE: 1.217\n",
      "Train MAE: 0.640\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_154 (LSTM)              (None, 1008, 45)          8460      \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "lstm_155 (LSTM)              (None, 1008, 45)          16380     \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_77 (TimeDis (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 60s 751ms/step - loss: 0.7025 - val_loss: 0.6602\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.7024 - val_loss: 0.6600\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 61s 768ms/step - loss: 0.7022 - val_loss: 0.6598\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 61s 763ms/step - loss: 0.7020 - val_loss: 0.6595\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.7019 - val_loss: 0.6593\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.7017 - val_loss: 0.6591\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.7015 - val_loss: 0.6588\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.7013 - val_loss: 0.6585\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.7012 - val_loss: 0.6583\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 62s 769ms/step - loss: 0.7010 - val_loss: 0.6580\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 62s 772ms/step - loss: 0.7008 - val_loss: 0.6577\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 61s 769ms/step - loss: 0.7005 - val_loss: 0.6574\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.7003 - val_loss: 0.6571\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 62s 772ms/step - loss: 0.7001 - val_loss: 0.6568\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.6999 - val_loss: 0.6564\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 62s 770ms/step - loss: 0.6997 - val_loss: 0.6561\n",
      "Epoch 17/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 62s 771ms/step - loss: 0.6995 - val_loss: 0.6558\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.6992 - val_loss: 0.6554\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 62s 772ms/step - loss: 0.6990 - val_loss: 0.6551\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.6988 - val_loss: 0.6547\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 61s 761ms/step - loss: 0.6986 - val_loss: 0.6544\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.6983 - val_loss: 0.6540\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.6981 - val_loss: 0.6536\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 61s 768ms/step - loss: 0.6978 - val_loss: 0.6533\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.6976 - val_loss: 0.6529\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 0.6974 - val_loss: 0.6525\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 61s 763ms/step - loss: 0.6971 - val_loss: 0.6521\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 62s 770ms/step - loss: 0.6969 - val_loss: 0.6517\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 61s 763ms/step - loss: 0.6966 - val_loss: 0.6513\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.6963 - val_loss: 0.6509\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 61s 763ms/step - loss: 0.6961 - val_loss: 0.6505\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 61s 769ms/step - loss: 0.6959 - val_loss: 0.6501\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.6956 - val_loss: 0.6497\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 0.6953 - val_loss: 0.6493\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.6951 - val_loss: 0.6489\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.6948 - val_loss: 0.6484\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.6945 - val_loss: 0.6480\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.6943 - val_loss: 0.6476\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 62s 770ms/step - loss: 0.6940 - val_loss: 0.6471\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.6937 - val_loss: 0.6467\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.6935 - val_loss: 0.6462\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.6932 - val_loss: 0.6458\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.6929 - val_loss: 0.6453\n",
      "Execution time:  2678.760439157486\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6816\n",
      "Root Mean Square Error: 1.0115\n",
      "Mean Square Error: 1.0231\n",
      "\n",
      "Train RMSE: 1.011\n",
      "Train MSE: 1.023\n",
      "Train MAE: 0.682\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_156 (LSTM)              (None, 1008, 43)          7740      \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "lstm_157 (LSTM)              (None, 1008, 43)          14964     \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_78 (TimeDis (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 122s 498ms/step - loss: 0.9812 - val_loss: 1.3328\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 122s 498ms/step - loss: 0.9810 - val_loss: 1.3324\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9807 - val_loss: 1.3321\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9804 - val_loss: 1.3316\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9801 - val_loss: 1.3312\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9798 - val_loss: 1.3307\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9794 - val_loss: 1.3302\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9790 - val_loss: 1.3297\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9786 - val_loss: 1.3291\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9782 - val_loss: 1.3285\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9778 - val_loss: 1.3279\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9774 - val_loss: 1.3272\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9769 - val_loss: 1.3266\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9764 - val_loss: 1.3259\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9759 - val_loss: 1.3251\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9754 - val_loss: 1.3244\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9748 - val_loss: 1.3236\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9743 - val_loss: 1.3228\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9737 - val_loss: 1.3219\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9731 - val_loss: 1.3210\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9725 - val_loss: 1.3200\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9718 - val_loss: 1.3191\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9711 - val_loss: 1.3180\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9704 - val_loss: 1.3169\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9696 - val_loss: 1.3158\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9688 - val_loss: 1.3146\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9680 - val_loss: 1.3134\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9671 - val_loss: 1.3121\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9662 - val_loss: 1.3107\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 122s 499ms/step - loss: 0.9652 - val_loss: 1.3092\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 122s 501ms/step - loss: 0.9642 - val_loss: 1.3076\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9631 - val_loss: 1.3060\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9620 - val_loss: 1.3042\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9608 - val_loss: 1.3024\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9595 - val_loss: 1.3004\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9581 - val_loss: 1.2982\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9566 - val_loss: 1.2960\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9550 - val_loss: 1.2935\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9533 - val_loss: 1.2909\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9515 - val_loss: 1.2880\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9496 - val_loss: 1.2850\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9475 - val_loss: 1.2816\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 122s 498ms/step - loss: 0.9452 - val_loss: 1.2781\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 122s 499ms/step - loss: 0.9428 - val_loss: 1.2742\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9402 - val_loss: 1.2699\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9373 - val_loss: 1.2653\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9343 - val_loss: 1.2603\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.9308 - val_loss: 1.2548\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9272 - val_loss: 1.2487\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9231 - val_loss: 1.2421\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9187 - val_loss: 1.2349\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 122s 500ms/step - loss: 0.9139 - val_loss: 1.2271\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 121s 498ms/step - loss: 0.9086 - val_loss: 1.2186\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.9029 - val_loss: 1.2094\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.8968 - val_loss: 1.1996\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 121s 496ms/step - loss: 0.8904 - val_loss: 1.1893\n",
      "Execution time:  6823.556005239487\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.8446\n",
      "Root Mean Square Error: 1.0735\n",
      "Mean Square Error: 1.1525\n",
      "\n",
      "Train RMSE: 1.074\n",
      "Train MSE: 1.152\n",
      "Train MAE: 0.845\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_158 (LSTM)              (None, 1008, 45)          8460      \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "lstm_159 (LSTM)              (None, 1008, 45)          16380     \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_79 (TimeDis (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 64s 804ms/step - loss: 0.9813 - val_loss: 1.1627\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 63s 794ms/step - loss: 0.9813 - val_loss: 1.1626\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 64s 799ms/step - loss: 0.9812 - val_loss: 1.1625\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 64s 801ms/step - loss: 0.9812 - val_loss: 1.1625\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 64s 803ms/step - loss: 0.9811 - val_loss: 1.1624\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 64s 801ms/step - loss: 0.9811 - val_loss: 1.1623\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 64s 803ms/step - loss: 0.9810 - val_loss: 1.1622\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 64s 802ms/step - loss: 0.9809 - val_loss: 1.1621\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 64s 800ms/step - loss: 0.9808 - val_loss: 1.1620\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 64s 796ms/step - loss: 0.9808 - val_loss: 1.1619\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 64s 800ms/step - loss: 0.9807 - val_loss: 1.1618\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 64s 798ms/step - loss: 0.9806 - val_loss: 1.1617\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 64s 798ms/step - loss: 0.9805 - val_loss: 1.1616\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 64s 801ms/step - loss: 0.9804 - val_loss: 1.1615\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 64s 799ms/step - loss: 0.9804 - val_loss: 1.1613\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 64s 800ms/step - loss: 0.9803 - val_loss: 1.1612\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 64s 806ms/step - loss: 0.9802 - val_loss: 1.1611\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 64s 799ms/step - loss: 0.9801 - val_loss: 1.1610\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 64s 802ms/step - loss: 0.9800 - val_loss: 1.1609\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 64s 801ms/step - loss: 0.9799 - val_loss: 1.1607\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 64s 801ms/step - loss: 0.9798 - val_loss: 1.1606\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 64s 804ms/step - loss: 0.9797 - val_loss: 1.1605\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 64s 801ms/step - loss: 0.9796 - val_loss: 1.1603\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 64s 802ms/step - loss: 0.9795 - val_loss: 1.1602\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 64s 800ms/step - loss: 0.9794 - val_loss: 1.1601\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 64s 803ms/step - loss: 0.9793 - val_loss: 1.1599\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 64s 798ms/step - loss: 0.9792 - val_loss: 1.1598\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 64s 803ms/step - loss: 0.9791 - val_loss: 1.1597\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 64s 801ms/step - loss: 0.9790 - val_loss: 1.1595\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 64s 799ms/step - loss: 0.9789 - val_loss: 1.1594\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 64s 798ms/step - loss: 0.9788 - val_loss: 1.1592\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 64s 796ms/step - loss: 0.9787 - val_loss: 1.1591\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 64s 799ms/step - loss: 0.9786 - val_loss: 1.1589\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 63s 793ms/step - loss: 0.9785 - val_loss: 1.1588\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 63s 791ms/step - loss: 0.9784 - val_loss: 1.1586\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 63s 793ms/step - loss: 0.9783 - val_loss: 1.1585\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 63s 791ms/step - loss: 0.9782 - val_loss: 1.1583\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 63s 787ms/step - loss: 0.9781 - val_loss: 1.1582\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 64s 795ms/step - loss: 0.9780 - val_loss: 1.1580\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 63s 790ms/step - loss: 0.9778 - val_loss: 1.1579\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 63s 790ms/step - loss: 0.9777 - val_loss: 1.1577\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 63s 788ms/step - loss: 0.9776 - val_loss: 1.1575\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 63s 790ms/step - loss: 0.9775 - val_loss: 1.1574\n",
      "Execution time:  2782.560327529907\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.9241\n",
      "Root Mean Square Error: 1.1337\n",
      "Mean Square Error: 1.2854\n",
      "\n",
      "Train RMSE: 1.134\n",
      "Train MSE: 1.285\n",
      "Train MAE: 0.924\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_160 (LSTM)              (None, 1008, 43)          7740      \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "lstm_161 (LSTM)              (None, 1008, 43)          14964     \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_80 (TimeDis (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 119s 487ms/step - loss: 0.6233 - val_loss: 0.4743\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 119s 486ms/step - loss: 0.5706 - val_loss: 0.4328\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 119s 486ms/step - loss: 0.5667 - val_loss: 0.4242\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5658 - val_loss: 0.3915\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 117s 482ms/step - loss: 0.5631 - val_loss: 0.3928\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5620 - val_loss: 0.3986\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5612 - val_loss: 0.3950\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5604 - val_loss: 0.3913\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5600 - val_loss: 0.3907\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5598 - val_loss: 0.3907\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5595 - val_loss: 0.3899\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5593 - val_loss: 0.3894\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5590 - val_loss: 0.3876\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5592 - val_loss: 0.3870\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5584 - val_loss: 0.4021\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5575 - val_loss: 0.4223\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5557 - val_loss: 0.4439\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 118s 485ms/step - loss: 0.5559 - val_loss: 0.4726\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5569 - val_loss: 0.4131\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5572 - val_loss: 0.4115\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5569 - val_loss: 0.4157\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5566 - val_loss: 0.4233\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5569 - val_loss: 0.4176\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5563 - val_loss: 0.4194\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.5559 - val_loss: 0.4305\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.5554 - val_loss: 0.4220\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5437 - val_loss: 0.4384\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5519 - val_loss: 0.4304\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5429 - val_loss: 0.4421\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5418 - val_loss: 0.4468\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5408 - val_loss: 0.4518\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5401 - val_loss: 0.4774\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.5399 - val_loss: 0.4744\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5385 - val_loss: 0.4747\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5383 - val_loss: 0.4806\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5380 - val_loss: 0.4727\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5377 - val_loss: 0.4762\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5389 - val_loss: 0.4916\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5365 - val_loss: 0.5512\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.5370 - val_loss: 0.4822\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.5363 - val_loss: 0.4850\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5361 - val_loss: 0.4883\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5360 - val_loss: 0.4915\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5358 - val_loss: 0.5002\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.5353 - val_loss: 0.5688\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5377 - val_loss: 0.4888\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5357 - val_loss: 0.4891\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 117s 482ms/step - loss: 0.5354 - val_loss: 0.4997\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5369 - val_loss: 0.5068\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5348 - val_loss: 0.5664\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5371 - val_loss: 0.4872\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5351 - val_loss: 0.4910\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5345 - val_loss: 0.5041\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.5377 - val_loss: 0.5591\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.5357 - val_loss: 0.4888\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.5343 - val_loss: 0.4936\n",
      "Execution time:  6629.472941875458\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6667\n",
      "Root Mean Square Error: 1.1497\n",
      "Mean Square Error: 1.3219\n",
      "\n",
      "Train RMSE: 1.150\n",
      "Train MSE: 1.322\n",
      "Train MAE: 0.667\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_162 (LSTM)              (None, 1008, 45)          8460      \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "lstm_163 (LSTM)              (None, 1008, 45)          16380     \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_81 (TimeDis (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.6795 - val_loss: 0.5521\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 0.6238 - val_loss: 0.4537\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 61s 768ms/step - loss: 0.6120 - val_loss: 0.4243\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.6069 - val_loss: 0.4063\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.6039 - val_loss: 0.3977\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 64s 794ms/step - loss: 0.6015 - val_loss: 0.3937\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5998 - val_loss: 0.3901\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5983 - val_loss: 0.3883\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 63s 782ms/step - loss: 0.5973 - val_loss: 0.3859\n",
      "Epoch 10/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 62s 778ms/step - loss: 0.5960 - val_loss: 0.3857\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5951 - val_loss: 0.3843\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 62s 770ms/step - loss: 0.5927 - val_loss: 0.3589\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.5834 - val_loss: 0.3623\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 63s 782ms/step - loss: 0.5809 - val_loss: 0.3533\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5763 - val_loss: 0.3646\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5737 - val_loss: 0.3749\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.5718 - val_loss: 0.3830\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.5705 - val_loss: 0.3915\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5694 - val_loss: 0.3994\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 62s 770ms/step - loss: 0.5686 - val_loss: 0.4057\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5681 - val_loss: 0.4117\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5675 - val_loss: 0.4162\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5671 - val_loss: 0.4201\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.5665 - val_loss: 0.4250\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 62s 780ms/step - loss: 0.5656 - val_loss: 0.4284\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 62s 772ms/step - loss: 0.5647 - val_loss: 0.4334\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5643 - val_loss: 0.4387\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5639 - val_loss: 0.4426\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5636 - val_loss: 0.4465\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5632 - val_loss: 0.4505\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5633 - val_loss: 0.4541\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.5632 - val_loss: 0.4590\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5625 - val_loss: 0.4585\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 0.5623 - val_loss: 0.4621\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.5615 - val_loss: 0.4629\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 62s 772ms/step - loss: 0.5605 - val_loss: 0.4642\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5594 - val_loss: 0.4684\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 0.5593 - val_loss: 0.4711\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 63s 782ms/step - loss: 0.5591 - val_loss: 0.4734\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5589 - val_loss: 0.4751\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 0.5584 - val_loss: 0.4757\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.5584 - val_loss: 0.4771\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5585 - val_loss: 0.4797\n",
      "Execution time:  2701.564010620117\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.7249\n",
      "Root Mean Square Error: 1.1887\n",
      "Mean Square Error: 1.4131\n",
      "\n",
      "Train RMSE: 1.189\n",
      "Train MSE: 1.413\n",
      "Train MAE: 0.725\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_164 (LSTM)              (None, 1008, 43)          7740      \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "lstm_165 (LSTM)              (None, 1008, 43)          14964     \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_82 (TimeDis (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 22,748\n",
      "Trainable params: 22,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 120s 493ms/step - loss: 0.8057 - val_loss: 0.8555\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 119s 488ms/step - loss: 0.6867 - val_loss: 0.8432\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 119s 487ms/step - loss: 0.6829 - val_loss: 0.8398\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 118s 486ms/step - loss: 0.6815 - val_loss: 0.8382\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 119s 487ms/step - loss: 0.6807 - val_loss: 0.8372\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 119s 486ms/step - loss: 0.6802 - val_loss: 0.8366\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 118s 485ms/step - loss: 0.6798 - val_loss: 0.8361\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6796 - val_loss: 0.8357\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6794 - val_loss: 0.8355\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6792 - val_loss: 0.8352\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6791 - val_loss: 0.8350\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 118s 485ms/step - loss: 0.6790 - val_loss: 0.8349\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6789 - val_loss: 0.8348\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6788 - val_loss: 0.8346\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 118s 485ms/step - loss: 0.6787 - val_loss: 0.8345\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6787 - val_loss: 0.8345\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6786 - val_loss: 0.8344\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8343\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 118s 485ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 118s 484ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 118s 482ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 118s 483ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 117s 481ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 117s 479ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 117s 480ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Execution time:  6619.18386220932\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6954\n",
      "Root Mean Square Error: 1.0192\n",
      "Mean Square Error: 1.0387\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.695\n",
      "###########################\n",
      "\n",
      "MODEL:  LSTM\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_166 (LSTM)              (None, 1008, 45)          8460      \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "lstm_167 (LSTM)              (None, 1008, 45)          16380     \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_83 (TimeDis (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 24,886\n",
      "Trainable params: 24,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 59s 734ms/step - loss: 0.9438 - val_loss: 0.9019\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 60s 744ms/step - loss: 0.7569 - val_loss: 0.7071\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 59s 742ms/step - loss: 0.7156 - val_loss: 0.6876\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 60s 744ms/step - loss: 0.7101 - val_loss: 0.6814\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 59s 742ms/step - loss: 0.7077 - val_loss: 0.6782\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7063 - val_loss: 0.6761\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 60s 747ms/step - loss: 0.7054 - val_loss: 0.6747\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7047 - val_loss: 0.6736\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 60s 754ms/step - loss: 0.7042 - val_loss: 0.6728\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 60s 752ms/step - loss: 0.7038 - val_loss: 0.6722\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 60s 750ms/step - loss: 0.7035 - val_loss: 0.6716\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7033 - val_loss: 0.6712\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 60s 748ms/step - loss: 0.7030 - val_loss: 0.6708\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 60s 751ms/step - loss: 0.7028 - val_loss: 0.6705\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 60s 750ms/step - loss: 0.7027 - val_loss: 0.6702\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 60s 750ms/step - loss: 0.7025 - val_loss: 0.6700\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7024 - val_loss: 0.6697\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 60s 751ms/step - loss: 0.7023 - val_loss: 0.6696\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7022 - val_loss: 0.6694\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 60s 753ms/step - loss: 0.7021 - val_loss: 0.6692\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 60s 747ms/step - loss: 0.7020 - val_loss: 0.6691\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 60s 751ms/step - loss: 0.7019 - val_loss: 0.6689\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 59s 742ms/step - loss: 0.7018 - val_loss: 0.6688\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 60s 746ms/step - loss: 0.7018 - val_loss: 0.6687\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 59s 743ms/step - loss: 0.7017 - val_loss: 0.6686\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 60s 744ms/step - loss: 0.7016 - val_loss: 0.6685\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 60s 746ms/step - loss: 0.7016 - val_loss: 0.6684\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7015 - val_loss: 0.6683\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7015 - val_loss: 0.6683\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 60s 754ms/step - loss: 0.7015 - val_loss: 0.6682\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 60s 752ms/step - loss: 0.7014 - val_loss: 0.6681\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 60s 749ms/step - loss: 0.7014 - val_loss: 0.6680\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 60s 753ms/step - loss: 0.7013 - val_loss: 0.6680\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 60s 754ms/step - loss: 0.7013 - val_loss: 0.6679\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 60s 752ms/step - loss: 0.7013 - val_loss: 0.6679\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 60s 752ms/step - loss: 0.7012 - val_loss: 0.6678\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 60s 751ms/step - loss: 0.7012 - val_loss: 0.6678\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.7012 - val_loss: 0.6677\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 60s 753ms/step - loss: 0.7012 - val_loss: 0.6677\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.7011 - val_loss: 0.6676\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 60s 750ms/step - loss: 0.7011 - val_loss: 0.6676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/43\n",
      "80/80 [==============================] - 60s 751ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 60s 748ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Execution time:  2611.783912420273\n",
      "LSTM:\n",
      "Mean Absolute Error: 0.6956\n",
      "Root Mean Square Error: 1.0193\n",
      "Mean Square Error: 1.0389\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.696\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 6, 43)             5934      \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 6, 43)             11352     \n",
      "_________________________________________________________________\n",
      "dropout_169 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_84 (TimeDis (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3738 - val_loss: 0.1992\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2903 - val_loss: 0.2041\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2838 - val_loss: 0.2156\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2825 - val_loss: 0.2119\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2813 - val_loss: 0.2220\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2817 - val_loss: 0.2226\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2800 - val_loss: 0.2111\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2803 - val_loss: 0.2223\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2790 - val_loss: 0.2113\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2783 - val_loss: 0.2045\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2778 - val_loss: 0.2080\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2776 - val_loss: 0.2050\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2767 - val_loss: 0.2054\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2748 - val_loss: 0.1950\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2755 - val_loss: 0.1929\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2730 - val_loss: 0.1800\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2729 - val_loss: 0.1783\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2723 - val_loss: 0.1718\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2719 - val_loss: 0.1673\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2715 - val_loss: 0.1676\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2704 - val_loss: 0.1591\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2705 - val_loss: 0.1572\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2698 - val_loss: 0.1522\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2687 - val_loss: 0.1458\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2688 - val_loss: 0.1392\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2694 - val_loss: 0.1459\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2674 - val_loss: 0.1350\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2678 - val_loss: 0.1292\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2678 - val_loss: 0.1291\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2676 - val_loss: 0.1205\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2670 - val_loss: 0.1206\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2664 - val_loss: 0.1192\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2657 - val_loss: 0.1228\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2658 - val_loss: 0.1196\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.1152\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2658 - val_loss: 0.1111\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.1097\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2648 - val_loss: 0.1110\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2643 - val_loss: 0.1109\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2655 - val_loss: 0.1059\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.2647 - val_loss: 0.1059\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2645 - val_loss: 0.1054\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2638 - val_loss: 0.0986\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2638 - val_loss: 0.1023\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2644 - val_loss: 0.0985\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2643 - val_loss: 0.1003\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.0985\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.0985\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2628 - val_loss: 0.0918\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2627 - val_loss: 0.1016\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2630 - val_loss: 0.0954\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2629 - val_loss: 0.0940\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2628 - val_loss: 0.0957\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2625 - val_loss: 0.0958\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2623 - val_loss: 0.0986\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2627 - val_loss: 0.0943\n",
      "Execution time:  84.43638944625854\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1696\n",
      "Root Mean Square Error: 0.5747\n",
      "Mean Square Error: 0.3303\n",
      "\n",
      "Train RMSE: 0.575\n",
      "Train MSE: 0.330\n",
      "Train MAE: 0.170\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (None, 6, 45)             6480      \n",
      "_________________________________________________________________\n",
      "dropout_170 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 6, 45)             12420     \n",
      "_________________________________________________________________\n",
      "dropout_171 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_85 (TimeDis (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4752 - val_loss: 0.2880\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.3125 - val_loss: 0.2340\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2844 - val_loss: 0.2182\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2763 - val_loss: 0.2164\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2740 - val_loss: 0.2154\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2724 - val_loss: 0.2128\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2728 - val_loss: 0.2138\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2719 - val_loss: 0.2147\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2713 - val_loss: 0.2134\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2711 - val_loss: 0.2137\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2712 - val_loss: 0.2119\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2727 - val_loss: 0.2153\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2704 - val_loss: 0.2128\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2712 - val_loss: 0.2144\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2703 - val_loss: 0.2110\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2704 - val_loss: 0.2111\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2695 - val_loss: 0.2103\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2694 - val_loss: 0.2081\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2699 - val_loss: 0.2087\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2699 - val_loss: 0.2094\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2685 - val_loss: 0.2085\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2693 - val_loss: 0.2100\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2690 - val_loss: 0.2098\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2681 - val_loss: 0.2086\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2681 - val_loss: 0.2053\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2672 - val_loss: 0.2039\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2670 - val_loss: 0.2047\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2670 - val_loss: 0.2031\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2670 - val_loss: 0.2015\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2663 - val_loss: 0.2015\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2666 - val_loss: 0.2012\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2654 - val_loss: 0.1976\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2652 - val_loss: 0.1957\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2659 - val_loss: 0.1974\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2651 - val_loss: 0.1948\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2660 - val_loss: 0.1950\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2657 - val_loss: 0.1973\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2637 - val_loss: 0.1926\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2653 - val_loss: 0.1934\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2655 - val_loss: 0.1932\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2636 - val_loss: 0.1912\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2634 - val_loss: 0.1916\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2636 - val_loss: 0.1872\n",
      "Execution time:  43.191447257995605\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1574\n",
      "Root Mean Square Error: 0.5736\n",
      "Mean Square Error: 0.3290\n",
      "\n",
      "Train RMSE: 0.574\n",
      "Train MSE: 0.329\n",
      "Train MAE: 0.157\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_4 (GRU)                  (None, 6, 43)             5934      \n",
      "_________________________________________________________________\n",
      "dropout_172 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 6, 43)             11352     \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_86 (TimeDis (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "  1/326 [..............................] - ETA: 0s - loss: 0.1185WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5953 - val_loss: 0.8097\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5135 - val_loss: 0.8066\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5029 - val_loss: 0.8062\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5021 - val_loss: 0.8061\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5018 - val_loss: 0.8061\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5014 - val_loss: 0.8060\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5014 - val_loss: 0.8060\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5008 - val_loss: 0.8060\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5011 - val_loss: 0.8060\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5008 - val_loss: 0.8060\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5005 - val_loss: 0.8060\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5009 - val_loss: 0.8060\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5005 - val_loss: 0.8059\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.8059\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.8059\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4998 - val_loss: 0.8059\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5000 - val_loss: 0.8059\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4997 - val_loss: 0.8059\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5000 - val_loss: 0.8059\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4995 - val_loss: 0.8059\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4996 - val_loss: 0.8059\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4993 - val_loss: 0.8059\n",
      "Epoch 23/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4992 - val_loss: 0.8059\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4991 - val_loss: 0.8059\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4990 - val_loss: 0.8059\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4986 - val_loss: 0.8059\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4988 - val_loss: 0.8059\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4986 - val_loss: 0.8059\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4989 - val_loss: 0.8059\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4988 - val_loss: 0.8059\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4985 - val_loss: 0.8059\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4982 - val_loss: 0.8059\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4982 - val_loss: 0.8059\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4980 - val_loss: 0.8059\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4977 - val_loss: 0.8059\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4981 - val_loss: 0.8059\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4981 - val_loss: 0.8059\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4980 - val_loss: 0.8059\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4980 - val_loss: 0.8059\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4978 - val_loss: 0.8059\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4980 - val_loss: 0.8059\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4980 - val_loss: 0.8059\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4973 - val_loss: 0.8059\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4979 - val_loss: 0.8059\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4976 - val_loss: 0.8059\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4975 - val_loss: 0.8059\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.8059\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4973 - val_loss: 0.8059\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4975 - val_loss: 0.8059\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4972 - val_loss: 0.8059\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4973 - val_loss: 0.8059\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4977 - val_loss: 0.8059\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4971 - val_loss: 0.8059\n",
      "Execution time:  82.23852682113647\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4937\n",
      "Root Mean Square Error: 0.7565\n",
      "Mean Square Error: 0.5723\n",
      "\n",
      "Train RMSE: 0.756\n",
      "Train MSE: 0.572\n",
      "Train MAE: 0.494\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, 6, 45)             6480      \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 6, 45)             12420     \n",
      "_________________________________________________________________\n",
      "dropout_175 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_87 (TimeDis (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.7047 - val_loss: 0.7135\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5393 - val_loss: 0.6789\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.5181 - val_loss: 0.6690\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5035 - val_loss: 0.6650\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5000 - val_loss: 0.6635\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4990 - val_loss: 0.6631\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4984 - val_loss: 0.6625\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4984 - val_loss: 0.6623\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4982 - val_loss: 0.6620\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4979 - val_loss: 0.6620\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4977 - val_loss: 0.6617\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4976 - val_loss: 0.6617\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4974 - val_loss: 0.6619\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4974 - val_loss: 0.6622\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4973 - val_loss: 0.6621\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4971 - val_loss: 0.6620\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4968 - val_loss: 0.6620\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4969 - val_loss: 0.6623\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4967 - val_loss: 0.6622\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4967 - val_loss: 0.6623\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4965 - val_loss: 0.6624\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4965 - val_loss: 0.6625\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4966 - val_loss: 0.6622\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4962 - val_loss: 0.6619\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4961 - val_loss: 0.6625\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4962 - val_loss: 0.6624\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4960 - val_loss: 0.6622\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4958 - val_loss: 0.6624\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4960 - val_loss: 0.6625\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4953 - val_loss: 0.6625\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4956 - val_loss: 0.6623\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4952 - val_loss: 0.6623\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4953 - val_loss: 0.6622\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4951 - val_loss: 0.6625\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4951 - val_loss: 0.6623\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4950 - val_loss: 0.6621\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4949 - val_loss: 0.6622\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4949 - val_loss: 0.6621\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4946 - val_loss: 0.6622\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4949 - val_loss: 0.6620\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4947 - val_loss: 0.6621\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4945 - val_loss: 0.6621\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4950 - val_loss: 0.6620\n",
      "Execution time:  45.2899649143219\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4938\n",
      "Root Mean Square Error: 0.7567\n",
      "Mean Square Error: 0.5726\n",
      "\n",
      "Train RMSE: 0.757\n",
      "Train MSE: 0.573\n",
      "Train MAE: 0.494\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_8 (GRU)                  (None, 6, 43)             5934      \n",
      "_________________________________________________________________\n",
      "dropout_176 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 6, 43)             11352     \n",
      "_________________________________________________________________\n",
      "dropout_177 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_88 (TimeDis (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "  1/326 [..............................] - ETA: 0s - loss: 0.4802WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7157 - val_loss: 0.8169\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7138 - val_loss: 0.8139\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7119 - val_loss: 0.8109\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7098 - val_loss: 0.8079\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7080 - val_loss: 0.8048\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7063 - val_loss: 0.8016\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7043 - val_loss: 0.7984\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7023 - val_loss: 0.7951\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.7004 - val_loss: 0.7918\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6981 - val_loss: 0.7885\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6964 - val_loss: 0.7850\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6947 - val_loss: 0.7816\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6924 - val_loss: 0.7781\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6903 - val_loss: 0.7746\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6882 - val_loss: 0.7710\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6859 - val_loss: 0.7674\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6842 - val_loss: 0.7637\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6819 - val_loss: 0.7600\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6798 - val_loss: 0.7562\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6775 - val_loss: 0.7523\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6751 - val_loss: 0.7485\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6731 - val_loss: 0.7445\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6706 - val_loss: 0.7405\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6685 - val_loss: 0.7365\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6663 - val_loss: 0.7324\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6639 - val_loss: 0.7283\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6616 - val_loss: 0.7242\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6590 - val_loss: 0.7200\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6566 - val_loss: 0.7158\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6544 - val_loss: 0.7116\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6518 - val_loss: 0.7074\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6492 - val_loss: 0.7031\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6470 - val_loss: 0.6987\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6445 - val_loss: 0.6943\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6420 - val_loss: 0.6899\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6397 - val_loss: 0.6854\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6371 - val_loss: 0.6809\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6344 - val_loss: 0.6763\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6317 - val_loss: 0.6717\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6290 - val_loss: 0.6669\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6261 - val_loss: 0.6621\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6233 - val_loss: 0.6572\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6206 - val_loss: 0.6522\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6175 - val_loss: 0.6471\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6152 - val_loss: 0.6420\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6119 - val_loss: 0.6368\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6086 - val_loss: 0.6314\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6057 - val_loss: 0.6260\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6025 - val_loss: 0.6205\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5991 - val_loss: 0.6148\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5958 - val_loss: 0.6091\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5923 - val_loss: 0.6033\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5893 - val_loss: 0.5974\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5862 - val_loss: 0.5915\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5825 - val_loss: 0.5854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5787 - val_loss: 0.5793\n",
      "Execution time:  80.02521181106567\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5707\n",
      "Root Mean Square Error: 0.8778\n",
      "Mean Square Error: 0.7705\n",
      "\n",
      "Train RMSE: 0.878\n",
      "Train MSE: 0.771\n",
      "Train MAE: 0.571\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, 6, 45)             6480      \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 6, 45)             12420     \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_89 (TimeDis (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.6999 - val_loss: 0.6865\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6992 - val_loss: 0.6856\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6983 - val_loss: 0.6847\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6977 - val_loss: 0.6837\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6969 - val_loss: 0.6828\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6962 - val_loss: 0.6818\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6953 - val_loss: 0.6808\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6947 - val_loss: 0.6799\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6939 - val_loss: 0.6789\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6931 - val_loss: 0.6779\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6923 - val_loss: 0.6769\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6915 - val_loss: 0.6758\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6907 - val_loss: 0.6748\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6899 - val_loss: 0.6738\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6889 - val_loss: 0.6727\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6883 - val_loss: 0.6717\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6874 - val_loss: 0.6706\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6868 - val_loss: 0.6696\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6858 - val_loss: 0.6685\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6851 - val_loss: 0.6674\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6842 - val_loss: 0.6664\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6834 - val_loss: 0.6653\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6823 - val_loss: 0.6642\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6818 - val_loss: 0.6631\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6809 - val_loss: 0.6620\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6800 - val_loss: 0.6608\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6792 - val_loss: 0.6597\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6783 - val_loss: 0.6586\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6775 - val_loss: 0.6574\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6765 - val_loss: 0.6563\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6758 - val_loss: 0.6551\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6748 - val_loss: 0.6540\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6740 - val_loss: 0.6528\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6731 - val_loss: 0.6516\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6722 - val_loss: 0.6504\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6715 - val_loss: 0.6492\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6705 - val_loss: 0.6480\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6696 - val_loss: 0.6468\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6687 - val_loss: 0.6456\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6677 - val_loss: 0.6444\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6669 - val_loss: 0.6431\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6660 - val_loss: 0.6419\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.6651 - val_loss: 0.6407\n",
      "Execution time:  42.630512952804565\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6572\n",
      "Root Mean Square Error: 0.9417\n",
      "Mean Square Error: 0.8868\n",
      "\n",
      "Train RMSE: 0.942\n",
      "Train MSE: 0.887\n",
      "Train MAE: 0.657\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_12 (GRU)                 (None, 6, 43)             5934      \n",
      "_________________________________________________________________\n",
      "dropout_180 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 6, 43)             11352     \n",
      "_________________________________________________________________\n",
      "dropout_181 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_90 (TimeDis (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8897 - val_loss: 1.3045\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8892 - val_loss: 1.3036\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8887 - val_loss: 1.3028\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8883 - val_loss: 1.3019\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8877 - val_loss: 1.3010\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8872 - val_loss: 1.3001\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8867 - val_loss: 1.2992\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8861 - val_loss: 1.2982\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8855 - val_loss: 1.2972\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8850 - val_loss: 1.2962\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8844 - val_loss: 1.2951\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8838 - val_loss: 1.2940\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8832 - val_loss: 1.2930\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8826 - val_loss: 1.2919\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8820 - val_loss: 1.2907\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8813 - val_loss: 1.2896\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8806 - val_loss: 1.2884\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8800 - val_loss: 1.2872\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8793 - val_loss: 1.2860\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8787 - val_loss: 1.2848\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8780 - val_loss: 1.2836\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8772 - val_loss: 1.2823\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8765 - val_loss: 1.2810\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8757 - val_loss: 1.2797\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8750 - val_loss: 1.2784\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8743 - val_loss: 1.2770\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8736 - val_loss: 1.2756\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8728 - val_loss: 1.2742\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8720 - val_loss: 1.2728\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8712 - val_loss: 1.2714\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8704 - val_loss: 1.2699\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8695 - val_loss: 1.2684\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8687 - val_loss: 1.2669\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8678 - val_loss: 1.2654\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8670 - val_loss: 1.2638\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8661 - val_loss: 1.2622\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8652 - val_loss: 1.2606\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8643 - val_loss: 1.2590\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8634 - val_loss: 1.2573\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8624 - val_loss: 1.2556\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8613 - val_loss: 1.2539\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8604 - val_loss: 1.2521\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8594 - val_loss: 1.2503\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8585 - val_loss: 1.2485\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8574 - val_loss: 1.2467\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8564 - val_loss: 1.2448\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8553 - val_loss: 1.2429\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8542 - val_loss: 1.2410\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8531 - val_loss: 1.2390\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8519 - val_loss: 1.2370\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8508 - val_loss: 1.2350\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8496 - val_loss: 1.2329\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8487 - val_loss: 1.2309\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8474 - val_loss: 1.2287\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8462 - val_loss: 1.2266\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.8449 - val_loss: 1.2244\n",
      "Execution time:  80.19839143753052\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8798\n",
      "Root Mean Square Error: 1.0685\n",
      "Mean Square Error: 1.1416\n",
      "\n",
      "Train RMSE: 1.068\n",
      "Train MSE: 1.142\n",
      "Train MAE: 0.880\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_14 (GRU)                 (None, 6, 45)             6480      \n",
      "_________________________________________________________________\n",
      "dropout_182 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 6, 45)             12420     \n",
      "_________________________________________________________________\n",
      "dropout_183 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_91 (TimeDis (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.8823 - val_loss: 1.1427\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8823 - val_loss: 1.1425\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8821 - val_loss: 1.1424\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8820 - val_loss: 1.1422\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8819 - val_loss: 1.1420\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8818 - val_loss: 1.1418\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8816 - val_loss: 1.1416\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8815 - val_loss: 1.1414\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8813 - val_loss: 1.1412\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8812 - val_loss: 1.1410\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8811 - val_loss: 1.1408\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8809 - val_loss: 1.1406\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8808 - val_loss: 1.1404\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8807 - val_loss: 1.1402\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8806 - val_loss: 1.1400\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8804 - val_loss: 1.1398\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8803 - val_loss: 1.1396\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8801 - val_loss: 1.1394\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8800 - val_loss: 1.1392\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8798 - val_loss: 1.1389\n",
      "Epoch 21/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8797 - val_loss: 1.1387\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8796 - val_loss: 1.1385\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8794 - val_loss: 1.1383\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8793 - val_loss: 1.1381\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.8791 - val_loss: 1.1379\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8790 - val_loss: 1.1377\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8789 - val_loss: 1.1375\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8787 - val_loss: 1.1372\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8786 - val_loss: 1.1370\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8784 - val_loss: 1.1368\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8783 - val_loss: 1.1366\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8782 - val_loss: 1.1364\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8780 - val_loss: 1.1361\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8778 - val_loss: 1.1359\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8777 - val_loss: 1.1357\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8776 - val_loss: 1.1355\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8774 - val_loss: 1.1353\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8773 - val_loss: 1.1350\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8771 - val_loss: 1.1348\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8770 - val_loss: 1.1346\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8768 - val_loss: 1.1344\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8767 - val_loss: 1.1341\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.8765 - val_loss: 1.1339\n",
      "Execution time:  44.818156480789185\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9267\n",
      "Root Mean Square Error: 1.1143\n",
      "Mean Square Error: 1.2416\n",
      "\n",
      "Train RMSE: 1.114\n",
      "Train MSE: 1.242\n",
      "Train MAE: 0.927\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_16 (GRU)                 (None, 6, 43)             5934      \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 6, 43)             11352     \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_92 (TimeDis (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4307 - val_loss: 0.2706\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3096 - val_loss: 0.1970\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2883 - val_loss: 0.1874\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2826 - val_loss: 0.1775\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2805 - val_loss: 0.1843\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2801 - val_loss: 0.1911\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2790 - val_loss: 0.1891\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2785 - val_loss: 0.1911\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2775 - val_loss: 0.1922\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2775 - val_loss: 0.1874\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2774 - val_loss: 0.1909\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2773 - val_loss: 0.1884\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2766 - val_loss: 0.1941\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2754 - val_loss: 0.1908\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2756 - val_loss: 0.1879\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2750 - val_loss: 0.1861\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2752 - val_loss: 0.1862\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2748 - val_loss: 0.1845\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2749 - val_loss: 0.1860\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2743 - val_loss: 0.1826\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2739 - val_loss: 0.1870\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2737 - val_loss: 0.1832\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2737 - val_loss: 0.1810\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2732 - val_loss: 0.1844\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2731 - val_loss: 0.1760\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2733 - val_loss: 0.1807\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2727 - val_loss: 0.1798\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2729 - val_loss: 0.1767\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2733 - val_loss: 0.1770\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2735 - val_loss: 0.1771\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2724 - val_loss: 0.1728\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2725 - val_loss: 0.1717\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2714 - val_loss: 0.1718\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2716 - val_loss: 0.1727\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2717 - val_loss: 0.1697\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2717 - val_loss: 0.1717\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2714 - val_loss: 0.1693\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2714 - val_loss: 0.1676\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2710 - val_loss: 0.1680\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2714 - val_loss: 0.1689\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2715 - val_loss: 0.1667\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2709 - val_loss: 0.1703\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.2704 - val_loss: 0.1647\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2703 - val_loss: 0.1656\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2700 - val_loss: 0.1622\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2703 - val_loss: 0.1675\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2699 - val_loss: 0.1627\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2696 - val_loss: 0.1651\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2689 - val_loss: 0.1637\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2694 - val_loss: 0.1615\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2684 - val_loss: 0.1632\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2692 - val_loss: 0.1589\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2686 - val_loss: 0.1583\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2688 - val_loss: 0.1606\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2684 - val_loss: 0.1609\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2679 - val_loss: 0.1587\n",
      "Execution time:  81.51253342628479\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1761\n",
      "Root Mean Square Error: 0.5767\n",
      "Mean Square Error: 0.3326\n",
      "\n",
      "Train RMSE: 0.577\n",
      "Train MSE: 0.333\n",
      "Train MAE: 0.176\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_18 (GRU)                 (None, 6, 45)             6480      \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, 6, 45)             12420     \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_93 (TimeDis (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.5053 - val_loss: 0.3227\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.3511 - val_loss: 0.2736\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.3126 - val_loss: 0.2395\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2944 - val_loss: 0.2265\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2858 - val_loss: 0.2182\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2793 - val_loss: 0.2130\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2762 - val_loss: 0.2117\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2738 - val_loss: 0.2110\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2733 - val_loss: 0.2112\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2724 - val_loss: 0.2112\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2729 - val_loss: 0.2115\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2726 - val_loss: 0.2115\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2721 - val_loss: 0.2115\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2724 - val_loss: 0.2114\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2715 - val_loss: 0.2116\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2713 - val_loss: 0.2118\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2711 - val_loss: 0.2116\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2706 - val_loss: 0.2113\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2706 - val_loss: 0.2109\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2711 - val_loss: 0.2106\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2703 - val_loss: 0.2109\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2705 - val_loss: 0.2107\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2707 - val_loss: 0.2107\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2704 - val_loss: 0.2102\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2703 - val_loss: 0.2100\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2698 - val_loss: 0.2098\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2699 - val_loss: 0.2098\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2699 - val_loss: 0.2097\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2699 - val_loss: 0.2093\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2694 - val_loss: 0.2091\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2692 - val_loss: 0.2088\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2692 - val_loss: 0.2084\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2687 - val_loss: 0.2085\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2687 - val_loss: 0.2077\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2686 - val_loss: 0.2080\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2691 - val_loss: 0.2078\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2684 - val_loss: 0.2077\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2688 - val_loss: 0.2073\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2688 - val_loss: 0.2071\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2687 - val_loss: 0.2076\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2684 - val_loss: 0.2066\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2682 - val_loss: 0.2069\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.2680 - val_loss: 0.2063\n",
      "Execution time:  42.36288595199585\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1638\n",
      "Root Mean Square Error: 0.5750\n",
      "Mean Square Error: 0.3306\n",
      "\n",
      "Train RMSE: 0.575\n",
      "Train MSE: 0.331\n",
      "Train MAE: 0.164\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_20 (GRU)                 (None, 6, 43)             5934      \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "gru_21 (GRU)                 (None, 6, 43)             11352     \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 6, 43)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_94 (TimeDis (None, 6, 1)              44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6643 - val_loss: 0.8404\n",
      "Epoch 2/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5384 - val_loss: 0.8157\n",
      "Epoch 3/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5248 - val_loss: 0.8096\n",
      "Epoch 4/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5186 - val_loss: 0.8077\n",
      "Epoch 5/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5129 - val_loss: 0.8070\n",
      "Epoch 6/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5072 - val_loss: 0.8066\n",
      "Epoch 7/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5039 - val_loss: 0.8064\n",
      "Epoch 8/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5022 - val_loss: 0.8063\n",
      "Epoch 9/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5016 - val_loss: 0.8062\n",
      "Epoch 10/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5016 - val_loss: 0.8061\n",
      "Epoch 11/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5010 - val_loss: 0.8061\n",
      "Epoch 12/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5013 - val_loss: 0.8061\n",
      "Epoch 13/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5011 - val_loss: 0.8061\n",
      "Epoch 14/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5008 - val_loss: 0.8060\n",
      "Epoch 15/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5009 - val_loss: 0.8060\n",
      "Epoch 16/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5008 - val_loss: 0.8060\n",
      "Epoch 17/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5007 - val_loss: 0.8060\n",
      "Epoch 18/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5007 - val_loss: 0.8060\n",
      "Epoch 19/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5007 - val_loss: 0.8060\n",
      "Epoch 20/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5007 - val_loss: 0.8060\n",
      "Epoch 21/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5006 - val_loss: 0.8060\n",
      "Epoch 22/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5005 - val_loss: 0.8060\n",
      "Epoch 23/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5003 - val_loss: 0.8060\n",
      "Epoch 24/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5006 - val_loss: 0.8060\n",
      "Epoch 25/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5004 - val_loss: 0.8060\n",
      "Epoch 26/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5005 - val_loss: 0.8060\n",
      "Epoch 27/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5003 - val_loss: 0.8060\n",
      "Epoch 28/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5005 - val_loss: 0.8060\n",
      "Epoch 29/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5006 - val_loss: 0.8060\n",
      "Epoch 30/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5006 - val_loss: 0.8060\n",
      "Epoch 31/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5004 - val_loss: 0.8060\n",
      "Epoch 32/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.8060\n",
      "Epoch 33/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5003 - val_loss: 0.8060\n",
      "Epoch 34/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 35/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.8060\n",
      "Epoch 36/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5000 - val_loss: 0.8060\n",
      "Epoch 37/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.8060\n",
      "Epoch 38/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5003 - val_loss: 0.8060\n",
      "Epoch 39/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 40/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5002 - val_loss: 0.8060\n",
      "Epoch 41/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5003 - val_loss: 0.8060\n",
      "Epoch 42/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.8060\n",
      "Epoch 43/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5002 - val_loss: 0.8060\n",
      "Epoch 44/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5002 - val_loss: 0.8060\n",
      "Epoch 45/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 46/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 47/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 48/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 49/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 50/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4998 - val_loss: 0.8060\n",
      "Epoch 51/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4997 - val_loss: 0.8060\n",
      "Epoch 52/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.8060\n",
      "Epoch 53/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4996 - val_loss: 0.8060\n",
      "Epoch 54/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4998 - val_loss: 0.8060\n",
      "Epoch 55/56\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4998 - val_loss: 0.8060\n",
      "Epoch 56/56\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4995 - val_loss: 0.8060\n",
      "Execution time:  81.205246925354\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4932\n",
      "Root Mean Square Error: 0.7504\n",
      "Mean Square Error: 0.5631\n",
      "\n",
      "Train RMSE: 0.750\n",
      "Train MSE: 0.563\n",
      "Train MAE: 0.493\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_22 (GRU)                 (None, 6, 45)             6480      \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "gru_23 (GRU)                 (None, 6, 45)             12420     \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 6, 45)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_95 (TimeDis (None, 6, 1)              46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.8027 - val_loss: 0.8321\n",
      "Epoch 2/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5870 - val_loss: 0.7217\n",
      "Epoch 3/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5517 - val_loss: 0.6978\n",
      "Epoch 4/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5381 - val_loss: 0.6854\n",
      "Epoch 5/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5295 - val_loss: 0.6780\n",
      "Epoch 6/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5233 - val_loss: 0.6733\n",
      "Epoch 7/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5183 - val_loss: 0.6701\n",
      "Epoch 8/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5134 - val_loss: 0.6676\n",
      "Epoch 9/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5085 - val_loss: 0.6663\n",
      "Epoch 10/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5044 - val_loss: 0.6652\n",
      "Epoch 11/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5023 - val_loss: 0.6644\n",
      "Epoch 12/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.5005 - val_loss: 0.6638\n",
      "Epoch 13/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4996 - val_loss: 0.6634\n",
      "Epoch 14/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4990 - val_loss: 0.6630\n",
      "Epoch 15/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4990 - val_loss: 0.6628\n",
      "Epoch 16/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4986 - val_loss: 0.6627\n",
      "Epoch 17/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4984 - val_loss: 0.6627\n",
      "Epoch 18/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4983 - val_loss: 0.6625\n",
      "Epoch 19/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4981 - val_loss: 0.6624\n",
      "Epoch 20/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4982 - val_loss: 0.6624\n",
      "Epoch 21/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4979 - val_loss: 0.6623\n",
      "Epoch 22/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4978 - val_loss: 0.6624\n",
      "Epoch 23/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4980 - val_loss: 0.6623\n",
      "Epoch 24/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4979 - val_loss: 0.6622\n",
      "Epoch 25/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4977 - val_loss: 0.6622\n",
      "Epoch 26/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4977 - val_loss: 0.6622\n",
      "Epoch 27/43\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4976 - val_loss: 0.6622\n",
      "Epoch 28/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4978 - val_loss: 0.6622\n",
      "Epoch 29/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4977 - val_loss: 0.6623\n",
      "Epoch 30/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4975 - val_loss: 0.6622\n",
      "Epoch 31/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4975 - val_loss: 0.6622\n",
      "Epoch 32/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4976 - val_loss: 0.6621\n",
      "Epoch 33/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4974 - val_loss: 0.6621\n",
      "Epoch 34/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4972 - val_loss: 0.6621\n",
      "Epoch 35/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4973 - val_loss: 0.6621\n",
      "Epoch 36/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4974 - val_loss: 0.6622\n",
      "Epoch 37/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4973 - val_loss: 0.6621\n",
      "Epoch 38/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4972 - val_loss: 0.6622\n",
      "Epoch 39/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4972 - val_loss: 0.6621\n",
      "Epoch 40/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4971 - val_loss: 0.6621\n",
      "Epoch 41/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4971 - val_loss: 0.6621\n",
      "Epoch 42/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4972 - val_loss: 0.6622\n",
      "Epoch 43/43\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 0.4973 - val_loss: 0.6622\n",
      "Execution time:  44.59614324569702\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4928\n",
      "Root Mean Square Error: 0.7502\n",
      "Mean Square Error: 0.5627\n",
      "\n",
      "Train RMSE: 0.750\n",
      "Train MSE: 0.563\n",
      "Train MAE: 0.493\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_24 (GRU)                 (None, 18, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_25 (GRU)                 (None, 18, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_193 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_96 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.4178 - val_loss: 0.2499\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3618 - val_loss: 0.2365\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3530 - val_loss: 0.2373\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3495 - val_loss: 0.2446\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3482 - val_loss: 0.2438\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3472 - val_loss: 0.2446\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3462 - val_loss: 0.2407\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3460 - val_loss: 0.2377\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3454 - val_loss: 0.2378\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3448 - val_loss: 0.2324\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3437 - val_loss: 0.2324\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3440 - val_loss: 0.2336\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3429 - val_loss: 0.2326\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3432 - val_loss: 0.2245\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3427 - val_loss: 0.2265\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3415 - val_loss: 0.2213\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3411 - val_loss: 0.2161\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3411 - val_loss: 0.2161\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3403 - val_loss: 0.2117\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3396 - val_loss: 0.2071\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3390 - val_loss: 0.2042\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3380 - val_loss: 0.2019\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3381 - val_loss: 0.2013\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3376 - val_loss: 0.1984\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3372 - val_loss: 0.1945\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3374 - val_loss: 0.1954\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3371 - val_loss: 0.1937\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3362 - val_loss: 0.1919\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3362 - val_loss: 0.1914\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3360 - val_loss: 0.1864\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3354 - val_loss: 0.1847\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3347 - val_loss: 0.1819\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3348 - val_loss: 0.1820\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3349 - val_loss: 0.1798\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3351 - val_loss: 0.1788\n",
      "Epoch 36/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3342 - val_loss: 0.1764\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3342 - val_loss: 0.1751\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3340 - val_loss: 0.1713\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3332 - val_loss: 0.1690\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3327 - val_loss: 0.1672\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3327 - val_loss: 0.1655\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3330 - val_loss: 0.1636\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3321 - val_loss: 0.1609\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3325 - val_loss: 0.1576\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3316 - val_loss: 0.1549\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3317 - val_loss: 0.1555\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3315 - val_loss: 0.1531\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3310 - val_loss: 0.1524\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3308 - val_loss: 0.1473\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3308 - val_loss: 0.1503\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3303 - val_loss: 0.1437\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3302 - val_loss: 0.1438\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3296 - val_loss: 0.1430\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3290 - val_loss: 0.1384\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3292 - val_loss: 0.1385\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3289 - val_loss: 0.1374\n",
      "Execution time:  157.61947011947632\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1921\n",
      "Root Mean Square Error: 0.6040\n",
      "Mean Square Error: 0.3648\n",
      "\n",
      "Train RMSE: 0.604\n",
      "Train MSE: 0.365\n",
      "Train MAE: 0.192\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_26 (GRU)                 (None, 18, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_194 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_27 (GRU)                 (None, 18, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_195 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_97 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.4674 - val_loss: 0.2937\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3695 - val_loss: 0.2766\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3559 - val_loss: 0.2687\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3486 - val_loss: 0.2619\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3442 - val_loss: 0.2592\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3413 - val_loss: 0.2577\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.3403 - val_loss: 0.2564\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3398 - val_loss: 0.2562\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3386 - val_loss: 0.2561\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3387 - val_loss: 0.2560\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3379 - val_loss: 0.2563\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3377 - val_loss: 0.2558\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3376 - val_loss: 0.2559\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3369 - val_loss: 0.2561\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3367 - val_loss: 0.2556\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3372 - val_loss: 0.2556\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3366 - val_loss: 0.2558\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3366 - val_loss: 0.2551\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3360 - val_loss: 0.2554\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3353 - val_loss: 0.2546\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3350 - val_loss: 0.2543\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3350 - val_loss: 0.2543\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3339 - val_loss: 0.2548\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3339 - val_loss: 0.2547\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.3335 - val_loss: 0.2565\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3338 - val_loss: 0.2533\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3328 - val_loss: 0.2552\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3331 - val_loss: 0.2547\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3321 - val_loss: 0.2541\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3321 - val_loss: 0.2544\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3322 - val_loss: 0.2539\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3320 - val_loss: 0.2546\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3317 - val_loss: 0.2528\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3309 - val_loss: 0.2545\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3314 - val_loss: 0.2537\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3310 - val_loss: 0.2529\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3305 - val_loss: 0.2540- ET\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3302 - val_loss: 0.2537\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3307 - val_loss: 0.2524\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3300 - val_loss: 0.2535\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3297 - val_loss: 0.2533\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3297 - val_loss: 0.2522\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.3294 - val_loss: 0.2529\n",
      "Execution time:  82.41637229919434\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1850\n",
      "Root Mean Square Error: 0.5977\n",
      "Mean Square Error: 0.3573\n",
      "\n",
      "Train RMSE: 0.598\n",
      "Train MSE: 0.357\n",
      "Train MAE: 0.185\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_28 (GRU)                 (None, 18, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_196 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_29 (GRU)                 (None, 18, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_197 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_98 (TimeDis (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.6180 - val_loss: 0.8139\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5505 - val_loss: 0.8071\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5438 - val_loss: 0.8061\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5421 - val_loss: 0.8059\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5411 - val_loss: 0.8057\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5408 - val_loss: 0.8057\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5403 - val_loss: 0.8057\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5399 - val_loss: 0.8056\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5398 - val_loss: 0.8056\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5398 - val_loss: 0.8056\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5394 - val_loss: 0.8056\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5392 - val_loss: 0.8056\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5390 - val_loss: 0.8056\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5390 - val_loss: 0.8056\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5390 - val_loss: 0.8056\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5391 - val_loss: 0.8056\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5387 - val_loss: 0.8056\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5387 - val_loss: 0.8056\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5385 - val_loss: 0.8056\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5383 - val_loss: 0.8056\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5386 - val_loss: 0.8056\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5385 - val_loss: 0.8056\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5384 - val_loss: 0.8056\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5383 - val_loss: 0.8056\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5382 - val_loss: 0.8056\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5381 - val_loss: 0.8056\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5383 - val_loss: 0.8056\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5378 - val_loss: 0.8056\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5376 - val_loss: 0.8056\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5376 - val_loss: 0.8056\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5374 - val_loss: 0.8056\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5372 - val_loss: 0.8056\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5372 - val_loss: 0.8056\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5372 - val_loss: 0.8056\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5369 - val_loss: 0.8056\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5371 - val_loss: 0.8056\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5369 - val_loss: 0.8056\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5368 - val_loss: 0.8056\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5369 - val_loss: 0.8056\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5368 - val_loss: 0.8056\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5367 - val_loss: 0.8056\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5365 - val_loss: 0.8056\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5367 - val_loss: 0.8056\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5366 - val_loss: 0.8056\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5363 - val_loss: 0.8056\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5364 - val_loss: 0.8056\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5363 - val_loss: 0.8056\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5363 - val_loss: 0.8056\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5363 - val_loss: 0.8056\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5360 - val_loss: 0.8056\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5361 - val_loss: 0.8056\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5359 - val_loss: 0.8056\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5359 - val_loss: 0.8056\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5358 - val_loss: 0.8056\n",
      "Execution time:  158.30796456336975\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5075\n",
      "Root Mean Square Error: 0.7646\n",
      "Mean Square Error: 0.5847\n",
      "\n",
      "Train RMSE: 0.765\n",
      "Train MSE: 0.585\n",
      "Train MAE: 0.507\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_30 (GRU)                 (None, 18, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_198 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_31 (GRU)                 (None, 18, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_199 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_99 (TimeDis (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.6912 - val_loss: 0.7204\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5772 - val_loss: 0.6978\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5552 - val_loss: 0.6884\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5475 - val_loss: 0.6839\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5433 - val_loss: 0.6819\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5408 - val_loss: 0.6808\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5393 - val_loss: 0.6803\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5385 - val_loss: 0.6799\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5378 - val_loss: 0.6796\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5372 - val_loss: 0.6795\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5370 - val_loss: 0.6792\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5366 - val_loss: 0.6792\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5366 - val_loss: 0.6790\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5364 - val_loss: 0.6788\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5362 - val_loss: 0.6788\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5361 - val_loss: 0.6788\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5361 - val_loss: 0.6791\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5360 - val_loss: 0.6787\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5359 - val_loss: 0.6788\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5358 - val_loss: 0.6786\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5356 - val_loss: 0.6790\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5357 - val_loss: 0.6786\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5356 - val_loss: 0.6786\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5357 - val_loss: 0.6785\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5354 - val_loss: 0.6785\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5354 - val_loss: 0.6782\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5354 - val_loss: 0.6785\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5354 - val_loss: 0.6783\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5352 - val_loss: 0.6783\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5352 - val_loss: 0.6781\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5351 - val_loss: 0.6782\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5351 - val_loss: 0.6784\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5348 - val_loss: 0.6785: 0s - loss: \n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5349 - val_loss: 0.6784\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5348 - val_loss: 0.6784\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5347 - val_loss: 0.6784\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5347 - val_loss: 0.6786\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5346 - val_loss: 0.6785\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5346 - val_loss: 0.6785\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5343 - val_loss: 0.6786\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5342 - val_loss: 0.6788\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5344 - val_loss: 0.6789\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5344 - val_loss: 0.6787\n",
      "Execution time:  84.3237042427063\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5063\n",
      "Root Mean Square Error: 0.7668\n",
      "Mean Square Error: 0.5879\n",
      "\n",
      "Train RMSE: 0.767\n",
      "Train MSE: 0.588\n",
      "Train MAE: 0.506\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_32 (GRU)                 (None, 18, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_200 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_33 (GRU)                 (None, 18, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_201 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_100 (TimeDi (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 3s 11ms/step - loss: 0.7082 - val_loss: 0.8058\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.7053 - val_loss: 0.8011\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.7023 - val_loss: 0.7964\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6998 - val_loss: 0.7916\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6970 - val_loss: 0.7868\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6940 - val_loss: 0.7818\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6914 - val_loss: 0.7768\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.6883 - val_loss: 0.7716\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6856 - val_loss: 0.7664\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6828 - val_loss: 0.7610\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6796 - val_loss: 0.7556\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6767 - val_loss: 0.7500\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6736 - val_loss: 0.7444\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.6703 - val_loss: 0.7386\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6673 - val_loss: 0.7328\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6638 - val_loss: 0.7268\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6608 - val_loss: 0.7208\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6578 - val_loss: 0.7147\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.6541 - val_loss: 0.7085\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.6509 - val_loss: 0.7022\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6475 - val_loss: 0.6958\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6441 - val_loss: 0.6893\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6406 - val_loss: 0.6828\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6373 - val_loss: 0.6761\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.6336 - val_loss: 0.6693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6302 - val_loss: 0.6624\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6266 - val_loss: 0.6553\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6227 - val_loss: 0.6480\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6189 - val_loss: 0.6406\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6150 - val_loss: 0.6330\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.6111 - val_loss: 0.6252\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6068 - val_loss: 0.6172\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.6025 - val_loss: 0.6090\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5981 - val_loss: 0.6007\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5940 - val_loss: 0.5921\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5893 - val_loss: 0.5833\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5850 - val_loss: 0.5743\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5802 - val_loss: 0.5651\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5753 - val_loss: 0.5557\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5707 - val_loss: 0.5461\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5656 - val_loss: 0.5364\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5609 - val_loss: 0.5266\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5560 - val_loss: 0.5167\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5510 - val_loss: 0.5067\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5462 - val_loss: 0.4967\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5410 - val_loss: 0.4867\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5361 - val_loss: 0.4768\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5309 - val_loss: 0.4669\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5262 - val_loss: 0.4571\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5211 - val_loss: 0.4474\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5165 - val_loss: 0.4378\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5124 - val_loss: 0.4281\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5073 - val_loss: 0.4187\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5030 - val_loss: 0.4092\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.4984 - val_loss: 0.3999\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.4946 - val_loss: 0.3908\n",
      "Execution time:  157.76559209823608\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4463\n",
      "Root Mean Square Error: 0.7825\n",
      "Mean Square Error: 0.6124\n",
      "\n",
      "Train RMSE: 0.783\n",
      "Train MSE: 0.612\n",
      "Train MAE: 0.446\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_34 (GRU)                 (None, 18, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_202 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_35 (GRU)                 (None, 18, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_101 (TimeDi (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.7116 - val_loss: 0.6981\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7105 - val_loss: 0.6969\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7097 - val_loss: 0.6957\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7088 - val_loss: 0.6944\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7078 - val_loss: 0.6932\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7070 - val_loss: 0.6919\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7060 - val_loss: 0.6907\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7050 - val_loss: 0.6894\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7039 - val_loss: 0.6881\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7029 - val_loss: 0.6868\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7020 - val_loss: 0.6855\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7010 - val_loss: 0.6841\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7000 - val_loss: 0.6828\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6990 - val_loss: 0.6815\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6979 - val_loss: 0.6801\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6968 - val_loss: 0.6787\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6959 - val_loss: 0.6774\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6949 - val_loss: 0.6760\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6939 - val_loss: 0.6746\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6928 - val_loss: 0.6732\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6919 - val_loss: 0.6718\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6908 - val_loss: 0.6703\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6897 - val_loss: 0.6689\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6887 - val_loss: 0.6675\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6877 - val_loss: 0.6660\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6865 - val_loss: 0.6646\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6856 - val_loss: 0.6631\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6844 - val_loss: 0.6616\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6834 - val_loss: 0.6601\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6822 - val_loss: 0.6586\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6813 - val_loss: 0.6571\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6800 - val_loss: 0.6556\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6790 - val_loss: 0.6540\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6778 - val_loss: 0.6525\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6767 - val_loss: 0.6509\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6756 - val_loss: 0.6494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6746 - val_loss: 0.6478\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6735 - val_loss: 0.6462\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6724 - val_loss: 0.6447\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6712 - val_loss: 0.6431\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6699 - val_loss: 0.6415\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6688 - val_loss: 0.6399\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6676 - val_loss: 0.6383\n",
      "Execution time:  81.83752202987671\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6547\n",
      "Root Mean Square Error: 0.9428\n",
      "Mean Square Error: 0.8889\n",
      "\n",
      "Train RMSE: 0.943\n",
      "Train MSE: 0.889\n",
      "Train MAE: 0.655\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_36 (GRU)                 (None, 18, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_37 (GRU)                 (None, 18, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_205 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_102 (TimeDi (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 12ms/step - loss: 0.8921 - val_loss: 1.3036\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8916 - val_loss: 1.3027\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8911 - val_loss: 1.3017\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8906 - val_loss: 1.3007\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8900 - val_loss: 1.2997\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8894 - val_loss: 1.2986\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8888 - val_loss: 1.2975\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8882 - val_loss: 1.2963\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8876 - val_loss: 1.2952\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8870 - val_loss: 1.2940\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8863 - val_loss: 1.2927\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8856 - val_loss: 1.2915\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8850 - val_loss: 1.2902\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8842 - val_loss: 1.2888\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8835 - val_loss: 1.2874\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8828 - val_loss: 1.2860\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8820 - val_loss: 1.2846\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8812 - val_loss: 1.2831\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8805 - val_loss: 1.2816\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8796 - val_loss: 1.2801\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8788 - val_loss: 1.2785\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8779 - val_loss: 1.2769\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8771 - val_loss: 1.2752\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8762 - val_loss: 1.2735\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8753 - val_loss: 1.2718\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8744 - val_loss: 1.2700\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8734 - val_loss: 1.2682\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8725 - val_loss: 1.2663\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8714 - val_loss: 1.2644\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8703 - val_loss: 1.2625\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8694 - val_loss: 1.2605\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8682 - val_loss: 1.2584\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8671 - val_loss: 1.2563\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8659 - val_loss: 1.2541\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8648 - val_loss: 1.2519\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8636 - val_loss: 1.2497\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8624 - val_loss: 1.2474\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8611 - val_loss: 1.2450\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8598 - val_loss: 1.2426\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8586 - val_loss: 1.2401\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8572 - val_loss: 1.2376\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8558 - val_loss: 1.2350\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8544 - val_loss: 1.2323\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8530 - val_loss: 1.2296\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8516 - val_loss: 1.2268\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8501 - val_loss: 1.2240\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8485 - val_loss: 1.2211\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8470 - val_loss: 1.2181\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8453 - val_loss: 1.2151\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8437 - val_loss: 1.2120\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8420 - val_loss: 1.2089\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8403 - val_loss: 1.2057\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8384 - val_loss: 1.2024\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.8368 - val_loss: 1.1991\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8350 - val_loss: 1.1957\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.8330 - val_loss: 1.1922\n",
      "Execution time:  158.44966626167297\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8605\n",
      "Root Mean Square Error: 1.0508\n",
      "Mean Square Error: 1.1042\n",
      "\n",
      "Train RMSE: 1.051\n",
      "Train MSE: 1.104\n",
      "Train MAE: 0.860\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_38 (GRU)                 (None, 18, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_206 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_39 (GRU)                 (None, 18, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_207 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_103 (TimeDi (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8812 - val_loss: 1.1381\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8810 - val_loss: 1.1379\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8809 - val_loss: 1.1376\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8807 - val_loss: 1.1373\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8805 - val_loss: 1.1370\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8803 - val_loss: 1.1367\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8801 - val_loss: 1.1364\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8799 - val_loss: 1.1361\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.8797 - val_loss: 1.1358\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8795 - val_loss: 1.1355\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8793 - val_loss: 1.1352\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8791 - val_loss: 1.1349\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8789 - val_loss: 1.1346\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8787 - val_loss: 1.1342\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8785 - val_loss: 1.1339\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8783 - val_loss: 1.1336\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8781 - val_loss: 1.1332\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8779 - val_loss: 1.1329\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8777 - val_loss: 1.1326\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8775 - val_loss: 1.1322\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8772 - val_loss: 1.1319\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8770 - val_loss: 1.1315\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8768 - val_loss: 1.1312\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8766 - val_loss: 1.1308\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8763 - val_loss: 1.1305\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8761 - val_loss: 1.1301\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8759 - val_loss: 1.1297\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8756 - val_loss: 1.1294\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8754 - val_loss: 1.1290\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8752 - val_loss: 1.1286\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8750 - val_loss: 1.1283\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8747 - val_loss: 1.1279\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8745 - val_loss: 1.1275\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8743 - val_loss: 1.1271\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8741 - val_loss: 1.1268\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8738 - val_loss: 1.1264\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8735 - val_loss: 1.1260\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8733 - val_loss: 1.1256\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8731 - val_loss: 1.1252\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8728 - val_loss: 1.1248\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8726 - val_loss: 1.1244\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8723 - val_loss: 1.1240\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8720 - val_loss: 1.1236\n",
      "Execution time:  82.48642182350159\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9183\n",
      "Root Mean Square Error: 1.1050\n",
      "Mean Square Error: 1.2211\n",
      "\n",
      "Train RMSE: 1.105\n",
      "Train MSE: 1.221\n",
      "Train MAE: 0.918\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_40 (GRU)                 (None, 18, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_208 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_41 (GRU)                 (None, 18, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_104 (TimeDi (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.4332 - val_loss: 0.2511\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3688 - val_loss: 0.2223\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3571 - val_loss: 0.2101\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3513 - val_loss: 0.2057\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3478 - val_loss: 0.2029\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3455 - val_loss: 0.2045\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3443 - val_loss: 0.2041\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3430 - val_loss: 0.2057\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3426 - val_loss: 0.2086\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3425 - val_loss: 0.2029\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3415 - val_loss: 0.2045\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3418 - val_loss: 0.2027\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3410 - val_loss: 0.2035\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3413 - val_loss: 0.2012\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3408 - val_loss: 0.2025\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3401 - val_loss: 0.2026\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3406 - val_loss: 0.1999\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3401 - val_loss: 0.2018\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3398 - val_loss: 0.2005\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3396 - val_loss: 0.1981\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3393 - val_loss: 0.1988\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3390 - val_loss: 0.1981\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3389 - val_loss: 0.1967\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3389 - val_loss: 0.1958\n",
      "Epoch 25/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3382 - val_loss: 0.1961\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3385 - val_loss: 0.1953\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3383 - val_loss: 0.1961\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3381 - val_loss: 0.1927\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3377 - val_loss: 0.1936\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3379 - val_loss: 0.1914\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3380 - val_loss: 0.1897\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3370 - val_loss: 0.1894\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3371 - val_loss: 0.1871\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3377 - val_loss: 0.1894\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3372 - val_loss: 0.1881\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3368 - val_loss: 0.1857\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3367 - val_loss: 0.1874\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3367 - val_loss: 0.1849\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3361 - val_loss: 0.1829\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3361 - val_loss: 0.1799\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3359 - val_loss: 0.1801\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3356 - val_loss: 0.1806\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3353 - val_loss: 0.1798\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3354 - val_loss: 0.1786\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3352 - val_loss: 0.1787\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3353 - val_loss: 0.1786\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3350 - val_loss: 0.1759\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3350 - val_loss: 0.1754\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3346 - val_loss: 0.1750\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3350 - val_loss: 0.1746\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3345 - val_loss: 0.1717\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.3342 - val_loss: 0.1729\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3343 - val_loss: 0.1731\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3339 - val_loss: 0.1722\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3341 - val_loss: 0.1694\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.3338 - val_loss: 0.1712\n",
      "Execution time:  157.90851163864136\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1778\n",
      "Root Mean Square Error: 0.5788\n",
      "Mean Square Error: 0.3350\n",
      "\n",
      "Train RMSE: 0.579\n",
      "Train MSE: 0.335\n",
      "Train MAE: 0.178\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_42 (GRU)                 (None, 18, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_210 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_43 (GRU)                 (None, 18, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_211 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_105 (TimeDi (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.4840 - val_loss: 0.3098\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3829 - val_loss: 0.2913\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3688 - val_loss: 0.2794\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3598 - val_loss: 0.2716\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3537 - val_loss: 0.2665\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3494 - val_loss: 0.2626\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3464 - val_loss: 0.2603\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3445 - val_loss: 0.2583\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3422 - val_loss: 0.2570\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3408 - val_loss: 0.2558\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3394 - val_loss: 0.2552\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3381 - val_loss: 0.2549\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3375 - val_loss: 0.2545\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3366 - val_loss: 0.2545\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3362 - val_loss: 0.2542\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3359 - val_loss: 0.2542\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3357 - val_loss: 0.2544\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3353 - val_loss: 0.2544\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3351 - val_loss: 0.2545\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3346 - val_loss: 0.2546\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3348 - val_loss: 0.2546\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3345 - val_loss: 0.2548\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3340 - val_loss: 0.2548\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3341 - val_loss: 0.2548\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3340 - val_loss: 0.2549\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3336 - val_loss: 0.2548\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3335 - val_loss: 0.2547\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3335 - val_loss: 0.2548\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3331 - val_loss: 0.2546\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3333 - val_loss: 0.2545\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3333 - val_loss: 0.2546\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3329 - val_loss: 0.2546\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3327 - val_loss: 0.2546\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3327 - val_loss: 0.2545\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3327 - val_loss: 0.2544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3329 - val_loss: 0.2543\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3325 - val_loss: 0.2541\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3322 - val_loss: 0.2539\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3325 - val_loss: 0.2539\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3321 - val_loss: 0.2538\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3319 - val_loss: 0.2539\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.3318 - val_loss: 0.2537\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.3319 - val_loss: 0.2536\n",
      "Execution time:  82.28378868103027\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1816\n",
      "Root Mean Square Error: 0.5798\n",
      "Mean Square Error: 0.3362\n",
      "\n",
      "Train RMSE: 0.580\n",
      "Train MSE: 0.336\n",
      "Train MAE: 0.182\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_44 (GRU)                 (None, 18, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_212 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_45 (GRU)                 (None, 18, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_213 (Dropout)        (None, 18, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_106 (TimeDi (None, 18, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.6641 - val_loss: 0.8329\n",
      "Epoch 2/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5743 - val_loss: 0.8187\n",
      "Epoch 3/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5609 - val_loss: 0.8127\n",
      "Epoch 4/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5522 - val_loss: 0.8091\n",
      "Epoch 5/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5469 - val_loss: 0.8074\n",
      "Epoch 6/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5447 - val_loss: 0.8066\n",
      "Epoch 7/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5431 - val_loss: 0.8063\n",
      "Epoch 8/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5423 - val_loss: 0.8061\n",
      "Epoch 9/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5418 - val_loss: 0.8059\n",
      "Epoch 10/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5412 - val_loss: 0.8059\n",
      "Epoch 11/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5405 - val_loss: 0.8058\n",
      "Epoch 12/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5402 - val_loss: 0.8058\n",
      "Epoch 13/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5398 - val_loss: 0.8057\n",
      "Epoch 14/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5397 - val_loss: 0.8057\n",
      "Epoch 15/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5393 - val_loss: 0.8057\n",
      "Epoch 16/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5393 - val_loss: 0.8057\n",
      "Epoch 17/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5390 - val_loss: 0.8057\n",
      "Epoch 18/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5388 - val_loss: 0.8057\n",
      "Epoch 19/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5388 - val_loss: 0.8056\n",
      "Epoch 20/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5386 - val_loss: 0.8056\n",
      "Epoch 21/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5388 - val_loss: 0.8056\n",
      "Epoch 22/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5386 - val_loss: 0.8056\n",
      "Epoch 23/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5384 - val_loss: 0.8056\n",
      "Epoch 24/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5384 - val_loss: 0.8056\n",
      "Epoch 25/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5383 - val_loss: 0.8056\n",
      "Epoch 26/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5383 - val_loss: 0.8056\n",
      "Epoch 27/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5383 - val_loss: 0.8056\n",
      "Epoch 28/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5381 - val_loss: 0.8056\n",
      "Epoch 29/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5380 - val_loss: 0.8056\n",
      "Epoch 30/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 31/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 32/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5380 - val_loss: 0.8056\n",
      "Epoch 33/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5377 - val_loss: 0.8056\n",
      "Epoch 34/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 35/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5378 - val_loss: 0.8056\n",
      "Epoch 36/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5377 - val_loss: 0.8056\n",
      "Epoch 37/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5376 - val_loss: 0.8056\n",
      "Epoch 38/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5379 - val_loss: 0.8056\n",
      "Epoch 39/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5375 - val_loss: 0.8056\n",
      "Epoch 40/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5375 - val_loss: 0.8056\n",
      "Epoch 41/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5376 - val_loss: 0.8056\n",
      "Epoch 42/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5376 - val_loss: 0.8056\n",
      "Epoch 43/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5374 - val_loss: 0.8056\n",
      "Epoch 44/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5375 - val_loss: 0.8056\n",
      "Epoch 45/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5374 - val_loss: 0.8056\n",
      "Epoch 46/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 47/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5374 - val_loss: 0.8056\n",
      "Epoch 48/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 49/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 50/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 51/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5374 - val_loss: 0.8056\n",
      "Epoch 52/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5372 - val_loss: 0.8056\n",
      "Epoch 53/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5373 - val_loss: 0.8056\n",
      "Epoch 54/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5371 - val_loss: 0.8056\n",
      "Epoch 55/56\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 0.5369 - val_loss: 0.8056\n",
      "Epoch 56/56\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.5372 - val_loss: 0.8056\n",
      "Execution time:  158.036141872406\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5023\n",
      "Root Mean Square Error: 0.7618\n",
      "Mean Square Error: 0.5804\n",
      "\n",
      "Train RMSE: 0.762\n",
      "Train MSE: 0.580\n",
      "Train MAE: 0.502\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_46 (GRU)                 (None, 18, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_214 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_47 (GRU)                 (None, 18, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_215 (Dropout)        (None, 18, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_107 (TimeDi (None, 18, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.7588 - val_loss: 0.7736\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6016 - val_loss: 0.7256\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5838 - val_loss: 0.7114\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5731 - val_loss: 0.7028\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5658 - val_loss: 0.6968\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5597 - val_loss: 0.6924\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5549 - val_loss: 0.6891\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5509 - val_loss: 0.6865\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5478 - val_loss: 0.6844\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5457 - val_loss: 0.6830\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5441 - val_loss: 0.6819\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5431 - val_loss: 0.6811\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5421 - val_loss: 0.6804\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5413 - val_loss: 0.6799\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5407 - val_loss: 0.6795\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5400 - val_loss: 0.6793\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5396 - val_loss: 0.6791\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5392 - val_loss: 0.6789\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5388 - val_loss: 0.6788\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5384 - val_loss: 0.6787\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5380 - val_loss: 0.6785\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5380 - val_loss: 0.6785\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5377 - val_loss: 0.6784\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5377 - val_loss: 0.6784\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5373 - val_loss: 0.6784\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5373 - val_loss: 0.6784\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5371 - val_loss: 0.6783\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5370 - val_loss: 0.6783\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5369 - val_loss: 0.6783\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5368 - val_loss: 0.6784\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5367 - val_loss: 0.6783\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5365 - val_loss: 0.6784\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5365 - val_loss: 0.6783\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5363 - val_loss: 0.6783\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5363 - val_loss: 0.6783\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5363 - val_loss: 0.6782\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5362 - val_loss: 0.6783\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5362 - val_loss: 0.6783\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5363 - val_loss: 0.6782\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5360 - val_loss: 0.6782\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5360 - val_loss: 0.6783\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5360 - val_loss: 0.6783\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5360 - val_loss: 0.6782\n",
      "Execution time:  82.04746532440186\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5006\n",
      "Root Mean Square Error: 0.7582\n",
      "Mean Square Error: 0.5748\n",
      "\n",
      "Train RMSE: 0.758\n",
      "Train MSE: 0.575\n",
      "Train MAE: 0.501\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_48 (GRU)                 (None, 36, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_216 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_49 (GRU)                 (None, 36, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_217 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_108 (TimeDi (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 10s 30ms/step - loss: 0.4704 - val_loss: 0.2691\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4305 - val_loss: 0.2520\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.4236 - val_loss: 0.2456\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.4201 - val_loss: 0.2512\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4183 - val_loss: 0.2509\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.4168 - val_loss: 0.2521\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4166 - val_loss: 0.2509\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4151 - val_loss: 0.2512\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4151 - val_loss: 0.2528\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4141 - val_loss: 0.2517\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4141 - val_loss: 0.2535\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4142 - val_loss: 0.2543\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4128 - val_loss: 0.2551\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4123 - val_loss: 0.2537\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4121 - val_loss: 0.2560\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4116 - val_loss: 0.2564\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.4105 - val_loss: 0.2552\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4099 - val_loss: 0.2565\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4098 - val_loss: 0.2547\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4089 - val_loss: 0.2543\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4089 - val_loss: 0.2576\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4086 - val_loss: 0.2563\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4083 - val_loss: 0.2583\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4071 - val_loss: 0.2566\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4065 - val_loss: 0.2549\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4062 - val_loss: 0.2554\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4054 - val_loss: 0.2563\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4050 - val_loss: 0.2560\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4049 - val_loss: 0.2556\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.4041 - val_loss: 0.2602\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4032 - val_loss: 0.2601\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4031 - val_loss: 0.2565\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4024 - val_loss: 0.2602\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4016 - val_loss: 0.2537\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4010 - val_loss: 0.2532\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4012 - val_loss: 0.2565\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4006 - val_loss: 0.2530\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3987 - val_loss: 0.2525\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3985 - val_loss: 0.2504\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3956 - val_loss: 0.2468\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.3955 - val_loss: 0.2506\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3958 - val_loss: 0.2483\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3956 - val_loss: 0.2519\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3955 - val_loss: 0.2474\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3929 - val_loss: 0.2450\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3921 - val_loss: 0.2412\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3923 - val_loss: 0.2446\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3922 - val_loss: 0.2434\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3901 - val_loss: 0.2404\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3919 - val_loss: 0.2447\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.3896 - val_loss: 0.2464\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3902 - val_loss: 0.2439\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3897 - val_loss: 0.2442\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3885 - val_loss: 0.2431\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3882 - val_loss: 0.2437\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.3873 - val_loss: 0.2418\n",
      "Execution time:  501.45494747161865\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2520\n",
      "Root Mean Square Error: 0.6385\n",
      "Mean Square Error: 0.4077\n",
      "\n",
      "Train RMSE: 0.638\n",
      "Train MSE: 0.408\n",
      "Train MAE: 0.252\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_50 (GRU)                 (None, 36, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_218 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_51 (GRU)                 (None, 36, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_219 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_109 (TimeDi (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 38ms/step - loss: 0.4937 - val_loss: 0.3290\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4247 - val_loss: 0.3213\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4183 - val_loss: 0.3168\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.4141 - val_loss: 0.3144\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4114 - val_loss: 0.3121\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4089 - val_loss: 0.3108\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4075 - val_loss: 0.3106\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4062 - val_loss: 0.3105\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4055 - val_loss: 0.3106\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4048 - val_loss: 0.3107\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4046 - val_loss: 0.3109\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4038 - val_loss: 0.3110\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4034 - val_loss: 0.3109\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4029 - val_loss: 0.3110\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4026 - val_loss: 0.3115\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4020 - val_loss: 0.3121\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4015 - val_loss: 0.3125\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4012 - val_loss: 0.3132\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4006 - val_loss: 0.3133\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4003 - val_loss: 0.3137\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.3997 - val_loss: 0.3138\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.3997 - val_loss: 0.3141\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3993 - val_loss: 0.3141\n",
      "Epoch 24/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3991 - val_loss: 0.3145\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3987 - val_loss: 0.3143\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3985 - val_loss: 0.3145\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3984 - val_loss: 0.3149\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3980 - val_loss: 0.3154\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.3977 - val_loss: 0.3159\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3975 - val_loss: 0.3151\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3973 - val_loss: 0.3167\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3968 - val_loss: 0.3156\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3969 - val_loss: 0.3173\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3962 - val_loss: 0.3157\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3961 - val_loss: 0.3176\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3955 - val_loss: 0.3171\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3960 - val_loss: 0.3178\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3955 - val_loss: 0.3172\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3951 - val_loss: 0.3186\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.3944 - val_loss: 0.3187\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3939 - val_loss: 0.3196\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.3938 - val_loss: 0.3194\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3929 - val_loss: 0.3201\n",
      "Execution time:  148.31544280052185\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2127\n",
      "Root Mean Square Error: 0.6202\n",
      "Mean Square Error: 0.3846\n",
      "\n",
      "Train RMSE: 0.620\n",
      "Train MSE: 0.385\n",
      "Train MAE: 0.213\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_52 (GRU)                 (None, 36, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_220 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_53 (GRU)                 (None, 36, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_221 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_110 (TimeDi (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 10s 30ms/step - loss: 0.6489 - val_loss: 0.8134\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5934 - val_loss: 0.8071\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5865 - val_loss: 0.8057\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5840 - val_loss: 0.8053\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.5829 - val_loss: 0.8051\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5826 - val_loss: 0.8051\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5822 - val_loss: 0.8050\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5819 - val_loss: 0.8050\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5813 - val_loss: 0.8050\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5807 - val_loss: 0.8050\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5805 - val_loss: 0.8050\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.5798 - val_loss: 0.8050\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5795 - val_loss: 0.8050\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5782 - val_loss: 0.8050\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5774 - val_loss: 0.8050\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5777 - val_loss: 0.8050\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5770 - val_loss: 0.8050\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5768 - val_loss: 0.8050\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5764 - val_loss: 0.8050\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5760 - val_loss: 0.8050\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5760 - val_loss: 0.8050\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5753 - val_loss: 0.8050\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5755 - val_loss: 0.8050\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5745 - val_loss: 0.8050\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5745 - val_loss: 0.8049\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5744 - val_loss: 0.8049\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5738 - val_loss: 0.8049\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5729 - val_loss: 0.8049\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5732 - val_loss: 0.8049\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5724 - val_loss: 0.8049\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5718 - val_loss: 0.8049\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5719 - val_loss: 0.8049\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5715 - val_loss: 0.8049\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5714 - val_loss: 0.8049\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5708 - val_loss: 0.8049\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5710 - val_loss: 0.8049\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5707 - val_loss: 0.8049\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5705 - val_loss: 0.8049\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5699 - val_loss: 0.8049\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5693 - val_loss: 0.8049\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5692 - val_loss: 0.8049\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5686 - val_loss: 0.8049\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5693 - val_loss: 0.8049\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5690 - val_loss: 0.8049\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 9s 28ms/step - loss: 0.5694 - val_loss: 0.8049\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5685 - val_loss: 0.8049\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5681 - val_loss: 0.8049\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5670 - val_loss: 0.8049\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5669 - val_loss: 0.8049\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5670 - val_loss: 0.8049\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5660 - val_loss: 0.8049\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5661 - val_loss: 0.8049\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5681 - val_loss: 0.8049\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5669 - val_loss: 0.8049\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5668 - val_loss: 0.8049\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5663 - val_loss: 0.8049\n",
      "Execution time:  498.457377910614\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5423\n",
      "Root Mean Square Error: 0.8088\n",
      "Mean Square Error: 0.6542\n",
      "\n",
      "Train RMSE: 0.809\n",
      "Train MSE: 0.654\n",
      "Train MAE: 0.542\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_54 (GRU)                 (None, 36, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_222 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_55 (GRU)                 (None, 36, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_223 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_111 (TimeDi (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 37ms/step - loss: 0.7060 - val_loss: 0.7398\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.6100 - val_loss: 0.7232\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - ETA: 0s - loss: 0.597 - 3s 30ms/step - loss: 0.5971 - val_loss: 0.7138\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5900 - val_loss: 0.7090\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5857 - val_loss: 0.7072\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5832 - val_loss: 0.7056\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5816 - val_loss: 0.7046\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5805 - val_loss: 0.7042\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5797 - val_loss: 0.7039\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5790 - val_loss: 0.7041\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5784 - val_loss: 0.7044\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5780 - val_loss: 0.7049\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5777 - val_loss: 0.7056\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5774 - val_loss: 0.7061\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5771 - val_loss: 0.7074\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5768 - val_loss: 0.7091\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5764 - val_loss: 0.7115\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5763 - val_loss: 0.7134s - loss: 0.5 - ETA: 0s - los\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5756 - val_loss: 0.7141\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5753 - val_loss: 0.7143\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5748 - val_loss: 0.7132\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5742 - val_loss: 0.7119\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5734 - val_loss: 0.7117\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5732 - val_loss: 0.7112\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5730 - val_loss: 0.7114\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5728 - val_loss: 0.7128\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5727 - val_loss: 0.7118\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5725 - val_loss: 0.7130\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5723 - val_loss: 0.7125\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5722 - val_loss: 0.7131\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5721 - val_loss: 0.7133\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5718 - val_loss: 0.7134\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5720 - val_loss: 0.7131A: 0s -\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5717 - val_loss: 0.7136\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5716 - val_loss: 0.7138\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5714 - val_loss: 0.7130\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5714 - val_loss: 0.7128\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5711 - val_loss: 0.7133\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5713 - val_loss: 0.7132\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5710 - val_loss: 0.7118\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5708 - val_loss: 0.7128\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5706 - val_loss: 0.7114\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5701 - val_loss: 0.7129\n",
      "Execution time:  146.09937977790833\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5375\n",
      "Root Mean Square Error: 0.8134\n",
      "Mean Square Error: 0.6616\n",
      "\n",
      "Train RMSE: 0.813\n",
      "Train MSE: 0.662\n",
      "Train MAE: 0.537\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_56 (GRU)                 (None, 36, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_224 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_57 (GRU)                 (None, 36, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_225 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_112 (TimeDi (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 9s 29ms/step - loss: 0.7025 - val_loss: 0.7929\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.7001 - val_loss: 0.7883\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6976 - val_loss: 0.7835\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6950 - val_loss: 0.7787\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6925 - val_loss: 0.7737\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6899 - val_loss: 0.7685\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6873 - val_loss: 0.7633\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6845 - val_loss: 0.7579\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6818 - val_loss: 0.7524\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6790 - val_loss: 0.7468\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6763 - val_loss: 0.7410\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6733 - val_loss: 0.7352\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6703 - val_loss: 0.7292\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6676 - val_loss: 0.7231\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6646 - val_loss: 0.7169\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6614 - val_loss: 0.7106\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6585 - val_loss: 0.7043\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6552 - val_loss: 0.6978\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6523 - val_loss: 0.6913\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6492 - val_loss: 0.6846\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6459 - val_loss: 0.6778\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6429 - val_loss: 0.6709\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6396 - val_loss: 0.6638\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6362 - val_loss: 0.6566\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6329 - val_loss: 0.6493\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6295 - val_loss: 0.6417\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6257 - val_loss: 0.6340\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6223 - val_loss: 0.6261\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6184 - val_loss: 0.6180\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6148 - val_loss: 0.6098\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6109 - val_loss: 0.6013\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6072 - val_loss: 0.5927\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6034 - val_loss: 0.5839\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5992 - val_loss: 0.5749\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5952 - val_loss: 0.5658\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5909 - val_loss: 0.5565\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5869 - val_loss: 0.5471\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5827 - val_loss: 0.5376\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5786 - val_loss: 0.5281\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5740 - val_loss: 0.5184\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5701 - val_loss: 0.5088\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5658 - val_loss: 0.4991\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5617 - val_loss: 0.4894\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5571 - val_loss: 0.4797\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5536 - val_loss: 0.4702\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5492 - val_loss: 0.4607\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5457 - val_loss: 0.4513\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5414 - val_loss: 0.4420\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5381 - val_loss: 0.4328\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5341 - val_loss: 0.4237\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5304 - val_loss: 0.4147\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5269 - val_loss: 0.4058\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5233 - val_loss: 0.3969\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5198 - val_loss: 0.3882\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5167 - val_loss: 0.3796\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5131 - val_loss: 0.3710\n",
      "Execution time:  497.1685929298401\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4296\n",
      "Root Mean Square Error: 0.7661\n",
      "Mean Square Error: 0.5870\n",
      "\n",
      "Train RMSE: 0.766\n",
      "Train MSE: 0.587\n",
      "Train MAE: 0.430\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_58 (GRU)                 (None, 36, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_226 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_59 (GRU)                 (None, 36, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_227 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_113 (TimeDi (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 38ms/step - loss: 0.7034 - val_loss: 0.6870\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7026 - val_loss: 0.6858\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.7017 - val_loss: 0.6847\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7007 - val_loss: 0.6834\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6998 - val_loss: 0.6822 0s - l\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6988 - val_loss: 0.6809\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6977 - val_loss: 0.6795\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.6968 - val_loss: 0.6782\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6958 - val_loss: 0.6769\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6949 - val_loss: 0.6755\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6938 - val_loss: 0.6741\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6927 - val_loss: 0.6727\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.6917 - val_loss: 0.6713\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6907 - val_loss: 0.6699\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6897 - val_loss: 0.6685\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6887 - val_loss: 0.6671\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6875 - val_loss: 0.6656\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.6865 - val_loss: 0.6642\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6855 - val_loss: 0.6627\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6844 - val_loss: 0.6612\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6833 - val_loss: 0.6597\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6823 - val_loss: 0.6582\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6811 - val_loss: 0.6568\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6801 - val_loss: 0.6552\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6790 - val_loss: 0.6537\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6779 - val_loss: 0.6522\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6768 - val_loss: 0.6507\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6757 - val_loss: 0.6491\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6746 - val_loss: 0.6476\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6735 - val_loss: 0.6461\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6725 - val_loss: 0.6445\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6712 - val_loss: 0.6429\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6702 - val_loss: 0.6414TA: 0s - lo\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6689 - val_loss: 0.6398\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6678 - val_loss: 0.6382\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6667 - val_loss: 0.6366\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.6655 - val_loss: 0.6350\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6644 - val_loss: 0.6334\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6631 - val_loss: 0.6318\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6620 - val_loss: 0.6301\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6607 - val_loss: 0.6285\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.6597 - val_loss: 0.6268\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6584 - val_loss: 0.6252\n",
      "Execution time:  146.48808813095093\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6342\n",
      "Root Mean Square Error: 0.9224\n",
      "Mean Square Error: 0.8508\n",
      "\n",
      "Train RMSE: 0.922\n",
      "Train MSE: 0.851\n",
      "Train MAE: 0.634\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_114\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_60 (GRU)                 (None, 36, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_228 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_61 (GRU)                 (None, 36, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_229 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_114 (TimeDi (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 10s 31ms/step - loss: 0.8960 - val_loss: 1.3030\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8954 - val_loss: 1.3019\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8947 - val_loss: 1.3006\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8941 - val_loss: 1.2993\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8935 - val_loss: 1.2980\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8928 - val_loss: 1.2966\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8921 - val_loss: 1.2952\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8913 - val_loss: 1.2937\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8906 - val_loss: 1.2922\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8898 - val_loss: 1.2906\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.8890 - val_loss: 1.2890\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8882 - val_loss: 1.2874\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8874 - val_loss: 1.2857\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8865 - val_loss: 1.2840\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8856 - val_loss: 1.2822\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8847 - val_loss: 1.2804\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8838 - val_loss: 1.2786\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8829 - val_loss: 1.2767\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8819 - val_loss: 1.2747\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8809 - val_loss: 1.2727\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8799 - val_loss: 1.2706\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8789 - val_loss: 1.2685\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8778 - val_loss: 1.2664\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8767 - val_loss: 1.2641\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8756 - val_loss: 1.2619\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8744 - val_loss: 1.2595\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8732 - val_loss: 1.2571\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8720 - val_loss: 1.2547\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8707 - val_loss: 1.2521\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8694 - val_loss: 1.2495\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8681 - val_loss: 1.2469\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8668 - val_loss: 1.2442\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8654 - val_loss: 1.2414\n",
      "Epoch 34/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8639 - val_loss: 1.2385\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.8625 - val_loss: 1.2355\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8609 - val_loss: 1.2325\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.8594 - val_loss: 1.2294\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8578 - val_loss: 1.2262\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8562 - val_loss: 1.2230\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8545 - val_loss: 1.2196\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8528 - val_loss: 1.2162\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8511 - val_loss: 1.2127\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8493 - val_loss: 1.2091\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8473 - val_loss: 1.2054\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8456 - val_loss: 1.2016\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.8436 - val_loss: 1.1977\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8416 - val_loss: 1.1937\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.8396 - val_loss: 1.1896\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8375 - val_loss: 1.1854\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8353 - val_loss: 1.1812\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8330 - val_loss: 1.1768\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8308 - val_loss: 1.1723\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8286 - val_loss: 1.1678\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8261 - val_loss: 1.1631\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.8236 - val_loss: 1.1584\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.8212 - val_loss: 1.1535\n",
      "Execution time:  493.7741816043854\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8344\n",
      "Root Mean Square Error: 1.0252\n",
      "Mean Square Error: 1.0510\n",
      "\n",
      "Train RMSE: 1.025\n",
      "Train MSE: 1.051\n",
      "Train MAE: 0.834\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_115\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_62 (GRU)                 (None, 36, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_230 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_63 (GRU)                 (None, 36, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_231 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_115 (TimeDi (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 40ms/step - loss: 0.8807 - val_loss: 1.1331\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8806 - val_loss: 1.1328\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8803 - val_loss: 1.1326\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.8802 - val_loss: 1.1323\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8800 - val_loss: 1.1320\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8798 - val_loss: 1.1317\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8796 - val_loss: 1.1314\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8794 - val_loss: 1.1312\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8792 - val_loss: 1.1309\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8790 - val_loss: 1.1305\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8788 - val_loss: 1.1302\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8786 - val_loss: 1.1299\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.8784 - val_loss: 1.1296\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8782 - val_loss: 1.1293\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8780 - val_loss: 1.1290\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8778 - val_loss: 1.1286\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8776 - val_loss: 1.1283\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8774 - val_loss: 1.1280\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8771 - val_loss: 1.1276\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8769 - val_loss: 1.1273\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8767 - val_loss: 1.1270\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8765 - val_loss: 1.1266\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8762 - val_loss: 1.1263\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8760 - val_loss: 1.1259\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8758 - val_loss: 1.1256\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8756 - val_loss: 1.1252\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8753 - val_loss: 1.1248\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8751 - val_loss: 1.1245\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8749 - val_loss: 1.1241\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8747 - val_loss: 1.1237\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8744 - val_loss: 1.1234\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8742 - val_loss: 1.1230\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8740 - val_loss: 1.1226\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8737 - val_loss: 1.1222\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8734 - val_loss: 1.1219\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8732 - val_loss: 1.1215\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8730 - val_loss: 1.1211\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8727 - val_loss: 1.1207\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8724 - val_loss: 1.1203\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8722 - val_loss: 1.1199\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8720 - val_loss: 1.1195\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8717 - val_loss: 1.1191\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.8715 - val_loss: 1.1187\n",
      "Execution time:  148.41799974441528\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9121\n",
      "Root Mean Square Error: 1.0984\n",
      "Mean Square Error: 1.2064\n",
      "\n",
      "Train RMSE: 1.098\n",
      "Train MSE: 1.206\n",
      "Train MAE: 0.912\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_116\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_64 (GRU)                 (None, 36, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_232 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_65 (GRU)                 (None, 36, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_233 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_116 (TimeDi (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 10s 29ms/step - loss: 0.4719 - val_loss: 0.2495\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4316 - val_loss: 0.2344\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4248 - val_loss: 0.2205\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4203 - val_loss: 0.2200\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4176 - val_loss: 0.2201\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4157 - val_loss: 0.2194\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4136 - val_loss: 0.2215\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4123 - val_loss: 0.2215\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4110 - val_loss: 0.2228\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4107 - val_loss: 0.2255\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4104 - val_loss: 0.2267\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4099 - val_loss: 0.2276\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4093 - val_loss: 0.2284\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4091 - val_loss: 0.2276\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4087 - val_loss: 0.2291\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4090 - val_loss: 0.2257\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4082 - val_loss: 0.2269\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4080 - val_loss: 0.2280\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4080 - val_loss: 0.2267\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4077 - val_loss: 0.2257\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4077 - val_loss: 0.2259\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4072 - val_loss: 0.2253\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4075 - val_loss: 0.2253\n",
      "Epoch 24/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4072 - val_loss: 0.2272\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4066 - val_loss: 0.2259\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4068 - val_loss: 0.2247\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4063 - val_loss: 0.2245\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4065 - val_loss: 0.2264\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4062 - val_loss: 0.2257\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4063 - val_loss: 0.2247\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4057 - val_loss: 0.2252\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4057 - val_loss: 0.2250\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4057 - val_loss: 0.2245\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4057 - val_loss: 0.2223\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4054 - val_loss: 0.2232\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4050 - val_loss: 0.2238\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4052 - val_loss: 0.2237\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4050 - val_loss: 0.2230\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4050 - val_loss: 0.2224\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4045 - val_loss: 0.2221\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4044 - val_loss: 0.2223\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4047 - val_loss: 0.2214\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4046 - val_loss: 0.2238\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4045 - val_loss: 0.2212\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4039 - val_loss: 0.2198\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4038 - val_loss: 0.2195\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4040 - val_loss: 0.2206\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4035 - val_loss: 0.2224\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4037 - val_loss: 0.2195\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4033 - val_loss: 0.2195\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 9s 29ms/step - loss: 0.4032 - val_loss: 0.2197\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4031 - val_loss: 0.2207\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4028 - val_loss: 0.2208\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4029 - val_loss: 0.2202\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4029 - val_loss: 0.2192\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.4025 - val_loss: 0.2186\n",
      "Execution time:  499.63963866233826\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2052\n",
      "Root Mean Square Error: 0.6004\n",
      "Mean Square Error: 0.3605\n",
      "\n",
      "Train RMSE: 0.600\n",
      "Train MSE: 0.360\n",
      "Train MAE: 0.205\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_117\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_66 (GRU)                 (None, 36, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_234 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_67 (GRU)                 (None, 36, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_235 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_117 (TimeDi (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 37ms/step - loss: 0.5174 - val_loss: 0.3390\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4334 - val_loss: 0.3323\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4265 - val_loss: 0.3272\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4215 - val_loss: 0.3232\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4176 - val_loss: 0.3204\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4141 - val_loss: 0.3180\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4124 - val_loss: 0.3159\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4106 - val_loss: 0.3146\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4092 - val_loss: 0.3134\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4080 - val_loss: 0.3125\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4066 - val_loss: 0.3117\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4059 - val_loss: 0.3109\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4051 - val_loss: 0.3104\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4041 - val_loss: 0.3098\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4038 - val_loss: 0.3092\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4030 - val_loss: 0.3090\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4022 - val_loss: 0.3087\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4015 - val_loss: 0.3088\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4008 - val_loss: 0.3088\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4007 - val_loss: 0.3087\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4005 - val_loss: 0.3089\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4000 - val_loss: 0.3089\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3997 - val_loss: 0.3089\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3996 - val_loss: 0.3089\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3992 - val_loss: 0.3090\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3990 - val_loss: 0.3089\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3991 - val_loss: 0.3091\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3989 - val_loss: 0.3092\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3988 - val_loss: 0.3091\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3986 - val_loss: 0.3093\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3983 - val_loss: 0.3094\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3982 - val_loss: 0.3094\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3981 - val_loss: 0.3095\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3980 - val_loss: 0.3095\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3980 - val_loss: 0.3095\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3978 - val_loss: 0.3096\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3980 - val_loss: 0.3096\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3979 - val_loss: 0.3096\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3978 - val_loss: 0.3097\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3973 - val_loss: 0.3098\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3975 - val_loss: 0.3097\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.3974 - val_loss: 0.3099\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.3972 - val_loss: 0.3097\n",
      "Execution time:  145.74998307228088\n",
      "GRU:\n",
      "Mean Absolute Error: 0.1904\n",
      "Root Mean Square Error: 0.5828\n",
      "Mean Square Error: 0.3396\n",
      "\n",
      "Train RMSE: 0.583\n",
      "Train MSE: 0.340\n",
      "Train MAE: 0.190\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_118\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_68 (GRU)                 (None, 36, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_236 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_69 (GRU)                 (None, 36, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_237 (Dropout)        (None, 36, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_118 (TimeDi (None, 36, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "324/324 [==============================] - 10s 30ms/step - loss: 0.6796 - val_loss: 0.8282\n",
      "Epoch 2/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6135 - val_loss: 0.8177\n",
      "Epoch 3/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.6035 - val_loss: 0.8129\n",
      "Epoch 4/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5962 - val_loss: 0.8097\n",
      "Epoch 5/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5916 - val_loss: 0.8078\n",
      "Epoch 6/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5886 - val_loss: 0.8067\n",
      "Epoch 7/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5862 - val_loss: 0.8060\n",
      "Epoch 8/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5847 - val_loss: 0.8057\n",
      "Epoch 9/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5837 - val_loss: 0.8054\n",
      "Epoch 10/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5832 - val_loss: 0.8053\n",
      "Epoch 11/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5828 - val_loss: 0.8052\n",
      "Epoch 12/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5822 - val_loss: 0.8052\n",
      "Epoch 13/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5820 - val_loss: 0.8051\n",
      "Epoch 14/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5816 - val_loss: 0.8051\n",
      "Epoch 15/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5815 - val_loss: 0.8051\n",
      "Epoch 16/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5813 - val_loss: 0.8050\n",
      "Epoch 17/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5810 - val_loss: 0.8050\n",
      "Epoch 18/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5811 - val_loss: 0.8050\n",
      "Epoch 19/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5809 - val_loss: 0.8050\n",
      "Epoch 20/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5805 - val_loss: 0.8050\n",
      "Epoch 21/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5807 - val_loss: 0.8050\n",
      "Epoch 22/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5804 - val_loss: 0.8050\n",
      "Epoch 23/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5805 - val_loss: 0.8050\n",
      "Epoch 24/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5802 - val_loss: 0.8050\n",
      "Epoch 25/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5802 - val_loss: 0.8050\n",
      "Epoch 26/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5802 - val_loss: 0.8050\n",
      "Epoch 27/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5798 - val_loss: 0.8050\n",
      "Epoch 28/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5799 - val_loss: 0.8050\n",
      "Epoch 29/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5797 - val_loss: 0.8050\n",
      "Epoch 30/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5796 - val_loss: 0.8050\n",
      "Epoch 31/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5796 - val_loss: 0.8050\n",
      "Epoch 32/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5798 - val_loss: 0.8050\n",
      "Epoch 33/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5796 - val_loss: 0.8050\n",
      "Epoch 34/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5795 - val_loss: 0.8050\n",
      "Epoch 35/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5795 - val_loss: 0.8050\n",
      "Epoch 36/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5794 - val_loss: 0.8050\n",
      "Epoch 37/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5794 - val_loss: 0.8050\n",
      "Epoch 38/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5793 - val_loss: 0.8049\n",
      "Epoch 39/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5792 - val_loss: 0.8049\n",
      "Epoch 40/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5792 - val_loss: 0.8049\n",
      "Epoch 41/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5792 - val_loss: 0.8049\n",
      "Epoch 42/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5792 - val_loss: 0.8049\n",
      "Epoch 43/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5793 - val_loss: 0.8049\n",
      "Epoch 44/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5790 - val_loss: 0.8049\n",
      "Epoch 45/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5790 - val_loss: 0.8049\n",
      "Epoch 46/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5789 - val_loss: 0.8049\n",
      "Epoch 47/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5788 - val_loss: 0.8049\n",
      "Epoch 48/56\n",
      "324/324 [==============================] - 9s 26ms/step - loss: 0.5790 - val_loss: 0.8049\n",
      "Epoch 49/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5787 - val_loss: 0.8049\n",
      "Epoch 50/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5790 - val_loss: 0.8049\n",
      "Epoch 51/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5790 - val_loss: 0.8049\n",
      "Epoch 52/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5787 - val_loss: 0.8049\n",
      "Epoch 53/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5786 - val_loss: 0.8049\n",
      "Epoch 54/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5786 - val_loss: 0.8049\n",
      "Epoch 55/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5788 - val_loss: 0.8049\n",
      "Epoch 56/56\n",
      "324/324 [==============================] - 9s 27ms/step - loss: 0.5785 - val_loss: 0.8049\n",
      "Execution time:  493.329874753952\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5066\n",
      "Root Mean Square Error: 0.7640\n",
      "Mean Square Error: 0.5837\n",
      "\n",
      "Train RMSE: 0.764\n",
      "Train MSE: 0.584\n",
      "Train MAE: 0.507\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  6h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_119\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_70 (GRU)                 (None, 36, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_238 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_71 (GRU)                 (None, 36, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_239 (Dropout)        (None, 36, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_119 (TimeDi (None, 36, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "106/106 [==============================] - 4s 40ms/step - loss: 0.7881 - val_loss: 0.7846\n",
      "Epoch 2/43\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.6244 - val_loss: 0.7420\n",
      "Epoch 3/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6116 - val_loss: 0.7317\n",
      "Epoch 4/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6051 - val_loss: 0.7256\n",
      "Epoch 5/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.6005 - val_loss: 0.7211\n",
      "Epoch 6/43\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.5968 - val_loss: 0.7177\n",
      "Epoch 7/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5938 - val_loss: 0.7149\n",
      "Epoch 8/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5910 - val_loss: 0.7125\n",
      "Epoch 9/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5888 - val_loss: 0.7105\n",
      "Epoch 10/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5869 - val_loss: 0.7091\n",
      "Epoch 11/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5856 - val_loss: 0.7079\n",
      "Epoch 12/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5845 - val_loss: 0.7070\n",
      "Epoch 13/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5836 - val_loss: 0.7063\n",
      "Epoch 14/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5830 - val_loss: 0.7058\n",
      "Epoch 15/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5824 - val_loss: 0.7053\n",
      "Epoch 16/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5820 - val_loss: 0.7049\n",
      "Epoch 17/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5815 - val_loss: 0.7045\n",
      "Epoch 18/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5811 - val_loss: 0.7042\n",
      "Epoch 19/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5807 - val_loss: 0.7040\n",
      "Epoch 20/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5802 - val_loss: 0.7037\n",
      "Epoch 21/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5799 - val_loss: 0.7036\n",
      "Epoch 22/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5796 - val_loss: 0.7033\n",
      "Epoch 23/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5794 - val_loss: 0.7032\n",
      "Epoch 24/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5791 - val_loss: 0.7031\n",
      "Epoch 25/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5789 - val_loss: 0.7030\n",
      "Epoch 26/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5785 - val_loss: 0.7029\n",
      "Epoch 27/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5786 - val_loss: 0.7029\n",
      "Epoch 28/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5783 - val_loss: 0.7028\n",
      "Epoch 29/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5782 - val_loss: 0.7028\n",
      "Epoch 30/43\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5779 - val_loss: 0.7028\n",
      "Epoch 31/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5778 - val_loss: 0.7028\n",
      "Epoch 32/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5776 - val_loss: 0.7027\n",
      "Epoch 33/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5777 - val_loss: 0.7027\n",
      "Epoch 34/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5773 - val_loss: 0.7027\n",
      "Epoch 35/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5772 - val_loss: 0.7027\n",
      "Epoch 36/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5771 - val_loss: 0.7026\n",
      "Epoch 37/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5771 - val_loss: 0.7027\n",
      "Epoch 38/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5771 - val_loss: 0.7027\n",
      "Epoch 39/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5770 - val_loss: 0.7027\n",
      "Epoch 40/43\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5769 - val_loss: 0.7027\n",
      "Epoch 41/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5768 - val_loss: 0.7027\n",
      "Epoch 42/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5767 - val_loss: 0.7027: 0.5\n",
      "Epoch 43/43\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5765 - val_loss: 0.7027\n",
      "Execution time:  150.9851996898651\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5105\n",
      "Root Mean Square Error: 0.7665\n",
      "Mean Square Error: 0.5875\n",
      "\n",
      "Train RMSE: 0.767\n",
      "Train MSE: 0.588\n",
      "Train MAE: 0.511\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_120\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_72 (GRU)                 (None, 72, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_240 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_73 (GRU)                 (None, 72, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_241 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_120 (TimeDi (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 17s 54ms/step - loss: 0.5485 - val_loss: 0.3351\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5166 - val_loss: 0.3176\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5099 - val_loss: 0.3154\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5075 - val_loss: 0.3157\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5060 - val_loss: 0.3169\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5048 - val_loss: 0.3267\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5055 - val_loss: 0.3382\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5056 - val_loss: 0.3538\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5045 - val_loss: 0.3556\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5036 - val_loss: 0.3611\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5028 - val_loss: 0.3596\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5004 - val_loss: 0.3500\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 16s 50ms/step - loss: 0.4990 - val_loss: 0.3612\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4972 - val_loss: 0.3564\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4958 - val_loss: 0.3481\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4919 - val_loss: 0.3495\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4923 - val_loss: 0.3558\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4899 - val_loss: 0.3408\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4883 - val_loss: 0.3393\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4834 - val_loss: 0.3324\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.4835 - val_loss: 0.3308\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4801 - val_loss: 0.3272\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4795 - val_loss: 0.3221\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4787 - val_loss: 0.3201\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4771 - val_loss: 0.3163\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4769 - val_loss: 0.3163\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4750 - val_loss: 0.3197\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4731 - val_loss: 0.3221\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4726 - val_loss: 0.3216\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4697 - val_loss: 0.3220\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4708 - val_loss: 0.3179\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4658 - val_loss: 0.3299\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4638 - val_loss: 0.3179\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4624 - val_loss: 0.3188\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4619 - val_loss: 0.3259\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4610 - val_loss: 0.3231\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4619 - val_loss: 0.3088\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4587 - val_loss: 0.3074\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4577 - val_loss: 0.3125\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4568 - val_loss: 0.3065\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4567 - val_loss: 0.3124\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4570 - val_loss: 0.3194\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4529 - val_loss: 0.3000\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4522 - val_loss: 0.3033\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4524 - val_loss: 0.3069\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4523 - val_loss: 0.2930\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4505 - val_loss: 0.3020\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4495 - val_loss: 0.3082\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4530 - val_loss: 0.3009\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4465 - val_loss: 0.3018\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4451 - val_loss: 0.2974\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4431 - val_loss: 0.3060\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4420 - val_loss: 0.2923\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4419 - val_loss: 0.2991\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4424 - val_loss: 0.2979\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4422 - val_loss: 0.3099\n",
      "Execution time:  924.7728035449982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU:\n",
      "Mean Absolute Error: 0.3708\n",
      "Root Mean Square Error: 0.8239\n",
      "Mean Square Error: 0.6788\n",
      "\n",
      "Train RMSE: 0.824\n",
      "Train MSE: 0.679\n",
      "Train MAE: 0.371\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_74 (GRU)                 (None, 72, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_242 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_75 (GRU)                 (None, 72, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_243 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_121 (TimeDi (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 7s 63ms/step - loss: 0.5734 - val_loss: 0.3899\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5099 - val_loss: 0.3867\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.5026 - val_loss: 0.3848\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 57ms/step - loss: 0.4993 - val_loss: 0.3834\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4973 - val_loss: 0.3822\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4960 - val_loss: 0.3814\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4949 - val_loss: 0.3804\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4941 - val_loss: 0.3797\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4933 - val_loss: 0.3795\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4920 - val_loss: 0.3790\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4912 - val_loss: 0.3788\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4905 - val_loss: 0.3787\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4896 - val_loss: 0.3792\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4887 - val_loss: 0.3799\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4868 - val_loss: 0.3805\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4851 - val_loss: 0.3756\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4816 - val_loss: 0.3683\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4849 - val_loss: 0.3735\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4780 - val_loss: 0.3702\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4755 - val_loss: 0.3706\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4768 - val_loss: 0.3729\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4703 - val_loss: 0.3723\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4754 - val_loss: 0.3741\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4676 - val_loss: 0.3737\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4710 - val_loss: 0.3748\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4676 - val_loss: 0.3753\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4673 - val_loss: 0.3766\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4672 - val_loss: 0.3760\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4644 - val_loss: 0.3783\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4641 - val_loss: 0.3785\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4609 - val_loss: 0.3793\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4613 - val_loss: 0.3799\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4599 - val_loss: 0.3805\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4592 - val_loss: 0.3807\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4574 - val_loss: 0.3808\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4579 - val_loss: 0.3817\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4562 - val_loss: 0.3818\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4564 - val_loss: 0.3825\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4547 - val_loss: 0.3836\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4547 - val_loss: 0.3839\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4540 - val_loss: 0.3846\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4524 - val_loss: 0.3851\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4531 - val_loss: 0.3851\n",
      "Execution time:  272.34867572784424\n",
      "GRU:\n",
      "Mean Absolute Error: 0.3070\n",
      "Root Mean Square Error: 0.7978\n",
      "Mean Square Error: 0.6365\n",
      "\n",
      "Train RMSE: 0.798\n",
      "Train MSE: 0.637\n",
      "Train MAE: 0.307\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_122\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_76 (GRU)                 (None, 72, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_244 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_77 (GRU)                 (None, 72, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_245 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_122 (TimeDi (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 17s 55ms/step - loss: 0.6954 - val_loss: 0.8142\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6455 - val_loss: 0.8072\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6422 - val_loss: 0.8057\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6397 - val_loss: 0.8050\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6366 - val_loss: 0.8047\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6317 - val_loss: 0.8048\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6282 - val_loss: 0.8047\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6250 - val_loss: 0.8040\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6199 - val_loss: 0.8040\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6174 - val_loss: 0.8039\n",
      "Epoch 11/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6681 - val_loss: 0.8039\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6582 - val_loss: 0.8039\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6236 - val_loss: 0.8039\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6162 - val_loss: 0.8040\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6153 - val_loss: 0.8038\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6156 - val_loss: 0.8038\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6136 - val_loss: 0.8038\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6164 - val_loss: 0.8038\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6127 - val_loss: 0.8038\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6111 - val_loss: 0.8038\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6111 - val_loss: 0.8039\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6070 - val_loss: 0.8038\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6079 - val_loss: 0.8038\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6068 - val_loss: 0.8038\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6062 - val_loss: 0.8038\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6057 - val_loss: 0.8038\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6049 - val_loss: 0.8037\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6064 - val_loss: 0.8038\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6040 - val_loss: 0.8037\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6040 - val_loss: 0.8038\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6015 - val_loss: 0.8037\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6048 - val_loss: 0.8038\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6008 - val_loss: 0.8037\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6003 - val_loss: 0.8037\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6002 - val_loss: 0.8038\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5985 - val_loss: 0.8037\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6000 - val_loss: 0.8037\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5981 - val_loss: 0.8037\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6007 - val_loss: 0.8038\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5991 - val_loss: 0.8037\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5958 - val_loss: 0.8037\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5990 - val_loss: 0.8037\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5957 - val_loss: 0.8037\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5972 - val_loss: 0.8037\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5949 - val_loss: 0.8037\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5961 - val_loss: 0.8038\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5939 - val_loss: 0.8037\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5937 - val_loss: 0.8037\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5944 - val_loss: 0.8037\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5950 - val_loss: 0.8037\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5921 - val_loss: 0.8037\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5921 - val_loss: 0.8038\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5902 - val_loss: 0.8037\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5926 - val_loss: 0.8037\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5917 - val_loss: 0.8037\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5960 - val_loss: 0.8038\n",
      "Execution time:  926.9670395851135\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6128\n",
      "Root Mean Square Error: 0.9128\n",
      "Mean Square Error: 0.8333\n",
      "\n",
      "Train RMSE: 0.913\n",
      "Train MSE: 0.833\n",
      "Train MAE: 0.613\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_78 (GRU)                 (None, 72, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_246 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_79 (GRU)                 (None, 72, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_247 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_123 (TimeDi (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 7s 70ms/step - loss: 0.7493 - val_loss: 0.7691\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6569 - val_loss: 0.7544\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6451 - val_loss: 0.7480\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6428 - val_loss: 0.7461\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6395 - val_loss: 0.7436\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6381 - val_loss: 0.7414\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6371 - val_loss: 0.7394\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6369 - val_loss: 0.7391\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6344 - val_loss: 0.7364\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6332 - val_loss: 0.7348\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6323 - val_loss: 0.7334\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6303 - val_loss: 0.7302\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6276 - val_loss: 0.7230\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6250 - val_loss: 0.7210\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6199 - val_loss: 0.7193\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6173 - val_loss: 0.7185\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6148 - val_loss: 0.7174\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6125 - val_loss: 0.7162\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6103 - val_loss: 0.7154\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6102 - val_loss: 0.7145\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6090 - val_loss: 0.7162\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6051 - val_loss: 0.7136\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6054 - val_loss: 0.7145\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6022 - val_loss: 0.7137\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6033 - val_loss: 0.7139\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6019 - val_loss: 0.7136\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6019 - val_loss: 0.7135\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6006 - val_loss: 0.7135\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5994 - val_loss: 0.7133\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6005 - val_loss: 0.7128\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6030 - val_loss: 0.7125\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6002 - val_loss: 0.7123\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5992 - val_loss: 0.7127\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5980 - val_loss: 0.7126\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.5973 - val_loss: 0.7124\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5985 - val_loss: 0.7128\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6031 - val_loss: 0.7119\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6008 - val_loss: 0.7117\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5997 - val_loss: 0.7125\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.5952 - val_loss: 0.7122\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5976 - val_loss: 0.7121\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.5943 - val_loss: 0.7119\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5975 - val_loss: 0.7119\n",
      "Execution time:  276.7366795539856\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5877\n",
      "Root Mean Square Error: 0.8815\n",
      "Mean Square Error: 0.7770\n",
      "\n",
      "Train RMSE: 0.881\n",
      "Train MSE: 0.777\n",
      "Train MAE: 0.588\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_124\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_80 (GRU)                 (None, 72, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_248 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_81 (GRU)                 (None, 72, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_249 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_124 (TimeDi (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 17s 54ms/step - loss: 0.7154 - val_loss: 0.8282\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.7137 - val_loss: 0.8241\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.7119 - val_loss: 0.8198\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.7100 - val_loss: 0.8154\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.7081 - val_loss: 0.8109\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.7063 - val_loss: 0.8063\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.7042 - val_loss: 0.8017\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.7024 - val_loss: 0.7970\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.7004 - val_loss: 0.7922\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6983 - val_loss: 0.7874\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6964 - val_loss: 0.7825\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6944 - val_loss: 0.7776\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6923 - val_loss: 0.7726\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6903 - val_loss: 0.7676\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6881 - val_loss: 0.7625\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6860 - val_loss: 0.7573\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6839 - val_loss: 0.7521\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6818 - val_loss: 0.7468\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6798 - val_loss: 0.7415\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6775 - val_loss: 0.7361\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6755 - val_loss: 0.7308\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6734 - val_loss: 0.7254\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6714 - val_loss: 0.7199\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6693 - val_loss: 0.7145\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6672 - val_loss: 0.7090\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6652 - val_loss: 0.7034\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6631 - val_loss: 0.6978\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6610 - val_loss: 0.6921\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6589 - val_loss: 0.6863\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6568 - val_loss: 0.6804\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6546 - val_loss: 0.6744\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6524 - val_loss: 0.6683\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6501 - val_loss: 0.6621\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6478 - val_loss: 0.6557\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 17s 53ms/step - loss: 0.6455 - val_loss: 0.6492\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6431 - val_loss: 0.6426\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6407 - val_loss: 0.6359\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6383 - val_loss: 0.6290\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6358 - val_loss: 0.6220\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6334 - val_loss: 0.6149\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6308 - val_loss: 0.6077\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6283 - val_loss: 0.6003\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6256 - val_loss: 0.5929\n",
      "Epoch 44/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6229 - val_loss: 0.5853\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6204 - val_loss: 0.5776\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6177 - val_loss: 0.5699\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6153 - val_loss: 0.5621\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6126 - val_loss: 0.5543\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6100 - val_loss: 0.5463\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6071 - val_loss: 0.5383\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6047 - val_loss: 0.5304\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6019 - val_loss: 0.5225\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5996 - val_loss: 0.5147\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5969 - val_loss: 0.5069\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5943 - val_loss: 0.4991\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.5919 - val_loss: 0.4913\n",
      "Execution time:  930.4160780906677\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5229\n",
      "Root Mean Square Error: 0.8483\n",
      "Mean Square Error: 0.7196\n",
      "\n",
      "Train RMSE: 0.848\n",
      "Train MSE: 0.720\n",
      "Train MAE: 0.523\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_82 (GRU)                 (None, 72, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_250 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_83 (GRU)                 (None, 72, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_251 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_125 (TimeDi (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 7s 63ms/step - loss: 0.7264 - val_loss: 0.7220\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7256 - val_loss: 0.7211\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7250 - val_loss: 0.7201\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7242 - val_loss: 0.7191\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7235 - val_loss: 0.7180\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7227 - val_loss: 0.7169\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7218 - val_loss: 0.7158\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7211 - val_loss: 0.7147\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7203 - val_loss: 0.7136\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7194 - val_loss: 0.7124\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7186 - val_loss: 0.7112\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7178 - val_loss: 0.7101\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.7169 - val_loss: 0.7089\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7161 - val_loss: 0.7077\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.7153 - val_loss: 0.7065\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7144 - val_loss: 0.7052\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7134 - val_loss: 0.7040\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7126 - val_loss: 0.7028\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7118 - val_loss: 0.7015\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7109 - val_loss: 0.7003\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7100 - val_loss: 0.6990\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7091 - val_loss: 0.6977\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 61ms/step - loss: 0.7082 - val_loss: 0.6964\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7073 - val_loss: 0.6951\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7064 - val_loss: 0.6938\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.7056 - val_loss: 0.6925\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7046 - val_loss: 0.6912\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.7037 - val_loss: 0.6899\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7028 - val_loss: 0.6885\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7019 - val_loss: 0.6872\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.7010 - val_loss: 0.6858\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.7000 - val_loss: 0.6844\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6991 - val_loss: 0.6831\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6981 - val_loss: 0.6817\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6971 - val_loss: 0.6803\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6961 - val_loss: 0.6788\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6952 - val_loss: 0.6774\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6943 - val_loss: 0.6760\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6933 - val_loss: 0.6745\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6923 - val_loss: 0.6731\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6913 - val_loss: 0.6716\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6904 - val_loss: 0.6702\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6894 - val_loss: 0.6687\n",
      "Execution time:  272.9143626689911\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6772\n",
      "Root Mean Square Error: 0.9682\n",
      "Mean Square Error: 0.9374\n",
      "\n",
      "Train RMSE: 0.968\n",
      "Train MSE: 0.937\n",
      "Train MAE: 0.677\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_126\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_84 (GRU)                 (None, 72, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_252 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_85 (GRU)                 (None, 72, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_253 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_126 (TimeDi (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 17s 54ms/step - loss: 0.8985 - val_loss: 1.3038\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8980 - val_loss: 1.3027\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.8975 - val_loss: 1.3014\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.8969 - val_loss: 1.3001\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.8964 - val_loss: 1.2988\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8958 - val_loss: 1.2974\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8952 - val_loss: 1.2959\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8945 - val_loss: 1.2944\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8938 - val_loss: 1.2929\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8932 - val_loss: 1.2913\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8924 - val_loss: 1.2897\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8917 - val_loss: 1.2880\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8910 - val_loss: 1.2863\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8902 - val_loss: 1.2846\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8894 - val_loss: 1.2828\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8886 - val_loss: 1.2809\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8878 - val_loss: 1.2790\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8869 - val_loss: 1.2771\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8860 - val_loss: 1.2751\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8851 - val_loss: 1.2730\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8842 - val_loss: 1.2709\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8832 - val_loss: 1.2688\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8823 - val_loss: 1.2666\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8813 - val_loss: 1.2643\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8802 - val_loss: 1.2620\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8792 - val_loss: 1.2597\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8781 - val_loss: 1.2572\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8770 - val_loss: 1.2548\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8759 - val_loss: 1.2522\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8747 - val_loss: 1.2496\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8736 - val_loss: 1.2469\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8723 - val_loss: 1.2442\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8711 - val_loss: 1.2414\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8698 - val_loss: 1.2385\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8685 - val_loss: 1.2356\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8671 - val_loss: 1.2326\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8658 - val_loss: 1.2295\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8644 - val_loss: 1.2264\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8630 - val_loss: 1.2232\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8615 - val_loss: 1.2199\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8601 - val_loss: 1.2165\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8585 - val_loss: 1.2130\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8569 - val_loss: 1.2095\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8553 - val_loss: 1.2059\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8536 - val_loss: 1.2021\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8519 - val_loss: 1.1983\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8502 - val_loss: 1.1945\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8484 - val_loss: 1.1905\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8466 - val_loss: 1.1864\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8448 - val_loss: 1.1823\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8428 - val_loss: 1.1780\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8409 - val_loss: 1.1737\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8389 - val_loss: 1.1693\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8369 - val_loss: 1.1647\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8348 - val_loss: 1.1601\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.8327 - val_loss: 1.1554\n",
      "Execution time:  926.9325835704803\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8403\n",
      "Root Mean Square Error: 1.0331\n",
      "Mean Square Error: 1.0673\n",
      "\n",
      "Train RMSE: 1.033\n",
      "Train MSE: 1.067\n",
      "Train MAE: 0.840\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_127\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_86 (GRU)                 (None, 72, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_254 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_87 (GRU)                 (None, 72, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_255 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_127 (TimeDi (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 7s 65ms/step - loss: 0.8876 - val_loss: 1.1407\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 61ms/step - loss: 0.8875 - val_loss: 1.1405\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 6s 61ms/step - loss: 0.8874 - val_loss: 1.1404\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8873 - val_loss: 1.1402\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8872 - val_loss: 1.1400\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8871 - val_loss: 1.1398\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8869 - val_loss: 1.1396\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8868 - val_loss: 1.1394\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8867 - val_loss: 1.1392\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8866 - val_loss: 1.1389\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8864 - val_loss: 1.1387\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8863 - val_loss: 1.1385\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8862 - val_loss: 1.1383\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8860 - val_loss: 1.1380\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8859 - val_loss: 1.1378\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8858 - val_loss: 1.1376\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8856 - val_loss: 1.1373\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8855 - val_loss: 1.1371\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8853 - val_loss: 1.1368\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8852 - val_loss: 1.1366\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8850 - val_loss: 1.1363\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8849 - val_loss: 1.1361\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8848 - val_loss: 1.1358\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8846 - val_loss: 1.1356\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8844 - val_loss: 1.1353\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8843 - val_loss: 1.1350\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8842 - val_loss: 1.1348\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8840 - val_loss: 1.1345\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8838 - val_loss: 1.1342\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8837 - val_loss: 1.1339\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8835 - val_loss: 1.1337\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8834 - val_loss: 1.1334\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8832 - val_loss: 1.1331\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8830 - val_loss: 1.1328\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 61ms/step - loss: 0.8829 - val_loss: 1.1325\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8827 - val_loss: 1.1323\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8825 - val_loss: 1.1320\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8824 - val_loss: 1.1317\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8822 - val_loss: 1.1314\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8820 - val_loss: 1.1311\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.8819 - val_loss: 1.1308\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.8817 - val_loss: 1.1305\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.8815 - val_loss: 1.1302\n",
      "Execution time:  273.94239115715027\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9243\n",
      "Root Mean Square Error: 1.1117\n",
      "Mean Square Error: 1.2359\n",
      "\n",
      "Train RMSE: 1.112\n",
      "Train MSE: 1.236\n",
      "Train MAE: 0.924\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_128\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_88 (GRU)                 (None, 72, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_256 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_89 (GRU)                 (None, 72, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_257 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_128 (TimeDi (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 18s 56ms/step - loss: 0.5399 - val_loss: 0.2881\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5130 - val_loss: 0.2848\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5083 - val_loss: 0.2849\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5057 - val_loss: 0.2860\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5035 - val_loss: 0.2843\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5017 - val_loss: 0.2841\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.5002 - val_loss: 0.2825\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4994 - val_loss: 0.2828\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4988 - val_loss: 0.2827\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4981 - val_loss: 0.2840\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4979 - val_loss: 0.2824\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4972 - val_loss: 0.2813\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4972 - val_loss: 0.2792\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4966 - val_loss: 0.2818\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4966 - val_loss: 0.2805\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4962 - val_loss: 0.2795\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4959 - val_loss: 0.2815\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4957 - val_loss: 0.2816\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4950 - val_loss: 0.2820\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4947 - val_loss: 0.2818\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4943 - val_loss: 0.2820\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4931 - val_loss: 0.2824\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4927 - val_loss: 0.2832\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4912 - val_loss: 0.2812\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4892 - val_loss: 0.2770\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4875 - val_loss: 0.2706\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4842 - val_loss: 0.2647\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4810 - val_loss: 0.2590\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4777 - val_loss: 0.2549\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4748 - val_loss: 0.2511\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4738 - val_loss: 0.2524\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4727 - val_loss: 0.2494\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4712 - val_loss: 0.2498\n",
      "Epoch 34/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4697 - val_loss: 0.2474\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4701 - val_loss: 0.2496\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4696 - val_loss: 0.2490\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4690 - val_loss: 0.2481\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4693 - val_loss: 0.2490\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4682 - val_loss: 0.2478\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4684 - val_loss: 0.2484\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4678 - val_loss: 0.2496\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4667 - val_loss: 0.2482\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4668 - val_loss: 0.2496\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4660 - val_loss: 0.2494\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4659 - val_loss: 0.2496\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.4653 - val_loss: 0.2507\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4654 - val_loss: 0.2488\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4650 - val_loss: 0.2503\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4648 - val_loss: 0.2500\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4646 - val_loss: 0.2498\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4645 - val_loss: 0.2488\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4635 - val_loss: 0.2502\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.4638 - val_loss: 0.2488\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4629 - val_loss: 0.2501\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.4627 - val_loss: 0.2495\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.4628 - val_loss: 0.2493\n",
      "Execution time:  941.049516916275\n",
      "GRU:\n",
      "Mean Absolute Error: 0.3091\n",
      "Root Mean Square Error: 0.7976\n",
      "Mean Square Error: 0.6361\n",
      "\n",
      "Train RMSE: 0.798\n",
      "Train MSE: 0.636\n",
      "Train MAE: 0.309\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_129\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_90 (GRU)                 (None, 72, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_258 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_91 (GRU)                 (None, 72, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_259 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_129 (TimeDi (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "105/105 [==============================] - 7s 67ms/step - loss: 0.5701 - val_loss: 0.3882\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5062 - val_loss: 0.3880\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.5031 - val_loss: 0.3868\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.5005 - val_loss: 0.3856\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4985 - val_loss: 0.3845\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4967 - val_loss: 0.3837\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4953 - val_loss: 0.3829\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4943 - val_loss: 0.3823\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4932 - val_loss: 0.3817\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4921 - val_loss: 0.3813\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4913 - val_loss: 0.3808\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4907 - val_loss: 0.3805\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4902 - val_loss: 0.3803\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4897 - val_loss: 0.3800\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4890 - val_loss: 0.3798\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4885 - val_loss: 0.3796\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4880 - val_loss: 0.3794\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4875 - val_loss: 0.3793\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4869 - val_loss: 0.3791\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4867 - val_loss: 0.3791\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4861 - val_loss: 0.3791\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4858 - val_loss: 0.3790\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4857 - val_loss: 0.3790\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4851 - val_loss: 0.3790\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4850 - val_loss: 0.3792\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4846 - val_loss: 0.3792\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4846 - val_loss: 0.3792\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4843 - val_loss: 0.3793\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4838 - val_loss: 0.3794\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4839 - val_loss: 0.3795\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4838 - val_loss: 0.3796\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 61ms/step - loss: 0.4835 - val_loss: 0.3797\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4830 - val_loss: 0.3798\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4830 - val_loss: 0.3800\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4824 - val_loss: 0.3802\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4824 - val_loss: 0.3804\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4822 - val_loss: 0.3805\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4819 - val_loss: 0.3807\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4816 - val_loss: 0.3809\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.4813 - val_loss: 0.3812\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4811 - val_loss: 0.3815\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.4809 - val_loss: 0.3817\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.4805 - val_loss: 0.3819\n",
      "Execution time:  274.6278533935547\n",
      "GRU:\n",
      "Mean Absolute Error: 0.2153\n",
      "Root Mean Square Error: 0.6089\n",
      "Mean Square Error: 0.3708\n",
      "\n",
      "Train RMSE: 0.609\n",
      "Train MSE: 0.371\n",
      "Train MAE: 0.215\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_130\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_92 (GRU)                 (None, 72, 43)            5934      \n",
      "_________________________________________________________________\n",
      "dropout_260 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "gru_93 (GRU)                 (None, 72, 43)            11352     \n",
      "_________________________________________________________________\n",
      "dropout_261 (Dropout)        (None, 72, 43)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_130 (TimeDi (None, 72, 1)             44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "321/321 [==============================] - 17s 54ms/step - loss: 0.7176 - val_loss: 0.8230\n",
      "Epoch 2/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6597 - val_loss: 0.8147\n",
      "Epoch 3/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6498 - val_loss: 0.8104\n",
      "Epoch 4/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6461 - val_loss: 0.8081\n",
      "Epoch 5/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6439 - val_loss: 0.8067\n",
      "Epoch 6/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6421 - val_loss: 0.8059\n",
      "Epoch 7/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6405 - val_loss: 0.8053\n",
      "Epoch 8/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6392 - val_loss: 0.8049\n",
      "Epoch 9/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6384 - val_loss: 0.8047\n",
      "Epoch 10/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6376 - val_loss: 0.8045\n",
      "Epoch 11/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6371 - val_loss: 0.8044\n",
      "Epoch 12/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6368 - val_loss: 0.8043\n",
      "Epoch 13/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6365 - val_loss: 0.8042\n",
      "Epoch 14/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6361 - val_loss: 0.8041\n",
      "Epoch 15/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6358 - val_loss: 0.8041\n",
      "Epoch 16/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6355 - val_loss: 0.8041\n",
      "Epoch 17/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6352 - val_loss: 0.8041\n",
      "Epoch 18/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6351 - val_loss: 0.8040\n",
      "Epoch 19/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6346 - val_loss: 0.8040\n",
      "Epoch 20/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6345 - val_loss: 0.8040\n",
      "Epoch 21/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6342 - val_loss: 0.8040\n",
      "Epoch 22/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6336 - val_loss: 0.8040\n",
      "Epoch 23/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6335 - val_loss: 0.8040\n",
      "Epoch 24/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6332 - val_loss: 0.8040\n",
      "Epoch 25/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6326 - val_loss: 0.8040\n",
      "Epoch 26/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6321 - val_loss: 0.8040\n",
      "Epoch 27/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6316 - val_loss: 0.8040\n",
      "Epoch 28/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6312 - val_loss: 0.8040\n",
      "Epoch 29/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6304 - val_loss: 0.8040\n",
      "Epoch 30/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6300 - val_loss: 0.8040\n",
      "Epoch 31/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6290 - val_loss: 0.8040\n",
      "Epoch 32/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6280 - val_loss: 0.8040\n",
      "Epoch 33/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6271 - val_loss: 0.8040\n",
      "Epoch 34/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6260 - val_loss: 0.8040\n",
      "Epoch 35/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6256 - val_loss: 0.8040\n",
      "Epoch 36/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6238 - val_loss: 0.8040\n",
      "Epoch 37/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6227 - val_loss: 0.8040\n",
      "Epoch 38/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6214 - val_loss: 0.8040\n",
      "Epoch 39/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6202 - val_loss: 0.8040\n",
      "Epoch 40/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6191 - val_loss: 0.8040\n",
      "Epoch 41/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6179 - val_loss: 0.8040\n",
      "Epoch 42/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6167 - val_loss: 0.8040\n",
      "Epoch 43/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6156 - val_loss: 0.8040\n",
      "Epoch 44/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6143 - val_loss: 0.8040\n",
      "Epoch 45/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6134 - val_loss: 0.8040\n",
      "Epoch 46/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6132 - val_loss: 0.8040\n",
      "Epoch 47/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6120 - val_loss: 0.8040\n",
      "Epoch 48/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6112 - val_loss: 0.8040\n",
      "Epoch 49/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6109 - val_loss: 0.8040\n",
      "Epoch 50/56\n",
      "321/321 [==============================] - 17s 52ms/step - loss: 0.6100 - val_loss: 0.8040\n",
      "Epoch 51/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6099 - val_loss: 0.8039\n",
      "Epoch 52/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6092 - val_loss: 0.8039\n",
      "Epoch 53/56\n",
      "321/321 [==============================] - 17s 51ms/step - loss: 0.6096 - val_loss: 0.8039\n",
      "Epoch 54/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6085 - val_loss: 0.8039\n",
      "Epoch 55/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6085 - val_loss: 0.8039\n",
      "Epoch 56/56\n",
      "321/321 [==============================] - 16s 51ms/step - loss: 0.6079 - val_loss: 0.8039\n",
      "Execution time:  931.0133748054504\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5757\n",
      "Root Mean Square Error: 0.8592\n",
      "Mean Square Error: 0.7382\n",
      "\n",
      "Train RMSE: 0.859\n",
      "Train MSE: 0.738\n",
      "Train MAE: 0.576\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  12h\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_131\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_94 (GRU)                 (None, 72, 45)            6480      \n",
      "_________________________________________________________________\n",
      "dropout_262 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "gru_95 (GRU)                 (None, 72, 45)            12420     \n",
      "_________________________________________________________________\n",
      "dropout_263 (Dropout)        (None, 72, 45)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_131 (TimeDi (None, 72, 1)             46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 7s 64ms/step - loss: 0.7794 - val_loss: 0.7982\n",
      "Epoch 2/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6608 - val_loss: 0.7652\n",
      "Epoch 3/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6521 - val_loss: 0.7572\n",
      "Epoch 4/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6480 - val_loss: 0.7527\n",
      "Epoch 5/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6451 - val_loss: 0.7497\n",
      "Epoch 6/43\n",
      "105/105 [==============================] - 6s 57ms/step - loss: 0.6430 - val_loss: 0.7474\n",
      "Epoch 7/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6412 - val_loss: 0.7456\n",
      "Epoch 8/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6397 - val_loss: 0.7441\n",
      "Epoch 9/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6385 - val_loss: 0.7428\n",
      "Epoch 10/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6374 - val_loss: 0.7416\n",
      "Epoch 11/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6364 - val_loss: 0.7404\n",
      "Epoch 12/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6354 - val_loss: 0.7393\n",
      "Epoch 13/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6346 - val_loss: 0.7383\n",
      "Epoch 14/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6339 - val_loss: 0.7374\n",
      "Epoch 15/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6334 - val_loss: 0.7369\n",
      "Epoch 16/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6328 - val_loss: 0.7365\n",
      "Epoch 17/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6324 - val_loss: 0.7363\n",
      "Epoch 18/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6320 - val_loss: 0.7361\n",
      "Epoch 19/43\n",
      "105/105 [==============================] - 6s 57ms/step - loss: 0.6317 - val_loss: 0.7359\n",
      "Epoch 20/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6313 - val_loss: 0.7358\n",
      "Epoch 21/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6310 - val_loss: 0.7357\n",
      "Epoch 22/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6308 - val_loss: 0.7355\n",
      "Epoch 23/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6306 - val_loss: 0.7355\n",
      "Epoch 24/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6304 - val_loss: 0.7353\n",
      "Epoch 25/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6302 - val_loss: 0.7350\n",
      "Epoch 26/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6300 - val_loss: 0.7349\n",
      "Epoch 27/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6297 - val_loss: 0.7348\n",
      "Epoch 28/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6297 - val_loss: 0.7345\n",
      "Epoch 29/43\n",
      "105/105 [==============================] - 6s 61ms/step - loss: 0.6295 - val_loss: 0.7345\n",
      "Epoch 30/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6295 - val_loss: 0.7345\n",
      "Epoch 31/43\n",
      "105/105 [==============================] - 7s 63ms/step - loss: 0.6293 - val_loss: 0.7342\n",
      "Epoch 32/43\n",
      "105/105 [==============================] - 6s 61ms/step - loss: 0.6291 - val_loss: 0.7341\n",
      "Epoch 33/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6290 - val_loss: 0.7341\n",
      "Epoch 34/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6287 - val_loss: 0.7339\n",
      "Epoch 35/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6288 - val_loss: 0.7339\n",
      "Epoch 36/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6286 - val_loss: 0.7337\n",
      "Epoch 37/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6284 - val_loss: 0.7333\n",
      "Epoch 38/43\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 0.6282 - val_loss: 0.7332\n",
      "Epoch 39/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6280 - val_loss: 0.7329\n",
      "Epoch 40/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6279 - val_loss: 0.7328\n",
      "Epoch 41/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6278 - val_loss: 0.7327\n",
      "Epoch 42/43\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 0.6277 - val_loss: 0.7325\n",
      "Epoch 43/43\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 0.6274 - val_loss: 0.7322\n",
      "Execution time:  273.1925628185272\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5320\n",
      "Root Mean Square Error: 0.7760\n",
      "Mean Square Error: 0.6022\n",
      "\n",
      "Train RMSE: 0.776\n",
      "Train MSE: 0.602\n",
      "Train MAE: 0.532\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_132\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_96 (GRU)                 (None, 144, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_264 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_97 (GRU)                 (None, 144, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_265 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_132 (TimeDi (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 35s 112ms/step - loss: 0.6089 - val_loss: 0.4306\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.5925 - val_loss: 0.4095\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5860 - val_loss: 0.4006\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5800 - val_loss: 0.3974\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5764 - val_loss: 0.3943\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5724 - val_loss: 0.3889\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5685 - val_loss: 0.3852\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5661 - val_loss: 0.3826\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5634 - val_loss: 0.3848\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5617 - val_loss: 0.3929\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5617 - val_loss: 0.3922\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5578 - val_loss: 0.4046\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5578 - val_loss: 0.3925\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5551 - val_loss: 0.3938\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5533 - val_loss: 0.3918\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5511 - val_loss: 0.3993\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5481 - val_loss: 0.4051\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5475 - val_loss: 0.3765\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5431 - val_loss: 0.4109\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5424 - val_loss: 0.3877\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5405 - val_loss: 0.4057\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5378 - val_loss: 0.3890\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5383 - val_loss: 0.3778\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5349 - val_loss: 0.3899\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5348 - val_loss: 0.3720\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5331 - val_loss: 0.3569\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5292 - val_loss: 0.3624\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5273 - val_loss: 0.3854\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5268 - val_loss: 0.3651\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5240 - val_loss: 0.3532\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5231 - val_loss: 0.3503\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5176 - val_loss: 0.3331\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5151 - val_loss: 0.3328\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5108 - val_loss: 0.3336\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5098 - val_loss: 0.3434\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5098 - val_loss: 0.3625\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5186 - val_loss: 0.3036\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5110 - val_loss: 0.3059\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5095 - val_loss: 0.3213\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5090 - val_loss: 0.3035\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5042 - val_loss: 0.3065\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5015 - val_loss: 0.3058\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5043 - val_loss: 0.3225\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5042 - val_loss: 0.3002\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5009 - val_loss: 0.3142\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.4974 - val_loss: 0.3080\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5160 - val_loss: 0.3187\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5071 - val_loss: 0.3134\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.4954 - val_loss: 0.3055\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5051 - val_loss: 0.3433\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5053 - val_loss: 0.3252\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5004 - val_loss: 0.3176\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.4935 - val_loss: 0.3090\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.4896 - val_loss: 0.3189\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.4959 - val_loss: 0.2940\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.4941 - val_loss: 0.3099\n",
      "Execution time:  1750.1785168647766\n",
      "GRU:\n",
      "Mean Absolute Error: 0.4351\n",
      "Root Mean Square Error: 0.8935\n",
      "Mean Square Error: 0.7984\n",
      "\n",
      "Train RMSE: 0.894\n",
      "Train MSE: 0.798\n",
      "Train MAE: 0.435\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_133\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_98 (GRU)                 (None, 144, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_266 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_99 (GRU)                 (None, 144, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_267 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_133 (TimeDi (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.6252 - val_loss: 0.4117\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6039 - val_loss: 0.4122\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5943 - val_loss: 0.4127\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5904 - val_loss: 0.4122\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5866 - val_loss: 0.4134\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5833 - val_loss: 0.4146\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5800 - val_loss: 0.4163\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5768 - val_loss: 0.4178\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5740 - val_loss: 0.4188\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5714 - val_loss: 0.4189\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5687 - val_loss: 0.4177\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5659 - val_loss: 0.4165\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5640 - val_loss: 0.4157\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5615 - val_loss: 0.4140\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5601 - val_loss: 0.4120\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5575 - val_loss: 0.4062\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5552 - val_loss: 0.4016\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5531 - val_loss: 0.4037\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5503 - val_loss: 0.4063\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5486 - val_loss: 0.4112\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5449 - val_loss: 0.4132\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5424 - val_loss: 0.4124\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5376 - val_loss: 0.4135\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5337 - val_loss: 0.4130\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5306 - val_loss: 0.4138\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5284 - val_loss: 0.4152\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5269 - val_loss: 0.4151\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5274 - val_loss: 0.4161\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5261 - val_loss: 0.4199\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5258 - val_loss: 0.4224\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5353 - val_loss: 0.4231\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5239 - val_loss: 0.4238\n",
      "Epoch 33/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5202 - val_loss: 0.4248\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5321 - val_loss: 0.4212\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5250 - val_loss: 0.4231\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5222 - val_loss: 0.4244\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5315 - val_loss: 0.4176\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5215 - val_loss: 0.4260\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5153 - val_loss: 0.4251\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5152 - val_loss: 0.4242\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5152 - val_loss: 0.4247\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5136 - val_loss: 0.4260\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5118 - val_loss: 0.4256\n",
      "Execution time:  498.31243920326233\n",
      "GRU:\n",
      "Mean Absolute Error: 0.3766\n",
      "Root Mean Square Error: 0.8788\n",
      "Mean Square Error: 0.7723\n",
      "\n",
      "Train RMSE: 0.879\n",
      "Train MSE: 0.772\n",
      "Train MAE: 0.377\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_100 (GRU)                (None, 144, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_268 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_101 (GRU)                (None, 144, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_269 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_134 (TimeDi (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 32s 101ms/step - loss: 0.7277 - val_loss: 0.8089\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6985 - val_loss: 0.8060\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6916 - val_loss: 0.8047\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6868 - val_loss: 0.8036\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6911 - val_loss: 0.8027\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6960 - val_loss: 0.8027\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6955 - val_loss: 0.8024\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6920 - val_loss: 0.8021\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6895 - val_loss: 0.8019\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6889 - val_loss: 0.8019\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6902 - val_loss: 0.8018\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6931 - val_loss: 0.8017\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7024 - val_loss: 0.8017\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7048 - val_loss: 0.8016\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7021 - val_loss: 0.8016\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7015 - val_loss: 0.8015\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7012 - val_loss: 0.8015\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7010 - val_loss: 0.8015\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7009 - val_loss: 0.8014\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7008 - val_loss: 0.8014\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7007 - val_loss: 0.8014\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7006 - val_loss: 0.8014\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7006 - val_loss: 0.8014\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7006 - val_loss: 0.8013\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7005 - val_loss: 0.8013\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7005 - val_loss: 0.8013\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7005 - val_loss: 0.8013\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7005 - val_loss: 0.8013\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7005 - val_loss: 0.8013\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7005 - val_loss: 0.8013\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7005 - val_loss: 0.8013\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.7004 - val_loss: 0.8013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.7004 - val_loss: 0.8013\n",
      "Execution time:  1743.939314365387\n",
      "GRU:\n",
      "Mean Absolute Error: 0.7139\n",
      "Root Mean Square Error: 1.0056\n",
      "Mean Square Error: 1.0112\n",
      "\n",
      "Train RMSE: 1.006\n",
      "Train MSE: 1.011\n",
      "Train MAE: 0.714\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_102 (GRU)                (None, 144, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_270 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_103 (GRU)                (None, 144, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_271 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_135 (TimeDi (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 12s 115ms/step - loss: 0.7964 - val_loss: 0.7865\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 0.7046 - val_loss: 0.7655\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.6988 - val_loss: 0.7603\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6936 - val_loss: 0.7566\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6859 - val_loss: 0.7433\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6842 - val_loss: 0.7386\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.6789 - val_loss: 0.7329\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6776 - val_loss: 0.7307\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6752 - val_loss: 0.7284\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.6736 - val_loss: 0.7271\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6706 - val_loss: 0.7242\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6728 - val_loss: 0.7255\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6676 - val_loss: 0.7231\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6649 - val_loss: 0.7211\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6649 - val_loss: 0.7191\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6638 - val_loss: 0.7180\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6629 - val_loss: 0.7170\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6594 - val_loss: 0.7166\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6544 - val_loss: 0.7158\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6546 - val_loss: 0.7156\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6491 - val_loss: 0.7149\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6491 - val_loss: 0.7150\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6483 - val_loss: 0.7147\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6454 - val_loss: 0.7142\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6421 - val_loss: 0.7139\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6391 - val_loss: 0.7136\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6385 - val_loss: 0.7144\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6407 - val_loss: 0.7137\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6373 - val_loss: 0.7136\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6389 - val_loss: 0.7134\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6347 - val_loss: 0.7134\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.6343 - val_loss: 0.7134\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6345 - val_loss: 0.7132\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6331 - val_loss: 0.7132\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6332 - val_loss: 0.7131\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6331 - val_loss: 0.7130\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6324 - val_loss: 0.7131\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6299 - val_loss: 0.7129\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6298 - val_loss: 0.7129\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6295 - val_loss: 0.7128\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6370 - val_loss: 0.7134\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.6575 - val_loss: 0.7134\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6381 - val_loss: 0.7128\n",
      "Execution time:  496.2300238609314\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6380\n",
      "Root Mean Square Error: 0.9338\n",
      "Mean Square Error: 0.8719\n",
      "\n",
      "Train RMSE: 0.934\n",
      "Train MSE: 0.872\n",
      "Train MAE: 0.638\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_104 (GRU)                (None, 144, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_272 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_105 (GRU)                (None, 144, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_273 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_136 (TimeDi (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 32s 102ms/step - loss: 0.6994 - val_loss: 0.7963\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6980 - val_loss: 0.7920\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6965 - val_loss: 0.7874\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 32s 100ms/step - loss: 0.6949 - val_loss: 0.7825\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6931 - val_loss: 0.7775\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6914 - val_loss: 0.7723\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6896 - val_loss: 0.7670\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6879 - val_loss: 0.7616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6860 - val_loss: 0.7561\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6842 - val_loss: 0.7505\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6824 - val_loss: 0.7449\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6805 - val_loss: 0.7393\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6788 - val_loss: 0.7336\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6768 - val_loss: 0.7279\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6751 - val_loss: 0.7222\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6733 - val_loss: 0.7165\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6715 - val_loss: 0.7108\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6698 - val_loss: 0.7050\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6680 - val_loss: 0.6993\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6663 - val_loss: 0.6935\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6647 - val_loss: 0.6877\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6630 - val_loss: 0.6818\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6612 - val_loss: 0.6758\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6594 - val_loss: 0.6698\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6578 - val_loss: 0.6638\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6560 - val_loss: 0.6577\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6543 - val_loss: 0.6515\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6524 - val_loss: 0.6452\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6506 - val_loss: 0.6389\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6489 - val_loss: 0.6325\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6471 - val_loss: 0.6260\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6453 - val_loss: 0.6195\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6435 - val_loss: 0.6130\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6417 - val_loss: 0.6064\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6398 - val_loss: 0.5998\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6380 - val_loss: 0.5931\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6362 - val_loss: 0.5864\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6346 - val_loss: 0.5797\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6327 - val_loss: 0.5730\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6310 - val_loss: 0.5663\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6293 - val_loss: 0.5596\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6276 - val_loss: 0.5530\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6259 - val_loss: 0.5463\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6243 - val_loss: 0.5397\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6227 - val_loss: 0.5331\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6209 - val_loss: 0.5266\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6195 - val_loss: 0.5202\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6178 - val_loss: 0.5138\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6165 - val_loss: 0.5076\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6151 - val_loss: 0.5014\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6137 - val_loss: 0.4953\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6124 - val_loss: 0.4892\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6111 - val_loss: 0.4833\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6099 - val_loss: 0.4775\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6086 - val_loss: 0.4718\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6073 - val_loss: 0.4661\n",
      "Execution time:  1747.803641319275\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5050\n",
      "Root Mean Square Error: 0.8408\n",
      "Mean Square Error: 0.7069\n",
      "\n",
      "Train RMSE: 0.841\n",
      "Train MSE: 0.707\n",
      "Train MAE: 0.505\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_137\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_106 (GRU)                (None, 144, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_274 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_107 (GRU)                (None, 144, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_275 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_137 (TimeDi (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 12s 117ms/step - loss: 0.7119 - val_loss: 0.7130\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.7116 - val_loss: 0.7123\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7111 - val_loss: 0.7115\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 12s 116ms/step - loss: 0.7107 - val_loss: 0.7107\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.7102 - val_loss: 0.7099\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.7097 - val_loss: 0.7090\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.7092 - val_loss: 0.7081\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.7087 - val_loss: 0.7072\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.7082 - val_loss: 0.7063\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7076 - val_loss: 0.7054\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7071 - val_loss: 0.7044\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.7066 - val_loss: 0.7035\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.7060 - val_loss: 0.7025\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7054 - val_loss: 0.7015\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.7049 - val_loss: 0.7005\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.7043 - val_loss: 0.6995\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.7037 - val_loss: 0.6985\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.7032 - val_loss: 0.6974\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7026 - val_loss: 0.6964\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7020 - val_loss: 0.6953\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.7014 - val_loss: 0.6943\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7008 - val_loss: 0.6932\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7003 - val_loss: 0.6921\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 12s 114ms/step - loss: 0.6996 - val_loss: 0.6910\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6991 - val_loss: 0.6899\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6984 - val_loss: 0.6888\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6978 - val_loss: 0.6877\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6972 - val_loss: 0.6866\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6966 - val_loss: 0.6854\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6960 - val_loss: 0.6843\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6954 - val_loss: 0.6831\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6948 - val_loss: 0.6820\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6941 - val_loss: 0.6808\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6934 - val_loss: 0.6796\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6928 - val_loss: 0.6784\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6922 - val_loss: 0.6772\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6916 - val_loss: 0.6761\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6909 - val_loss: 0.6749\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6903 - val_loss: 0.6737\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6897 - val_loss: 0.6725\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6890 - val_loss: 0.6713\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6884 - val_loss: 0.6701\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6878 - val_loss: 0.6689\n",
      "Execution time:  506.42418336868286\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6691\n",
      "Root Mean Square Error: 0.9652\n",
      "Mean Square Error: 0.9316\n",
      "\n",
      "Train RMSE: 0.965\n",
      "Train MSE: 0.932\n",
      "Train MAE: 0.669\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_138\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_108 (GRU)                (None, 144, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_276 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_109 (GRU)                (None, 144, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_277 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_138 (TimeDi (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 32s 102ms/step - loss: 0.9072 - val_loss: 1.2928\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9069 - val_loss: 1.2920\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9066 - val_loss: 1.2912\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.9062 - val_loss: 1.2902\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9058 - val_loss: 1.2893\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9054 - val_loss: 1.2883\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9050 - val_loss: 1.2872\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.9045 - val_loss: 1.2861\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9040 - val_loss: 1.2849\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9035 - val_loss: 1.2837\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.9030 - val_loss: 1.2825\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9024 - val_loss: 1.2812\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9019 - val_loss: 1.2799\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9013 - val_loss: 1.2785\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9007 - val_loss: 1.2771\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.9001 - val_loss: 1.2757\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8994 - val_loss: 1.2742\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8988 - val_loss: 1.2727\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8981 - val_loss: 1.2711\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8974 - val_loss: 1.2695\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8968 - val_loss: 1.2679\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8960 - val_loss: 1.2662\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8953 - val_loss: 1.2645\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8945 - val_loss: 1.2627\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8937 - val_loss: 1.2608\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8929 - val_loss: 1.2590\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8920 - val_loss: 1.2571\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8912 - val_loss: 1.2551\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8903 - val_loss: 1.2531\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8894 - val_loss: 1.2510\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8885 - val_loss: 1.2489\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8876 - val_loss: 1.2467\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8866 - val_loss: 1.2445\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8856 - val_loss: 1.2422\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.8846 - val_loss: 1.2399\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8835 - val_loss: 1.2375\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8825 - val_loss: 1.2350\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8814 - val_loss: 1.2325\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.8803 - val_loss: 1.2299\n",
      "Epoch 40/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8792 - val_loss: 1.2273\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8780 - val_loss: 1.2246\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8768 - val_loss: 1.2219\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8756 - val_loss: 1.2190\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8744 - val_loss: 1.2162\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8731 - val_loss: 1.2132\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8718 - val_loss: 1.2102\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8704 - val_loss: 1.2071\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8691 - val_loss: 1.2039\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8677 - val_loss: 1.2007\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8663 - val_loss: 1.1974\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8648 - val_loss: 1.1940\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8633 - val_loss: 1.1905\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8618 - val_loss: 1.1870\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.8602 - val_loss: 1.1833\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8586 - val_loss: 1.1796\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.8569 - val_loss: 1.1759\n",
      "Execution time:  1754.0065641403198\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8566\n",
      "Root Mean Square Error: 1.0501\n",
      "Mean Square Error: 1.1026\n",
      "\n",
      "Train RMSE: 1.050\n",
      "Train MSE: 1.103\n",
      "Train MAE: 0.857\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_139\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_110 (GRU)                (None, 144, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_278 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_111 (GRU)                (None, 144, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_279 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_139 (TimeDi (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 12s 114ms/step - loss: 0.9010 - val_loss: 1.1446\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.9009 - val_loss: 1.1444\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.9008 - val_loss: 1.1442\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.9007 - val_loss: 1.1440\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.9006 - val_loss: 1.1438\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.9005 - val_loss: 1.1436\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.9004 - val_loss: 1.1433\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.9002 - val_loss: 1.1431\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.9001 - val_loss: 1.1429\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.9000 - val_loss: 1.1426\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8998 - val_loss: 1.1424\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.8997 - val_loss: 1.1421\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8996 - val_loss: 1.1419\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8994 - val_loss: 1.1416\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.8993 - val_loss: 1.1414\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8991 - val_loss: 1.1411\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8990 - val_loss: 1.1408\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8988 - val_loss: 1.1406\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.8987 - val_loss: 1.1403\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8985 - val_loss: 1.1400\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8984 - val_loss: 1.1397\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8982 - val_loss: 1.1395\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8981 - val_loss: 1.1392\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8979 - val_loss: 1.1389\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.8978 - val_loss: 1.1386\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8976 - val_loss: 1.1383\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8975 - val_loss: 1.1380\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8973 - val_loss: 1.1378\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8971 - val_loss: 1.1375\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8970 - val_loss: 1.1372\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8968 - val_loss: 1.1369\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.8967 - val_loss: 1.1366\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8965 - val_loss: 1.1363\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8963 - val_loss: 1.1360\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8962 - val_loss: 1.1357\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.8960 - val_loss: 1.1354\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8958 - val_loss: 1.1351\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8956 - val_loss: 1.1347\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8955 - val_loss: 1.1344\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.8953 - val_loss: 1.1341\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8951 - val_loss: 1.1338\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.8950 - val_loss: 1.1335\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.8948 - val_loss: 1.1332\n",
      "Execution time:  499.97774839401245\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9267\n",
      "Root Mean Square Error: 1.1157\n",
      "Mean Square Error: 1.2448\n",
      "\n",
      "Train RMSE: 1.116\n",
      "Train MSE: 1.245\n",
      "Train MAE: 0.927\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_140\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_112 (GRU)                (None, 144, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_280 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_113 (GRU)                (None, 144, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_281 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_140 (TimeDi (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 32s 101ms/step - loss: 0.6122 - val_loss: 0.4021\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5917 - val_loss: 0.3972\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5881 - val_loss: 0.3877\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5835 - val_loss: 0.3713\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5796 - val_loss: 0.3496\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5765 - val_loss: 0.3388\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5743 - val_loss: 0.3318\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5726 - val_loss: 0.3255\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5709 - val_loss: 0.3200\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5700 - val_loss: 0.3162\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5682 - val_loss: 0.3143\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5661 - val_loss: 0.3088\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5644 - val_loss: 0.3092\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5621 - val_loss: 0.3024\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5596 - val_loss: 0.2972\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5571 - val_loss: 0.2922\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5539 - val_loss: 0.2895\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5521 - val_loss: 0.2865\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5484 - val_loss: 0.2857\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5465 - val_loss: 0.2821\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5433 - val_loss: 0.2784\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 33s 105ms/step - loss: 0.5408 - val_loss: 0.2762\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 34s 107ms/step - loss: 0.5393 - val_loss: 0.2773\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 34s 107ms/step - loss: 0.5379 - val_loss: 0.2781\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5371 - val_loss: 0.2787\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5361 - val_loss: 0.2823\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5354 - val_loss: 0.2784\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5354 - val_loss: 0.2809\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5331 - val_loss: 0.2815\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5328 - val_loss: 0.2817\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 31s 97ms/step - loss: 0.5315 - val_loss: 0.2789\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5310 - val_loss: 0.2819\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5308 - val_loss: 0.2846\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5304 - val_loss: 0.2838\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5299 - val_loss: 0.2869\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5289 - val_loss: 0.2833\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5284 - val_loss: 0.2850\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5269 - val_loss: 0.2820\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5264 - val_loss: 0.2827\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5260 - val_loss: 0.2868\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5256 - val_loss: 0.2841\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5247 - val_loss: 0.2857\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5250 - val_loss: 0.2856\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5226 - val_loss: 0.2856\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5223 - val_loss: 0.2861\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5216 - val_loss: 0.2855\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5222 - val_loss: 0.2882\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5229 - val_loss: 0.2855\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5215 - val_loss: 0.2885\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5204 - val_loss: 0.2882\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5176 - val_loss: 0.2902\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5197 - val_loss: 0.2849\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5177 - val_loss: 0.2854\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5184 - val_loss: 0.2865\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.5160 - val_loss: 0.2891\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.5159 - val_loss: 0.2870\n",
      "Execution time:  1746.3181297779083\n",
      "GRU:\n",
      "Mean Absolute Error: 0.3813\n",
      "Root Mean Square Error: 0.8586\n",
      "Mean Square Error: 0.7373\n",
      "\n",
      "Train RMSE: 0.859\n",
      "Train MSE: 0.737\n",
      "Train MAE: 0.381\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_141\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_114 (GRU)                (None, 144, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_282 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_115 (GRU)                (None, 144, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_283 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_141 (TimeDi (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 12s 114ms/step - loss: 0.6263 - val_loss: 0.4104\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5934 - val_loss: 0.4133\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5891 - val_loss: 0.4147\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5860 - val_loss: 0.4157\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5828 - val_loss: 0.4163\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5807 - val_loss: 0.4168\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.5783 - val_loss: 0.4171\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5760 - val_loss: 0.4172\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5742 - val_loss: 0.4175\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5723 - val_loss: 0.4176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5706 - val_loss: 0.4175\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5690 - val_loss: 0.4176\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5675 - val_loss: 0.4174\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5663 - val_loss: 0.4175\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.5652 - val_loss: 0.4172\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5639 - val_loss: 0.4168\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5630 - val_loss: 0.4167\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5617 - val_loss: 0.4163\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5607 - val_loss: 0.4159\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5598 - val_loss: 0.4153\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5585 - val_loss: 0.4148\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.5577 - val_loss: 0.4139\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5563 - val_loss: 0.4129\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5546 - val_loss: 0.4115\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.5534 - val_loss: 0.4103\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5515 - val_loss: 0.4088\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5496 - val_loss: 0.4072\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.5476 - val_loss: 0.4059\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5450 - val_loss: 0.4054\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5428 - val_loss: 0.4056\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5407 - val_loss: 0.4068\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5393 - val_loss: 0.4084\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5377 - val_loss: 0.4102\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5360 - val_loss: 0.4118\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5343 - val_loss: 0.4129\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5337 - val_loss: 0.4141\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5331 - val_loss: 0.4147\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.5328 - val_loss: 0.4154\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5318 - val_loss: 0.4160\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5309 - val_loss: 0.4165\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5302 - val_loss: 0.4171\n",
      "Epoch 42/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.5291 - val_loss: 0.4169\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.5287 - val_loss: 0.4184\n",
      "Execution time:  496.63371229171753\n",
      "GRU:\n",
      "Mean Absolute Error: 0.3544\n",
      "Root Mean Square Error: 0.8462\n",
      "Mean Square Error: 0.7160\n",
      "\n",
      "Train RMSE: 0.846\n",
      "Train MSE: 0.716\n",
      "Train MAE: 0.354\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_142\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_116 (GRU)                (None, 144, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_284 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_117 (GRU)                (None, 144, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_285 (Dropout)        (None, 144, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_142 (TimeDi (None, 144, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "315/315 [==============================] - 32s 102ms/step - loss: 0.7602 - val_loss: 0.8265\n",
      "Epoch 2/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.7058 - val_loss: 0.8142\n",
      "Epoch 3/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6963 - val_loss: 0.8105\n",
      "Epoch 4/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6892 - val_loss: 0.8088\n",
      "Epoch 5/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6869 - val_loss: 0.8074\n",
      "Epoch 6/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6853 - val_loss: 0.8064\n",
      "Epoch 7/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6839 - val_loss: 0.8057\n",
      "Epoch 8/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6825 - val_loss: 0.8051\n",
      "Epoch 9/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6813 - val_loss: 0.8047\n",
      "Epoch 10/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6800 - val_loss: 0.8044\n",
      "Epoch 11/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6788 - val_loss: 0.8042\n",
      "Epoch 12/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6774 - val_loss: 0.8040\n",
      "Epoch 13/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6760 - val_loss: 0.8038\n",
      "Epoch 14/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6746 - val_loss: 0.8036\n",
      "Epoch 15/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6735 - val_loss: 0.8035\n",
      "Epoch 16/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6718 - val_loss: 0.8033\n",
      "Epoch 17/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6701 - val_loss: 0.8032\n",
      "Epoch 18/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6683 - val_loss: 0.8030\n",
      "Epoch 19/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6660 - val_loss: 0.8029\n",
      "Epoch 20/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6633 - val_loss: 0.8028\n",
      "Epoch 21/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6611 - val_loss: 0.8027\n",
      "Epoch 22/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6585 - val_loss: 0.8027\n",
      "Epoch 23/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6561 - val_loss: 0.8026\n",
      "Epoch 24/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6526 - val_loss: 0.8026\n",
      "Epoch 25/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6496 - val_loss: 0.8025\n",
      "Epoch 26/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6460 - val_loss: 0.8025\n",
      "Epoch 27/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6434 - val_loss: 0.8025\n",
      "Epoch 28/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6417 - val_loss: 0.8025\n",
      "Epoch 29/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6401 - val_loss: 0.8025\n",
      "Epoch 30/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6386 - val_loss: 0.8024\n",
      "Epoch 31/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6378 - val_loss: 0.8024\n",
      "Epoch 32/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6374 - val_loss: 0.8024\n",
      "Epoch 33/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6365 - val_loss: 0.8024\n",
      "Epoch 34/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6358 - val_loss: 0.8023\n",
      "Epoch 35/56\n",
      "315/315 [==============================] - 34s 106ms/step - loss: 0.6353 - val_loss: 0.8023\n",
      "Epoch 36/56\n",
      "315/315 [==============================] - 31s 100ms/step - loss: 0.6350 - val_loss: 0.8023\n",
      "Epoch 37/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6345 - val_loss: 0.8023\n",
      "Epoch 38/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6341 - val_loss: 0.8023\n",
      "Epoch 39/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6342 - val_loss: 0.8023\n",
      "Epoch 40/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6337 - val_loss: 0.8023\n",
      "Epoch 41/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6339 - val_loss: 0.8022\n",
      "Epoch 42/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6336 - val_loss: 0.8022\n",
      "Epoch 43/56\n",
      "315/315 [==============================] - 32s 100ms/step - loss: 0.6332 - val_loss: 0.8022\n",
      "Epoch 44/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6331 - val_loss: 0.8022\n",
      "Epoch 45/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6327 - val_loss: 0.8022\n",
      "Epoch 46/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6325 - val_loss: 0.8022\n",
      "Epoch 47/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6327 - val_loss: 0.8022\n",
      "Epoch 48/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6326 - val_loss: 0.8022\n",
      "Epoch 49/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6324 - val_loss: 0.8022\n",
      "Epoch 50/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6322 - val_loss: 0.8022\n",
      "Epoch 51/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6319 - val_loss: 0.8022\n",
      "Epoch 52/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6319 - val_loss: 0.8022\n",
      "Epoch 53/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6315 - val_loss: 0.8021\n",
      "Epoch 54/56\n",
      "315/315 [==============================] - 31s 98ms/step - loss: 0.6312 - val_loss: 0.8021\n",
      "Epoch 55/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6313 - val_loss: 0.8022\n",
      "Epoch 56/56\n",
      "315/315 [==============================] - 31s 99ms/step - loss: 0.6311 - val_loss: 0.8021\n",
      "Execution time:  1766.272943496704\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6386\n",
      "Root Mean Square Error: 0.9354\n",
      "Mean Square Error: 0.8750\n",
      "\n",
      "Train RMSE: 0.935\n",
      "Train MSE: 0.875\n",
      "Train MAE: 0.639\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  1d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_143\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_118 (GRU)                (None, 144, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_286 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_119 (GRU)                (None, 144, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_287 (Dropout)        (None, 144, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_143 (TimeDi (None, 144, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "103/103 [==============================] - 12s 115ms/step - loss: 0.8369 - val_loss: 0.8324\n",
      "Epoch 2/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.7150 - val_loss: 0.7822\n",
      "Epoch 3/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.7054 - val_loss: 0.7726\n",
      "Epoch 4/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.7008 - val_loss: 0.7676\n",
      "Epoch 5/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6975 - val_loss: 0.7644\n",
      "Epoch 6/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6947 - val_loss: 0.7618\n",
      "Epoch 7/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6922 - val_loss: 0.7596\n",
      "Epoch 8/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6897 - val_loss: 0.7571\n",
      "Epoch 9/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6873 - val_loss: 0.7542\n",
      "Epoch 10/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6847 - val_loss: 0.7508\n",
      "Epoch 11/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6828 - val_loss: 0.7479\n",
      "Epoch 12/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6814 - val_loss: 0.7458\n",
      "Epoch 13/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6803 - val_loss: 0.7442\n",
      "Epoch 14/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6792 - val_loss: 0.7429\n",
      "Epoch 15/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6783 - val_loss: 0.7416\n",
      "Epoch 16/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6773 - val_loss: 0.7405\n",
      "Epoch 17/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6765 - val_loss: 0.7393\n",
      "Epoch 18/43\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.6756 - val_loss: 0.7382\n",
      "Epoch 19/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6748 - val_loss: 0.7372\n",
      "Epoch 20/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6740 - val_loss: 0.7362\n",
      "Epoch 21/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6733 - val_loss: 0.7352\n",
      "Epoch 22/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6726 - val_loss: 0.7344\n",
      "Epoch 23/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6721 - val_loss: 0.7335\n",
      "Epoch 24/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6715 - val_loss: 0.7328\n",
      "Epoch 25/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6707 - val_loss: 0.7324\n",
      "Epoch 26/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6703 - val_loss: 0.7314\n",
      "Epoch 27/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6698 - val_loss: 0.7313\n",
      "Epoch 28/43\n",
      "103/103 [==============================] - 12s 114ms/step - loss: 0.6690 - val_loss: 0.7306\n",
      "Epoch 29/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6687 - val_loss: 0.7302\n",
      "Epoch 30/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6682 - val_loss: 0.7295\n",
      "Epoch 31/43\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 0.6678 - val_loss: 0.7293\n",
      "Epoch 32/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6672 - val_loss: 0.7286\n",
      "Epoch 33/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6668 - val_loss: 0.7286\n",
      "Epoch 34/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6661 - val_loss: 0.7276\n",
      "Epoch 35/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6661 - val_loss: 0.7281\n",
      "Epoch 36/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6647 - val_loss: 0.7273\n",
      "Epoch 37/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6645 - val_loss: 0.7269\n",
      "Epoch 38/43\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.6639 - val_loss: 0.7263\n",
      "Epoch 39/43\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.6625 - val_loss: 0.7263\n",
      "Epoch 40/43\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.6594 - val_loss: 0.7251\n",
      "Epoch 41/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6583 - val_loss: 0.7237\n",
      "Epoch 42/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 12s 113ms/step - loss: 0.6581 - val_loss: 0.7235\n",
      "Epoch 43/43\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 0.6575 - val_loss: 0.7221\n",
      "Execution time:  503.6986105442047\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6278\n",
      "Root Mean Square Error: 0.9179\n",
      "Mean Square Error: 0.8426\n",
      "\n",
      "Train RMSE: 0.918\n",
      "Train MSE: 0.843\n",
      "Train MAE: 0.628\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_144\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_120 (GRU)                (None, 432, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_288 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_121 (GRU)                (None, 432, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_289 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_144 (TimeDi (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.6396 - val_loss: 0.5336\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6337 - val_loss: 0.4956\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6212 - val_loss: 0.4529\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6066 - val_loss: 0.4628\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6042 - val_loss: 0.4670\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6016 - val_loss: 0.4675\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5996 - val_loss: 0.4688\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.5987 - val_loss: 0.4640\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5979 - val_loss: 0.4702\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5965 - val_loss: 0.4666\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5959 - val_loss: 0.4701\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5953 - val_loss: 0.4683\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5941 - val_loss: 0.4644\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5938 - val_loss: 0.4663\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.5938 - val_loss: 0.4634\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5944 - val_loss: 0.4549\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5936 - val_loss: 0.4602\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.5938 - val_loss: 0.4513\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5935 - val_loss: 0.4572\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5931 - val_loss: 0.4550\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.5930 - val_loss: 0.4594\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5931 - val_loss: 0.4546\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5931 - val_loss: 0.4543\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.5931 - val_loss: 0.4539\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5935 - val_loss: 0.4519\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5930 - val_loss: 0.4521\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.5926 - val_loss: 0.4506\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5926 - val_loss: 0.4511\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5929 - val_loss: 0.4536\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.5922 - val_loss: 0.4528\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5922 - val_loss: 0.4508\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5921 - val_loss: 0.4544\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5924 - val_loss: 0.4554\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5920 - val_loss: 0.4526\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5919 - val_loss: 0.4556\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5915 - val_loss: 0.4527\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5919 - val_loss: 0.4582\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5912 - val_loss: 0.4551\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5912 - val_loss: 0.4544\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5911 - val_loss: 0.4578\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5905 - val_loss: 0.4573\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5906 - val_loss: 0.4590\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5899 - val_loss: 0.4592\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5903 - val_loss: 0.4580\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5899 - val_loss: 0.4556\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5896 - val_loss: 0.4609\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5891 - val_loss: 0.4593\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5941 - val_loss: 0.4548\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5895 - val_loss: 0.4559\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5881 - val_loss: 0.4586\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5890 - val_loss: 0.4597\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5877 - val_loss: 0.4596\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5878 - val_loss: 0.4587\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5881 - val_loss: 0.4567\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.5875 - val_loss: 0.4583\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5883 - val_loss: 0.4546\n",
      "Execution time:  4543.565344810486\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5515\n",
      "Root Mean Square Error: 0.9958\n",
      "Mean Square Error: 0.9916\n",
      "\n",
      "Train RMSE: 0.996\n",
      "Train MSE: 0.992\n",
      "Train MAE: 0.552\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_145\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_122 (GRU)                (None, 432, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_290 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_123 (GRU)                (None, 432, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_291 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_145 (TimeDi (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6907 - val_loss: 0.3693\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.6773 - val_loss: 0.3355\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.6723 - val_loss: 0.3339\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.6684 - val_loss: 0.3388\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.6649 - val_loss: 0.3407\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.6620 - val_loss: 0.3397\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 29s 309ms/step - loss: 0.6599 - val_loss: 0.3385\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.6521 - val_loss: 0.3585\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.6432 - val_loss: 0.3926\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.6373 - val_loss: 0.3846\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6432 - val_loss: 0.3854\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6353 - val_loss: 0.3814\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.6353 - val_loss: 0.3948\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6323 - val_loss: 0.3879\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6312 - val_loss: 0.3831\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6295 - val_loss: 0.3780\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6289 - val_loss: 0.3797\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6285 - val_loss: 0.3825\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.6281 - val_loss: 0.3826\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6276 - val_loss: 0.3842\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6271 - val_loss: 0.3835\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6269 - val_loss: 0.3837\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.6267 - val_loss: 0.3848\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6265 - val_loss: 0.3852\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6266 - val_loss: 0.3872\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6261 - val_loss: 0.3854\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 31s 326ms/step - loss: 0.6260 - val_loss: 0.3850\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 31s 324ms/step - loss: 0.6260 - val_loss: 0.3884\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6252 - val_loss: 0.3875\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.6252 - val_loss: 0.3881\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6239 - val_loss: 0.3831\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.6255 - val_loss: 0.3880\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.6230 - val_loss: 0.3850\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6240 - val_loss: 0.3852\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.6232 - val_loss: 0.3861\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6230 - val_loss: 0.3871\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6223 - val_loss: 0.3886\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.6221 - val_loss: 0.3911\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6220 - val_loss: 0.3928\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.6218 - val_loss: 0.3939\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.6215 - val_loss: 0.3941\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6212 - val_loss: 0.3942\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.6211 - val_loss: 0.3947\n",
      "Execution time:  1305.1391537189484\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5307\n",
      "Root Mean Square Error: 0.9617\n",
      "Mean Square Error: 0.9249\n",
      "\n",
      "Train RMSE: 0.962\n",
      "Train MSE: 0.925\n",
      "Train MAE: 0.531\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_146\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_124 (GRU)                (None, 432, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_292 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_125 (GRU)                (None, 432, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_293 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_146 (TimeDi (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.7522 - val_loss: 0.8001\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 80s 274ms/step - loss: 0.6778 - val_loss: 0.7956\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6768 - val_loss: 0.7944\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6764 - val_loss: 0.7939\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6762 - val_loss: 0.7936\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6761 - val_loss: 0.7934\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6760 - val_loss: 0.7932\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6759 - val_loss: 0.7931\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6759 - val_loss: 0.7930\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6759 - val_loss: 0.7929\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6758 - val_loss: 0.7929\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Execution time:  4536.104122877121\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6930\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9904\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_147\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_126 (GRU)                (None, 432, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_294 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_127 (GRU)                (None, 432, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_295 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_147 (TimeDi (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 34s 359ms/step - loss: 0.8607 - val_loss: 0.6577\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7109 - val_loss: 0.6277\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7075 - val_loss: 0.6240\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7066 - val_loss: 0.6223\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7061 - val_loss: 0.6213\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7058 - val_loss: 0.6207\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7056 - val_loss: 0.6203\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7055 - val_loss: 0.6200\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7054 - val_loss: 0.6198\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7053 - val_loss: 0.6196\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7052 - val_loss: 0.6194\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7052 - val_loss: 0.6193\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7051 - val_loss: 0.6192\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7051 - val_loss: 0.6191\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7050 - val_loss: 0.6190\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7050 - val_loss: 0.6189\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7050 - val_loss: 0.6189\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7050 - val_loss: 0.6188\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 31s 325ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 31s 325ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7049 - val_loss: 0.6185\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7049 - val_loss: 0.6185\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 31s 324ms/step - loss: 0.7048 - val_loss: 0.6185\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6185\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 31s 324ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 34/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 31s 324ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7048 - val_loss: 0.6183\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7048 - val_loss: 0.6183\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6183\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6183\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6183\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7048 - val_loss: 0.6183\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.7048 - val_loss: 0.6183\n",
      "Execution time:  1328.6985881328583\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6930\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9904\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_148\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_128 (GRU)                (None, 432, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_296 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_129 (GRU)                (None, 432, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_297 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_148 (TimeDi (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.6758 - val_loss: 0.7913\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6749 - val_loss: 0.7882\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6739 - val_loss: 0.7848\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6730 - val_loss: 0.7811\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6719 - val_loss: 0.7774\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6707 - val_loss: 0.7734\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6695 - val_loss: 0.7694\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6683 - val_loss: 0.7652\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6671 - val_loss: 0.7609\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6659 - val_loss: 0.7566\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6645 - val_loss: 0.7522\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6632 - val_loss: 0.7477\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6620 - val_loss: 0.7433\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6607 - val_loss: 0.7389\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6595 - val_loss: 0.7344\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6583 - val_loss: 0.7301\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6571 - val_loss: 0.7258\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6560 - val_loss: 0.7215\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6549 - val_loss: 0.7172\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6539 - val_loss: 0.7129\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6529 - val_loss: 0.7087\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6519 - val_loss: 0.7044\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6508 - val_loss: 0.7001\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6498 - val_loss: 0.6958\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6488 - val_loss: 0.6914\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6478 - val_loss: 0.6869\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6467 - val_loss: 0.6824\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6457 - val_loss: 0.6779\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6446 - val_loss: 0.6733\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 80s 274ms/step - loss: 0.6435 - val_loss: 0.6686\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6424 - val_loss: 0.6639\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6414 - val_loss: 0.6592\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6403 - val_loss: 0.6544\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6393 - val_loss: 0.6496\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6381 - val_loss: 0.6448\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6371 - val_loss: 0.6400\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6360 - val_loss: 0.6351\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6350 - val_loss: 0.6303\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6339 - val_loss: 0.6255\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6330 - val_loss: 0.6207\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6319 - val_loss: 0.6160\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6310 - val_loss: 0.6112\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6300 - val_loss: 0.6065\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6292 - val_loss: 0.6019\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6281 - val_loss: 0.5973\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6273 - val_loss: 0.5927\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6264 - val_loss: 0.5881\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6256 - val_loss: 0.5836\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6248 - val_loss: 0.5791\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6240 - val_loss: 0.5747\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6232 - val_loss: 0.5703\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6224 - val_loss: 0.5659\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6217 - val_loss: 0.5616\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6210 - val_loss: 0.5574\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6202 - val_loss: 0.5532\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 80s 276ms/step - loss: 0.6196 - val_loss: 0.5490\n",
      "Execution time:  4529.597225427628\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5616\n",
      "Root Mean Square Error: 0.9229\n",
      "Mean Square Error: 0.8518\n",
      "\n",
      "Train RMSE: 0.923\n",
      "Train MSE: 0.852\n",
      "Train MAE: 0.562\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_149\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_130 (GRU)                (None, 432, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_298 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_131 (GRU)                (None, 432, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_299 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_149 (TimeDi (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.7083 - val_loss: 0.6309\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7082 - val_loss: 0.6303\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7080 - val_loss: 0.6297\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7078 - val_loss: 0.6290\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7076 - val_loss: 0.6283\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7074 - val_loss: 0.6276\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7072 - val_loss: 0.6269\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7070 - val_loss: 0.6261\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7067 - val_loss: 0.6254\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7065 - val_loss: 0.6246\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7063 - val_loss: 0.6239\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.7060 - val_loss: 0.6231\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7058 - val_loss: 0.6223\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7055 - val_loss: 0.6215\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7053 - val_loss: 0.6207\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7051 - val_loss: 0.6199\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7048 - val_loss: 0.6191\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7046 - val_loss: 0.6182\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7043 - val_loss: 0.6174\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.7040 - val_loss: 0.6166\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7038 - val_loss: 0.6157\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7036 - val_loss: 0.6149\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7033 - val_loss: 0.6140\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7030 - val_loss: 0.6132\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7028 - val_loss: 0.6123\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7025 - val_loss: 0.6115\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7022 - val_loss: 0.6106\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7020 - val_loss: 0.6097\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7017 - val_loss: 0.6089\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7014 - val_loss: 0.6080\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7012 - val_loss: 0.6071\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7009 - val_loss: 0.6062\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7006 - val_loss: 0.6053\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7003 - val_loss: 0.6045\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7001 - val_loss: 0.6036\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6998 - val_loss: 0.6027\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6995 - val_loss: 0.6018\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6992 - val_loss: 0.6009\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6990 - val_loss: 0.6000\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6987 - val_loss: 0.5991\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6984 - val_loss: 0.5982\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.6981 - val_loss: 0.5973\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6979 - val_loss: 0.5964\n",
      "Execution time:  1297.3414912223816\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6863\n",
      "Root Mean Square Error: 0.9985\n",
      "Mean Square Error: 0.9970\n",
      "\n",
      "Train RMSE: 0.998\n",
      "Train MSE: 0.997\n",
      "Train MAE: 0.686\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_150\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_132 (GRU)                (None, 432, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_300 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_133 (GRU)                (None, 432, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_301 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_150 (TimeDi (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.9064 - val_loss: 1.2858\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.9061 - val_loss: 1.2851\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.9057 - val_loss: 1.2842\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.9053 - val_loss: 1.2833\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.9049 - val_loss: 1.2824\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.9044 - val_loss: 1.2814\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.9039 - val_loss: 1.2803\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.9034 - val_loss: 1.2792\n",
      "Epoch 9/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 81s 277ms/step - loss: 0.9029 - val_loss: 1.2780\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.9023 - val_loss: 1.2768\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.9017 - val_loss: 1.2755\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.9011 - val_loss: 1.2742\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.9005 - val_loss: 1.2728\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8998 - val_loss: 1.2714\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8991 - val_loss: 1.2699\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8984 - val_loss: 1.2684\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8976 - val_loss: 1.2668\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8969 - val_loss: 1.2652\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8961 - val_loss: 1.2635\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8953 - val_loss: 1.2618\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8944 - val_loss: 1.2600\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8936 - val_loss: 1.2581\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8927 - val_loss: 1.2562\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8917 - val_loss: 1.2542\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8907 - val_loss: 1.2521\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8897 - val_loss: 1.2500\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8887 - val_loss: 1.2478\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8876 - val_loss: 1.2455\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8865 - val_loss: 1.2431\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8854 - val_loss: 1.2407\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8842 - val_loss: 1.2382\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8830 - val_loss: 1.2356\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8818 - val_loss: 1.2329\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8804 - val_loss: 1.2302\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8791 - val_loss: 1.2273\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8777 - val_loss: 1.2244\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8764 - val_loss: 1.2214\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8749 - val_loss: 1.2182\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8734 - val_loss: 1.2150\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 87s 297ms/step - loss: 0.8718 - val_loss: 1.2116\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8702 - val_loss: 1.2082\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8686 - val_loss: 1.2046\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8668 - val_loss: 1.2009\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8650 - val_loss: 1.1970\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.8632 - val_loss: 1.1930\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8613 - val_loss: 1.1889\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8594 - val_loss: 1.1847\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8573 - val_loss: 1.1802\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.8552 - val_loss: 1.1757\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.8530 - val_loss: 1.1710\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.8508 - val_loss: 1.1661\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8485 - val_loss: 1.1611\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8461 - val_loss: 1.1559\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.8436 - val_loss: 1.1505\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.8410 - val_loss: 1.1450\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.8384 - val_loss: 1.1393\n",
      "Execution time:  4554.418386936188\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8267\n",
      "Root Mean Square Error: 1.0308\n",
      "Mean Square Error: 1.0626\n",
      "\n",
      "Train RMSE: 1.031\n",
      "Train MSE: 1.063\n",
      "Train MAE: 0.827\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_151\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_134 (GRU)                (None, 432, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_302 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_135 (GRU)                (None, 432, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_303 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_151 (TimeDi (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.9077 - val_loss: 1.1012\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.9076 - val_loss: 1.1011\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9075 - val_loss: 1.1009\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.9075 - val_loss: 1.1008\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9074 - val_loss: 1.1006\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9073 - val_loss: 1.1004\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9072 - val_loss: 1.1003\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.9071 - val_loss: 1.1001\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9071 - val_loss: 1.0999\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.9070 - val_loss: 1.0997\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9069 - val_loss: 1.0995\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9068 - val_loss: 1.0993\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.9067 - val_loss: 1.0991\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.9066 - val_loss: 1.0989\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.9065 - val_loss: 1.0987\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9064 - val_loss: 1.0985\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9063 - val_loss: 1.0983\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9062 - val_loss: 1.0980\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.9061 - val_loss: 1.0978\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9060 - val_loss: 1.0976\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9059 - val_loss: 1.0974\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9058 - val_loss: 1.0971\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9057 - val_loss: 1.0969\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9055 - val_loss: 1.0967\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9054 - val_loss: 1.0964\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9053 - val_loss: 1.0962\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.9052 - val_loss: 1.0959\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9051 - val_loss: 1.0957\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9050 - val_loss: 1.0954\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9049 - val_loss: 1.0952\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.9047 - val_loss: 1.0949\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9046 - val_loss: 1.0947\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.9045 - val_loss: 1.0944\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9044 - val_loss: 1.0942\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.9043 - val_loss: 1.0939\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9041 - val_loss: 1.0937\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9040 - val_loss: 1.0934\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9039 - val_loss: 1.0931\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.9038 - val_loss: 1.0929\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9036 - val_loss: 1.0926\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9035 - val_loss: 1.0923\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.9034 - val_loss: 1.0921\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.9032 - val_loss: 1.0918\n",
      "Execution time:  1312.5420167446136\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9195\n",
      "Root Mean Square Error: 1.1123\n",
      "Mean Square Error: 1.2373\n",
      "\n",
      "Train RMSE: 1.112\n",
      "Train MSE: 1.237\n",
      "Train MAE: 0.919\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_152\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_136 (GRU)                (None, 432, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_304 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_137 (GRU)                (None, 432, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_305 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_152 (TimeDi (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6405 - val_loss: 0.4631\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 80s 273ms/step - loss: 0.6254 - val_loss: 0.4107\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 80s 273ms/step - loss: 0.6226 - val_loss: 0.3797\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 80s 272ms/step - loss: 0.6210 - val_loss: 0.3705\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6199 - val_loss: 0.3712\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6186 - val_loss: 0.3735\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6174 - val_loss: 0.3753\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6162 - val_loss: 0.3762\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6149 - val_loss: 0.3779\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6134 - val_loss: 0.3762\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 83s 284ms/step - loss: 0.6116 - val_loss: 0.3867\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.6076 - val_loss: 0.4176\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6000 - val_loss: 0.4562\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5967 - val_loss: 0.4650\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5954 - val_loss: 0.4663\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5948 - val_loss: 0.4679\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5942 - val_loss: 0.4688\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5935 - val_loss: 0.4695\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 82s 281ms/step - loss: 0.5929 - val_loss: 0.4696\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5929 - val_loss: 0.4722\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5913 - val_loss: 0.4709\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5909 - val_loss: 0.4694\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5901 - val_loss: 0.4683\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5902 - val_loss: 0.4639\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 82s 281ms/step - loss: 0.5894 - val_loss: 0.4643\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5891 - val_loss: 0.4655\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5895 - val_loss: 0.4653\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.5889 - val_loss: 0.4645\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5892 - val_loss: 0.4633\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5887 - val_loss: 0.4621\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5884 - val_loss: 0.4635\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5884 - val_loss: 0.4621\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.5885 - val_loss: 0.4604\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.5879 - val_loss: 0.4606\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5884 - val_loss: 0.4578\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.5880 - val_loss: 0.4578\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.5878 - val_loss: 0.4586\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5880 - val_loss: 0.4577\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.5876 - val_loss: 0.4566\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 82s 279ms/step - loss: 0.5881 - val_loss: 0.4547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5874 - val_loss: 0.4540\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5873 - val_loss: 0.4551\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5877 - val_loss: 0.4517\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5874 - val_loss: 0.4535\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5870 - val_loss: 0.4524\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5875 - val_loss: 0.4512\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5869 - val_loss: 0.4501\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5869 - val_loss: 0.4539\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5873 - val_loss: 0.4513\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5872 - val_loss: 0.4513\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5872 - val_loss: 0.4488\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.5872 - val_loss: 0.4486\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5870 - val_loss: 0.4476\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.5870 - val_loss: 0.4482\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5870 - val_loss: 0.4471\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.5865 - val_loss: 0.4496\n",
      "Execution time:  4571.128044366837\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5462\n",
      "Root Mean Square Error: 0.9930\n",
      "Mean Square Error: 0.9860\n",
      "\n",
      "Train RMSE: 0.993\n",
      "Train MSE: 0.986\n",
      "Train MAE: 0.546\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_153\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_138 (GRU)                (None, 432, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_306 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_139 (GRU)                (None, 432, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_307 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_153 (TimeDi (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.6974 - val_loss: 0.5047\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6664 - val_loss: 0.3687\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6638 - val_loss: 0.3461\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6647 - val_loss: 0.3487\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6624 - val_loss: 0.3479\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6612 - val_loss: 0.3478\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6602 - val_loss: 0.3475\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6590 - val_loss: 0.3474\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.6584 - val_loss: 0.3471\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6574 - val_loss: 0.3467\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.6568 - val_loss: 0.3465\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.6560 - val_loss: 0.3461\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6554 - val_loss: 0.3457\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6548 - val_loss: 0.3454\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.6542 - val_loss: 0.3453\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.6536 - val_loss: 0.3450\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6530 - val_loss: 0.3447\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.6523 - val_loss: 0.3443\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.6515 - val_loss: 0.3443\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 31s 324ms/step - loss: 0.6505 - val_loss: 0.3442\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6497 - val_loss: 0.3459\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6483 - val_loss: 0.3464\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.6469 - val_loss: 0.3505\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 30s 321ms/step - loss: 0.6416 - val_loss: 0.3300\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 30s 321ms/step - loss: 0.6405 - val_loss: 0.3564\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.6347 - val_loss: 0.3706\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 30s 321ms/step - loss: 0.6306 - val_loss: 0.3751\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6307 - val_loss: 0.3759\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6286 - val_loss: 0.3835\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.6267 - val_loss: 0.3814\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 30s 321ms/step - loss: 0.6264 - val_loss: 0.3833\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 30s 321ms/step - loss: 0.6265 - val_loss: 0.3819\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 31s 321ms/step - loss: 0.6256 - val_loss: 0.3843\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 31s 326ms/step - loss: 0.6255 - val_loss: 0.3825\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6250 - val_loss: 0.3851\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6240 - val_loss: 0.3825\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6243 - val_loss: 0.3866\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.6235 - val_loss: 0.3830\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6237 - val_loss: 0.3855\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6238 - val_loss: 0.3834\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.6233 - val_loss: 0.3839\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.6234 - val_loss: 0.3823\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 30s 321ms/step - loss: 0.6231 - val_loss: 0.3896\n",
      "Execution time:  1322.5656213760376\n",
      "GRU:\n",
      "Mean Absolute Error: 0.5395\n",
      "Root Mean Square Error: 0.9651\n",
      "Mean Square Error: 0.9315\n",
      "\n",
      "Train RMSE: 0.965\n",
      "Train MSE: 0.931\n",
      "Train MAE: 0.540\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_154\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_140 (GRU)                (None, 432, 43)           5934      \n",
      "_________________________________________________________________\n",
      "dropout_308 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "gru_141 (GRU)                (None, 432, 43)           11352     \n",
      "_________________________________________________________________\n",
      "dropout_309 (Dropout)        (None, 432, 43)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_154 (TimeDi (None, 432, 1)            44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 82s 280ms/step - loss: 0.7911 - val_loss: 0.8092\n",
      "Epoch 2/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6802 - val_loss: 0.7992\n",
      "Epoch 3/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6780 - val_loss: 0.7965\n",
      "Epoch 4/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6772 - val_loss: 0.7952\n",
      "Epoch 5/56\n",
      "292/292 [==============================] - 80s 275ms/step - loss: 0.6767 - val_loss: 0.7945\n",
      "Epoch 6/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6764 - val_loss: 0.7940\n",
      "Epoch 7/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6763 - val_loss: 0.7937\n",
      "Epoch 8/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6761 - val_loss: 0.7934\n",
      "Epoch 9/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6760 - val_loss: 0.7932\n",
      "Epoch 10/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6760 - val_loss: 0.7931\n",
      "Epoch 11/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6759 - val_loss: 0.7930\n",
      "Epoch 12/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6759 - val_loss: 0.7929\n",
      "Epoch 13/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 14/56\n",
      "292/292 [==============================] - 81s 276ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 15/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 16/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7928\n",
      "Epoch 17/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 18/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 19/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 20/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 21/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6758 - val_loss: 0.7927\n",
      "Epoch 22/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 23/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 24/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 25/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 26/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 27/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 28/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 29/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 30/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 31/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 32/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 33/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 34/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 35/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 36/56\n",
      "292/292 [==============================] - 82s 280ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 37/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 38/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 39/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 40/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 41/56\n",
      "292/292 [==============================] - 81s 279ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 42/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 43/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 44/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 45/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 46/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 47/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 48/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 49/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 50/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 51/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 52/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 53/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 54/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 55/56\n",
      "292/292 [==============================] - 81s 277ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Epoch 56/56\n",
      "292/292 [==============================] - 81s 278ms/step - loss: 0.6757 - val_loss: 0.7927\n",
      "Execution time:  4557.967628240585\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6930\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9904\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  3d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_155\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_142 (GRU)                (None, 432, 45)           6480      \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "gru_143 (GRU)                (None, 432, 45)           12420     \n",
      "_________________________________________________________________\n",
      "dropout_311 (Dropout)        (None, 432, 45)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_155 (TimeDi (None, 432, 1)            46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "95/95 [==============================] - 31s 323ms/step - loss: 0.8869 - val_loss: 0.9335\n",
      "Epoch 2/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7546 - val_loss: 0.6507\n",
      "Epoch 3/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7128 - val_loss: 0.6345\n",
      "Epoch 4/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7098 - val_loss: 0.6298\n",
      "Epoch 5/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7085 - val_loss: 0.6272\n",
      "Epoch 6/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7077 - val_loss: 0.6255\n",
      "Epoch 7/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7072 - val_loss: 0.6242\n",
      "Epoch 8/43\n",
      "95/95 [==============================] - 30s 311ms/step - loss: 0.7068 - val_loss: 0.6233\n",
      "Epoch 9/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7065 - val_loss: 0.6226\n",
      "Epoch 10/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7063 - val_loss: 0.6220\n",
      "Epoch 11/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7061 - val_loss: 0.6216\n",
      "Epoch 12/43\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.7059 - val_loss: 0.6212\n",
      "Epoch 13/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7058 - val_loss: 0.6208\n",
      "Epoch 14/43\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.7057 - val_loss: 0.6206\n",
      "Epoch 15/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7056 - val_loss: 0.6203\n",
      "Epoch 16/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7055 - val_loss: 0.6201\n",
      "Epoch 17/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7054 - val_loss: 0.6199\n",
      "Epoch 18/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7054 - val_loss: 0.6198\n",
      "Epoch 19/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7053 - val_loss: 0.6196\n",
      "Epoch 20/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7053 - val_loss: 0.6195\n",
      "Epoch 21/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7052 - val_loss: 0.6194\n",
      "Epoch 22/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7052 - val_loss: 0.6193\n",
      "Epoch 23/43\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.7051 - val_loss: 0.6192\n",
      "Epoch 24/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7051 - val_loss: 0.6191\n",
      "Epoch 25/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7051 - val_loss: 0.6190\n",
      "Epoch 26/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7050 - val_loss: 0.6189\n",
      "Epoch 27/43\n",
      "95/95 [==============================] - 31s 330ms/step - loss: 0.7050 - val_loss: 0.6189\n",
      "Epoch 28/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7050 - val_loss: 0.6188\n",
      "Epoch 29/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7050 - val_loss: 0.6188\n",
      "Epoch 30/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 31/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7049 - val_loss: 0.6187\n",
      "Epoch 32/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 33/43\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.7049 - val_loss: 0.6186\n",
      "Epoch 34/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7049 - val_loss: 0.6185\n",
      "Epoch 35/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7049 - val_loss: 0.6185\n",
      "Epoch 36/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7048 - val_loss: 0.6185\n",
      "Epoch 37/43\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.7048 - val_loss: 0.6185\n",
      "Epoch 38/43\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 39/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 40/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 41/43\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 42/43\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Epoch 43/43\n",
      "95/95 [==============================] - 31s 322ms/step - loss: 0.7048 - val_loss: 0.6184\n",
      "Execution time:  1307.531947851181\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6930\n",
      "Root Mean Square Error: 0.9952\n",
      "Mean Square Error: 0.9904\n",
      "\n",
      "Train RMSE: 0.995\n",
      "Train MSE: 0.990\n",
      "Train MAE: 0.693\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_144 (GRU)                (None, 1008, 43)          5934      \n",
      "_________________________________________________________________\n",
      "dropout_312 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "gru_145 (GRU)                (None, 1008, 43)          11352     \n",
      "_________________________________________________________________\n",
      "dropout_313 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_156 (TimeDi (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.6112 - val_loss: 0.4391\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 106s 435ms/step - loss: 0.5732 - val_loss: 0.4797\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 106s 433ms/step - loss: 0.5706 - val_loss: 0.4595\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 106s 433ms/step - loss: 0.5684 - val_loss: 0.4519\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 106s 434ms/step - loss: 0.5654 - val_loss: 0.4433\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 106s 433ms/step - loss: 0.5625 - val_loss: 0.4238\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5605 - val_loss: 0.4231\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 106s 433ms/step - loss: 0.5592 - val_loss: 0.4130\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 106s 435ms/step - loss: 0.5582 - val_loss: 0.4205\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 106s 435ms/step - loss: 0.5573 - val_loss: 0.4218\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 106s 433ms/step - loss: 0.5559 - val_loss: 0.4208\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5541 - val_loss: 0.4335\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5519 - val_loss: 0.4312\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5483 - val_loss: 0.4447\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5470 - val_loss: 0.4795\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 107s 437ms/step - loss: 0.5423 - val_loss: 0.4833\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5408 - val_loss: 0.5206\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 107s 437ms/step - loss: 0.5405 - val_loss: 0.5523\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5382 - val_loss: 0.5686\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 107s 437ms/step - loss: 0.5391 - val_loss: 0.5799\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5389 - val_loss: 0.5736\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5392 - val_loss: 0.5633\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5382 - val_loss: 0.5518\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5400 - val_loss: 0.4907\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5398 - val_loss: 0.4985\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5345 - val_loss: 0.4856\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5372 - val_loss: 0.5170\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5342 - val_loss: 0.4967\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5343 - val_loss: 0.5143\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5347 - val_loss: 0.5228\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5352 - val_loss: 0.5238\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5350 - val_loss: 0.5116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5345 - val_loss: 0.5109\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5344 - val_loss: 0.5097\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5338 - val_loss: 0.5146\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5331 - val_loss: 0.5105\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5328 - val_loss: 0.5132\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5322 - val_loss: 0.5139\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5321 - val_loss: 0.5159\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5313 - val_loss: 0.5148\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5316 - val_loss: 0.5136\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5308 - val_loss: 0.5147\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5296 - val_loss: 0.5162\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5295 - val_loss: 0.5182\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5291 - val_loss: 0.5179\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5291 - val_loss: 0.5176\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5277 - val_loss: 0.5210\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5278 - val_loss: 0.5253\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5285 - val_loss: 0.5139\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5279 - val_loss: 0.5049\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5268 - val_loss: 0.5015\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5265 - val_loss: 0.5016\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5250 - val_loss: 0.4984\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5240 - val_loss: 0.4905\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.5244 - val_loss: 0.4870\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5242 - val_loss: 0.4814\n",
      "Execution time:  6012.78852725029\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6887\n",
      "Root Mean Square Error: 1.1500\n",
      "Mean Square Error: 1.3225\n",
      "\n",
      "Train RMSE: 1.150\n",
      "Train MSE: 1.323\n",
      "Train MAE: 0.689\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_157\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_146 (GRU)                (None, 1008, 45)          6480      \n",
      "_________________________________________________________________\n",
      "dropout_314 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "gru_147 (GRU)                (None, 1008, 45)          12420     \n",
      "_________________________________________________________________\n",
      "dropout_315 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_157 (TimeDi (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 65s 812ms/step - loss: 0.6716 - val_loss: 0.4595\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 66s 825ms/step - loss: 0.6269 - val_loss: 0.4557\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 66s 821ms/step - loss: 0.6178 - val_loss: 0.4236\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 65s 816ms/step - loss: 0.6117 - val_loss: 0.4029\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 65s 813ms/step - loss: 0.6078 - val_loss: 0.3963\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 66s 823ms/step - loss: 0.6050 - val_loss: 0.3921\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 65s 817ms/step - loss: 0.6025 - val_loss: 0.3887\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.6003 - val_loss: 0.3863\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 66s 819ms/step - loss: 0.5982 - val_loss: 0.3852\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.5961 - val_loss: 0.3833\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 65s 818ms/step - loss: 0.5928 - val_loss: 0.3821\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.5885 - val_loss: 0.3679\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 65s 813ms/step - loss: 0.5807 - val_loss: 0.3885\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.5759 - val_loss: 0.4103\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.5739 - val_loss: 0.4144\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 65s 816ms/step - loss: 0.5720 - val_loss: 0.4237\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.5712 - val_loss: 0.4293\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.5703 - val_loss: 0.4340\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.5696 - val_loss: 0.4370\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 65s 812ms/step - loss: 0.5690 - val_loss: 0.4390\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 66s 819ms/step - loss: 0.5684 - val_loss: 0.4399\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.5679 - val_loss: 0.4407\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 65s 818ms/step - loss: 0.5674 - val_loss: 0.4406\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.5668 - val_loss: 0.4404\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 65s 814ms/step - loss: 0.5663 - val_loss: 0.4399\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.5658 - val_loss: 0.4395\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.5654 - val_loss: 0.4386\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.5648 - val_loss: 0.4390\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 65s 810ms/step - loss: 0.5642 - val_loss: 0.4393\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 66s 821ms/step - loss: 0.5636 - val_loss: 0.4420\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.5632 - val_loss: 0.4455\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.5627 - val_loss: 0.4486\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 66s 823ms/step - loss: 0.5621 - val_loss: 0.4530\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 66s 823ms/step - loss: 0.5616 - val_loss: 0.4573\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 66s 819ms/step - loss: 0.5613 - val_loss: 0.4611\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.5607 - val_loss: 0.4631\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 65s 817ms/step - loss: 0.5604 - val_loss: 0.4642\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 66s 819ms/step - loss: 0.5599 - val_loss: 0.4635\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 65s 818ms/step - loss: 0.5593 - val_loss: 0.4623\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 65s 814ms/step - loss: 0.5583 - val_loss: 0.4640\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 66s 823ms/step - loss: 0.5573 - val_loss: 0.4678\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 66s 821ms/step - loss: 0.5593 - val_loss: 0.4808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/43\n",
      "80/80 [==============================] - 66s 821ms/step - loss: 0.5603 - val_loss: 0.4730\n",
      "Execution time:  2853.5444486141205\n",
      "GRU:\n",
      "Mean Absolute Error: 0.7225\n",
      "Root Mean Square Error: 1.1774\n",
      "Mean Square Error: 1.3862\n",
      "\n",
      "Train RMSE: 1.177\n",
      "Train MSE: 1.386\n",
      "Train MAE: 0.722\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_158\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_148 (GRU)                (None, 1008, 43)          5934      \n",
      "_________________________________________________________________\n",
      "dropout_316 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "gru_149 (GRU)                (None, 1008, 43)          11352     \n",
      "_________________________________________________________________\n",
      "dropout_317 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_158 (TimeDi (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 112s 460ms/step - loss: 0.7656 - val_loss: 0.8393\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 110s 453ms/step - loss: 0.6805 - val_loss: 0.8361\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 111s 454ms/step - loss: 0.6794 - val_loss: 0.8352\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 111s 453ms/step - loss: 0.6791 - val_loss: 0.8349\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 111s 454ms/step - loss: 0.6789 - val_loss: 0.8347\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6788 - val_loss: 0.8345\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6787 - val_loss: 0.8344\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6786 - val_loss: 0.8344\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 111s 454ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 111s 454ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 112s 458ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 115s 472ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 112s 458ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 121s 497ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 112s 460ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 111s 454ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 111s 454ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 112s 458ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 112s 459ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 111s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 112s 459ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 112s 458ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 112s 457ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 112s 458ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 111s 455ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Execution time:  6280.207427024841\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6954\n",
      "Root Mean Square Error: 1.0192\n",
      "Mean Square Error: 1.0387\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.695\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_159\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_150 (GRU)                (None, 1008, 45)          6480      \n",
      "_________________________________________________________________\n",
      "dropout_318 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "gru_151 (GRU)                (None, 1008, 45)          12420     \n",
      "_________________________________________________________________\n",
      "dropout_319 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_159 (TimeDi (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 65s 812ms/step - loss: 0.9039 - val_loss: 0.6908\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 67s 839ms/step - loss: 0.7072 - val_loss: 0.6741\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 68s 848ms/step - loss: 0.7039 - val_loss: 0.6712\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 68s 849ms/step - loss: 0.7028 - val_loss: 0.6699\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7022 - val_loss: 0.6692\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 68s 851ms/step - loss: 0.7019 - val_loss: 0.6687\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 68s 853ms/step - loss: 0.7017 - val_loss: 0.6684\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 68s 849ms/step - loss: 0.7016 - val_loss: 0.6682\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 68s 850ms/step - loss: 0.7014 - val_loss: 0.6680\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 68s 853ms/step - loss: 0.7014 - val_loss: 0.6679\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7013 - val_loss: 0.6678\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7012 - val_loss: 0.6677\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7012 - val_loss: 0.6676\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 69s 861ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 68s 852ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 68s 852ms/step - loss: 0.7011 - val_loss: 0.6674\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7011 - val_loss: 0.6674\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 69s 860ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 68s 851ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 68s 852ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 68s 856ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 68s 856ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 69s 856ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 68s 851ms/step - loss: 0.7010 - val_loss: 0.6672\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 69s 858ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 68s 854ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 68s 854ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 69s 862ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 68s 851ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 69s 857ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 68s 853ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 68s 852ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 69s 858ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 68s 853ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 68s 854ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 68s 852ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 68s 851ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 68s 855ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 69s 859ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 68s 854ms/step - loss: 0.7009 - val_loss: 0.6671\n",
      "Execution time:  2974.136693716049\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6955\n",
      "Root Mean Square Error: 1.0192\n",
      "Mean Square Error: 1.0387\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.695\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_160\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_152 (GRU)                (None, 1008, 43)          5934      \n",
      "_________________________________________________________________\n",
      "dropout_320 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "gru_153 (GRU)                (None, 1008, 43)          11352     \n",
      "_________________________________________________________________\n",
      "dropout_321 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_160 (TimeDi (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6771 - val_loss: 0.8429\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6763 - val_loss: 0.8413\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.6754 - val_loss: 0.8396\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6743 - val_loss: 0.8378\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.6733 - val_loss: 0.8357\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 109s 445ms/step - loss: 0.6721 - val_loss: 0.8335\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.6709 - val_loss: 0.8312\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6695 - val_loss: 0.8288\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6682 - val_loss: 0.8262\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 108s 445ms/step - loss: 0.6668 - val_loss: 0.8235\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6653 - val_loss: 0.8206\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6638 - val_loss: 0.8177\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6622 - val_loss: 0.8146\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6606 - val_loss: 0.8114\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6590 - val_loss: 0.8081\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6572 - val_loss: 0.8047\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6555 - val_loss: 0.8012\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.6537 - val_loss: 0.7976\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6519 - val_loss: 0.7939\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6501 - val_loss: 0.7902\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6483 - val_loss: 0.7865\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 108s 444ms/step - loss: 0.6466 - val_loss: 0.7827\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.6448 - val_loss: 0.7790\n",
      "Epoch 24/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 108s 442ms/step - loss: 0.6431 - val_loss: 0.7752\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.6413 - val_loss: 0.7714\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.6396 - val_loss: 0.7676\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.6378 - val_loss: 0.7638\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.6361 - val_loss: 0.7599\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.6343 - val_loss: 0.7559\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 107s 441ms/step - loss: 0.6326 - val_loss: 0.7519\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.6307 - val_loss: 0.7478\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.6289 - val_loss: 0.7436\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.6270 - val_loss: 0.7393\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.6251 - val_loss: 0.7349\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.6232 - val_loss: 0.7304\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.6212 - val_loss: 0.7258\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.6193 - val_loss: 0.7211\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.6172 - val_loss: 0.7162\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.6152 - val_loss: 0.7112\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.6131 - val_loss: 0.7062\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.6110 - val_loss: 0.7010\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.6089 - val_loss: 0.6957\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.6067 - val_loss: 0.6903\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 107s 437ms/step - loss: 0.6046 - val_loss: 0.6849\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.6024 - val_loss: 0.6794\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.6004 - val_loss: 0.6739\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5984 - val_loss: 0.6683\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 107s 437ms/step - loss: 0.5962 - val_loss: 0.6628\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 106s 435ms/step - loss: 0.5942 - val_loss: 0.6572\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5922 - val_loss: 0.6516\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5902 - val_loss: 0.6460\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5884 - val_loss: 0.6403\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 106s 436ms/step - loss: 0.5864 - val_loss: 0.6346\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 106s 435ms/step - loss: 0.5846 - val_loss: 0.6289\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 107s 438ms/step - loss: 0.5827 - val_loss: 0.6232\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 106s 435ms/step - loss: 0.5808 - val_loss: 0.6174\n",
      "Execution time:  6060.734648704529\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6566\n",
      "Root Mean Square Error: 1.0724\n",
      "Mean Square Error: 1.1500\n",
      "\n",
      "Train RMSE: 1.072\n",
      "Train MSE: 1.150\n",
      "Train MAE: 0.657\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_161\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_154 (GRU)                (None, 1008, 45)          6480      \n",
      "_________________________________________________________________\n",
      "dropout_322 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "gru_155 (GRU)                (None, 1008, 45)          12420     \n",
      "_________________________________________________________________\n",
      "dropout_323 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_161 (TimeDi (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 64s 805ms/step - loss: 0.7000 - val_loss: 0.6744\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.6997 - val_loss: 0.6741\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 66s 821ms/step - loss: 0.6995 - val_loss: 0.6738\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.6993 - val_loss: 0.6735\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 66s 819ms/step - loss: 0.6991 - val_loss: 0.6732\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 66s 821ms/step - loss: 0.6988 - val_loss: 0.6729\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 66s 819ms/step - loss: 0.6985 - val_loss: 0.6725\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.6983 - val_loss: 0.6721\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 65s 817ms/step - loss: 0.6980 - val_loss: 0.6718\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 66s 821ms/step - loss: 0.6977 - val_loss: 0.6714\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 65s 818ms/step - loss: 0.6974 - val_loss: 0.6710\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.6971 - val_loss: 0.6705\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 65s 818ms/step - loss: 0.6968 - val_loss: 0.6701\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 65s 818ms/step - loss: 0.6965 - val_loss: 0.6697\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 66s 819ms/step - loss: 0.6962 - val_loss: 0.6692\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 65s 816ms/step - loss: 0.6958 - val_loss: 0.6687\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 65s 816ms/step - loss: 0.6956 - val_loss: 0.6683\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.6952 - val_loss: 0.6678\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 66s 824ms/step - loss: 0.6949 - val_loss: 0.6673\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 63s 793ms/step - loss: 0.6946 - val_loss: 0.6668\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 63s 793ms/step - loss: 0.6942 - val_loss: 0.6663\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 63s 792ms/step - loss: 0.6939 - val_loss: 0.6658\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 64s 794ms/step - loss: 0.6936 - val_loss: 0.6652\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 63s 790ms/step - loss: 0.6932 - val_loss: 0.6647\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 64s 795ms/step - loss: 0.6928 - val_loss: 0.6641\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 63s 791ms/step - loss: 0.6925 - val_loss: 0.6636\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 63s 790ms/step - loss: 0.6921 - val_loss: 0.6630\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 64s 795ms/step - loss: 0.6918 - val_loss: 0.6625\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 64s 794ms/step - loss: 0.6914 - val_loss: 0.6619\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 63s 791ms/step - loss: 0.6910 - val_loss: 0.6613\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 63s 788ms/step - loss: 0.6907 - val_loss: 0.6607\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 63s 791ms/step - loss: 0.6903 - val_loss: 0.6601\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 63s 794ms/step - loss: 0.6900 - val_loss: 0.6595\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 63s 791ms/step - loss: 0.6896 - val_loss: 0.6589\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 63s 787ms/step - loss: 0.6892 - val_loss: 0.6583\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 63s 791ms/step - loss: 0.6888 - val_loss: 0.6577\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 63s 786ms/step - loss: 0.6884 - val_loss: 0.6571\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 63s 788ms/step - loss: 0.6881 - val_loss: 0.6564\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 63s 785ms/step - loss: 0.6877 - val_loss: 0.6558\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 63s 786ms/step - loss: 0.6873 - val_loss: 0.6552\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 63s 790ms/step - loss: 0.6869 - val_loss: 0.6545\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 63s 789ms/step - loss: 0.6866 - val_loss: 0.6539\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 63s 788ms/step - loss: 0.6862 - val_loss: 0.6532\n",
      "Execution time:  2801.0457088947296\n",
      "GRU:\n",
      "Mean Absolute Error: 0.7071\n",
      "Root Mean Square Error: 1.0438\n",
      "Mean Square Error: 1.0895\n",
      "\n",
      "Train RMSE: 1.044\n",
      "Train MSE: 1.089\n",
      "Train MAE: 0.707\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_162\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_156 (GRU)                (None, 1008, 43)          5934      \n",
      "_________________________________________________________________\n",
      "dropout_324 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "gru_157 (GRU)                (None, 1008, 43)          11352     \n",
      "_________________________________________________________________\n",
      "dropout_325 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_162 (TimeDi (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 104s 427ms/step - loss: 0.9816 - val_loss: 1.3307\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9814 - val_loss: 1.3303\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9811 - val_loss: 1.3299\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9808 - val_loss: 1.3295\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9805 - val_loss: 1.3291\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9802 - val_loss: 1.3286\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9798 - val_loss: 1.3281\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9795 - val_loss: 1.3275\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 103s 421ms/step - loss: 0.9791 - val_loss: 1.3270\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9787 - val_loss: 1.3264\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9783 - val_loss: 1.3257\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9778 - val_loss: 1.3251\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9774 - val_loss: 1.3244\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9769 - val_loss: 1.3237\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9764 - val_loss: 1.3229\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9759 - val_loss: 1.3222\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9754 - val_loss: 1.3213\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9748 - val_loss: 1.3205\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9742 - val_loss: 1.3196\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9737 - val_loss: 1.3187\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9730 - val_loss: 1.3178\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9724 - val_loss: 1.3168\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9717 - val_loss: 1.3157\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9710 - val_loss: 1.3147\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9703 - val_loss: 1.3135\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9695 - val_loss: 1.3124\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9687 - val_loss: 1.3111\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 104s 424ms/step - loss: 0.9679 - val_loss: 1.3098\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9671 - val_loss: 1.3085\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9662 - val_loss: 1.3071\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9652 - val_loss: 1.3057\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9642 - val_loss: 1.3041\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9632 - val_loss: 1.3025\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9622 - val_loss: 1.3009\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9610 - val_loss: 1.2991\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9598 - val_loss: 1.2973\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 104s 426ms/step - loss: 0.9586 - val_loss: 1.2954\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9573 - val_loss: 1.2934\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9560 - val_loss: 1.2913\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9546 - val_loss: 1.2892\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9531 - val_loss: 1.2869\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 103s 420ms/step - loss: 0.9516 - val_loss: 1.2845\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9500 - val_loss: 1.2820\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9484 - val_loss: 1.2794\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9466 - val_loss: 1.2767\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 104s 425ms/step - loss: 0.9449 - val_loss: 1.2739\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 103s 424ms/step - loss: 0.9430 - val_loss: 1.2709\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9410 - val_loss: 1.2678\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9390 - val_loss: 1.2646\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9368 - val_loss: 1.2612\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9346 - val_loss: 1.2577\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9323 - val_loss: 1.2540\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 103s 422ms/step - loss: 0.9299 - val_loss: 1.2501\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9273 - val_loss: 1.2460\n",
      "Epoch 55/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 104s 424ms/step - loss: 0.9246 - val_loss: 1.2418\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 103s 423ms/step - loss: 0.9218 - val_loss: 1.2373\n",
      "Execution time:  5809.358271121979\n",
      "GRU:\n",
      "Mean Absolute Error: 0.8663\n",
      "Root Mean Square Error: 1.0861\n",
      "Mean Square Error: 1.1796\n",
      "\n",
      "Train RMSE: 1.086\n",
      "Train MSE: 1.180\n",
      "Train MAE: 0.866\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_163\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_158 (GRU)                (None, 1008, 45)          6480      \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "gru_159 (GRU)                (None, 1008, 45)          12420     \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_163 (TimeDi (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 63s 783ms/step - loss: 0.9813 - val_loss: 1.1606\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9812 - val_loss: 1.1605\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 66s 831ms/step - loss: 0.9811 - val_loss: 1.1604\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9810 - val_loss: 1.1602\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 66s 824ms/step - loss: 0.9809 - val_loss: 1.1601\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9808 - val_loss: 1.1600\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9807 - val_loss: 1.1598\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 66s 824ms/step - loss: 0.9806 - val_loss: 1.1597\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.9805 - val_loss: 1.1595\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 66s 829ms/step - loss: 0.9804 - val_loss: 1.1594\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 66s 830ms/step - loss: 0.9803 - val_loss: 1.1592\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 66s 829ms/step - loss: 0.9801 - val_loss: 1.1591\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 66s 825ms/step - loss: 0.9800 - val_loss: 1.1589\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 66s 831ms/step - loss: 0.9799 - val_loss: 1.1587\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 66s 830ms/step - loss: 0.9797 - val_loss: 1.1585\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 66s 828ms/step - loss: 0.9796 - val_loss: 1.1584\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9795 - val_loss: 1.1582\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 66s 823ms/step - loss: 0.9793 - val_loss: 1.1580\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 66s 828ms/step - loss: 0.9792 - val_loss: 1.1578\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9791 - val_loss: 1.1576\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9789 - val_loss: 1.1574\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 66s 827ms/step - loss: 0.9788 - val_loss: 1.1572\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 66s 824ms/step - loss: 0.9786 - val_loss: 1.1570\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 66s 830ms/step - loss: 0.9785 - val_loss: 1.1568\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 66s 823ms/step - loss: 0.9783 - val_loss: 1.1566\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9782 - val_loss: 1.1563\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9780 - val_loss: 1.1561\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9778 - val_loss: 1.1559\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9777 - val_loss: 1.1557\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 66s 829ms/step - loss: 0.9775 - val_loss: 1.1555\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9773 - val_loss: 1.1552\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 66s 829ms/step - loss: 0.9772 - val_loss: 1.1550\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 66s 830ms/step - loss: 0.9770 - val_loss: 1.1548\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 66s 828ms/step - loss: 0.9768 - val_loss: 1.1545\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 66s 825ms/step - loss: 0.9767 - val_loss: 1.1543\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 66s 827ms/step - loss: 0.9765 - val_loss: 1.1540\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 66s 827ms/step - loss: 0.9763 - val_loss: 1.1538\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9761 - val_loss: 1.1535\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9760 - val_loss: 1.1533\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 66s 823ms/step - loss: 0.9758 - val_loss: 1.1530\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 66s 822ms/step - loss: 0.9756 - val_loss: 1.1528\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 66s 826ms/step - loss: 0.9754 - val_loss: 1.1525\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.9752 - val_loss: 1.1523\n",
      "Execution time:  2878.6958146095276\n",
      "GRU:\n",
      "Mean Absolute Error: 0.9214\n",
      "Root Mean Square Error: 1.1318\n",
      "Mean Square Error: 1.2810\n",
      "\n",
      "Train RMSE: 1.132\n",
      "Train MSE: 1.281\n",
      "Train MAE: 0.921\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_164\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_160 (GRU)                (None, 1008, 43)          5934      \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "gru_161 (GRU)                (None, 1008, 43)          11352     \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_164 (TimeDi (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6177 - val_loss: 0.4614\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5694 - val_loss: 0.4397\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5664 - val_loss: 0.4374\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5655 - val_loss: 0.4357\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5639 - val_loss: 0.4284\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5625 - val_loss: 0.4185\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5611 - val_loss: 0.4090\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5601 - val_loss: 0.4017\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5596 - val_loss: 0.3967\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5590 - val_loss: 0.3932\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5587 - val_loss: 0.3916\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5583 - val_loss: 0.3894\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5581 - val_loss: 0.3890\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5579 - val_loss: 0.3876\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5578 - val_loss: 0.3910\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5576 - val_loss: 0.3921\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5573 - val_loss: 0.3922\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5569 - val_loss: 0.3939\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5568 - val_loss: 0.3962\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5563 - val_loss: 0.3978\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5560 - val_loss: 0.4013\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 108s 445ms/step - loss: 0.5556 - val_loss: 0.4039\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5553 - val_loss: 0.4064\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5547 - val_loss: 0.4116\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 109s 446ms/step - loss: 0.5542 - val_loss: 0.4160\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5538 - val_loss: 0.4225\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5531 - val_loss: 0.4315\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 109s 445ms/step - loss: 0.5522 - val_loss: 0.4493\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5497 - val_loss: 0.4654\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5478 - val_loss: 0.4790\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5459 - val_loss: 0.4903\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5432 - val_loss: 0.5018\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5412 - val_loss: 0.5032\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5399 - val_loss: 0.5035\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5393 - val_loss: 0.5054\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5388 - val_loss: 0.5042\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5383 - val_loss: 0.5076\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5381 - val_loss: 0.5008\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5376 - val_loss: 0.4961\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5373 - val_loss: 0.4971\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5369 - val_loss: 0.4928\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5366 - val_loss: 0.4959\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5360 - val_loss: 0.4925\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 107s 439ms/step - loss: 0.5357 - val_loss: 0.4923\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 108s 443ms/step - loss: 0.5352 - val_loss: 0.4840\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5351 - val_loss: 0.4831\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5346 - val_loss: 0.4771\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5344 - val_loss: 0.4757\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5341 - val_loss: 0.4743\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5339 - val_loss: 0.4743\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5337 - val_loss: 0.4748\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 107s 441ms/step - loss: 0.5336 - val_loss: 0.4733\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 108s 442ms/step - loss: 0.5331 - val_loss: 0.4740\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 108s 441ms/step - loss: 0.5330 - val_loss: 0.4743\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5325 - val_loss: 0.4706\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 107s 440ms/step - loss: 0.5320 - val_loss: 0.4719\n",
      "Execution time:  6066.962269544601\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6738\n",
      "Root Mean Square Error: 1.1399\n",
      "Mean Square Error: 1.2993\n",
      "\n",
      "Train RMSE: 1.140\n",
      "Train MSE: 1.299\n",
      "Train MAE: 0.674\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_165\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_162 (GRU)                (None, 1008, 45)          6480      \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "gru_163 (GRU)                (None, 1008, 45)          12420     \n",
      "_________________________________________________________________\n",
      "dropout_331 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_165 (TimeDi (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 65s 807ms/step - loss: 0.6717 - val_loss: 0.5261\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 67s 837ms/step - loss: 0.6160 - val_loss: 0.4133\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 67s 839ms/step - loss: 0.6093 - val_loss: 0.3988\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 67s 836ms/step - loss: 0.6064 - val_loss: 0.3928\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 67s 834ms/step - loss: 0.6046 - val_loss: 0.3894\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 67s 836ms/step - loss: 0.6032 - val_loss: 0.3874\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 67s 839ms/step - loss: 0.6020 - val_loss: 0.3856\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 67s 834ms/step - loss: 0.6010 - val_loss: 0.3844\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 67s 835ms/step - loss: 0.6001 - val_loss: 0.3837\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 67s 838ms/step - loss: 0.5993 - val_loss: 0.3828\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 66s 830ms/step - loss: 0.5986 - val_loss: 0.3822\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 68s 844ms/step - loss: 0.5979 - val_loss: 0.3815\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 67s 838ms/step - loss: 0.5972 - val_loss: 0.3812\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 67s 835ms/step - loss: 0.5965 - val_loss: 0.3807\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 68s 844ms/step - loss: 0.5959 - val_loss: 0.3804\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 67s 839ms/step - loss: 0.5954 - val_loss: 0.3798\n",
      "Epoch 17/43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 67s 835ms/step - loss: 0.5949 - val_loss: 0.3797\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 67s 833ms/step - loss: 0.5942 - val_loss: 0.3791\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 67s 834ms/step - loss: 0.5937 - val_loss: 0.3790\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 67s 835ms/step - loss: 0.5932 - val_loss: 0.3787\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 67s 834ms/step - loss: 0.5927 - val_loss: 0.3787\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 67s 832ms/step - loss: 0.5922 - val_loss: 0.3788\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 67s 834ms/step - loss: 0.5917 - val_loss: 0.3789\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 67s 838ms/step - loss: 0.5911 - val_loss: 0.3794\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 67s 840ms/step - loss: 0.5905 - val_loss: 0.3799\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 67s 838ms/step - loss: 0.5900 - val_loss: 0.3806\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 67s 835ms/step - loss: 0.5892 - val_loss: 0.3814\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 67s 835ms/step - loss: 0.5887 - val_loss: 0.3822\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 67s 836ms/step - loss: 0.5875 - val_loss: 0.3836\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 68s 844ms/step - loss: 0.5866 - val_loss: 0.3840\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 67s 833ms/step - loss: 0.5842 - val_loss: 0.3841\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 67s 832ms/step - loss: 0.5801 - val_loss: 0.3858\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 67s 835ms/step - loss: 0.5738 - val_loss: 0.3954\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 67s 839ms/step - loss: 0.5720 - val_loss: 0.4051\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 67s 834ms/step - loss: 0.5715 - val_loss: 0.4072\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 67s 840ms/step - loss: 0.5706 - val_loss: 0.4089\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 67s 838ms/step - loss: 0.5699 - val_loss: 0.4101\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 67s 842ms/step - loss: 0.5691 - val_loss: 0.4110\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 67s 837ms/step - loss: 0.5685 - val_loss: 0.4127\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 67s 839ms/step - loss: 0.5680 - val_loss: 0.4129\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 67s 838ms/step - loss: 0.5676 - val_loss: 0.4133\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 67s 837ms/step - loss: 0.5672 - val_loss: 0.4137\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 67s 833ms/step - loss: 0.5668 - val_loss: 0.4148\n",
      "Execution time:  2915.387079000473\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6780\n",
      "Root Mean Square Error: 1.1594\n",
      "Mean Square Error: 1.3442\n",
      "\n",
      "Train RMSE: 1.159\n",
      "Train MSE: 1.344\n",
      "Train MAE: 0.678\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  43\n",
      "dropout1:  0.40519643149940265\n",
      "dropout2:  0.3312343747027119\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 56\n",
      "batchsize: 11\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_166\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_164 (GRU)                (None, 1008, 43)          5934      \n",
      "_________________________________________________________________\n",
      "dropout_332 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "gru_165 (GRU)                (None, 1008, 43)          11352     \n",
      "_________________________________________________________________\n",
      "dropout_333 (Dropout)        (None, 1008, 43)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_166 (TimeDi (None, 1008, 1)           44        \n",
      "=================================================================\n",
      "Total params: 17,330\n",
      "Trainable params: 17,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/56\n",
      "244/244 [==============================] - 111s 456ms/step - loss: 0.8250 - val_loss: 0.8497\n",
      "Epoch 2/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6841 - val_loss: 0.8401\n",
      "Epoch 3/56\n",
      "244/244 [==============================] - 111s 453ms/step - loss: 0.6813 - val_loss: 0.8375\n",
      "Epoch 4/56\n",
      "244/244 [==============================] - 111s 454ms/step - loss: 0.6802 - val_loss: 0.8364\n",
      "Epoch 5/56\n",
      "244/244 [==============================] - 110s 453ms/step - loss: 0.6797 - val_loss: 0.8357\n",
      "Epoch 6/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6794 - val_loss: 0.8353\n",
      "Epoch 7/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6791 - val_loss: 0.8350\n",
      "Epoch 8/56\n",
      "244/244 [==============================] - 110s 452ms/step - loss: 0.6790 - val_loss: 0.8348\n",
      "Epoch 9/56\n",
      "244/244 [==============================] - 110s 449ms/step - loss: 0.6788 - val_loss: 0.8346\n",
      "Epoch 10/56\n",
      "244/244 [==============================] - 110s 452ms/step - loss: 0.6788 - val_loss: 0.8345\n",
      "Epoch 11/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6787 - val_loss: 0.8344\n",
      "Epoch 12/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6786 - val_loss: 0.8344\n",
      "Epoch 13/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 14/56\n",
      "244/244 [==============================] - 110s 452ms/step - loss: 0.6786 - val_loss: 0.8343\n",
      "Epoch 15/56\n",
      "244/244 [==============================] - 110s 452ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 16/56\n",
      "244/244 [==============================] - 111s 453ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 17/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 18/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 19/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6785 - val_loss: 0.8342\n",
      "Epoch 20/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 21/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 22/56\n",
      "244/244 [==============================] - 110s 452ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 23/56\n",
      "244/244 [==============================] - 115s 470ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 24/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 25/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 26/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 27/56\n",
      "244/244 [==============================] - 110s 449ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 28/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 29/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 30/56\n",
      "244/244 [==============================] - 110s 451ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 31/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 32/56\n",
      "244/244 [==============================] - 110s 449ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 33/56\n",
      "244/244 [==============================] - 109s 448ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 34/56\n",
      "244/244 [==============================] - 110s 452ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 35/56\n",
      "244/244 [==============================] - 110s 449ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 36/56\n",
      "244/244 [==============================] - 110s 449ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 37/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 38/56\n",
      "244/244 [==============================] - 110s 449ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 39/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 40/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 41/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 42/56\n",
      "244/244 [==============================] - 109s 449ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 43/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 44/56\n",
      "244/244 [==============================] - 109s 448ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 45/56\n",
      "244/244 [==============================] - 109s 448ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 46/56\n",
      "244/244 [==============================] - 110s 450ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 47/56\n",
      "244/244 [==============================] - 109s 448ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 48/56\n",
      "244/244 [==============================] - 109s 446ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 49/56\n",
      "244/244 [==============================] - 109s 446ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 50/56\n",
      "244/244 [==============================] - 109s 446ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 51/56\n",
      "244/244 [==============================] - 109s 445ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 52/56\n",
      "244/244 [==============================] - 109s 446ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 53/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 54/56\n",
      "244/244 [==============================] - 109s 446ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 55/56\n",
      "244/244 [==============================] - 109s 447ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Epoch 56/56\n",
      "244/244 [==============================] - 109s 448ms/step - loss: 0.6785 - val_loss: 0.8341\n",
      "Execution time:  6175.698762893677\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6954\n",
      "Root Mean Square Error: 1.0192\n",
      "Mean Square Error: 1.0387\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.695\n",
      "###########################\n",
      "\n",
      "MODEL:  GRU\n",
      "sequence:  7d\n",
      "units:  45\n",
      "dropout1:  0.11814836227952394\n",
      "dropout2:  0.24325404382648977\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 43\n",
      "batchsize: 30\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_167\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_166 (GRU)                (None, 1008, 45)          6480      \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "gru_167 (GRU)                (None, 1008, 45)          12420     \n",
      "_________________________________________________________________\n",
      "dropout_335 (Dropout)        (None, 1008, 45)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_167 (TimeDi (None, 1008, 1)           46        \n",
      "=================================================================\n",
      "Total params: 18,946\n",
      "Trainable params: 18,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/43\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.9498 - val_loss: 0.9685\n",
      "Epoch 2/43\n",
      "80/80 [==============================] - 64s 799ms/step - loss: 0.7420 - val_loss: 0.6900\n",
      "Epoch 3/43\n",
      "80/80 [==============================] - 64s 802ms/step - loss: 0.7101 - val_loss: 0.6803\n",
      "Epoch 4/43\n",
      "80/80 [==============================] - 64s 805ms/step - loss: 0.7068 - val_loss: 0.6763\n",
      "Epoch 5/43\n",
      "80/80 [==============================] - 64s 805ms/step - loss: 0.7052 - val_loss: 0.6741\n",
      "Epoch 6/43\n",
      "80/80 [==============================] - 65s 807ms/step - loss: 0.7043 - val_loss: 0.6726\n",
      "Epoch 7/43\n",
      "80/80 [==============================] - 64s 804ms/step - loss: 0.7037 - val_loss: 0.6716\n",
      "Epoch 8/43\n",
      "80/80 [==============================] - 64s 803ms/step - loss: 0.7032 - val_loss: 0.6709\n",
      "Epoch 9/43\n",
      "80/80 [==============================] - 65s 809ms/step - loss: 0.7029 - val_loss: 0.6704\n",
      "Epoch 10/43\n",
      "80/80 [==============================] - 64s 802ms/step - loss: 0.7026 - val_loss: 0.6699\n",
      "Epoch 11/43\n",
      "80/80 [==============================] - 64s 802ms/step - loss: 0.7024 - val_loss: 0.6696\n",
      "Epoch 12/43\n",
      "80/80 [==============================] - 64s 803ms/step - loss: 0.7022 - val_loss: 0.6693\n",
      "Epoch 13/43\n",
      "80/80 [==============================] - 64s 805ms/step - loss: 0.7020 - val_loss: 0.6690\n",
      "Epoch 14/43\n",
      "80/80 [==============================] - 65s 807ms/step - loss: 0.7019 - val_loss: 0.6688\n",
      "Epoch 15/43\n",
      "80/80 [==============================] - 65s 809ms/step - loss: 0.7018 - val_loss: 0.6686\n",
      "Epoch 16/43\n",
      "80/80 [==============================] - 65s 807ms/step - loss: 0.7017 - val_loss: 0.6685\n",
      "Epoch 17/43\n",
      "80/80 [==============================] - 64s 805ms/step - loss: 0.7016 - val_loss: 0.6683\n",
      "Epoch 18/43\n",
      "80/80 [==============================] - 65s 811ms/step - loss: 0.7016 - val_loss: 0.6682\n",
      "Epoch 19/43\n",
      "80/80 [==============================] - 65s 810ms/step - loss: 0.7015 - val_loss: 0.6681\n",
      "Epoch 20/43\n",
      "80/80 [==============================] - 65s 807ms/step - loss: 0.7014 - val_loss: 0.6680\n",
      "Epoch 21/43\n",
      "80/80 [==============================] - 64s 803ms/step - loss: 0.7014 - val_loss: 0.6679\n",
      "Epoch 22/43\n",
      "80/80 [==============================] - 65s 814ms/step - loss: 0.7013 - val_loss: 0.6679\n",
      "Epoch 23/43\n",
      "80/80 [==============================] - 65s 811ms/step - loss: 0.7013 - val_loss: 0.6678\n",
      "Epoch 24/43\n",
      "80/80 [==============================] - 64s 805ms/step - loss: 0.7013 - val_loss: 0.6677\n",
      "Epoch 25/43\n",
      "80/80 [==============================] - 65s 813ms/step - loss: 0.7012 - val_loss: 0.6677\n",
      "Epoch 26/43\n",
      "80/80 [==============================] - 65s 813ms/step - loss: 0.7012 - val_loss: 0.6676\n",
      "Epoch 27/43\n",
      "80/80 [==============================] - 65s 810ms/step - loss: 0.7012 - val_loss: 0.6676\n",
      "Epoch 28/43\n",
      "80/80 [==============================] - 65s 811ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 29/43\n",
      "80/80 [==============================] - 65s 811ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 30/43\n",
      "80/80 [==============================] - 65s 815ms/step - loss: 0.7011 - val_loss: 0.6675\n",
      "Epoch 31/43\n",
      "80/80 [==============================] - 65s 810ms/step - loss: 0.7011 - val_loss: 0.6674\n",
      "Epoch 32/43\n",
      "80/80 [==============================] - 65s 811ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 33/43\n",
      "80/80 [==============================] - 65s 813ms/step - loss: 0.7010 - val_loss: 0.6674\n",
      "Epoch 34/43\n",
      "80/80 [==============================] - 65s 813ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 35/43\n",
      "80/80 [==============================] - 65s 814ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 36/43\n",
      "80/80 [==============================] - 65s 814ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 37/43\n",
      "80/80 [==============================] - 65s 811ms/step - loss: 0.7010 - val_loss: 0.6673\n",
      "Epoch 38/43\n",
      "80/80 [==============================] - 65s 810ms/step - loss: 0.7010 - val_loss: 0.6672\n",
      "Epoch 39/43\n",
      "80/80 [==============================] - 65s 817ms/step - loss: 0.7010 - val_loss: 0.6672\n",
      "Epoch 40/43\n",
      "80/80 [==============================] - 65s 810ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 41/43\n",
      "80/80 [==============================] - 65s 807ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 42/43\n",
      "80/80 [==============================] - 65s 811ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Epoch 43/43\n",
      "80/80 [==============================] - 65s 819ms/step - loss: 0.7009 - val_loss: 0.6672\n",
      "Execution time:  2818.140313386917\n",
      "GRU:\n",
      "Mean Absolute Error: 0.6955\n",
      "Root Mean Square Error: 1.0192\n",
      "Mean Square Error: 1.0388\n",
      "\n",
      "Train RMSE: 1.019\n",
      "Train MSE: 1.039\n",
      "Train MAE: 0.695\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _model(cmodel,units,activationDense,dropout1,dropout2,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(cmodel(units = units, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(rate=dropout1))\n",
    "    model.add(cmodel(units = units, return_sequences=True))\n",
    "    model.add(Dropout(rate=dropout2))\n",
    "    model.add(TimeDistributed(Dense(1,kernel_initializer='normal',activation=activationDense)))\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "models = [LSTM,GRU]\n",
    "nmodels = [\"LSTM\",\"GRU\"]\n",
    "sequences = [\"1h\",\"3h\",\"6h\",\"12h\",\"1d\", \"3d\", \"7d\"]\n",
    "X_trains = [X_train1h,X_train3h, X_train6h, X_train12h,X_train1d,X_train3d, X_train7d]\n",
    "y_trains= [y_train1h,y_train3h, y_train6h,y_train12h,y_train1d,y_train3d, y_train7d]\n",
    "activations = ['relu']\n",
    "activationsDense = ['tanh','sigmoid']\n",
    "optimizers = ['adam','adadelta','adamax']\n",
    "list_validationSplit = [0.1,0.2]\n",
    "list_dropout1 =  np.random.uniform(0.1,0.8,5)   \n",
    "list_dropout2 =  np.random.uniform(0.1,0.8,5)   \n",
    "list_units = np.random.randint(6,high=100, size=5) \n",
    "list_epochs = np.random.randint(5,high=100, size=5)  \n",
    "list_batchsize = np.random.randint(6,high=64, size=5)    \n",
    "\n",
    "list_results = pd.DataFrame()\n",
    "\n",
    "for cmodel,nmodel in zip(models,nmodels):\n",
    "    for X_train, y_train,sequence in zip(X_trains,y_trains,sequences):\n",
    "        for optimizer in optimizers:\n",
    "            #for activation in activations:\n",
    "                for activationDense in activationsDense:\n",
    "                    for units,epochs,batchsize,dropout1,dropout2,validationsplit in zip(list_units,list_epochs,list_batchsize,list_dropout1,list_dropout2,list_validationSplit): \n",
    "                        start = time()\n",
    "                        print(\"###########################\\n\")\n",
    "                        print(\"MODEL: \", nmodel)\n",
    "                        print('sequence: ',sequence)\n",
    "                        print('units: ',units)\n",
    "                        print('dropout1: ',dropout1)\n",
    "                        print('dropout2: ',dropout2)\n",
    "                        print('optimizer:',optimizer)\n",
    "                        #print('activation:',activation)\n",
    "                        print('activationDense:',activationDense)\n",
    "                        print('epochs:',epochs)\n",
    "                        print('batchsize:',batchsize)\n",
    "                        print('validation_split:',validationsplit)\n",
    "                    \n",
    "                        model = _model(cmodel,units,activationDense,dropout1,dropout2,optimizer)\n",
    "                        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batchsize, validation_split=validationsplit, shuffle=False)\n",
    "                        end = time()\n",
    "                        totalTime = end-start\n",
    "                        print ('Execution time: ',totalTime)\n",
    "\n",
    "                        import matplotlib.pyplot as plt\n",
    "                        plt.plot(history.history['loss'], label='Training loss')\n",
    "                        plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "                        plt.legend();\n",
    "\n",
    "                        X_train_pred = model.predict(X_train, verbose=0)\n",
    "                        \n",
    "                        mae,rmse,mse = evaluate_prediction(X_train_pred, X_train,nmodel)\n",
    "                        \n",
    "                        print('Train RMSE: %.3f' % rmse);\n",
    "                        print('Train MSE: %.3f' % mse);\n",
    "                        print('Train MAE: %.3f' % mae);\n",
    "                        \n",
    "                        result = pd.DataFrame({\n",
    "                                               #'activation':[activation],\n",
    "                                               'model':[nmodel],\n",
    "                                               'sequence':[sequence],\n",
    "                                               'activationDense':[activationDense],\n",
    "                                               'optimizer':[optimizer],\n",
    "                                               'dropout1':[dropout1],\n",
    "                                               'dropout2':[dropout2],\n",
    "                                               'units':[units],\n",
    "                                               'epochs':[epochs],\n",
    "                                               'batchsize':[batchsize],\n",
    "                                               'validation_split':[validationsplit],\n",
    "                                               \n",
    "                                               'RMSE':[rmse],\n",
    "                                               'MSE':[mse],\n",
    "                                               'MAE':[mae],                            \n",
    "                                               'Time':[totalTime]})\n",
    "                        list_results = list_results.append(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_results.to_csv(\"resultats-cerca-optim-lstm-dues-capes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sequence</th>\n",
       "      <th>activationDense</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout1</th>\n",
       "      <th>dropout2</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.574725</td>\n",
       "      <td>0.330309</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>92.219330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.573424</td>\n",
       "      <td>0.328815</td>\n",
       "      <td>0.155201</td>\n",
       "      <td>40.125178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.750276</td>\n",
       "      <td>0.562914</td>\n",
       "      <td>0.490691</td>\n",
       "      <td>89.099901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.751313</td>\n",
       "      <td>0.564471</td>\n",
       "      <td>0.492566</td>\n",
       "      <td>41.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.961165</td>\n",
       "      <td>0.923839</td>\n",
       "      <td>0.660268</td>\n",
       "      <td>88.287473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.990478</td>\n",
       "      <td>0.981046</td>\n",
       "      <td>0.704085</td>\n",
       "      <td>41.589166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.103773</td>\n",
       "      <td>1.218315</td>\n",
       "      <td>0.915444</td>\n",
       "      <td>90.688455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.113190</td>\n",
       "      <td>1.239192</td>\n",
       "      <td>0.926062</td>\n",
       "      <td>41.509876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.574382</td>\n",
       "      <td>0.329915</td>\n",
       "      <td>0.162329</td>\n",
       "      <td>91.402002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.575712</td>\n",
       "      <td>0.331444</td>\n",
       "      <td>0.163877</td>\n",
       "      <td>41.700037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.748386</td>\n",
       "      <td>0.560081</td>\n",
       "      <td>0.489979</td>\n",
       "      <td>90.458614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.749006</td>\n",
       "      <td>0.561010</td>\n",
       "      <td>0.490869</td>\n",
       "      <td>41.399106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.606701</td>\n",
       "      <td>0.368086</td>\n",
       "      <td>0.186657</td>\n",
       "      <td>188.618612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.598933</td>\n",
       "      <td>0.358721</td>\n",
       "      <td>0.180815</td>\n",
       "      <td>79.699731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.769110</td>\n",
       "      <td>0.591530</td>\n",
       "      <td>0.509607</td>\n",
       "      <td>185.335225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.763945</td>\n",
       "      <td>0.583612</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>80.900866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.873589</td>\n",
       "      <td>0.763159</td>\n",
       "      <td>0.535442</td>\n",
       "      <td>187.653540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.935514</td>\n",
       "      <td>0.875186</td>\n",
       "      <td>0.647101</td>\n",
       "      <td>81.965687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.074432</td>\n",
       "      <td>1.154405</td>\n",
       "      <td>0.884578</td>\n",
       "      <td>179.522077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.115584</td>\n",
       "      <td>1.244527</td>\n",
       "      <td>0.928211</td>\n",
       "      <td>79.244838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model sequence activationDense optimizer  dropout1  dropout2  units  epochs  \\\n",
       "0  LSTM       1h            tanh      adam  0.405196  0.331234     43      56   \n",
       "0  LSTM       1h            tanh      adam  0.118148  0.243254     45      43   \n",
       "0  LSTM       1h         sigmoid      adam  0.405196  0.331234     43      56   \n",
       "0  LSTM       1h         sigmoid      adam  0.118148  0.243254     45      43   \n",
       "0  LSTM       1h            tanh  adadelta  0.405196  0.331234     43      56   \n",
       "0  LSTM       1h            tanh  adadelta  0.118148  0.243254     45      43   \n",
       "0  LSTM       1h         sigmoid  adadelta  0.405196  0.331234     43      56   \n",
       "0  LSTM       1h         sigmoid  adadelta  0.118148  0.243254     45      43   \n",
       "0  LSTM       1h            tanh    adamax  0.405196  0.331234     43      56   \n",
       "0  LSTM       1h            tanh    adamax  0.118148  0.243254     45      43   \n",
       "0  LSTM       1h         sigmoid    adamax  0.405196  0.331234     43      56   \n",
       "0  LSTM       1h         sigmoid    adamax  0.118148  0.243254     45      43   \n",
       "0  LSTM       3h            tanh      adam  0.405196  0.331234     43      56   \n",
       "0  LSTM       3h            tanh      adam  0.118148  0.243254     45      43   \n",
       "0  LSTM       3h         sigmoid      adam  0.405196  0.331234     43      56   \n",
       "0  LSTM       3h         sigmoid      adam  0.118148  0.243254     45      43   \n",
       "0  LSTM       3h            tanh  adadelta  0.405196  0.331234     43      56   \n",
       "0  LSTM       3h            tanh  adadelta  0.118148  0.243254     45      43   \n",
       "0  LSTM       3h         sigmoid  adadelta  0.405196  0.331234     43      56   \n",
       "0  LSTM       3h         sigmoid  adadelta  0.118148  0.243254     45      43   \n",
       "\n",
       "   batchsize  validation_split      RMSE       MSE       MAE        Time  \n",
       "0         11               0.1  0.574725  0.330309  0.165477   92.219330  \n",
       "0         30               0.2  0.573424  0.328815  0.155201   40.125178  \n",
       "0         11               0.1  0.750276  0.562914  0.490691   89.099901  \n",
       "0         30               0.2  0.751313  0.564471  0.492566   41.000820  \n",
       "0         11               0.1  0.961165  0.923839  0.660268   88.287473  \n",
       "0         30               0.2  0.990478  0.981046  0.704085   41.589166  \n",
       "0         11               0.1  1.103773  1.218315  0.915444   90.688455  \n",
       "0         30               0.2  1.113190  1.239192  0.926062   41.509876  \n",
       "0         11               0.1  0.574382  0.329915  0.162329   91.402002  \n",
       "0         30               0.2  0.575712  0.331444  0.163877   41.700037  \n",
       "0         11               0.1  0.748386  0.560081  0.489979   90.458614  \n",
       "0         30               0.2  0.749006  0.561010  0.490869   41.399106  \n",
       "0         11               0.1  0.606701  0.368086  0.186657  188.618612  \n",
       "0         30               0.2  0.598933  0.358721  0.180815   79.699731  \n",
       "0         11               0.1  0.769110  0.591530  0.509607  185.335225  \n",
       "0         30               0.2  0.763945  0.583612  0.506826   80.900866  \n",
       "0         11               0.1  0.873589  0.763159  0.535442  187.653540  \n",
       "0         30               0.2  0.935514  0.875186  0.647101   81.965687  \n",
       "0         11               0.1  1.074432  1.154405  0.884578  179.522077  \n",
       "0         30               0.2  1.115584  1.244527  0.928211   79.244838  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sequence</th>\n",
       "      <th>activationDense</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout1</th>\n",
       "      <th>dropout2</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.573424</td>\n",
       "      <td>0.328815</td>\n",
       "      <td>0.155201</td>\n",
       "      <td>40.125178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.573611</td>\n",
       "      <td>0.329030</td>\n",
       "      <td>0.157387</td>\n",
       "      <td>43.191447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.574382</td>\n",
       "      <td>0.329915</td>\n",
       "      <td>0.162329</td>\n",
       "      <td>91.402002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.574710</td>\n",
       "      <td>0.330291</td>\n",
       "      <td>0.169553</td>\n",
       "      <td>84.436389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.574725</td>\n",
       "      <td>0.330309</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>92.219330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.405196</td>\n",
       "      <td>0.331234</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.150011</td>\n",
       "      <td>1.322525</td>\n",
       "      <td>0.688702</td>\n",
       "      <td>6012.788527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.156215</td>\n",
       "      <td>1.336834</td>\n",
       "      <td>0.672385</td>\n",
       "      <td>2708.811306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.159410</td>\n",
       "      <td>1.344231</td>\n",
       "      <td>0.678044</td>\n",
       "      <td>2915.387079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.177376</td>\n",
       "      <td>1.386213</td>\n",
       "      <td>0.722471</td>\n",
       "      <td>2853.544449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.243254</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.188749</td>\n",
       "      <td>1.413123</td>\n",
       "      <td>0.724854</td>\n",
       "      <td>2701.564011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model sequence activationDense optimizer  dropout1  dropout2  units  \\\n",
       "0   LSTM       1h            tanh      adam  0.118148  0.243254     45   \n",
       "0    GRU       1h            tanh      adam  0.118148  0.243254     45   \n",
       "0   LSTM       1h            tanh    adamax  0.405196  0.331234     43   \n",
       "0    GRU       1h            tanh      adam  0.405196  0.331234     43   \n",
       "0   LSTM       1h            tanh      adam  0.405196  0.331234     43   \n",
       "..   ...      ...             ...       ...       ...       ...    ...   \n",
       "0    GRU       7d            tanh      adam  0.405196  0.331234     43   \n",
       "0   LSTM       7d            tanh      adam  0.118148  0.243254     45   \n",
       "0    GRU       7d            tanh    adamax  0.118148  0.243254     45   \n",
       "0    GRU       7d            tanh      adam  0.118148  0.243254     45   \n",
       "0   LSTM       7d            tanh    adamax  0.118148  0.243254     45   \n",
       "\n",
       "    epochs  batchsize  validation_split      RMSE       MSE       MAE  \\\n",
       "0       43         30               0.2  0.573424  0.328815  0.155201   \n",
       "0       43         30               0.2  0.573611  0.329030  0.157387   \n",
       "0       56         11               0.1  0.574382  0.329915  0.162329   \n",
       "0       56         11               0.1  0.574710  0.330291  0.169553   \n",
       "0       56         11               0.1  0.574725  0.330309  0.165477   \n",
       "..     ...        ...               ...       ...       ...       ...   \n",
       "0       56         11               0.1  1.150011  1.322525  0.688702   \n",
       "0       43         30               0.2  1.156215  1.336834  0.672385   \n",
       "0       43         30               0.2  1.159410  1.344231  0.678044   \n",
       "0       43         30               0.2  1.177376  1.386213  0.722471   \n",
       "0       43         30               0.2  1.188749  1.413123  0.724854   \n",
       "\n",
       "           Time  \n",
       "0     40.125178  \n",
       "0     43.191447  \n",
       "0     91.402002  \n",
       "0     84.436389  \n",
       "0     92.219330  \n",
       "..          ...  \n",
       "0   6012.788527  \n",
       "0   2708.811306  \n",
       "0   2915.387079  \n",
       "0   2853.544449  \n",
       "0   2701.564011  \n",
       "\n",
       "[168 rows x 14 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results.sort_values(by=['RMSE', 'sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PAC3_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
