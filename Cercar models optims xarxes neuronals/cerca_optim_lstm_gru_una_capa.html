<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>cerca_optim_lstm_gru_una_capa</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>




<style type="text/css">
    pre { line-height: 125%; margin: 0; }
td.linenos pre { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }
span.linenos { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }
td.linenos pre.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>



<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/*
 * Webkit scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-corner {
  background: var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-thumb {
  background: rgb(var(--jp-scrollbar-thumb-color));
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-right: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-bottom: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar */

[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-corner,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-corner {
  background-color: transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid transparent;
  border-right: var(--jp-scrollbar-endpad) solid transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid transparent;
  border-bottom: var(--jp-scrollbar-endpad) solid transparent;
}

/*
 * Phosphor
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Widget, /* </DEPRECATED> */
.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  cursor: default;
}


/* <DEPRECATED> */ .p-Widget.p-mod-hidden, /* </DEPRECATED> */
.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-CommandPalette, /* </DEPRECATED> */
.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-CommandPalette-search, /* </DEPRECATED> */
.lm-CommandPalette-search {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-content, /* </DEPRECATED> */
.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-CommandPalette-header, /* </DEPRECATED> */
.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}


/* <DEPRECATED> */ .p-CommandPalette-item, /* </DEPRECATED> */
.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}


/* <DEPRECATED> */ .p-CommandPalette-itemIcon, /* </DEPRECATED> */
.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemContent, /* </DEPRECATED> */
.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}


/* <DEPRECATED> */ .p-CommandPalette-itemShortcut, /* </DEPRECATED> */
.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemLabel, /* </DEPRECATED> */
.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-DockPanel, /* </DEPRECATED> */
.lm-DockPanel {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-widget, /* </DEPRECATED> */
.lm-DockPanel-widget {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-tabBar, /* </DEPRECATED> */
.lm-DockPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-DockPanel-handle, /* </DEPRECATED> */
.lm-DockPanel-handle {
  z-index: 2;
}


/* <DEPRECATED> */ .p-DockPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-DockPanel-handle:after, /* </DEPRECATED> */
.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}


/* <DEPRECATED> */ .p-DockPanel-overlay, /* </DEPRECATED> */
.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}


/* <DEPRECATED> */ .p-DockPanel-overlay.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Menu, /* </DEPRECATED> */
.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-Menu-content, /* </DEPRECATED> */
.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-Menu-item, /* </DEPRECATED> */
.lm-Menu-item {
  display: table-row;
}


/* <DEPRECATED> */
.p-Menu-item.p-mod-hidden,
.p-Menu-item.p-mod-collapsed,
/* </DEPRECATED> */
.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}


/* <DEPRECATED> */
.p-Menu-itemIcon,
.p-Menu-itemSubmenuIcon,
/* </DEPRECATED> */
.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}


/* <DEPRECATED> */ .p-Menu-itemLabel, /* </DEPRECATED> */
.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}


/* <DEPRECATED> */ .p-Menu-itemShortcut, /* </DEPRECATED> */
.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-MenuBar, /* </DEPRECATED> */
.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-MenuBar-content, /* </DEPRECATED> */
.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}


/* <DEPRECATED> */ .p--MenuBar-item, /* </DEPRECATED> */
.lm-MenuBar-item {
  box-sizing: border-box;
}


/* <DEPRECATED> */
.p-MenuBar-itemIcon,
.p-MenuBar-itemLabel,
/* </DEPRECATED> */
.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-ScrollBar, /* </DEPRECATED> */
.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-ScrollBar-button, /* </DEPRECATED> */
.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-track, /* </DEPRECATED> */
.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-thumb, /* </DEPRECATED> */
.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-SplitPanel-child, /* </DEPRECATED> */
.lm-SplitPanel-child {
  z-index: 0;
}


/* <DEPRECATED> */ .p-SplitPanel-handle, /* </DEPRECATED> */
.lm-SplitPanel-handle {
  z-index: 1;
}


/* <DEPRECATED> */ .p-SplitPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-SplitPanel-handle:after, /* </DEPRECATED> */
.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabBar, /* </DEPRECATED> */
.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-TabBar-content, /* </DEPRECATED> */
.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='horizontal'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='vertical'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
}


/* <DEPRECATED> */
.p-TabBar-tabIcon,
.p-TabBar-tabCloseIcon,
/* </DEPRECATED> */
.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-TabBar-tabLabel, /* </DEPRECATED> */
.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}


/* <DEPRECATED> */ .p-TabBar-tab.p-mod-hidden, /* </DEPRECATED> */
.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='horizontal'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='vertical'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabPanel-tabBar, /* </DEPRECATED> */
.lm-TabPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-TabPanel-stackedPanel, /* </DEPRECATED> */
.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

@charset "UTF-8";
/*!

Copyright 2015-present Palantir Technologies, Inc. All rights reserved.
Licensed under the Apache License, Version 2.0.

*/
html{
  -webkit-box-sizing:border-box;
          box-sizing:border-box; }

*,
*::before,
*::after{
  -webkit-box-sizing:inherit;
          box-sizing:inherit; }

body{
  text-transform:none;
  line-height:1.28581;
  letter-spacing:0;
  font-size:14px;
  font-weight:400;
  color:#182026;
  font-family:-apple-system, "BlinkMacSystemFont", "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Open Sans", "Helvetica Neue", "Icons16", sans-serif; }

p{
  margin-top:0;
  margin-bottom:10px; }

small{
  font-size:12px; }

strong{
  font-weight:600; }

::-moz-selection{
  background:rgba(125, 188, 255, 0.6); }

::selection{
  background:rgba(125, 188, 255, 0.6); }
.bp3-heading{
  color:#182026;
  font-weight:600;
  margin:0 0 10px;
  padding:0; }
  .bp3-dark .bp3-heading{
    color:#f5f8fa; }

h1.bp3-heading, .bp3-running-text h1{
  line-height:40px;
  font-size:36px; }

h2.bp3-heading, .bp3-running-text h2{
  line-height:32px;
  font-size:28px; }

h3.bp3-heading, .bp3-running-text h3{
  line-height:25px;
  font-size:22px; }

h4.bp3-heading, .bp3-running-text h4{
  line-height:21px;
  font-size:18px; }

h5.bp3-heading, .bp3-running-text h5{
  line-height:19px;
  font-size:16px; }

h6.bp3-heading, .bp3-running-text h6{
  line-height:16px;
  font-size:14px; }
.bp3-ui-text{
  text-transform:none;
  line-height:1.28581;
  letter-spacing:0;
  font-size:14px;
  font-weight:400; }

.bp3-monospace-text{
  text-transform:none;
  font-family:monospace; }

.bp3-text-muted{
  color:#5c7080; }
  .bp3-dark .bp3-text-muted{
    color:#a7b6c2; }

.bp3-text-disabled{
  color:rgba(92, 112, 128, 0.6); }
  .bp3-dark .bp3-text-disabled{
    color:rgba(167, 182, 194, 0.6); }

.bp3-text-overflow-ellipsis{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal; }
.bp3-running-text{
  line-height:1.5;
  font-size:14px; }
  .bp3-running-text h1{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h1{
      color:#f5f8fa; }
  .bp3-running-text h2{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h2{
      color:#f5f8fa; }
  .bp3-running-text h3{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h3{
      color:#f5f8fa; }
  .bp3-running-text h4{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h4{
      color:#f5f8fa; }
  .bp3-running-text h5{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h5{
      color:#f5f8fa; }
  .bp3-running-text h6{
    color:#182026;
    font-weight:600;
    margin-top:40px;
    margin-bottom:20px; }
    .bp3-dark .bp3-running-text h6{
      color:#f5f8fa; }
  .bp3-running-text hr{
    margin:20px 0;
    border:none;
    border-bottom:1px solid rgba(16, 22, 26, 0.15); }
    .bp3-dark .bp3-running-text hr{
      border-color:rgba(255, 255, 255, 0.15); }
  .bp3-running-text p{
    margin:0 0 10px;
    padding:0; }

.bp3-text-large{
  font-size:16px; }

.bp3-text-small{
  font-size:12px; }
a{
  text-decoration:none;
  color:#106ba3; }
  a:hover{
    cursor:pointer;
    text-decoration:underline;
    color:#106ba3; }
  a .bp3-icon, a .bp3-icon-standard, a .bp3-icon-large{
    color:inherit; }
  a code,
  .bp3-dark a code{
    color:inherit; }
  .bp3-dark a,
  .bp3-dark a:hover{
    color:#48aff0; }
    .bp3-dark a .bp3-icon, .bp3-dark a .bp3-icon-standard, .bp3-dark a .bp3-icon-large,
    .bp3-dark a:hover .bp3-icon,
    .bp3-dark a:hover .bp3-icon-standard,
    .bp3-dark a:hover .bp3-icon-large{
      color:inherit; }
.bp3-running-text code, .bp3-code{
  text-transform:none;
  font-family:monospace;
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
  background:rgba(255, 255, 255, 0.7);
  padding:2px 5px;
  color:#5c7080;
  font-size:smaller; }
  .bp3-dark .bp3-running-text code, .bp3-running-text .bp3-dark code, .bp3-dark .bp3-code{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#a7b6c2; }
  .bp3-running-text a > code, a > .bp3-code{
    color:#137cbd; }
    .bp3-dark .bp3-running-text a > code, .bp3-running-text .bp3-dark a > code, .bp3-dark a > .bp3-code{
      color:inherit; }

.bp3-running-text pre, .bp3-code-block{
  text-transform:none;
  font-family:monospace;
  display:block;
  margin:10px 0;
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
  background:rgba(255, 255, 255, 0.7);
  padding:13px 15px 12px;
  line-height:1.4;
  color:#182026;
  font-size:13px;
  word-break:break-all;
  word-wrap:break-word; }
  .bp3-dark .bp3-running-text pre, .bp3-running-text .bp3-dark pre, .bp3-dark .bp3-code-block{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa; }
  .bp3-running-text pre > code, .bp3-code-block > code{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none;
    padding:0;
    color:inherit;
    font-size:inherit; }

.bp3-running-text kbd, .bp3-key{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  min-width:24px;
  height:24px;
  padding:3px 6px;
  vertical-align:middle;
  line-height:24px;
  color:#5c7080;
  font-family:inherit;
  font-size:12px; }
  .bp3-running-text kbd .bp3-icon, .bp3-key .bp3-icon, .bp3-running-text kbd .bp3-icon-standard, .bp3-key .bp3-icon-standard, .bp3-running-text kbd .bp3-icon-large, .bp3-key .bp3-icon-large{
    margin-right:5px; }
  .bp3-dark .bp3-running-text kbd, .bp3-running-text .bp3-dark kbd, .bp3-dark .bp3-key{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
    background:#394b59;
    color:#a7b6c2; }
.bp3-running-text blockquote, .bp3-blockquote{
  margin:0 0 10px;
  border-left:solid 4px rgba(167, 182, 194, 0.5);
  padding:0 20px; }
  .bp3-dark .bp3-running-text blockquote, .bp3-running-text .bp3-dark blockquote, .bp3-dark .bp3-blockquote{
    border-color:rgba(115, 134, 148, 0.5); }
.bp3-running-text ul,
.bp3-running-text ol, .bp3-list{
  margin:10px 0;
  padding-left:30px; }
  .bp3-running-text ul li:not(:last-child), .bp3-running-text ol li:not(:last-child), .bp3-list li:not(:last-child){
    margin-bottom:5px; }
  .bp3-running-text ul ol, .bp3-running-text ol ol, .bp3-list ol,
  .bp3-running-text ul ul,
  .bp3-running-text ol ul,
  .bp3-list ul{
    margin-top:5px; }

.bp3-list-unstyled{
  margin:0;
  padding:0;
  list-style:none; }
  .bp3-list-unstyled li{
    padding:0; }
.bp3-rtl{
  text-align:right; }

.bp3-dark{
  color:#f5f8fa; }

:focus{
  outline:rgba(19, 124, 189, 0.6) auto 2px;
  outline-offset:2px;
  -moz-outline-radius:6px; }

.bp3-focus-disabled :focus{
  outline:none !important; }
  .bp3-focus-disabled :focus ~ .bp3-control-indicator{
    outline:none !important; }

.bp3-alert{
  max-width:400px;
  padding:20px; }

.bp3-alert-body{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-alert-body .bp3-icon{
    margin-top:0;
    margin-right:20px;
    font-size:40px; }

.bp3-alert-footer{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:reverse;
      -ms-flex-direction:row-reverse;
          flex-direction:row-reverse;
  margin-top:10px; }
  .bp3-alert-footer .bp3-button{
    margin-left:10px; }
.bp3-breadcrumbs{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:wrap;
      flex-wrap:wrap;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  margin:0;
  cursor:default;
  height:30px;
  padding:0;
  list-style:none; }
  .bp3-breadcrumbs > li{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center; }
    .bp3-breadcrumbs > li::after{
      display:block;
      margin:0 5px;
      background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e");
      width:16px;
      height:16px;
      content:""; }
    .bp3-breadcrumbs > li:last-of-type::after{
      display:none; }

.bp3-breadcrumb,
.bp3-breadcrumb-current,
.bp3-breadcrumbs-collapsed{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  font-size:16px; }

.bp3-breadcrumb,
.bp3-breadcrumbs-collapsed{
  color:#5c7080; }

.bp3-breadcrumb:hover{
  text-decoration:none; }

.bp3-breadcrumb.bp3-disabled{
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-breadcrumb .bp3-icon{
  margin-right:5px; }

.bp3-breadcrumb-current{
  color:inherit;
  font-weight:600; }
  .bp3-breadcrumb-current .bp3-input{
    vertical-align:baseline;
    font-size:inherit;
    font-weight:inherit; }

.bp3-breadcrumbs-collapsed{
  margin-right:2px;
  border:none;
  border-radius:3px;
  background:#ced9e0;
  cursor:pointer;
  padding:1px 5px;
  vertical-align:text-bottom; }
  .bp3-breadcrumbs-collapsed::before{
    display:block;
    background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e") center no-repeat;
    width:16px;
    height:16px;
    content:""; }
  .bp3-breadcrumbs-collapsed:hover{
    background:#bfccd6;
    text-decoration:none;
    color:#182026; }

.bp3-dark .bp3-breadcrumb,
.bp3-dark .bp3-breadcrumbs-collapsed{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumbs > li::after{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumb.bp3-disabled{
  color:rgba(167, 182, 194, 0.6); }

.bp3-dark .bp3-breadcrumb-current{
  color:#f5f8fa; }

.bp3-dark .bp3-breadcrumbs-collapsed{
  background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-breadcrumbs-collapsed:hover{
    background:rgba(16, 22, 26, 0.6);
    color:#f5f8fa; }
.bp3-button{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  padding:5px 10px;
  vertical-align:middle;
  text-align:left;
  font-size:14px;
  min-width:30px;
  min-height:30px; }
  .bp3-button > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-button > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-button::before,
  .bp3-button > *{
    margin-right:7px; }
  .bp3-button:empty::before,
  .bp3-button > :last-child{
    margin-right:0; }
  .bp3-button:empty{
    padding:0 !important; }
  .bp3-button:disabled, .bp3-button.bp3-disabled{
    cursor:not-allowed; }
  .bp3-button.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button.bp3-align-right,
  .bp3-align-right .bp3-button{
    text-align:right; }
  .bp3-button.bp3-align-left,
  .bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-button:not([class*="bp3-intent-"]){
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    color:#182026; }
    .bp3-button:not([class*="bp3-intent-"]):hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
      background-clip:padding-box;
      background-color:#ebf1f5; }
    .bp3-button:not([class*="bp3-intent-"]):active, .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#d8e1e8;
      background-image:none; }
    .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      outline:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active:hover, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-button.bp3-intent-primary{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover, .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#106ba3; }
    .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#0e5a8a;
      background-image:none; }
    .bp3-button.bp3-intent-primary:disabled, .bp3-button.bp3-intent-primary.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(19, 124, 189, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-success{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#0f9960;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-success:hover, .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-success:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#0d8050; }
    .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#0a6640;
      background-image:none; }
    .bp3-button.bp3-intent-success:disabled, .bp3-button.bp3-intent-success.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(15, 153, 96, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-warning{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#d9822b;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover, .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#bf7326; }
    .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#a66321;
      background-image:none; }
    .bp3-button.bp3-intent-warning:disabled, .bp3-button.bp3-intent-warning.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(217, 130, 43, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-danger{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#db3737;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover, .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
      background-color:#c23030; }
    .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#a82a2a;
      background-image:none; }
    .bp3-button.bp3-intent-danger:disabled, .bp3-button.bp3-intent-danger.bp3-disabled{
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(219, 55, 55, 0.5);
      background-image:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
    stroke:#ffffff; }
  .bp3-button.bp3-large,
  .bp3-large .bp3-button{
    min-width:40px;
    min-height:40px;
    padding:5px 15px;
    font-size:16px; }
    .bp3-button.bp3-large::before,
    .bp3-button.bp3-large > *,
    .bp3-large .bp3-button::before,
    .bp3-large .bp3-button > *{
      margin-right:10px; }
    .bp3-button.bp3-large:empty::before,
    .bp3-button.bp3-large > :last-child,
    .bp3-large .bp3-button:empty::before,
    .bp3-large .bp3-button > :last-child{
      margin-right:0; }
  .bp3-button.bp3-small,
  .bp3-small .bp3-button{
    min-width:24px;
    min-height:24px;
    padding:0 7px; }
  .bp3-button.bp3-loading{
    position:relative; }
    .bp3-button.bp3-loading[class*="bp3-icon-"]::before{
      visibility:hidden; }
    .bp3-button.bp3-loading .bp3-button-spinner{
      position:absolute;
      margin:0; }
    .bp3-button.bp3-loading > :not(.bp3-button-spinner){
      visibility:hidden; }
  .bp3-button[class*="bp3-icon-"]::before{
    line-height:1;
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-weight:400;
    font-style:normal;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    color:#5c7080; }
  .bp3-button .bp3-icon, .bp3-button .bp3-icon-standard, .bp3-button .bp3-icon-large{
    color:#5c7080; }
    .bp3-button .bp3-icon.bp3-align-right, .bp3-button .bp3-icon-standard.bp3-align-right, .bp3-button .bp3-icon-large.bp3-align-right{
      margin-left:7px; }
  .bp3-button .bp3-icon:first-child:last-child,
  .bp3-button .bp3-spinner + .bp3-icon:last-child{
    margin:0 -7px; }
  .bp3-dark .bp3-button:not([class*="bp3-intent-"]){
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover, .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#30404d; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#202b33;
      background-image:none; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"])[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-large{
      color:#a7b6c2; }
  .bp3-dark .bp3-button[class*="bp3-intent-"]{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:active, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:disabled, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background-image:none;
      color:rgba(255, 255, 255, 0.3); }
    .bp3-dark .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
      stroke:#8a9ba8; }
  .bp3-button:disabled::before,
  .bp3-button:disabled .bp3-icon, .bp3-button:disabled .bp3-icon-standard, .bp3-button:disabled .bp3-icon-large, .bp3-button.bp3-disabled::before,
  .bp3-button.bp3-disabled .bp3-icon, .bp3-button.bp3-disabled .bp3-icon-standard, .bp3-button.bp3-disabled .bp3-icon-large, .bp3-button[class*="bp3-intent-"]::before,
  .bp3-button[class*="bp3-intent-"] .bp3-icon, .bp3-button[class*="bp3-intent-"] .bp3-icon-standard, .bp3-button[class*="bp3-intent-"] .bp3-icon-large{
    color:inherit !important; }
  .bp3-button.bp3-minimal{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none; }
    .bp3-button.bp3-minimal:hover{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(167, 182, 194, 0.3);
      text-decoration:none;
      color:#182026; }
    .bp3-button.bp3-minimal:active, .bp3-button.bp3-minimal.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(115, 134, 148, 0.3);
      color:#182026; }
    .bp3-button.bp3-minimal:disabled, .bp3-button.bp3-minimal:disabled:hover, .bp3-button.bp3-minimal.bp3-disabled, .bp3-button.bp3-minimal.bp3-disabled:hover{
      background:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button.bp3-minimal{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:inherit; }
      .bp3-dark .bp3-button.bp3-minimal:hover, .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none; }
      .bp3-dark .bp3-button.bp3-minimal:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button.bp3-minimal:disabled, .bp3-dark .bp3-button.bp3-minimal:disabled:hover, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{
        background:none;
        cursor:not-allowed;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover, .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-success{
      color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover, .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover, .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-danger{
      color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover, .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }

a.bp3-button{
  text-align:center;
  text-decoration:none;
  -webkit-transition:none;
  transition:none; }
  a.bp3-button, a.bp3-button:hover, a.bp3-button:active{
    color:#182026; }
  a.bp3-button.bp3-disabled{
    color:rgba(92, 112, 128, 0.6); }

.bp3-button-text{
  -webkit-box-flex:0;
      -ms-flex:0 1 auto;
          flex:0 1 auto; }

.bp3-button.bp3-align-left .bp3-button-text, .bp3-button.bp3-align-right .bp3-button-text,
.bp3-button-group.bp3-align-left .bp3-button-text,
.bp3-button-group.bp3-align-right .bp3-button-text{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto; }
.bp3-button-group{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex; }
  .bp3-button-group .bp3-button{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    position:relative;
    z-index:4; }
    .bp3-button-group .bp3-button:focus{
      z-index:5; }
    .bp3-button-group .bp3-button:hover{
      z-index:6; }
    .bp3-button-group .bp3-button:active, .bp3-button-group .bp3-button.bp3-active{
      z-index:7; }
    .bp3-button-group .bp3-button:disabled, .bp3-button-group .bp3-button.bp3-disabled{
      z-index:3; }
    .bp3-button-group .bp3-button[class*="bp3-intent-"]{
      z-index:9; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:focus{
        z-index:10; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:hover{
        z-index:11; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:active, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-active{
        z-index:12; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:disabled, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-disabled{
        z-index:8; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:first-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:first-child){
    border-top-left-radius:0;
    border-bottom-left-radius:0; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    margin-right:-1px;
    border-top-right-radius:0;
    border-bottom-right-radius:0; }
  .bp3-button-group.bp3-minimal .bp3-button{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none; }
    .bp3-button-group.bp3-minimal .bp3-button:hover{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(167, 182, 194, 0.3);
      text-decoration:none;
      color:#182026; }
    .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(115, 134, 148, 0.3);
      color:#182026; }
    .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
      background:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:inherit; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
        background:none;
        cursor:not-allowed;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
      color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
      color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:none;
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
  .bp3-button-group .bp3-popover-wrapper,
  .bp3-button-group .bp3-popover-target{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button-group .bp3-button.bp3-fill,
  .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-vertical{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column;
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch;
    vertical-align:top; }
    .bp3-button-group.bp3-vertical.bp3-fill{
      width:unset;
      height:100%; }
    .bp3-button-group.bp3-vertical .bp3-button{
      margin-right:0 !important;
      width:100%; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:first-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:first-child{
      border-radius:3px 3px 0 0; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:last-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:last-child{
      border-radius:0 0 3px 3px; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:not(:last-child){
      margin-bottom:-1px; }
  .bp3-button-group.bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    margin-right:1px; }
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-button:not(:last-child){
    margin-bottom:1px; }
.bp3-callout{
  line-height:1.5;
  font-size:14px;
  position:relative;
  border-radius:3px;
  background-color:rgba(138, 155, 168, 0.15);
  width:100%;
  padding:10px 12px 9px; }
  .bp3-callout[class*="bp3-icon-"]{
    padding-left:40px; }
    .bp3-callout[class*="bp3-icon-"]::before{
      line-height:1;
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-weight:400;
      font-style:normal;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      position:absolute;
      top:10px;
      left:10px;
      color:#5c7080; }
  .bp3-callout.bp3-callout-icon{
    padding-left:40px; }
    .bp3-callout.bp3-callout-icon > .bp3-icon:first-child{
      position:absolute;
      top:10px;
      left:10px;
      color:#5c7080; }
  .bp3-callout .bp3-heading{
    margin-top:0;
    margin-bottom:5px;
    line-height:20px; }
    .bp3-callout .bp3-heading:last-child{
      margin-bottom:0; }
  .bp3-dark .bp3-callout{
    background-color:rgba(138, 155, 168, 0.2); }
    .bp3-dark .bp3-callout[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
  .bp3-callout.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15); }
    .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-primary .bp3-heading{
      color:#106ba3; }
    .bp3-dark .bp3-callout.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{
        color:#48aff0; }
  .bp3-callout.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15); }
    .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-success .bp3-heading{
      color:#0d8050; }
    .bp3-dark .bp3-callout.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{
        color:#3dcc91; }
  .bp3-callout.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15); }
    .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-warning .bp3-heading{
      color:#bf7326; }
    .bp3-dark .bp3-callout.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{
        color:#ffb366; }
  .bp3-callout.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15); }
    .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-danger .bp3-heading{
      color:#c23030; }
    .bp3-dark .bp3-callout.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{
        color:#ff7373; }
  .bp3-running-text .bp3-callout{
    margin:20px 0; }
.bp3-card{
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
  background-color:#ffffff;
  padding:20px;
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-card.bp3-dark,
  .bp3-dark .bp3-card{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
    background-color:#30404d; }

.bp3-elevation-0{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }
  .bp3-elevation-0.bp3-dark,
  .bp3-dark .bp3-elevation-0{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }

.bp3-elevation-1{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-1.bp3-dark,
  .bp3-dark .bp3-elevation-1{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-elevation-2{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-2.bp3-dark,
  .bp3-dark .bp3-elevation-2{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); }

.bp3-elevation-3{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-3.bp3-dark,
  .bp3-dark .bp3-elevation-3{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-elevation-4{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-4.bp3-dark,
  .bp3-dark .bp3-elevation-4{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:hover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  cursor:pointer; }
  .bp3-card.bp3-interactive:hover.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:hover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:active{
  opacity:0.9;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  -webkit-transition-duration:0;
          transition-duration:0; }
  .bp3-card.bp3-interactive:active.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:active{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-collapse{
  height:0;
  overflow-y:hidden;
  -webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-collapse .bp3-collapse-body{
    -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-collapse .bp3-collapse-body[aria-hidden="true"]{
      display:none; }

.bp3-context-menu .bp3-popover-target{
  display:block; }

.bp3-context-menu-popover-target{
  position:fixed; }

.bp3-divider{
  margin:5px;
  border-right:1px solid rgba(16, 22, 26, 0.15);
  border-bottom:1px solid rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-divider{
    border-color:rgba(16, 22, 26, 0.4); }
.bp3-dialog-container{
  opacity:1;
  -webkit-transform:scale(1);
          transform:scale(1);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  width:100%;
  min-height:100%;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-dialog-container.bp3-overlay-enter > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5); }
  .bp3-dialog-container.bp3-overlay-enter-active > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear-active > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-dialog-container.bp3-overlay-exit > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-dialog-container.bp3-overlay-exit-active > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5);
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }

.bp3-dialog{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:30px 0;
  border-radius:6px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  background:#ebf1f5;
  width:500px;
  padding-bottom:20px;
  pointer-events:all;
  -webkit-user-select:text;
     -moz-user-select:text;
      -ms-user-select:text;
          user-select:text; }
  .bp3-dialog:focus{
    outline:0; }
  .bp3-dialog.bp3-dark,
  .bp3-dark .bp3-dialog{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    background:#293742;
    color:#f5f8fa; }

.bp3-dialog-header{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border-radius:6px 6px 0 0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  background:#ffffff;
  min-height:40px;
  padding-right:5px;
  padding-left:20px; }
  .bp3-dialog-header .bp3-icon-large,
  .bp3-dialog-header .bp3-icon{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px;
    color:#5c7080; }
  .bp3-dialog-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    margin:0;
    line-height:inherit; }
    .bp3-dialog-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-dialog-header{
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
    background:#30404d; }
    .bp3-dark .bp3-dialog-header .bp3-icon-large,
    .bp3-dark .bp3-dialog-header .bp3-icon{
      color:#a7b6c2; }

.bp3-dialog-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  margin:20px;
  line-height:18px; }

.bp3-dialog-footer{
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  margin:0 20px; }

.bp3-dialog-footer-actions{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:end;
      -ms-flex-pack:end;
          justify-content:flex-end; }
  .bp3-dialog-footer-actions .bp3-button{
    margin-left:10px; }
.bp3-drawer{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  padding:0; }
  .bp3-drawer:focus{
    outline:0; }
  .bp3-drawer.bp3-position-top{
    top:0;
    right:0;
    left:0;
    height:50%; }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter, .bp3-drawer.bp3-position-top.bp3-overlay-appear{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%); }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter-active, .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-position-bottom{
    right:0;
    bottom:0;
    left:0;
    height:50%; }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-position-left{
    top:0;
    bottom:0;
    left:0;
    width:50%; }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter, .bp3-drawer.bp3-position-left.bp3-overlay-appear{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%); }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter-active, .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-position-right{
    top:0;
    right:0;
    bottom:0;
    width:50%; }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter, .bp3-drawer.bp3-position-right.bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter-active, .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right):not(.bp3-vertical){
    top:0;
    right:0;
    bottom:0;
    width:50%; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right).bp3-vertical{
    right:0;
    bottom:0;
    left:0;
    height:50%; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
      -webkit-transition-delay:0;
              transition-delay:0; }
  .bp3-drawer.bp3-dark,
  .bp3-dark .bp3-drawer{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    background:#30404d;
    color:#f5f8fa; }

.bp3-drawer-header{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  position:relative;
  border-radius:0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  min-height:40px;
  padding:5px;
  padding-left:20px; }
  .bp3-drawer-header .bp3-icon-large,
  .bp3-drawer-header .bp3-icon{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px;
    color:#5c7080; }
  .bp3-drawer-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    margin:0;
    line-height:inherit; }
    .bp3-drawer-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-drawer-header{
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-drawer-header .bp3-icon-large,
    .bp3-dark .bp3-drawer-header .bp3-icon{
      color:#a7b6c2; }

.bp3-drawer-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  overflow:auto;
  line-height:18px; }

.bp3-drawer-footer{
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  position:relative;
  -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
  padding:10px 20px; }
  .bp3-dark .bp3-drawer-footer{
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); }
.bp3-editable-text{
  display:inline-block;
  position:relative;
  cursor:text;
  max-width:100%;
  vertical-align:top;
  white-space:nowrap; }
  .bp3-editable-text::before{
    position:absolute;
    top:-3px;
    right:-3px;
    bottom:-3px;
    left:-3px;
    border-radius:3px;
    content:"";
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-editable-text.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
    background-color:#ffffff; }
  .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#137cbd; }
  .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); }
  .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#0f9960; }
  .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); }
  .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#d9822b; }
  .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); }
  .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#db3737; }
  .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); }
  .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); }
  .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background-color:rgba(16, 22, 26, 0.3); }
  .bp3-dark .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#48aff0; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4);
            box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#3dcc91; }
  .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4);
            box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#ffb366; }
  .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4);
            box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#ff7373; }
  .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4);
            box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-editable-text-input,
.bp3-editable-text-content{
  display:inherit;
  position:relative;
  min-width:inherit;
  max-width:inherit;
  vertical-align:top;
  text-transform:inherit;
  letter-spacing:inherit;
  color:inherit;
  font:inherit;
  resize:none; }

.bp3-editable-text-input{
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  background:none;
  width:100%;
  padding:0;
  white-space:pre-wrap; }
  .bp3-editable-text-input::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-editable-text-input:focus{
    outline:none; }
  .bp3-editable-text-input::-ms-clear{
    display:none; }

.bp3-editable-text-content{
  overflow:hidden;
  padding-right:2px;
  text-overflow:ellipsis;
  white-space:pre; }
  .bp3-editable-text-editing > .bp3-editable-text-content{
    position:absolute;
    left:0;
    visibility:hidden; }
  .bp3-editable-text-placeholder > .bp3-editable-text-content{
    color:rgba(92, 112, 128, 0.6); }
    .bp3-dark .bp3-editable-text-placeholder > .bp3-editable-text-content{
      color:rgba(167, 182, 194, 0.6); }

.bp3-editable-text.bp3-multiline{
  display:block; }
  .bp3-editable-text.bp3-multiline .bp3-editable-text-content{
    overflow:auto;
    white-space:pre-wrap;
    word-wrap:break-word; }
.bp3-control-group{
  -webkit-transform:translateZ(0);
          transform:translateZ(0);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:stretch;
      -ms-flex-align:stretch;
          align-items:stretch; }
  .bp3-control-group > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select,
  .bp3-control-group .bp3-input,
  .bp3-control-group .bp3-select{
    position:relative; }
  .bp3-control-group .bp3-input{
    z-index:2;
    border-radius:inherit; }
    .bp3-control-group .bp3-input:focus{
      z-index:14;
      border-radius:3px; }
    .bp3-control-group .bp3-input[class*="bp3-intent"]{
      z-index:13; }
      .bp3-control-group .bp3-input[class*="bp3-intent"]:focus{
        z-index:15; }
    .bp3-control-group .bp3-input[readonly], .bp3-control-group .bp3-input:disabled, .bp3-control-group .bp3-input.bp3-disabled{
      z-index:1; }
  .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input{
    z-index:13; }
    .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input:focus{
      z-index:15; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select select,
  .bp3-control-group .bp3-select select{
    -webkit-transform:translateZ(0);
            transform:translateZ(0);
    z-index:4;
    border-radius:inherit; }
    .bp3-control-group .bp3-button:focus,
    .bp3-control-group .bp3-html-select select:focus,
    .bp3-control-group .bp3-select select:focus{
      z-index:5; }
    .bp3-control-group .bp3-button:hover,
    .bp3-control-group .bp3-html-select select:hover,
    .bp3-control-group .bp3-select select:hover{
      z-index:6; }
    .bp3-control-group .bp3-button:active,
    .bp3-control-group .bp3-html-select select:active,
    .bp3-control-group .bp3-select select:active{
      z-index:7; }
    .bp3-control-group .bp3-button[readonly], .bp3-control-group .bp3-button:disabled, .bp3-control-group .bp3-button.bp3-disabled,
    .bp3-control-group .bp3-html-select select[readonly],
    .bp3-control-group .bp3-html-select select:disabled,
    .bp3-control-group .bp3-html-select select.bp3-disabled,
    .bp3-control-group .bp3-select select[readonly],
    .bp3-control-group .bp3-select select:disabled,
    .bp3-control-group .bp3-select select.bp3-disabled{
      z-index:3; }
    .bp3-control-group .bp3-button[class*="bp3-intent"],
    .bp3-control-group .bp3-html-select select[class*="bp3-intent"],
    .bp3-control-group .bp3-select select[class*="bp3-intent"]{
      z-index:9; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:focus{
        z-index:10; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:hover{
        z-index:11; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:active{
        z-index:12; }
      .bp3-control-group .bp3-button[class*="bp3-intent"][readonly], .bp3-control-group .bp3-button[class*="bp3-intent"]:disabled, .bp3-control-group .bp3-button[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"].bp3-disabled{
        z-index:8; }
  .bp3-control-group .bp3-input-group > .bp3-icon,
  .bp3-control-group .bp3-input-group > .bp3-button,
  .bp3-control-group .bp3-input-group > .bp3-input-action{
    z-index:16; }
  .bp3-control-group .bp3-select::after,
  .bp3-control-group .bp3-html-select::after,
  .bp3-control-group .bp3-select > .bp3-icon,
  .bp3-control-group .bp3-html-select > .bp3-icon{
    z-index:17; }
  .bp3-control-group:not(.bp3-vertical) > *{
    margin-right:-1px; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > *{
    margin-right:0; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > .bp3-button + .bp3-button{
    margin-left:1px; }
  .bp3-control-group .bp3-popover-wrapper,
  .bp3-control-group .bp3-popover-target{
    border-radius:inherit; }
  .bp3-control-group > :first-child{
    border-radius:3px 0 0 3px; }
  .bp3-control-group > :last-child{
    margin-right:0;
    border-radius:0 3px 3px 0; }
  .bp3-control-group > :only-child{
    margin-right:0;
    border-radius:3px; }
  .bp3-control-group .bp3-input-group .bp3-button{
    border-radius:3px; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-fill > *:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-vertical{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column; }
    .bp3-control-group.bp3-vertical > *{
      margin-top:-1px; }
    .bp3-control-group.bp3-vertical > :first-child{
      margin-top:0;
      border-radius:3px 3px 0 0; }
    .bp3-control-group.bp3-vertical > :last-child{
      border-radius:0 0 3px 3px; }
.bp3-control{
  display:block;
  position:relative;
  margin-bottom:10px;
  cursor:pointer;
  text-transform:none; }
  .bp3-control input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
  .bp3-control:hover input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#106ba3; }
  .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background:#0e5a8a; }
  .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(19, 124, 189, 0.5); }
  .bp3-dark .bp3-control input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control:hover input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#106ba3; }
  .bp3-dark .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#0e5a8a; }
  .bp3-dark .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(14, 90, 138, 0.5); }
  .bp3-control:not(.bp3-align-right){
    padding-left:26px; }
    .bp3-control:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-26px; }
  .bp3-control.bp3-align-right{
    padding-right:26px; }
    .bp3-control.bp3-align-right .bp3-control-indicator{
      margin-right:-26px; }
  .bp3-control.bp3-disabled{
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-control.bp3-inline{
    display:inline-block;
    margin-right:20px; }
  .bp3-control input{
    position:absolute;
    top:0;
    left:0;
    opacity:0;
    z-index:-1; }
  .bp3-control .bp3-control-indicator{
    display:inline-block;
    position:relative;
    margin-top:-3px;
    margin-right:10px;
    border:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    cursor:pointer;
    width:1em;
    height:1em;
    vertical-align:middle;
    font-size:16px;
    -webkit-user-select:none;
       -moz-user-select:none;
        -ms-user-select:none;
            user-select:none; }
    .bp3-control .bp3-control-indicator::before{
      display:block;
      width:1em;
      height:1em;
      content:""; }
  .bp3-control:hover .bp3-control-indicator{
    background-color:#ebf1f5; }
  .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background:#d8e1e8; }
  .bp3-control input:disabled ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(206, 217, 224, 0.5);
    cursor:not-allowed; }
  .bp3-control input:focus ~ .bp3-control-indicator{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:2px;
    -moz-outline-radius:6px; }
  .bp3-control.bp3-align-right .bp3-control-indicator{
    float:right;
    margin-top:1px;
    margin-left:10px; }
  .bp3-control.bp3-large{
    font-size:16px; }
    .bp3-control.bp3-large:not(.bp3-align-right){
      padding-left:30px; }
      .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
        margin-left:-30px; }
    .bp3-control.bp3-large.bp3-align-right{
      padding-right:30px; }
      .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
        margin-right:-30px; }
    .bp3-control.bp3-large .bp3-control-indicator{
      font-size:20px; }
    .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-top:0; }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    color:#ffffff; }
  .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    background-color:#106ba3; }
  .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background:#0e5a8a; }
  .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(19, 124, 189, 0.5); }
  .bp3-dark .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#106ba3; }
  .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#0e5a8a; }
  .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(14, 90, 138, 0.5); }
  .bp3-control.bp3-checkbox .bp3-control-indicator{
    border-radius:3px; }
  .bp3-control.bp3-checkbox input:checked ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-radio .bp3-control-indicator{
    border-radius:50%; }
  .bp3-control.bp3-radio input:checked ~ .bp3-control-indicator::before{
    background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%); }
  .bp3-control.bp3-radio input:checked:disabled ~ .bp3-control-indicator::before{
    opacity:0.5; }
  .bp3-control.bp3-radio input:focus ~ .bp3-control-indicator{
    -moz-outline-radius:16px; }
  .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(167, 182, 194, 0.5); }
  .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(115, 134, 148, 0.5); }
  .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(92, 112, 128, 0.5); }
  .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(206, 217, 224, 0.5); }
    .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5); }
    .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch:not(.bp3-align-right){
    padding-left:38px; }
    .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-38px; }
  .bp3-control.bp3-switch.bp3-align-right{
    padding-right:38px; }
    .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{
      margin-right:-38px; }
  .bp3-control.bp3-switch .bp3-control-indicator{
    border:none;
    border-radius:1.75em;
    -webkit-box-shadow:none !important;
            box-shadow:none !important;
    width:auto;
    min-width:1.75em;
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-control.bp3-switch .bp3-control-indicator::before{
      position:absolute;
      left:0;
      margin:2px;
      border-radius:50%;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
      background:#ffffff;
      width:calc(1em - 4px);
      height:calc(1em - 4px);
      -webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
      transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    left:calc(100% - 1em); }
  .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){
    padding-left:45px; }
    .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-45px; }
  .bp3-control.bp3-switch.bp3-large.bp3-align-right{
    padding-right:45px; }
    .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-right:-45px; }
  .bp3-dark .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.7); }
  .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.9); }
  .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(57, 75, 89, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-dark .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background:#394b59; }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-control.bp3-switch .bp3-switch-inner-text{
    text-align:center;
    font-size:0.7em; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{
    visibility:hidden;
    margin-right:1.2em;
    margin-left:0.5em;
    line-height:0; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{
    visibility:visible;
    margin-right:0.5em;
    margin-left:1.2em;
    line-height:1em; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:first-child{
    visibility:visible;
    line-height:1em; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:last-child{
    visibility:hidden;
    line-height:0; }
  .bp3-dark .bp3-control{
    color:#f5f8fa; }
    .bp3-dark .bp3-control.bp3-disabled{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-control .bp3-control-indicator{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); }
    .bp3-dark .bp3-control:hover .bp3-control-indicator{
      background-color:#30404d; }
    .bp3-dark .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background:#202b33; }
    .bp3-dark .bp3-control input:disabled ~ .bp3-control-indicator{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      cursor:not-allowed; }
    .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked ~ .bp3-control-indicator, .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
      color:rgba(167, 182, 194, 0.6); }
.bp3-file-input{
  display:inline-block;
  position:relative;
  cursor:pointer;
  height:30px; }
  .bp3-file-input input{
    opacity:0;
    margin:0;
    min-width:200px; }
    .bp3-file-input input:disabled + .bp3-file-upload-input,
    .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(206, 217, 224, 0.5);
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6);
      resize:none; }
      .bp3-file-input input:disabled + .bp3-file-upload-input::after,
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
        outline:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        background-color:rgba(206, 217, 224, 0.5);
        background-image:none;
        cursor:not-allowed;
        color:rgba(92, 112, 128, 0.6); }
        .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active:hover,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active:hover{
          background:rgba(206, 217, 224, 0.7); }
      .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-dark
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
        -webkit-box-shadow:none;
                box-shadow:none;
        background:rgba(57, 75, 89, 0.5);
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-dark
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
          -webkit-box-shadow:none;
                  box-shadow:none;
          background-color:rgba(57, 75, 89, 0.5);
          background-image:none;
          color:rgba(167, 182, 194, 0.6); }
          .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-dark
          .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active{
            background:rgba(57, 75, 89, 0.7); }
  .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#182026; }
  .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#f5f8fa; }
  .bp3-file-input.bp3-fill{
    width:100%; }
  .bp3-file-input.bp3-large,
  .bp3-large .bp3-file-input{
    height:40px; }
  .bp3-file-input .bp3-file-upload-input-custom-text::after{
    content:attr(bp3-button-text); }

.bp3-file-upload-input{
  outline:none;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  height:30px;
  padding:0 10px;
  vertical-align:middle;
  line-height:30px;
  color:#182026;
  font-size:14px;
  font-weight:400;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none;
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  position:absolute;
  top:0;
  right:0;
  left:0;
  padding-right:80px;
  color:rgba(92, 112, 128, 0.6);
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-file-upload-input::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-file-upload-input:focus, .bp3-file-upload-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-file-upload-input[type="search"], .bp3-file-upload-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-file-upload-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-file-upload-input:disabled, .bp3-file-upload-input.bp3-disabled{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(206, 217, 224, 0.5);
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6);
    resize:none; }
  .bp3-file-upload-input::after{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    color:#182026;
    min-width:24px;
    min-height:24px;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    position:absolute;
    top:0;
    right:0;
    margin:3px;
    border-radius:3px;
    width:70px;
    text-align:center;
    line-height:24px;
    content:"Browse"; }
    .bp3-file-upload-input::after:hover{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
      background-clip:padding-box;
      background-color:#ebf1f5; }
    .bp3-file-upload-input::after:active, .bp3-file-upload-input::after.bp3-active{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#d8e1e8;
      background-image:none; }
    .bp3-file-upload-input::after:disabled, .bp3-file-upload-input::after.bp3-disabled{
      outline:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      cursor:not-allowed;
      color:rgba(92, 112, 128, 0.6); }
      .bp3-file-upload-input::after:disabled.bp3-active, .bp3-file-upload-input::after:disabled.bp3-active:hover, .bp3-file-upload-input::after.bp3-disabled.bp3-active, .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-file-upload-input:hover::after{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5; }
  .bp3-file-upload-input:active::after{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none; }
  .bp3-large .bp3-file-upload-input{
    height:40px;
    line-height:40px;
    font-size:16px;
    padding-right:95px; }
    .bp3-large .bp3-file-upload-input[type="search"], .bp3-large .bp3-file-upload-input.bp3-round{
      padding:0 15px; }
    .bp3-large .bp3-file-upload-input::after{
      min-width:30px;
      min-height:30px;
      margin:5px;
      width:85px;
      line-height:30px; }
  .bp3-dark .bp3-file-upload-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input:disabled, .bp3-dark .bp3-file-upload-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::after{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
      color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover, .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover{
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
        background-color:#30404d; }
      .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
        background-color:#202b33;
        background-image:none; }
      .bp3-dark .bp3-file-upload-input::after:disabled, .bp3-dark .bp3-file-upload-input::after.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none;
        background-color:rgba(57, 75, 89, 0.5);
        background-image:none;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active, .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{
          background:rgba(57, 75, 89, 0.7); }
      .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{
        background:rgba(16, 22, 26, 0.5);
        stroke:#8a9ba8; }
    .bp3-dark .bp3-file-upload-input:hover::after{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#30404d; }
    .bp3-dark .bp3-file-upload-input:active::after{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#202b33;
      background-image:none; }

.bp3-file-upload-input::after{
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
.bp3-form-group{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0 0 15px; }
  .bp3-form-group label.bp3-label{
    margin-bottom:5px; }
  .bp3-form-group .bp3-control{
    margin-top:7px; }
  .bp3-form-group .bp3-form-helper-text{
    margin-top:5px;
    color:#5c7080;
    font-size:12px; }
  .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#106ba3; }
  .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#0d8050; }
  .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#bf7326; }
  .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#c23030; }
  .bp3-form-group.bp3-inline{
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row;
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
    .bp3-form-group.bp3-inline.bp3-large label.bp3-label{
      margin:0 10px 0 0;
      line-height:40px; }
    .bp3-form-group.bp3-inline label.bp3-label{
      margin:0 10px 0 0;
      line-height:30px; }
  .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#48aff0; }
  .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#3dcc91; }
  .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#ffb366; }
  .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#ff7373; }
  .bp3-dark .bp3-form-group .bp3-form-helper-text{
    color:#a7b6c2; }
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(167, 182, 194, 0.6) !important; }
.bp3-input-group{
  display:block;
  position:relative; }
  .bp3-input-group .bp3-input{
    position:relative;
    width:100%; }
    .bp3-input-group .bp3-input:not(:first-child){
      padding-left:30px; }
    .bp3-input-group .bp3-input:not(:last-child){
      padding-right:30px; }
  .bp3-input-group .bp3-input-action,
  .bp3-input-group > .bp3-button,
  .bp3-input-group > .bp3-icon{
    position:absolute;
    top:0; }
    .bp3-input-group .bp3-input-action:first-child,
    .bp3-input-group > .bp3-button:first-child,
    .bp3-input-group > .bp3-icon:first-child{
      left:0; }
    .bp3-input-group .bp3-input-action:last-child,
    .bp3-input-group > .bp3-button:last-child,
    .bp3-input-group > .bp3-icon:last-child{
      right:0; }
  .bp3-input-group .bp3-button{
    min-width:24px;
    min-height:24px;
    margin:3px;
    padding:0 7px; }
    .bp3-input-group .bp3-button:empty{
      padding:0; }
  .bp3-input-group > .bp3-icon{
    z-index:1;
    color:#5c7080; }
    .bp3-input-group > .bp3-icon:empty{
      line-height:1;
      font-family:"Icons16", sans-serif;
      font-size:16px;
      font-weight:400;
      font-style:normal;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased; }
  .bp3-input-group > .bp3-icon,
  .bp3-input-group .bp3-input-action > .bp3-spinner{
    margin:7px; }
  .bp3-input-group .bp3-tag{
    margin:5px; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus),
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
    color:#5c7080; }
    .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-dark
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
      color:#a7b6c2; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{
      color:#5c7080; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled,
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled{
    color:rgba(92, 112, 128, 0.6) !important; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-input-group.bp3-disabled{
    cursor:not-allowed; }
    .bp3-input-group.bp3-disabled .bp3-icon{
      color:rgba(92, 112, 128, 0.6); }
  .bp3-input-group.bp3-large .bp3-button{
    min-width:30px;
    min-height:30px;
    margin:5px; }
  .bp3-input-group.bp3-large > .bp3-icon,
  .bp3-input-group.bp3-large .bp3-input-action > .bp3-spinner{
    margin:12px; }
  .bp3-input-group.bp3-large .bp3-input{
    height:40px;
    line-height:40px;
    font-size:16px; }
    .bp3-input-group.bp3-large .bp3-input[type="search"], .bp3-input-group.bp3-large .bp3-input.bp3-round{
      padding:0 15px; }
    .bp3-input-group.bp3-large .bp3-input:not(:first-child){
      padding-left:40px; }
    .bp3-input-group.bp3-large .bp3-input:not(:last-child){
      padding-right:40px; }
  .bp3-input-group.bp3-small .bp3-button{
    min-width:20px;
    min-height:20px;
    margin:2px; }
  .bp3-input-group.bp3-small .bp3-tag{
    min-width:20px;
    min-height:20px;
    margin:2px; }
  .bp3-input-group.bp3-small > .bp3-icon,
  .bp3-input-group.bp3-small .bp3-input-action > .bp3-spinner{
    margin:4px; }
  .bp3-input-group.bp3-small .bp3-input{
    height:24px;
    padding-right:8px;
    padding-left:8px;
    line-height:24px;
    font-size:12px; }
    .bp3-input-group.bp3-small .bp3-input[type="search"], .bp3-input-group.bp3-small .bp3-input.bp3-round{
      padding:0 12px; }
    .bp3-input-group.bp3-small .bp3-input:not(:first-child){
      padding-left:24px; }
    .bp3-input-group.bp3-small .bp3-input:not(:last-child){
      padding-right:24px; }
  .bp3-input-group.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-input-group.bp3-round .bp3-button,
  .bp3-input-group.bp3-round .bp3-input,
  .bp3-input-group.bp3-round .bp3-tag{
    border-radius:30px; }
  .bp3-dark .bp3-input-group .bp3-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-input-group.bp3-intent-primary .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input-group.bp3-intent-primary .bp3-input:disabled, .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-primary > .bp3-icon{
    color:#106ba3; }
    .bp3-dark .bp3-input-group.bp3-intent-primary > .bp3-icon{
      color:#48aff0; }
  .bp3-input-group.bp3-intent-success .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input-group.bp3-intent-success .bp3-input:disabled, .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-success > .bp3-icon{
    color:#0d8050; }
    .bp3-dark .bp3-input-group.bp3-intent-success > .bp3-icon{
      color:#3dcc91; }
  .bp3-input-group.bp3-intent-warning .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input-group.bp3-intent-warning .bp3-input:disabled, .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-warning > .bp3-icon{
    color:#bf7326; }
    .bp3-dark .bp3-input-group.bp3-intent-warning > .bp3-icon{
      color:#ffb366; }
  .bp3-input-group.bp3-intent-danger .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input-group.bp3-intent-danger .bp3-input:disabled, .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-danger > .bp3-icon{
    color:#c23030; }
    .bp3-dark .bp3-input-group.bp3-intent-danger > .bp3-icon{
      color:#ff7373; }
.bp3-input{
  outline:none;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  background:#ffffff;
  height:30px;
  padding:0 10px;
  vertical-align:middle;
  line-height:30px;
  color:#182026;
  font-size:14px;
  font-weight:400;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none; }
  .bp3-input::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input:focus, .bp3-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-input[type="search"], .bp3-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-input:disabled, .bp3-input.bp3-disabled{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(206, 217, 224, 0.5);
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6);
    resize:none; }
  .bp3-input.bp3-large{
    height:40px;
    line-height:40px;
    font-size:16px; }
    .bp3-input.bp3-large[type="search"], .bp3-input.bp3-large.bp3-round{
      padding:0 15px; }
  .bp3-input.bp3-small{
    height:24px;
    padding-right:8px;
    padding-left:8px;
    line-height:24px;
    font-size:12px; }
    .bp3-input.bp3-small[type="search"], .bp3-input.bp3-small.bp3-round{
      padding:0 12px; }
  .bp3-input.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-dark .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa; }
    .bp3-dark .bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input:disabled, .bp3-dark .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      color:rgba(167, 182, 194, 0.6); }
  .bp3-input.bp3-intent-primary{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input.bp3-intent-primary:disabled, .bp3-input.bp3-intent-primary.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary:focus{
        -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #137cbd;
                box-shadow:inset 0 0 0 1px #137cbd; }
      .bp3-dark .bp3-input.bp3-intent-primary:disabled, .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-success{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input.bp3-intent-success:disabled, .bp3-input.bp3-intent-success.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-success{
      -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success:focus{
        -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #0f9960;
                box-shadow:inset 0 0 0 1px #0f9960; }
      .bp3-dark .bp3-input.bp3-intent-success:disabled, .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-warning{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input.bp3-intent-warning:disabled, .bp3-input.bp3-intent-warning.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning:focus{
        -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #d9822b;
                box-shadow:inset 0 0 0 1px #d9822b; }
      .bp3-dark .bp3-input.bp3-intent-warning:disabled, .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-danger{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input.bp3-intent-danger:disabled, .bp3-input.bp3-intent-danger.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger:focus{
        -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #db3737;
                box-shadow:inset 0 0 0 1px #db3737; }
      .bp3-dark .bp3-input.bp3-intent-danger:disabled, .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input::-ms-clear{
    display:none; }
textarea.bp3-input{
  max-width:100%;
  padding:10px; }
  textarea.bp3-input, textarea.bp3-input.bp3-large, textarea.bp3-input.bp3-small{
    height:auto;
    line-height:inherit; }
  textarea.bp3-input.bp3-small{
    padding:8px; }
  .bp3-dark textarea.bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background:rgba(16, 22, 26, 0.3);
    color:#f5f8fa; }
    .bp3-dark textarea.bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input:disabled, .bp3-dark textarea.bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:rgba(57, 75, 89, 0.5);
      color:rgba(167, 182, 194, 0.6); }
label.bp3-label{
  display:block;
  margin-top:0;
  margin-bottom:15px; }
  label.bp3-label .bp3-html-select,
  label.bp3-label .bp3-input,
  label.bp3-label .bp3-select,
  label.bp3-label .bp3-slider,
  label.bp3-label .bp3-popover-wrapper{
    display:block;
    margin-top:5px;
    text-transform:none; }
  label.bp3-label .bp3-button-group{
    margin-top:5px; }
  label.bp3-label .bp3-select select,
  label.bp3-label .bp3-html-select select{
    width:100%;
    vertical-align:top;
    font-weight:400; }
  label.bp3-label.bp3-disabled,
  label.bp3-label.bp3-disabled .bp3-text-muted{
    color:rgba(92, 112, 128, 0.6); }
  label.bp3-label.bp3-inline{
    line-height:30px; }
    label.bp3-label.bp3-inline .bp3-html-select,
    label.bp3-label.bp3-inline .bp3-input,
    label.bp3-label.bp3-inline .bp3-input-group,
    label.bp3-label.bp3-inline .bp3-select,
    label.bp3-label.bp3-inline .bp3-popover-wrapper{
      display:inline-block;
      margin:0 0 0 5px;
      vertical-align:top; }
    label.bp3-label.bp3-inline .bp3-button-group{
      margin:0 0 0 5px; }
    label.bp3-label.bp3-inline .bp3-input-group .bp3-input{
      margin-left:0; }
    label.bp3-label.bp3-inline.bp3-large{
      line-height:40px; }
  label.bp3-label:not(.bp3-inline) .bp3-popover-target{
    display:block; }
  .bp3-dark label.bp3-label{
    color:#f5f8fa; }
    .bp3-dark label.bp3-label.bp3-disabled,
    .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{
      color:rgba(167, 182, 194, 0.6); }
.bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button{
  -webkit-box-flex:1;
      -ms-flex:1 1 14px;
          flex:1 1 14px;
  width:30px;
  min-height:0;
  padding:0; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:first-child{
    border-radius:0 3px 0 0; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:last-child{
    border-radius:0 0 3px 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:first-child{
  border-radius:3px 0 0 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:last-child{
  border-radius:0 0 0 3px; }

.bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical > .bp3-button{
  width:40px; }

form{
  display:block; }
.bp3-html-select select,
.bp3-select select{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  padding:5px 10px;
  vertical-align:middle;
  text-align:left;
  font-size:14px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  color:#182026;
  border-radius:3px;
  width:100%;
  height:30px;
  padding:0 25px 0 10px;
  -moz-appearance:none;
  -webkit-appearance:none; }
  .bp3-html-select select > *, .bp3-select select > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-html-select select > .bp3-fill, .bp3-select select > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-html-select select::before,
  .bp3-select select::before, .bp3-html-select select > *, .bp3-select select > *{
    margin-right:7px; }
  .bp3-html-select select:empty::before,
  .bp3-select select:empty::before,
  .bp3-html-select select > :last-child,
  .bp3-select select > :last-child{
    margin-right:0; }
  .bp3-html-select select:hover,
  .bp3-select select:hover{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5; }
  .bp3-html-select select:active,
  .bp3-select select:active, .bp3-html-select select.bp3-active,
  .bp3-select select.bp3-active{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none; }
  .bp3-html-select select:disabled,
  .bp3-select select:disabled, .bp3-html-select select.bp3-disabled,
  .bp3-select select.bp3-disabled{
    outline:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
    .bp3-html-select select:disabled.bp3-active,
    .bp3-select select:disabled.bp3-active, .bp3-html-select select:disabled.bp3-active:hover,
    .bp3-select select:disabled.bp3-active:hover, .bp3-html-select select.bp3-disabled.bp3-active,
    .bp3-select select.bp3-disabled.bp3-active, .bp3-html-select select.bp3-disabled.bp3-active:hover,
    .bp3-select select.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }

.bp3-html-select.bp3-minimal select,
.bp3-select.bp3-minimal select{
  -webkit-box-shadow:none;
          box-shadow:none;
  background:none; }
  .bp3-html-select.bp3-minimal select:hover,
  .bp3-select.bp3-minimal select:hover{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(167, 182, 194, 0.3);
    text-decoration:none;
    color:#182026; }
  .bp3-html-select.bp3-minimal select:active,
  .bp3-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal select.bp3-active,
  .bp3-select.bp3-minimal select.bp3-active{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:rgba(115, 134, 148, 0.3);
    color:#182026; }
  .bp3-html-select.bp3-minimal select:disabled,
  .bp3-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal select:disabled:hover,
  .bp3-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal select.bp3-disabled,
  .bp3-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal select.bp3-disabled:hover,
  .bp3-select.bp3-minimal select.bp3-disabled:hover{
    background:none;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
    .bp3-html-select.bp3-minimal select:disabled.bp3-active,
    .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{
      background:rgba(115, 134, 148, 0.3); }
  .bp3-dark .bp3-html-select.bp3-minimal select, .bp3-html-select.bp3-minimal .bp3-dark select,
  .bp3-dark .bp3-select.bp3-minimal select, .bp3-select.bp3-minimal .bp3-dark select{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:none;
    color:inherit; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover{
      background:rgba(138, 155, 168, 0.15); }
    .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      background:rgba(138, 155, 168, 0.3);
      color:#f5f8fa; }
    .bp3-dark .bp3-html-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal .bp3-dark select:disabled,
    .bp3-dark .bp3-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{
      background:none;
      cursor:not-allowed;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{
        background:rgba(138, 155, 168, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-primary,
  .bp3-select.bp3-minimal select.bp3-intent-primary{
    color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover{
      background:rgba(19, 124, 189, 0.15);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      background:rgba(19, 124, 189, 0.3);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{
      background:none;
      color:rgba(16, 107, 163, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{
        background:rgba(19, 124, 189, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
      stroke:#106ba3; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{
      color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.2);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(72, 175, 240, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-success,
  .bp3-select.bp3-minimal select.bp3-intent-success{
    color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover{
      background:rgba(15, 153, 96, 0.15);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      background:rgba(15, 153, 96, 0.3);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{
      background:none;
      color:rgba(13, 128, 80, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{
        background:rgba(15, 153, 96, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
      stroke:#0d8050; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{
      color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.2);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(61, 204, 145, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-warning,
  .bp3-select.bp3-minimal select.bp3-intent-warning{
    color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover{
      background:rgba(217, 130, 43, 0.15);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      background:rgba(217, 130, 43, 0.3);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{
      background:none;
      color:rgba(191, 115, 38, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{
        background:rgba(217, 130, 43, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
      stroke:#bf7326; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{
      color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.2);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(255, 179, 102, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-danger,
  .bp3-select.bp3-minimal select.bp3-intent-danger{
    color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      -webkit-box-shadow:none;
              box-shadow:none;
      background:none;
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover{
      background:rgba(219, 55, 55, 0.15);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      background:rgba(219, 55, 55, 0.3);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{
      background:none;
      color:rgba(194, 48, 48, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{
        background:rgba(219, 55, 55, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
      stroke:#c23030; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{
      color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.2);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(255, 115, 115, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }

.bp3-html-select.bp3-large select,
.bp3-select.bp3-large select{
  height:40px;
  padding-right:35px;
  font-size:16px; }

.bp3-dark .bp3-html-select select, .bp3-dark .bp3-select select{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
  background-color:#394b59;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
  color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover, .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#30404d; }
  .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#202b33;
    background-image:none; }
  .bp3-dark .bp3-html-select select:disabled, .bp3-dark .bp3-select select:disabled, .bp3-dark .bp3-html-select select.bp3-disabled, .bp3-dark .bp3-select select.bp3-disabled{
    -webkit-box-shadow:none;
            box-shadow:none;
    background-color:rgba(57, 75, 89, 0.5);
    background-image:none;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-html-select select:disabled.bp3-active, .bp3-dark .bp3-select select:disabled.bp3-active, .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active, .bp3-dark .bp3-select select.bp3-disabled.bp3-active{
      background:rgba(57, 75, 89, 0.7); }
  .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head, .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{
    background:rgba(16, 22, 26, 0.5);
    stroke:#8a9ba8; }

.bp3-html-select select:disabled,
.bp3-select select:disabled{
  -webkit-box-shadow:none;
          box-shadow:none;
  background-color:rgba(206, 217, 224, 0.5);
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-html-select .bp3-icon,
.bp3-select .bp3-icon, .bp3-select::after{
  position:absolute;
  top:7px;
  right:7px;
  color:#5c7080;
  pointer-events:none; }
  .bp3-html-select .bp3-disabled.bp3-icon,
  .bp3-select .bp3-disabled.bp3-icon, .bp3-disabled.bp3-select::after{
    color:rgba(92, 112, 128, 0.6); }
.bp3-html-select,
.bp3-select{
  display:inline-block;
  position:relative;
  vertical-align:middle;
  letter-spacing:normal; }
  .bp3-html-select select::-ms-expand,
  .bp3-select select::-ms-expand{
    display:none; }
  .bp3-html-select .bp3-icon,
  .bp3-select .bp3-icon{
    color:#5c7080; }
    .bp3-html-select .bp3-icon:hover,
    .bp3-select .bp3-icon:hover{
      color:#182026; }
    .bp3-dark .bp3-html-select .bp3-icon, .bp3-dark
    .bp3-select .bp3-icon{
      color:#a7b6c2; }
      .bp3-dark .bp3-html-select .bp3-icon:hover, .bp3-dark
      .bp3-select .bp3-icon:hover{
        color:#f5f8fa; }
  .bp3-html-select.bp3-large::after,
  .bp3-html-select.bp3-large .bp3-icon,
  .bp3-select.bp3-large::after,
  .bp3-select.bp3-large .bp3-icon{
    top:12px;
    right:12px; }
  .bp3-html-select.bp3-fill,
  .bp3-html-select.bp3-fill select,
  .bp3-select.bp3-fill,
  .bp3-select.bp3-fill select{
    width:100%; }
  .bp3-dark .bp3-html-select option, .bp3-dark
  .bp3-select option{
    background-color:#30404d;
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select::after, .bp3-dark
  .bp3-select::after{
    color:#a7b6c2; }

.bp3-select::after{
  line-height:1;
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-weight:400;
  font-style:normal;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  content:""; }
.bp3-running-text table, table.bp3-html-table{
  border-spacing:0;
  font-size:14px; }
  .bp3-running-text table th, table.bp3-html-table th,
  .bp3-running-text table td,
  table.bp3-html-table td{
    padding:11px;
    vertical-align:top;
    text-align:left; }
  .bp3-running-text table th, table.bp3-html-table th{
    color:#182026;
    font-weight:600; }
  
  .bp3-running-text table td,
  table.bp3-html-table td{
    color:#182026; }
  .bp3-running-text table tbody tr:first-child th, table.bp3-html-table tbody tr:first-child th,
  .bp3-running-text table tbody tr:first-child td,
  table.bp3-html-table tbody tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-running-text table th, .bp3-running-text .bp3-dark table th, .bp3-dark table.bp3-html-table th{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table td, .bp3-running-text .bp3-dark table td, .bp3-dark table.bp3-html-table td{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table tbody tr:first-child th, .bp3-running-text .bp3-dark table tbody tr:first-child th, .bp3-dark table.bp3-html-table tbody tr:first-child th,
  .bp3-dark .bp3-running-text table tbody tr:first-child td,
  .bp3-running-text .bp3-dark table tbody tr:first-child td,
  .bp3-dark table.bp3-html-table tbody tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }

table.bp3-html-table.bp3-html-table-condensed th,
table.bp3-html-table.bp3-html-table-condensed td, table.bp3-html-table.bp3-small th,
table.bp3-html-table.bp3-small td{
  padding-top:6px;
  padding-bottom:6px; }

table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
  background:rgba(191, 204, 214, 0.15); }

table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
  -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered tbody tr td{
  -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){
    -webkit-box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
  -webkit-box-shadow:none;
          box-shadow:none; }
  table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){
    -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-interactive tbody tr:hover td{
  background-color:rgba(191, 204, 214, 0.3);
  cursor:pointer; }

table.bp3-html-table.bp3-interactive tbody tr:active td{
  background-color:rgba(191, 204, 214, 0.4); }

.bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
  background:rgba(92, 112, 128, 0.15); }

.bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
  -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }

.bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{
  -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
          box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){
    -webkit-box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); }

.bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
  -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{
    -webkit-box-shadow:none;
            box-shadow:none; }

.bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{
  background-color:rgba(92, 112, 128, 0.3);
  cursor:pointer; }

.bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{
  background-color:rgba(92, 112, 128, 0.4); }

.bp3-key-combo{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center; }
  .bp3-key-combo > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-key-combo > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-key-combo::before,
  .bp3-key-combo > *{
    margin-right:5px; }
  .bp3-key-combo:empty::before,
  .bp3-key-combo > :last-child{
    margin-right:0; }

.bp3-hotkey-dialog{
  top:40px;
  padding-bottom:0; }
  .bp3-hotkey-dialog .bp3-dialog-body{
    margin:0;
    padding:0; }
  .bp3-hotkey-dialog .bp3-hotkey-label{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1; }

.bp3-hotkey-column{
  margin:auto;
  max-height:80vh;
  overflow-y:auto;
  padding:30px; }
  .bp3-hotkey-column .bp3-heading{
    margin-bottom:20px; }
    .bp3-hotkey-column .bp3-heading:not(:first-child){
      margin-top:40px; }

.bp3-hotkey{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:justify;
      -ms-flex-pack:justify;
          justify-content:space-between;
  margin-right:0;
  margin-left:0; }
  .bp3-hotkey:not(:last-child){
    margin-bottom:10px; }
.bp3-icon{
  display:inline-block;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  vertical-align:text-bottom; }
  .bp3-icon:not(:empty)::before{
    content:"" !important;
    content:unset !important; }
  .bp3-icon > svg{
    display:block; }
    .bp3-icon > svg:not([fill]){
      fill:currentColor; }

.bp3-icon.bp3-intent-primary, .bp3-icon-standard.bp3-intent-primary, .bp3-icon-large.bp3-intent-primary{
  color:#106ba3; }
  .bp3-dark .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-icon-large.bp3-intent-primary{
    color:#48aff0; }

.bp3-icon.bp3-intent-success, .bp3-icon-standard.bp3-intent-success, .bp3-icon-large.bp3-intent-success{
  color:#0d8050; }
  .bp3-dark .bp3-icon.bp3-intent-success, .bp3-dark .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-icon-large.bp3-intent-success{
    color:#3dcc91; }

.bp3-icon.bp3-intent-warning, .bp3-icon-standard.bp3-intent-warning, .bp3-icon-large.bp3-intent-warning{
  color:#bf7326; }
  .bp3-dark .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-icon-large.bp3-intent-warning{
    color:#ffb366; }

.bp3-icon.bp3-intent-danger, .bp3-icon-standard.bp3-intent-danger, .bp3-icon-large.bp3-intent-danger{
  color:#c23030; }
  .bp3-dark .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-icon-large.bp3-intent-danger{
    color:#ff7373; }

span.bp3-icon-standard{
  line-height:1;
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-weight:400;
  font-style:normal;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon-large{
  line-height:1;
  font-family:"Icons20", sans-serif;
  font-size:20px;
  font-weight:400;
  font-style:normal;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon:empty{
  line-height:1;
  font-family:"Icons20";
  font-size:inherit;
  font-weight:400;
  font-style:normal; }
  span.bp3-icon:empty::before{
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased; }

.bp3-icon-add::before{
  content:""; }

.bp3-icon-add-column-left::before{
  content:""; }

.bp3-icon-add-column-right::before{
  content:""; }

.bp3-icon-add-row-bottom::before{
  content:""; }

.bp3-icon-add-row-top::before{
  content:""; }

.bp3-icon-add-to-artifact::before{
  content:""; }

.bp3-icon-add-to-folder::before{
  content:""; }

.bp3-icon-airplane::before{
  content:""; }

.bp3-icon-align-center::before{
  content:""; }

.bp3-icon-align-justify::before{
  content:""; }

.bp3-icon-align-left::before{
  content:""; }

.bp3-icon-align-right::before{
  content:""; }

.bp3-icon-alignment-bottom::before{
  content:""; }

.bp3-icon-alignment-horizontal-center::before{
  content:""; }

.bp3-icon-alignment-left::before{
  content:""; }

.bp3-icon-alignment-right::before{
  content:""; }

.bp3-icon-alignment-top::before{
  content:""; }

.bp3-icon-alignment-vertical-center::before{
  content:""; }

.bp3-icon-annotation::before{
  content:""; }

.bp3-icon-application::before{
  content:""; }

.bp3-icon-applications::before{
  content:""; }

.bp3-icon-archive::before{
  content:""; }

.bp3-icon-arrow-bottom-left::before{
  content:"↙"; }

.bp3-icon-arrow-bottom-right::before{
  content:"↘"; }

.bp3-icon-arrow-down::before{
  content:"↓"; }

.bp3-icon-arrow-left::before{
  content:"←"; }

.bp3-icon-arrow-right::before{
  content:"→"; }

.bp3-icon-arrow-top-left::before{
  content:"↖"; }

.bp3-icon-arrow-top-right::before{
  content:"↗"; }

.bp3-icon-arrow-up::before{
  content:"↑"; }

.bp3-icon-arrows-horizontal::before{
  content:"↔"; }

.bp3-icon-arrows-vertical::before{
  content:"↕"; }

.bp3-icon-asterisk::before{
  content:"*"; }

.bp3-icon-automatic-updates::before{
  content:""; }

.bp3-icon-badge::before{
  content:""; }

.bp3-icon-ban-circle::before{
  content:""; }

.bp3-icon-bank-account::before{
  content:""; }

.bp3-icon-barcode::before{
  content:""; }

.bp3-icon-blank::before{
  content:""; }

.bp3-icon-blocked-person::before{
  content:""; }

.bp3-icon-bold::before{
  content:""; }

.bp3-icon-book::before{
  content:""; }

.bp3-icon-bookmark::before{
  content:""; }

.bp3-icon-box::before{
  content:""; }

.bp3-icon-briefcase::before{
  content:""; }

.bp3-icon-bring-data::before{
  content:""; }

.bp3-icon-build::before{
  content:""; }

.bp3-icon-calculator::before{
  content:""; }

.bp3-icon-calendar::before{
  content:""; }

.bp3-icon-camera::before{
  content:""; }

.bp3-icon-caret-down::before{
  content:"⌄"; }

.bp3-icon-caret-left::before{
  content:"〈"; }

.bp3-icon-caret-right::before{
  content:"〉"; }

.bp3-icon-caret-up::before{
  content:"⌃"; }

.bp3-icon-cell-tower::before{
  content:""; }

.bp3-icon-changes::before{
  content:""; }

.bp3-icon-chart::before{
  content:""; }

.bp3-icon-chat::before{
  content:""; }

.bp3-icon-chevron-backward::before{
  content:""; }

.bp3-icon-chevron-down::before{
  content:""; }

.bp3-icon-chevron-forward::before{
  content:""; }

.bp3-icon-chevron-left::before{
  content:""; }

.bp3-icon-chevron-right::before{
  content:""; }

.bp3-icon-chevron-up::before{
  content:""; }

.bp3-icon-circle::before{
  content:""; }

.bp3-icon-circle-arrow-down::before{
  content:""; }

.bp3-icon-circle-arrow-left::before{
  content:""; }

.bp3-icon-circle-arrow-right::before{
  content:""; }

.bp3-icon-circle-arrow-up::before{
  content:""; }

.bp3-icon-citation::before{
  content:""; }

.bp3-icon-clean::before{
  content:""; }

.bp3-icon-clipboard::before{
  content:""; }

.bp3-icon-cloud::before{
  content:"☁"; }

.bp3-icon-cloud-download::before{
  content:""; }

.bp3-icon-cloud-upload::before{
  content:""; }

.bp3-icon-code::before{
  content:""; }

.bp3-icon-code-block::before{
  content:""; }

.bp3-icon-cog::before{
  content:""; }

.bp3-icon-collapse-all::before{
  content:""; }

.bp3-icon-column-layout::before{
  content:""; }

.bp3-icon-comment::before{
  content:""; }

.bp3-icon-comparison::before{
  content:""; }

.bp3-icon-compass::before{
  content:""; }

.bp3-icon-compressed::before{
  content:""; }

.bp3-icon-confirm::before{
  content:""; }

.bp3-icon-console::before{
  content:""; }

.bp3-icon-contrast::before{
  content:""; }

.bp3-icon-control::before{
  content:""; }

.bp3-icon-credit-card::before{
  content:""; }

.bp3-icon-cross::before{
  content:"✗"; }

.bp3-icon-crown::before{
  content:""; }

.bp3-icon-cube::before{
  content:""; }

.bp3-icon-cube-add::before{
  content:""; }

.bp3-icon-cube-remove::before{
  content:""; }

.bp3-icon-curved-range-chart::before{
  content:""; }

.bp3-icon-cut::before{
  content:""; }

.bp3-icon-dashboard::before{
  content:""; }

.bp3-icon-data-lineage::before{
  content:""; }

.bp3-icon-database::before{
  content:""; }

.bp3-icon-delete::before{
  content:""; }

.bp3-icon-delta::before{
  content:"Δ"; }

.bp3-icon-derive-column::before{
  content:""; }

.bp3-icon-desktop::before{
  content:""; }

.bp3-icon-diagram-tree::before{
  content:""; }

.bp3-icon-direction-left::before{
  content:""; }

.bp3-icon-direction-right::before{
  content:""; }

.bp3-icon-disable::before{
  content:""; }

.bp3-icon-document::before{
  content:""; }

.bp3-icon-document-open::before{
  content:""; }

.bp3-icon-document-share::before{
  content:""; }

.bp3-icon-dollar::before{
  content:"$"; }

.bp3-icon-dot::before{
  content:"•"; }

.bp3-icon-double-caret-horizontal::before{
  content:""; }

.bp3-icon-double-caret-vertical::before{
  content:""; }

.bp3-icon-double-chevron-down::before{
  content:""; }

.bp3-icon-double-chevron-left::before{
  content:""; }

.bp3-icon-double-chevron-right::before{
  content:""; }

.bp3-icon-double-chevron-up::before{
  content:""; }

.bp3-icon-doughnut-chart::before{
  content:""; }

.bp3-icon-download::before{
  content:""; }

.bp3-icon-drag-handle-horizontal::before{
  content:""; }

.bp3-icon-drag-handle-vertical::before{
  content:""; }

.bp3-icon-draw::before{
  content:""; }

.bp3-icon-drive-time::before{
  content:""; }

.bp3-icon-duplicate::before{
  content:""; }

.bp3-icon-edit::before{
  content:"✎"; }

.bp3-icon-eject::before{
  content:"⏏"; }

.bp3-icon-endorsed::before{
  content:""; }

.bp3-icon-envelope::before{
  content:"✉"; }

.bp3-icon-equals::before{
  content:""; }

.bp3-icon-eraser::before{
  content:""; }

.bp3-icon-error::before{
  content:""; }

.bp3-icon-euro::before{
  content:"€"; }

.bp3-icon-exchange::before{
  content:""; }

.bp3-icon-exclude-row::before{
  content:""; }

.bp3-icon-expand-all::before{
  content:""; }

.bp3-icon-export::before{
  content:""; }

.bp3-icon-eye-off::before{
  content:""; }

.bp3-icon-eye-on::before{
  content:""; }

.bp3-icon-eye-open::before{
  content:""; }

.bp3-icon-fast-backward::before{
  content:""; }

.bp3-icon-fast-forward::before{
  content:""; }

.bp3-icon-feed::before{
  content:""; }

.bp3-icon-feed-subscribed::before{
  content:""; }

.bp3-icon-film::before{
  content:""; }

.bp3-icon-filter::before{
  content:""; }

.bp3-icon-filter-keep::before{
  content:""; }

.bp3-icon-filter-list::before{
  content:""; }

.bp3-icon-filter-open::before{
  content:""; }

.bp3-icon-filter-remove::before{
  content:""; }

.bp3-icon-flag::before{
  content:"⚑"; }

.bp3-icon-flame::before{
  content:""; }

.bp3-icon-flash::before{
  content:""; }

.bp3-icon-floppy-disk::before{
  content:""; }

.bp3-icon-flow-branch::before{
  content:""; }

.bp3-icon-flow-end::before{
  content:""; }

.bp3-icon-flow-linear::before{
  content:""; }

.bp3-icon-flow-review::before{
  content:""; }

.bp3-icon-flow-review-branch::before{
  content:""; }

.bp3-icon-flows::before{
  content:""; }

.bp3-icon-folder-close::before{
  content:""; }

.bp3-icon-folder-new::before{
  content:""; }

.bp3-icon-folder-open::before{
  content:""; }

.bp3-icon-folder-shared::before{
  content:""; }

.bp3-icon-folder-shared-open::before{
  content:""; }

.bp3-icon-follower::before{
  content:""; }

.bp3-icon-following::before{
  content:""; }

.bp3-icon-font::before{
  content:""; }

.bp3-icon-fork::before{
  content:""; }

.bp3-icon-form::before{
  content:""; }

.bp3-icon-full-circle::before{
  content:""; }

.bp3-icon-full-stacked-chart::before{
  content:""; }

.bp3-icon-fullscreen::before{
  content:""; }

.bp3-icon-function::before{
  content:""; }

.bp3-icon-gantt-chart::before{
  content:""; }

.bp3-icon-geolocation::before{
  content:""; }

.bp3-icon-geosearch::before{
  content:""; }

.bp3-icon-git-branch::before{
  content:""; }

.bp3-icon-git-commit::before{
  content:""; }

.bp3-icon-git-merge::before{
  content:""; }

.bp3-icon-git-new-branch::before{
  content:""; }

.bp3-icon-git-pull::before{
  content:""; }

.bp3-icon-git-push::before{
  content:""; }

.bp3-icon-git-repo::before{
  content:""; }

.bp3-icon-glass::before{
  content:""; }

.bp3-icon-globe::before{
  content:""; }

.bp3-icon-globe-network::before{
  content:""; }

.bp3-icon-graph::before{
  content:""; }

.bp3-icon-graph-remove::before{
  content:""; }

.bp3-icon-greater-than::before{
  content:""; }

.bp3-icon-greater-than-or-equal-to::before{
  content:""; }

.bp3-icon-grid::before{
  content:""; }

.bp3-icon-grid-view::before{
  content:""; }

.bp3-icon-group-objects::before{
  content:""; }

.bp3-icon-grouped-bar-chart::before{
  content:""; }

.bp3-icon-hand::before{
  content:""; }

.bp3-icon-hand-down::before{
  content:""; }

.bp3-icon-hand-left::before{
  content:""; }

.bp3-icon-hand-right::before{
  content:""; }

.bp3-icon-hand-up::before{
  content:""; }

.bp3-icon-header::before{
  content:""; }

.bp3-icon-header-one::before{
  content:""; }

.bp3-icon-header-two::before{
  content:""; }

.bp3-icon-headset::before{
  content:""; }

.bp3-icon-heart::before{
  content:"♥"; }

.bp3-icon-heart-broken::before{
  content:""; }

.bp3-icon-heat-grid::before{
  content:""; }

.bp3-icon-heatmap::before{
  content:""; }

.bp3-icon-help::before{
  content:"?"; }

.bp3-icon-helper-management::before{
  content:""; }

.bp3-icon-highlight::before{
  content:""; }

.bp3-icon-history::before{
  content:""; }

.bp3-icon-home::before{
  content:"⌂"; }

.bp3-icon-horizontal-bar-chart::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-asc::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-desc::before{
  content:""; }

.bp3-icon-horizontal-distribution::before{
  content:""; }

.bp3-icon-id-number::before{
  content:""; }

.bp3-icon-image-rotate-left::before{
  content:""; }

.bp3-icon-image-rotate-right::before{
  content:""; }

.bp3-icon-import::before{
  content:""; }

.bp3-icon-inbox::before{
  content:""; }

.bp3-icon-inbox-filtered::before{
  content:""; }

.bp3-icon-inbox-geo::before{
  content:""; }

.bp3-icon-inbox-search::before{
  content:""; }

.bp3-icon-inbox-update::before{
  content:""; }

.bp3-icon-info-sign::before{
  content:"ℹ"; }

.bp3-icon-inheritance::before{
  content:""; }

.bp3-icon-inner-join::before{
  content:""; }

.bp3-icon-insert::before{
  content:""; }

.bp3-icon-intersection::before{
  content:""; }

.bp3-icon-ip-address::before{
  content:""; }

.bp3-icon-issue::before{
  content:""; }

.bp3-icon-issue-closed::before{
  content:""; }

.bp3-icon-issue-new::before{
  content:""; }

.bp3-icon-italic::before{
  content:""; }

.bp3-icon-join-table::before{
  content:""; }

.bp3-icon-key::before{
  content:""; }

.bp3-icon-key-backspace::before{
  content:""; }

.bp3-icon-key-command::before{
  content:""; }

.bp3-icon-key-control::before{
  content:""; }

.bp3-icon-key-delete::before{
  content:""; }

.bp3-icon-key-enter::before{
  content:""; }

.bp3-icon-key-escape::before{
  content:""; }

.bp3-icon-key-option::before{
  content:""; }

.bp3-icon-key-shift::before{
  content:""; }

.bp3-icon-key-tab::before{
  content:""; }

.bp3-icon-known-vehicle::before{
  content:""; }

.bp3-icon-label::before{
  content:""; }

.bp3-icon-layer::before{
  content:""; }

.bp3-icon-layers::before{
  content:""; }

.bp3-icon-layout::before{
  content:""; }

.bp3-icon-layout-auto::before{
  content:""; }

.bp3-icon-layout-balloon::before{
  content:""; }

.bp3-icon-layout-circle::before{
  content:""; }

.bp3-icon-layout-grid::before{
  content:""; }

.bp3-icon-layout-group-by::before{
  content:""; }

.bp3-icon-layout-hierarchy::before{
  content:""; }

.bp3-icon-layout-linear::before{
  content:""; }

.bp3-icon-layout-skew-grid::before{
  content:""; }

.bp3-icon-layout-sorted-clusters::before{
  content:""; }

.bp3-icon-learning::before{
  content:""; }

.bp3-icon-left-join::before{
  content:""; }

.bp3-icon-less-than::before{
  content:""; }

.bp3-icon-less-than-or-equal-to::before{
  content:""; }

.bp3-icon-lifesaver::before{
  content:""; }

.bp3-icon-lightbulb::before{
  content:""; }

.bp3-icon-link::before{
  content:""; }

.bp3-icon-list::before{
  content:"☰"; }

.bp3-icon-list-columns::before{
  content:""; }

.bp3-icon-list-detail-view::before{
  content:""; }

.bp3-icon-locate::before{
  content:""; }

.bp3-icon-lock::before{
  content:""; }

.bp3-icon-log-in::before{
  content:""; }

.bp3-icon-log-out::before{
  content:""; }

.bp3-icon-manual::before{
  content:""; }

.bp3-icon-manually-entered-data::before{
  content:""; }

.bp3-icon-map::before{
  content:""; }

.bp3-icon-map-create::before{
  content:""; }

.bp3-icon-map-marker::before{
  content:""; }

.bp3-icon-maximize::before{
  content:""; }

.bp3-icon-media::before{
  content:""; }

.bp3-icon-menu::before{
  content:""; }

.bp3-icon-menu-closed::before{
  content:""; }

.bp3-icon-menu-open::before{
  content:""; }

.bp3-icon-merge-columns::before{
  content:""; }

.bp3-icon-merge-links::before{
  content:""; }

.bp3-icon-minimize::before{
  content:""; }

.bp3-icon-minus::before{
  content:"−"; }

.bp3-icon-mobile-phone::before{
  content:""; }

.bp3-icon-mobile-video::before{
  content:""; }

.bp3-icon-moon::before{
  content:""; }

.bp3-icon-more::before{
  content:""; }

.bp3-icon-mountain::before{
  content:""; }

.bp3-icon-move::before{
  content:""; }

.bp3-icon-mugshot::before{
  content:""; }

.bp3-icon-multi-select::before{
  content:""; }

.bp3-icon-music::before{
  content:""; }

.bp3-icon-new-drawing::before{
  content:""; }

.bp3-icon-new-grid-item::before{
  content:""; }

.bp3-icon-new-layer::before{
  content:""; }

.bp3-icon-new-layers::before{
  content:""; }

.bp3-icon-new-link::before{
  content:""; }

.bp3-icon-new-object::before{
  content:""; }

.bp3-icon-new-person::before{
  content:""; }

.bp3-icon-new-prescription::before{
  content:""; }

.bp3-icon-new-text-box::before{
  content:""; }

.bp3-icon-ninja::before{
  content:""; }

.bp3-icon-not-equal-to::before{
  content:""; }

.bp3-icon-notifications::before{
  content:""; }

.bp3-icon-notifications-updated::before{
  content:""; }

.bp3-icon-numbered-list::before{
  content:""; }

.bp3-icon-numerical::before{
  content:""; }

.bp3-icon-office::before{
  content:""; }

.bp3-icon-offline::before{
  content:""; }

.bp3-icon-oil-field::before{
  content:""; }

.bp3-icon-one-column::before{
  content:""; }

.bp3-icon-outdated::before{
  content:""; }

.bp3-icon-page-layout::before{
  content:""; }

.bp3-icon-panel-stats::before{
  content:""; }

.bp3-icon-panel-table::before{
  content:""; }

.bp3-icon-paperclip::before{
  content:""; }

.bp3-icon-paragraph::before{
  content:""; }

.bp3-icon-path::before{
  content:""; }

.bp3-icon-path-search::before{
  content:""; }

.bp3-icon-pause::before{
  content:""; }

.bp3-icon-people::before{
  content:""; }

.bp3-icon-percentage::before{
  content:""; }

.bp3-icon-person::before{
  content:""; }

.bp3-icon-phone::before{
  content:"☎"; }

.bp3-icon-pie-chart::before{
  content:""; }

.bp3-icon-pin::before{
  content:""; }

.bp3-icon-pivot::before{
  content:""; }

.bp3-icon-pivot-table::before{
  content:""; }

.bp3-icon-play::before{
  content:""; }

.bp3-icon-plus::before{
  content:"+"; }

.bp3-icon-polygon-filter::before{
  content:""; }

.bp3-icon-power::before{
  content:""; }

.bp3-icon-predictive-analysis::before{
  content:""; }

.bp3-icon-prescription::before{
  content:""; }

.bp3-icon-presentation::before{
  content:""; }

.bp3-icon-print::before{
  content:"⎙"; }

.bp3-icon-projects::before{
  content:""; }

.bp3-icon-properties::before{
  content:""; }

.bp3-icon-property::before{
  content:""; }

.bp3-icon-publish-function::before{
  content:""; }

.bp3-icon-pulse::before{
  content:""; }

.bp3-icon-random::before{
  content:""; }

.bp3-icon-record::before{
  content:""; }

.bp3-icon-redo::before{
  content:""; }

.bp3-icon-refresh::before{
  content:""; }

.bp3-icon-regression-chart::before{
  content:""; }

.bp3-icon-remove::before{
  content:""; }

.bp3-icon-remove-column::before{
  content:""; }

.bp3-icon-remove-column-left::before{
  content:""; }

.bp3-icon-remove-column-right::before{
  content:""; }

.bp3-icon-remove-row-bottom::before{
  content:""; }

.bp3-icon-remove-row-top::before{
  content:""; }

.bp3-icon-repeat::before{
  content:""; }

.bp3-icon-reset::before{
  content:""; }

.bp3-icon-resolve::before{
  content:""; }

.bp3-icon-rig::before{
  content:""; }

.bp3-icon-right-join::before{
  content:""; }

.bp3-icon-ring::before{
  content:""; }

.bp3-icon-rotate-document::before{
  content:""; }

.bp3-icon-rotate-page::before{
  content:""; }

.bp3-icon-satellite::before{
  content:""; }

.bp3-icon-saved::before{
  content:""; }

.bp3-icon-scatter-plot::before{
  content:""; }

.bp3-icon-search::before{
  content:""; }

.bp3-icon-search-around::before{
  content:""; }

.bp3-icon-search-template::before{
  content:""; }

.bp3-icon-search-text::before{
  content:""; }

.bp3-icon-segmented-control::before{
  content:""; }

.bp3-icon-select::before{
  content:""; }

.bp3-icon-selection::before{
  content:"⦿"; }

.bp3-icon-send-to::before{
  content:""; }

.bp3-icon-send-to-graph::before{
  content:""; }

.bp3-icon-send-to-map::before{
  content:""; }

.bp3-icon-series-add::before{
  content:""; }

.bp3-icon-series-configuration::before{
  content:""; }

.bp3-icon-series-derived::before{
  content:""; }

.bp3-icon-series-filtered::before{
  content:""; }

.bp3-icon-series-search::before{
  content:""; }

.bp3-icon-settings::before{
  content:""; }

.bp3-icon-share::before{
  content:""; }

.bp3-icon-shield::before{
  content:""; }

.bp3-icon-shop::before{
  content:""; }

.bp3-icon-shopping-cart::before{
  content:""; }

.bp3-icon-signal-search::before{
  content:""; }

.bp3-icon-sim-card::before{
  content:""; }

.bp3-icon-slash::before{
  content:""; }

.bp3-icon-small-cross::before{
  content:""; }

.bp3-icon-small-minus::before{
  content:""; }

.bp3-icon-small-plus::before{
  content:""; }

.bp3-icon-small-tick::before{
  content:""; }

.bp3-icon-snowflake::before{
  content:""; }

.bp3-icon-social-media::before{
  content:""; }

.bp3-icon-sort::before{
  content:""; }

.bp3-icon-sort-alphabetical::before{
  content:""; }

.bp3-icon-sort-alphabetical-desc::before{
  content:""; }

.bp3-icon-sort-asc::before{
  content:""; }

.bp3-icon-sort-desc::before{
  content:""; }

.bp3-icon-sort-numerical::before{
  content:""; }

.bp3-icon-sort-numerical-desc::before{
  content:""; }

.bp3-icon-split-columns::before{
  content:""; }

.bp3-icon-square::before{
  content:""; }

.bp3-icon-stacked-chart::before{
  content:""; }

.bp3-icon-star::before{
  content:"★"; }

.bp3-icon-star-empty::before{
  content:"☆"; }

.bp3-icon-step-backward::before{
  content:""; }

.bp3-icon-step-chart::before{
  content:""; }

.bp3-icon-step-forward::before{
  content:""; }

.bp3-icon-stop::before{
  content:""; }

.bp3-icon-stopwatch::before{
  content:""; }

.bp3-icon-strikethrough::before{
  content:""; }

.bp3-icon-style::before{
  content:""; }

.bp3-icon-swap-horizontal::before{
  content:""; }

.bp3-icon-swap-vertical::before{
  content:""; }

.bp3-icon-symbol-circle::before{
  content:""; }

.bp3-icon-symbol-cross::before{
  content:""; }

.bp3-icon-symbol-diamond::before{
  content:""; }

.bp3-icon-symbol-square::before{
  content:""; }

.bp3-icon-symbol-triangle-down::before{
  content:""; }

.bp3-icon-symbol-triangle-up::before{
  content:""; }

.bp3-icon-tag::before{
  content:""; }

.bp3-icon-take-action::before{
  content:""; }

.bp3-icon-taxi::before{
  content:""; }

.bp3-icon-text-highlight::before{
  content:""; }

.bp3-icon-th::before{
  content:""; }

.bp3-icon-th-derived::before{
  content:""; }

.bp3-icon-th-disconnect::before{
  content:""; }

.bp3-icon-th-filtered::before{
  content:""; }

.bp3-icon-th-list::before{
  content:""; }

.bp3-icon-thumbs-down::before{
  content:""; }

.bp3-icon-thumbs-up::before{
  content:""; }

.bp3-icon-tick::before{
  content:"✓"; }

.bp3-icon-tick-circle::before{
  content:""; }

.bp3-icon-time::before{
  content:"⏲"; }

.bp3-icon-timeline-area-chart::before{
  content:""; }

.bp3-icon-timeline-bar-chart::before{
  content:""; }

.bp3-icon-timeline-events::before{
  content:""; }

.bp3-icon-timeline-line-chart::before{
  content:""; }

.bp3-icon-tint::before{
  content:""; }

.bp3-icon-torch::before{
  content:""; }

.bp3-icon-tractor::before{
  content:""; }

.bp3-icon-train::before{
  content:""; }

.bp3-icon-translate::before{
  content:""; }

.bp3-icon-trash::before{
  content:""; }

.bp3-icon-tree::before{
  content:""; }

.bp3-icon-trending-down::before{
  content:""; }

.bp3-icon-trending-up::before{
  content:""; }

.bp3-icon-truck::before{
  content:""; }

.bp3-icon-two-columns::before{
  content:""; }

.bp3-icon-unarchive::before{
  content:""; }

.bp3-icon-underline::before{
  content:"⎁"; }

.bp3-icon-undo::before{
  content:"⎌"; }

.bp3-icon-ungroup-objects::before{
  content:""; }

.bp3-icon-unknown-vehicle::before{
  content:""; }

.bp3-icon-unlock::before{
  content:""; }

.bp3-icon-unpin::before{
  content:""; }

.bp3-icon-unresolve::before{
  content:""; }

.bp3-icon-updated::before{
  content:""; }

.bp3-icon-upload::before{
  content:""; }

.bp3-icon-user::before{
  content:""; }

.bp3-icon-variable::before{
  content:""; }

.bp3-icon-vertical-bar-chart-asc::before{
  content:""; }

.bp3-icon-vertical-bar-chart-desc::before{
  content:""; }

.bp3-icon-vertical-distribution::before{
  content:""; }

.bp3-icon-video::before{
  content:""; }

.bp3-icon-volume-down::before{
  content:""; }

.bp3-icon-volume-off::before{
  content:""; }

.bp3-icon-volume-up::before{
  content:""; }

.bp3-icon-walk::before{
  content:""; }

.bp3-icon-warning-sign::before{
  content:""; }

.bp3-icon-waterfall-chart::before{
  content:""; }

.bp3-icon-widget::before{
  content:""; }

.bp3-icon-widget-button::before{
  content:""; }

.bp3-icon-widget-footer::before{
  content:""; }

.bp3-icon-widget-header::before{
  content:""; }

.bp3-icon-wrench::before{
  content:""; }

.bp3-icon-zoom-in::before{
  content:""; }

.bp3-icon-zoom-out::before{
  content:""; }

.bp3-icon-zoom-to-fit::before{
  content:""; }
.bp3-submenu > .bp3-popover-wrapper{
  display:block; }

.bp3-submenu .bp3-popover-target{
  display:block; }

.bp3-submenu.bp3-popover{
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0 5px; }
  .bp3-submenu.bp3-popover > .bp3-popover-content{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-submenu.bp3-popover, .bp3-submenu.bp3-popover.bp3-dark{
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-dark .bp3-submenu.bp3-popover > .bp3-popover-content, .bp3-submenu.bp3-popover.bp3-dark > .bp3-popover-content{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
.bp3-menu{
  margin:0;
  border-radius:3px;
  background:#ffffff;
  min-width:180px;
  padding:5px;
  list-style:none;
  text-align:left;
  color:#182026; }

.bp3-menu-divider{
  display:block;
  margin:5px;
  border-top:1px solid rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-menu-divider{
    border-top-color:rgba(255, 255, 255, 0.15); }

.bp3-menu-item{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  border-radius:2px;
  padding:5px 7px;
  text-decoration:none;
  line-height:20px;
  color:inherit;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-menu-item > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-menu-item > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-menu-item::before,
  .bp3-menu-item > *{
    margin-right:7px; }
  .bp3-menu-item:empty::before,
  .bp3-menu-item > :last-child{
    margin-right:0; }
  .bp3-menu-item > .bp3-fill{
    word-break:break-word; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    background-color:rgba(167, 182, 194, 0.3);
    cursor:pointer;
    text-decoration:none; }
  .bp3-menu-item.bp3-disabled{
    background-color:inherit;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-dark .bp3-menu-item{
    color:inherit; }
    .bp3-dark .bp3-menu-item:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
      background-color:rgba(138, 155, 168, 0.15);
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-disabled{
      background-color:inherit;
      color:rgba(167, 182, 194, 0.6); }
  .bp3-menu-item.bp3-intent-primary{
    color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-primary::before, .bp3-menu-item.bp3-intent-primary::after,
    .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
      color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary.bp3-active{
      background-color:#137cbd; }
    .bp3-menu-item.bp3-intent-primary:active{
      background-color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary:active, .bp3-menu-item.bp3-intent-primary:active::before, .bp3-menu-item.bp3-intent-primary:active::after,
    .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-menu-item.bp3-intent-primary.bp3-active::after,
    .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-success{
    color:#0d8050; }
    .bp3-menu-item.bp3-intent-success .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-success::before, .bp3-menu-item.bp3-intent-success::after,
    .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
      color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success.bp3-active{
      background-color:#0f9960; }
    .bp3-menu-item.bp3-intent-success:active{
      background-color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-menu-item.bp3-intent-success:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success:active, .bp3-menu-item.bp3-intent-success:active::before, .bp3-menu-item.bp3-intent-success:active::after,
    .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-menu-item.bp3-intent-success.bp3-active::after,
    .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-warning{
    color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-warning::before, .bp3-menu-item.bp3-intent-warning::after,
    .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
      color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning.bp3-active{
      background-color:#d9822b; }
    .bp3-menu-item.bp3-intent-warning:active{
      background-color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning:active, .bp3-menu-item.bp3-intent-warning:active::before, .bp3-menu-item.bp3-intent-warning:active::after,
    .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-menu-item.bp3-intent-warning.bp3-active::after,
    .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-danger{
    color:#c23030; }
    .bp3-menu-item.bp3-intent-danger .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-danger::before, .bp3-menu-item.bp3-intent-danger::after,
    .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
      color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger.bp3-active{
      background-color:#db3737; }
    .bp3-menu-item.bp3-intent-danger:active{
      background-color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger:active, .bp3-menu-item.bp3-intent-danger:active::before, .bp3-menu-item.bp3-intent-danger:active::after,
    .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-menu-item.bp3-intent-danger.bp3-active::after,
    .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item::before{
    line-height:1;
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-weight:400;
    font-style:normal;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    margin-right:7px; }
  .bp3-menu-item::before,
  .bp3-menu-item > .bp3-icon{
    margin-top:2px;
    color:#5c7080; }
  .bp3-menu-item .bp3-menu-item-label{
    color:#5c7080; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    color:inherit; }
  .bp3-menu-item.bp3-active, .bp3-menu-item:active{
    background-color:rgba(115, 134, 148, 0.3); }
  .bp3-menu-item.bp3-disabled{
    outline:none !important;
    background-color:inherit !important;
    cursor:not-allowed !important;
    color:rgba(92, 112, 128, 0.6) !important; }
    .bp3-menu-item.bp3-disabled::before,
    .bp3-menu-item.bp3-disabled > .bp3-icon,
    .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-large .bp3-menu-item{
    padding:9px 7px;
    line-height:22px;
    font-size:16px; }
    .bp3-large .bp3-menu-item .bp3-icon{
      margin-top:3px; }
    .bp3-large .bp3-menu-item::before{
      line-height:1;
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-weight:400;
      font-style:normal;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      margin-top:1px;
      margin-right:10px; }

button.bp3-menu-item{
  border:none;
  background:none;
  width:100%;
  text-align:left; }
.bp3-menu-header{
  display:block;
  margin:5px;
  border-top:1px solid rgba(16, 22, 26, 0.15);
  cursor:default;
  padding-left:2px; }
  .bp3-dark .bp3-menu-header{
    border-top-color:rgba(255, 255, 255, 0.15); }
  .bp3-menu-header:first-of-type{
    border-top:none; }
  .bp3-menu-header > h6{
    color:#182026;
    font-weight:600;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    margin:0;
    padding:10px 7px 0 1px;
    line-height:17px; }
    .bp3-dark .bp3-menu-header > h6{
      color:#f5f8fa; }
  .bp3-menu-header:first-of-type > h6{
    padding-top:0; }
  .bp3-large .bp3-menu-header > h6{
    padding-top:15px;
    padding-bottom:5px;
    font-size:18px; }
  .bp3-large .bp3-menu-header:first-of-type > h6{
    padding-top:0; }

.bp3-dark .bp3-menu{
  background:#30404d;
  color:#f5f8fa; }

.bp3-dark .bp3-menu-item.bp3-intent-primary{
  color:#48aff0; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary::before, .bp3-dark .bp3-menu-item.bp3-intent-primary::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
    color:#48aff0; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{
    background-color:#137cbd; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary:active{
    background-color:#106ba3; }
  .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary:active, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item.bp3-intent-success{
  color:#3dcc91; }
  .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-success::before, .bp3-dark .bp3-menu-item.bp3-intent-success::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
    color:#3dcc91; }
  .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{
    background-color:#0f9960; }
  .bp3-dark .bp3-menu-item.bp3-intent-success:active{
    background-color:#0d8050; }
  .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success:active, .bp3-dark .bp3-menu-item.bp3-intent-success:active::before, .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item.bp3-intent-warning{
  color:#ffb366; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning::before, .bp3-dark .bp3-menu-item.bp3-intent-warning::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
    color:#ffb366; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{
    background-color:#d9822b; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning:active{
    background-color:#bf7326; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning:active, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item.bp3-intent-danger{
  color:#ff7373; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{
    color:inherit; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger::before, .bp3-dark .bp3-menu-item.bp3-intent-danger::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
    color:#ff7373; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{
    background-color:#db3737; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger:active{
    background-color:#c23030; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
  .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,
  .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger:active, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,
  .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
    color:#ffffff; }

.bp3-dark .bp3-menu-item::before,
.bp3-dark .bp3-menu-item > .bp3-icon{
  color:#a7b6c2; }

.bp3-dark .bp3-menu-item .bp3-menu-item-label{
  color:#a7b6c2; }

.bp3-dark .bp3-menu-item.bp3-active, .bp3-dark .bp3-menu-item:active{
  background-color:rgba(138, 155, 168, 0.3); }

.bp3-dark .bp3-menu-item.bp3-disabled{
  color:rgba(167, 182, 194, 0.6) !important; }
  .bp3-dark .bp3-menu-item.bp3-disabled::before,
  .bp3-dark .bp3-menu-item.bp3-disabled > .bp3-icon,
  .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
    color:rgba(167, 182, 194, 0.6) !important; }

.bp3-dark .bp3-menu-divider,
.bp3-dark .bp3-menu-header{
  border-color:rgba(255, 255, 255, 0.15); }

.bp3-dark .bp3-menu-header > h6{
  color:#f5f8fa; }

.bp3-label .bp3-menu{
  margin-top:5px; }
.bp3-navbar{
  position:relative;
  z-index:10;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  background-color:#ffffff;
  width:100%;
  height:50px;
  padding:0 15px; }
  .bp3-navbar.bp3-dark,
  .bp3-dark .bp3-navbar{
    background-color:#394b59; }
  .bp3-navbar.bp3-dark{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-navbar{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-navbar.bp3-fixed-top{
    position:fixed;
    top:0;
    right:0;
    left:0; }

.bp3-navbar-heading{
  margin-right:15px;
  font-size:16px; }

.bp3-navbar-group{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  height:50px; }
  .bp3-navbar-group.bp3-align-left{
    float:left; }
  .bp3-navbar-group.bp3-align-right{
    float:right; }

.bp3-navbar-divider{
  margin:0 10px;
  border-left:1px solid rgba(16, 22, 26, 0.15);
  height:20px; }
  .bp3-dark .bp3-navbar-divider{
    border-left-color:rgba(255, 255, 255, 0.15); }
.bp3-non-ideal-state{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  width:100%;
  height:100%;
  text-align:center; }
  .bp3-non-ideal-state > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-non-ideal-state > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-non-ideal-state::before,
  .bp3-non-ideal-state > *{
    margin-bottom:20px; }
  .bp3-non-ideal-state:empty::before,
  .bp3-non-ideal-state > :last-child{
    margin-bottom:0; }
  .bp3-non-ideal-state > *{
    max-width:400px; }

.bp3-non-ideal-state-visual{
  color:rgba(92, 112, 128, 0.6);
  font-size:60px; }
  .bp3-dark .bp3-non-ideal-state-visual{
    color:rgba(167, 182, 194, 0.6); }

.bp3-overflow-list{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:nowrap;
      flex-wrap:nowrap;
  min-width:0; }

.bp3-overflow-list-spacer{
  -ms-flex-negative:1;
      flex-shrink:1;
  width:1px; }

body.bp3-overlay-open{
  overflow:hidden; }

.bp3-overlay{
  position:static;
  top:0;
  right:0;
  bottom:0;
  left:0;
  z-index:20; }
  .bp3-overlay:not(.bp3-overlay-open){
    pointer-events:none; }
  .bp3-overlay.bp3-overlay-container{
    position:fixed;
    overflow:hidden; }
    .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-scroll-container{
    position:fixed;
    overflow:auto; }
    .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-inline{
    display:inline;
    overflow:visible; }

.bp3-overlay-content{
  position:fixed;
  z-index:20; }
  .bp3-overlay-inline .bp3-overlay-content,
  .bp3-overlay-scroll-container .bp3-overlay-content{
    position:absolute; }

.bp3-overlay-backdrop{
  position:fixed;
  top:0;
  right:0;
  bottom:0;
  left:0;
  opacity:1;
  z-index:20;
  background-color:rgba(16, 22, 26, 0.7);
  overflow:auto;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-overlay-backdrop.bp3-overlay-enter, .bp3-overlay-backdrop.bp3-overlay-appear{
    opacity:0; }
  .bp3-overlay-backdrop.bp3-overlay-enter-active, .bp3-overlay-backdrop.bp3-overlay-appear-active{
    opacity:1;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-overlay-backdrop.bp3-overlay-exit{
    opacity:1; }
  .bp3-overlay-backdrop.bp3-overlay-exit-active{
    opacity:0;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-overlay-backdrop:focus{
    outline:none; }
  .bp3-overlay-inline .bp3-overlay-backdrop{
    position:absolute; }
.bp3-panel-stack{
  position:relative;
  overflow:hidden; }

.bp3-panel-stack-header{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-negative:0;
      flex-shrink:0;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  z-index:1;
  -webkit-box-shadow:0 1px rgba(16, 22, 26, 0.15);
          box-shadow:0 1px rgba(16, 22, 26, 0.15);
  height:30px; }
  .bp3-dark .bp3-panel-stack-header{
    -webkit-box-shadow:0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 1px rgba(255, 255, 255, 0.15); }
  .bp3-panel-stack-header > span{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1;
            flex:1;
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch; }
  .bp3-panel-stack-header .bp3-heading{
    margin:0 5px; }

.bp3-button.bp3-panel-stack-header-back{
  margin-left:5px;
  padding-left:0;
  white-space:nowrap; }
  .bp3-button.bp3-panel-stack-header-back .bp3-icon{
    margin:0 2px; }

.bp3-panel-stack-view{
  position:absolute;
  top:0;
  right:0;
  bottom:0;
  left:0;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin-right:-1px;
  border-right:1px solid rgba(16, 22, 26, 0.15);
  background-color:#ffffff;
  overflow-y:auto; }
  .bp3-dark .bp3-panel-stack-view{
    background-color:#30404d; }

.bp3-panel-stack-push .bp3-panel-stack-enter, .bp3-panel-stack-push .bp3-panel-stack-appear{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0; }

.bp3-panel-stack-push .bp3-panel-stack-enter-active, .bp3-panel-stack-push .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }

.bp3-panel-stack-push .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-push .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }

.bp3-panel-stack-pop .bp3-panel-stack-enter, .bp3-panel-stack-pop .bp3-panel-stack-appear{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0; }

.bp3-panel-stack-pop .bp3-panel-stack-enter-active, .bp3-panel-stack-pop .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }

.bp3-panel-stack-pop .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-pop .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease;
  -webkit-transition-delay:0;
          transition-delay:0; }
.bp3-popover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1);
  display:inline-block;
  z-index:20;
  border-radius:3px; }
  .bp3-popover .bp3-popover-arrow{
    position:absolute;
    width:30px;
    height:30px; }
    .bp3-popover .bp3-popover-arrow::before{
      margin:5px;
      width:20px;
      height:20px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover{
    margin-top:-17px;
    margin-bottom:17px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
      bottom:-11px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover{
    margin-left:17px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
      left:-11px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover{
    margin-top:17px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
      top:-11px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover{
    margin-right:17px;
    margin-left:-17px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
      right:-11px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-popover > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-popover > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
    top:-0.3934px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
    right:-0.3934px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
    left:-0.3934px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
    bottom:-0.3934px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-popover .bp3-popover-content{
    background:#ffffff;
    color:inherit; }
  .bp3-popover .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-popover .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-popover .bp3-popover-arrow-fill{
    fill:#ffffff; }
  .bp3-popover-enter > .bp3-popover, .bp3-popover-appear > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3); }
  .bp3-popover-enter-active > .bp3-popover, .bp3-popover-appear-active > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-popover-exit > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-popover .bp3-popover-content{
    position:relative;
    border-radius:3px; }
  .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{
    max-width:350px;
    padding:20px; }
  .bp3-popover-target + .bp3-overlay .bp3-popover.bp3-popover-content-sizing{
    width:350px; }
  .bp3-popover.bp3-minimal{
    margin:0 !important; }
    .bp3-popover.bp3-minimal .bp3-popover-arrow{
      display:none; }
    .bp3-popover.bp3-minimal.bp3-popover{
      -webkit-transform:scale(1);
              transform:scale(1); }
      .bp3-popover-enter > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-enter-active > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
        -webkit-transition-delay:0;
                transition-delay:0; }
      .bp3-popover-exit > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-exit-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
        -webkit-transition-delay:0;
                transition-delay:0; }
  .bp3-popover.bp3-dark,
  .bp3-dark .bp3-popover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-popover .bp3-popover-content{
      background:#30404d;
      color:inherit; }
    .bp3-popover.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-popover .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-popover .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-popover.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-popover .bp3-popover-arrow-fill{
      fill:#30404d; }

.bp3-popover-arrow::before{
  display:block;
  position:absolute;
  -webkit-transform:rotate(45deg);
          transform:rotate(45deg);
  border-radius:2px;
  content:""; }

.bp3-tether-pinned .bp3-popover-arrow{
  display:none; }

.bp3-popover-backdrop{
  background:rgba(255, 255, 255, 0); }

.bp3-transition-container{
  opacity:1;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  z-index:20; }
  .bp3-transition-container.bp3-popover-enter, .bp3-transition-container.bp3-popover-appear{
    opacity:0; }
  .bp3-transition-container.bp3-popover-enter-active, .bp3-transition-container.bp3-popover-appear-active{
    opacity:1;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-transition-container.bp3-popover-exit{
    opacity:1; }
  .bp3-transition-container.bp3-popover-exit-active{
    opacity:0;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-transition-container:focus{
    outline:none; }
  .bp3-transition-container.bp3-popover-leave .bp3-popover-content{
    pointer-events:none; }
  .bp3-transition-container[data-x-out-of-boundaries]{
    display:none; }

span.bp3-popover-target{
  display:inline-block; }

.bp3-popover-wrapper.bp3-fill{
  width:100%; }

.bp3-portal{
  position:absolute;
  top:0;
  right:0;
  left:0; }
@-webkit-keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }
@keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }

.bp3-progress-bar{
  display:block;
  position:relative;
  border-radius:40px;
  background:rgba(92, 112, 128, 0.2);
  width:100%;
  height:8px;
  overflow:hidden; }
  .bp3-progress-bar .bp3-progress-meter{
    position:absolute;
    border-radius:40px;
    background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);
    background-color:rgba(92, 112, 128, 0.8);
    background-size:30px 30px;
    width:100%;
    height:100%;
    -webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{
    animation:linear-progress-bar-stripes 300ms linear infinite reverse; }
  .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{
    background-image:none; }

.bp3-dark .bp3-progress-bar{
  background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-progress-bar .bp3-progress-meter{
    background-color:#8a9ba8; }

.bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{
  background-color:#137cbd; }

.bp3-progress-bar.bp3-intent-success .bp3-progress-meter{
  background-color:#0f9960; }

.bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{
  background-color:#d9822b; }

.bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{
  background-color:#db3737; }
@-webkit-keyframes skeleton-glow{
  from{
    border-color:rgba(206, 217, 224, 0.2);
    background:rgba(206, 217, 224, 0.2); }
  to{
    border-color:rgba(92, 112, 128, 0.2);
    background:rgba(92, 112, 128, 0.2); } }
@keyframes skeleton-glow{
  from{
    border-color:rgba(206, 217, 224, 0.2);
    background:rgba(206, 217, 224, 0.2); }
  to{
    border-color:rgba(92, 112, 128, 0.2);
    background:rgba(92, 112, 128, 0.2); } }
.bp3-skeleton{
  border-color:rgba(206, 217, 224, 0.2) !important;
  border-radius:2px;
  -webkit-box-shadow:none !important;
          box-shadow:none !important;
  background:rgba(206, 217, 224, 0.2);
  background-clip:padding-box !important;
  cursor:default;
  color:transparent !important;
  -webkit-animation:1000ms linear infinite alternate skeleton-glow;
          animation:1000ms linear infinite alternate skeleton-glow;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-skeleton::before, .bp3-skeleton::after,
  .bp3-skeleton *{
    visibility:hidden !important; }
.bp3-slider{
  width:100%;
  min-width:150px;
  height:40px;
  position:relative;
  outline:none;
  cursor:default;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-slider:hover{
    cursor:pointer; }
  .bp3-slider:active{
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-slider.bp3-disabled{
    opacity:0.5;
    cursor:not-allowed; }
  .bp3-slider.bp3-slider-unlabeled{
    height:16px; }

.bp3-slider-track,
.bp3-slider-progress{
  top:5px;
  right:0;
  left:0;
  height:6px;
  position:absolute; }

.bp3-slider-track{
  border-radius:3px;
  overflow:hidden; }

.bp3-slider-progress{
  background:rgba(92, 112, 128, 0.2); }
  .bp3-dark .bp3-slider-progress{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-slider-progress.bp3-intent-primary{
    background-color:#137cbd; }
  .bp3-slider-progress.bp3-intent-success{
    background-color:#0f9960; }
  .bp3-slider-progress.bp3-intent-warning{
    background-color:#d9822b; }
  .bp3-slider-progress.bp3-intent-danger{
    background-color:#db3737; }

.bp3-slider-handle{
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  color:#182026;
  position:absolute;
  top:0;
  left:0;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
  cursor:pointer;
  width:16px;
  height:16px; }
  .bp3-slider-handle:hover{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5; }
  .bp3-slider-handle:active, .bp3-slider-handle.bp3-active{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none; }
  .bp3-slider-handle:disabled, .bp3-slider-handle.bp3-disabled{
    outline:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
    .bp3-slider-handle:disabled.bp3-active, .bp3-slider-handle:disabled.bp3-active:hover, .bp3-slider-handle.bp3-disabled.bp3-active, .bp3-slider-handle.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }
  .bp3-slider-handle:focus{
    z-index:1; }
  .bp3-slider-handle:hover{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    background-clip:padding-box;
    background-color:#ebf1f5;
    z-index:2;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
    cursor:-webkit-grab;
    cursor:grab; }
  .bp3-slider-handle.bp3-active{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-disabled .bp3-slider-handle{
    -webkit-box-shadow:none;
            box-shadow:none;
    background:#bfccd6;
    pointer-events:none; }
  .bp3-dark .bp3-slider-handle{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover, .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      background-color:#30404d; }
    .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
      background-color:#202b33;
      background-image:none; }
    .bp3-dark .bp3-slider-handle:disabled, .bp3-dark .bp3-slider-handle.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none;
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-slider-handle:disabled.bp3-active, .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-slider-handle, .bp3-dark .bp3-slider-handle:hover{
      background-color:#394b59; }
    .bp3-dark .bp3-slider-handle.bp3-active{
      background-color:#293742; }
  .bp3-dark .bp3-disabled .bp3-slider-handle{
    border-color:#5c7080;
    -webkit-box-shadow:none;
            box-shadow:none;
    background:#5c7080; }
  .bp3-slider-handle .bp3-slider-label{
    margin-left:8px;
    border-radius:3px;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
    background:#394b59;
    color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle .bp3-slider-label{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
      background:#e1e8ed;
      color:#394b59; }
    .bp3-disabled .bp3-slider-handle .bp3-slider-label{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-slider-handle.bp3-start, .bp3-slider-handle.bp3-end{
    width:8px; }
  .bp3-slider-handle.bp3-start{
    border-top-right-radius:0;
    border-bottom-right-radius:0; }
  .bp3-slider-handle.bp3-end{
    margin-left:8px;
    border-top-left-radius:0;
    border-bottom-left-radius:0; }
    .bp3-slider-handle.bp3-end .bp3-slider-label{
      margin-left:0; }

.bp3-slider-label{
  -webkit-transform:translate(-50%, 20px);
          transform:translate(-50%, 20px);
  display:inline-block;
  position:absolute;
  padding:2px 5px;
  vertical-align:top;
  line-height:1;
  font-size:12px; }

.bp3-slider.bp3-vertical{
  width:40px;
  min-width:40px;
  height:150px; }
  .bp3-slider.bp3-vertical .bp3-slider-track,
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    top:0;
    bottom:0;
    left:5px;
    width:6px;
    height:auto; }
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    top:auto; }
  .bp3-slider.bp3-vertical .bp3-slider-label{
    -webkit-transform:translate(20px, 50%);
            transform:translate(20px, 50%); }
  .bp3-slider.bp3-vertical .bp3-slider-handle{
    top:auto; }
    .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{
      margin-top:-8px;
      margin-left:0; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end, .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      margin-left:0;
      width:16px;
      height:8px; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      border-top-left-radius:0;
      border-bottom-right-radius:3px; }
      .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{
        -webkit-transform:translate(20px);
                transform:translate(20px); }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{
      margin-bottom:8px;
      border-top-left-radius:3px;
      border-bottom-left-radius:0;
      border-bottom-right-radius:0; }

@-webkit-keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

@keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

.bp3-spinner{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  overflow:visible;
  vertical-align:middle; }
  .bp3-spinner svg{
    display:block; }
  .bp3-spinner path{
    fill-opacity:0; }
  .bp3-spinner .bp3-spinner-head{
    -webkit-transform-origin:center;
            transform-origin:center;
    -webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    stroke:rgba(92, 112, 128, 0.8);
    stroke-linecap:round; }
  .bp3-spinner .bp3-spinner-track{
    stroke:rgba(92, 112, 128, 0.2); }

.bp3-spinner-animation{
  -webkit-animation:pt-spinner-animation 500ms linear infinite;
          animation:pt-spinner-animation 500ms linear infinite; }
  .bp3-no-spin > .bp3-spinner-animation{
    -webkit-animation:none;
            animation:none; }

.bp3-dark .bp3-spinner .bp3-spinner-head{
  stroke:#8a9ba8; }

.bp3-dark .bp3-spinner .bp3-spinner-track{
  stroke:rgba(16, 22, 26, 0.5); }

.bp3-spinner.bp3-intent-primary .bp3-spinner-head{
  stroke:#137cbd; }

.bp3-spinner.bp3-intent-success .bp3-spinner-head{
  stroke:#0f9960; }

.bp3-spinner.bp3-intent-warning .bp3-spinner-head{
  stroke:#d9822b; }

.bp3-spinner.bp3-intent-danger .bp3-spinner-head{
  stroke:#db3737; }
.bp3-tabs.bp3-vertical{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-tabs.bp3-vertical > .bp3-tab-list{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column;
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab{
      border-radius:3px;
      width:100%;
      padding:0 10px; }
      .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab[aria-selected="true"]{
        -webkit-box-shadow:none;
                box-shadow:none;
        background-color:rgba(19, 124, 189, 0.2); }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{
      top:0;
      right:0;
      bottom:0;
      left:0;
      border-radius:3px;
      background-color:rgba(19, 124, 189, 0.2);
      height:auto; }
  .bp3-tabs.bp3-vertical > .bp3-tab-panel{
    margin-top:0;
    padding-left:20px; }

.bp3-tab-list{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  -webkit-box-align:end;
      -ms-flex-align:end;
          align-items:flex-end;
  position:relative;
  margin:0;
  border:none;
  padding:0;
  list-style:none; }
  .bp3-tab-list > *:not(:last-child){
    margin-right:20px; }

.bp3-tab{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  position:relative;
  cursor:pointer;
  max-width:100%;
  vertical-align:top;
  line-height:30px;
  color:#182026;
  font-size:14px; }
  .bp3-tab a{
    display:block;
    text-decoration:none;
    color:inherit; }
  .bp3-tab-indicator-wrapper ~ .bp3-tab{
    -webkit-box-shadow:none !important;
            box-shadow:none !important;
    background-color:transparent !important; }
  .bp3-tab[aria-disabled="true"]{
    cursor:not-allowed;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-tab[aria-selected="true"]{
    border-radius:0;
    -webkit-box-shadow:inset 0 -3px 0 #106ba3;
            box-shadow:inset 0 -3px 0 #106ba3; }
  .bp3-tab[aria-selected="true"], .bp3-tab:not([aria-disabled="true"]):hover{
    color:#106ba3; }
  .bp3-tab:focus{
    -moz-outline-radius:0; }
  .bp3-large > .bp3-tab{
    line-height:40px;
    font-size:16px; }

.bp3-tab-panel{
  margin-top:20px; }
  .bp3-tab-panel[aria-hidden="true"]{
    display:none; }

.bp3-tab-indicator-wrapper{
  position:absolute;
  top:0;
  left:0;
  -webkit-transform:translateX(0), translateY(0);
          transform:translateX(0), translateY(0);
  -webkit-transition:height, width, -webkit-transform;
  transition:height, width, -webkit-transform;
  transition:height, transform, width;
  transition:height, transform, width, -webkit-transform;
  -webkit-transition-duration:200ms;
          transition-duration:200ms;
  -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
          transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
  pointer-events:none; }
  .bp3-tab-indicator-wrapper .bp3-tab-indicator{
    position:absolute;
    right:0;
    bottom:0;
    left:0;
    background-color:#106ba3;
    height:3px; }
  .bp3-tab-indicator-wrapper.bp3-no-animation{
    -webkit-transition:none;
    transition:none; }

.bp3-dark .bp3-tab{
  color:#f5f8fa; }
  .bp3-dark .bp3-tab[aria-disabled="true"]{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tab[aria-selected="true"]{
    -webkit-box-shadow:inset 0 -3px 0 #48aff0;
            box-shadow:inset 0 -3px 0 #48aff0; }
  .bp3-dark .bp3-tab[aria-selected="true"], .bp3-dark .bp3-tab:not([aria-disabled="true"]):hover{
    color:#48aff0; }

.bp3-dark .bp3-tab-indicator{
  background-color:#48aff0; }

.bp3-flex-expander{
  -webkit-box-flex:1;
      -ms-flex:1 1;
          flex:1 1; }
.bp3-tag{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  position:relative;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:none;
          box-shadow:none;
  background-color:#5c7080;
  min-width:20px;
  max-width:100%;
  min-height:20px;
  padding:2px 6px;
  line-height:16px;
  color:#f5f8fa;
  font-size:12px; }
  .bp3-tag.bp3-interactive{
    cursor:pointer; }
    .bp3-tag.bp3-interactive:hover{
      background-color:rgba(92, 112, 128, 0.85); }
    .bp3-tag.bp3-interactive.bp3-active, .bp3-tag.bp3-interactive:active{
      background-color:rgba(92, 112, 128, 0.7); }
  .bp3-tag > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag::before,
  .bp3-tag > *{
    margin-right:4px; }
  .bp3-tag:empty::before,
  .bp3-tag > :last-child{
    margin-right:0; }
  .bp3-tag:focus{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:0;
    -moz-outline-radius:6px; }
  .bp3-tag.bp3-round{
    border-radius:30px;
    padding-right:8px;
    padding-left:8px; }
  .bp3-dark .bp3-tag{
    background-color:#bfccd6;
    color:#182026; }
    .bp3-dark .bp3-tag.bp3-interactive{
      cursor:pointer; }
      .bp3-dark .bp3-tag.bp3-interactive:hover{
        background-color:rgba(191, 204, 214, 0.85); }
      .bp3-dark .bp3-tag.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-interactive:active{
        background-color:rgba(191, 204, 214, 0.7); }
    .bp3-dark .bp3-tag > .bp3-icon, .bp3-dark .bp3-tag .bp3-icon-standard, .bp3-dark .bp3-tag .bp3-icon-large{
      fill:currentColor; }
  .bp3-tag > .bp3-icon, .bp3-tag .bp3-icon-standard, .bp3-tag .bp3-icon-large{
    fill:#ffffff; }
  .bp3-tag.bp3-large,
  .bp3-large .bp3-tag{
    min-width:30px;
    min-height:30px;
    padding:0 10px;
    line-height:20px;
    font-size:14px; }
    .bp3-tag.bp3-large::before,
    .bp3-tag.bp3-large > *,
    .bp3-large .bp3-tag::before,
    .bp3-large .bp3-tag > *{
      margin-right:7px; }
    .bp3-tag.bp3-large:empty::before,
    .bp3-tag.bp3-large > :last-child,
    .bp3-large .bp3-tag:empty::before,
    .bp3-large .bp3-tag > :last-child{
      margin-right:0; }
    .bp3-tag.bp3-large.bp3-round,
    .bp3-large .bp3-tag.bp3-round{
      padding-right:12px;
      padding-left:12px; }
  .bp3-tag.bp3-intent-primary{
    background:#137cbd;
    color:#ffffff; }
    .bp3-tag.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.85); }
      .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.7); }
  .bp3-tag.bp3-intent-success{
    background:#0f9960;
    color:#ffffff; }
    .bp3-tag.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.85); }
      .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.7); }
  .bp3-tag.bp3-intent-warning{
    background:#d9822b;
    color:#ffffff; }
    .bp3-tag.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.85); }
      .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.7); }
  .bp3-tag.bp3-intent-danger{
    background:#db3737;
    color:#ffffff; }
    .bp3-tag.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.85); }
      .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.7); }
  .bp3-tag.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-tag.bp3-minimal > .bp3-icon, .bp3-tag.bp3-minimal .bp3-icon-standard, .bp3-tag.bp3-minimal .bp3-icon-large{
    fill:#5c7080; }
  .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
    background-color:rgba(138, 155, 168, 0.2);
    color:#182026; }
    .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
        background-color:rgba(92, 112, 128, 0.3); }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
        background-color:rgba(92, 112, 128, 0.4); }
    .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
      color:#f5f8fa; }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
          background-color:rgba(191, 204, 214, 0.3); }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
          background-color:rgba(191, 204, 214, 0.4); }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) > .bp3-icon, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-large{
        fill:#a7b6c2; }
  .bp3-tag.bp3-minimal.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15);
    color:#106ba3; }
    .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-primary > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{
      fill:#137cbd; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25);
      color:#48aff0; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
          background-color:rgba(19, 124, 189, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
          background-color:rgba(19, 124, 189, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15);
    color:#0d8050; }
    .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-success > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{
      fill:#0f9960; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25);
      color:#3dcc91; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
          background-color:rgba(15, 153, 96, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
          background-color:rgba(15, 153, 96, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15);
    color:#bf7326; }
    .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-warning > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{
      fill:#d9822b; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25);
      color:#ffb366; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
          background-color:rgba(217, 130, 43, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
          background-color:rgba(217, 130, 43, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15);
    color:#c23030; }
    .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-danger > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{
      fill:#db3737; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25);
      color:#ff7373; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
          background-color:rgba(219, 55, 55, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
          background-color:rgba(219, 55, 55, 0.45); }

.bp3-tag-remove{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  opacity:0.5;
  margin-top:-2px;
  margin-right:-6px !important;
  margin-bottom:-2px;
  border:none;
  background:none;
  cursor:pointer;
  padding:2px;
  padding-left:0;
  color:inherit; }
  .bp3-tag-remove:hover{
    opacity:0.8;
    background:none;
    text-decoration:none; }
  .bp3-tag-remove:active{
    opacity:1; }
  .bp3-tag-remove:empty::before{
    line-height:1;
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-weight:400;
    font-style:normal;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    content:""; }
  .bp3-large .bp3-tag-remove{
    margin-right:-10px !important;
    padding:5px;
    padding-left:0; }
    .bp3-large .bp3-tag-remove:empty::before{
      line-height:1;
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-weight:400;
      font-style:normal; }
.bp3-tag-input{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  cursor:text;
  height:auto;
  min-height:30px;
  padding-right:0;
  padding-left:5px;
  line-height:inherit; }
  .bp3-tag-input > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag-input > .bp3-tag-input-values{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag-input .bp3-tag-input-icon{
    margin-top:7px;
    margin-right:7px;
    margin-left:2px;
    color:#5c7080; }
  .bp3-tag-input .bp3-tag-input-values{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row;
    -ms-flex-wrap:wrap;
        flex-wrap:wrap;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    -ms-flex-item-align:stretch;
        align-self:stretch;
    margin-top:5px;
    margin-right:7px;
    min-width:0; }
    .bp3-tag-input .bp3-tag-input-values > *{
      -webkit-box-flex:0;
          -ms-flex-positive:0;
              flex-grow:0;
      -ms-flex-negative:0;
          flex-shrink:0; }
    .bp3-tag-input .bp3-tag-input-values > .bp3-fill{
      -webkit-box-flex:1;
          -ms-flex-positive:1;
              flex-grow:1;
      -ms-flex-negative:1;
          flex-shrink:1; }
    .bp3-tag-input .bp3-tag-input-values::before,
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-right:5px; }
    .bp3-tag-input .bp3-tag-input-values:empty::before,
    .bp3-tag-input .bp3-tag-input-values > :last-child{
      margin-right:0; }
    .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{
      padding-left:5px; }
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-bottom:5px; }
  .bp3-tag-input .bp3-tag{
    overflow-wrap:break-word; }
    .bp3-tag-input .bp3-tag.bp3-active{
      outline:rgba(19, 124, 189, 0.6) auto 2px;
      outline-offset:0;
      -moz-outline-radius:6px; }
  .bp3-tag-input .bp3-input-ghost{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:80px;
    line-height:20px; }
    .bp3-tag-input .bp3-input-ghost:disabled, .bp3-tag-input .bp3-input-ghost.bp3-disabled{
      cursor:not-allowed; }
  .bp3-tag-input .bp3-button,
  .bp3-tag-input .bp3-spinner{
    margin:3px;
    margin-left:0; }
  .bp3-tag-input .bp3-button{
    min-width:24px;
    min-height:24px;
    padding:0 7px; }
  .bp3-tag-input.bp3-large{
    height:auto;
    min-height:40px; }
    .bp3-tag-input.bp3-large::before,
    .bp3-tag-input.bp3-large > *{
      margin-right:10px; }
    .bp3-tag-input.bp3-large:empty::before,
    .bp3-tag-input.bp3-large > :last-child{
      margin-right:0; }
    .bp3-tag-input.bp3-large .bp3-tag-input-icon{
      margin-top:10px;
      margin-left:5px; }
    .bp3-tag-input.bp3-large .bp3-input-ghost{
      line-height:30px; }
    .bp3-tag-input.bp3-large .bp3-button{
      min-width:30px;
      min-height:30px;
      padding:5px 10px;
      margin:5px;
      margin-left:0; }
    .bp3-tag-input.bp3-large .bp3-spinner{
      margin:8px;
      margin-left:0; }
  .bp3-tag-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
    background-color:#ffffff; }
    .bp3-tag-input.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-tag-input .bp3-tag-input-icon, .bp3-tag-input.bp3-dark .bp3-tag-input-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-tag-input .bp3-input-ghost, .bp3-tag-input.bp3-dark .bp3-input-ghost{
    color:#f5f8fa; }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{
      color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tag-input.bp3-active, .bp3-tag-input.bp3-dark.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    background-color:rgba(16, 22, 26, 0.3); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-input-ghost{
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  background:none;
  padding:0; }
  .bp3-input-ghost::-webkit-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost::-moz-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost:-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost::-ms-input-placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost::placeholder{
    opacity:1;
    color:rgba(92, 112, 128, 0.6); }
  .bp3-input-ghost:focus{
    outline:none !important; }
.bp3-toast{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  position:relative !important;
  margin:20px 0 0;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  background-color:#ffffff;
  min-width:300px;
  max-width:500px;
  pointer-events:all; }
  .bp3-toast.bp3-toast-enter, .bp3-toast.bp3-toast-appear{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active, .bp3-toast.bp3-toast-appear-active{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-toast.bp3-toast-enter ~ .bp3-toast, .bp3-toast.bp3-toast-appear ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active ~ .bp3-toast, .bp3-toast.bp3-toast-appear-active ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-toast.bp3-toast-exit{
    opacity:1;
    -webkit-filter:blur(0);
            filter:blur(0); }
  .bp3-toast.bp3-toast-exit-active{
    opacity:0;
    -webkit-filter:blur(10px);
            filter:blur(10px);
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:opacity, filter;
    transition-property:opacity, filter, -webkit-filter;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-toast.bp3-toast-exit ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0); }
  .bp3-toast.bp3-toast-exit-active ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:50ms;
            transition-delay:50ms; }
  .bp3-toast .bp3-button-group{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    padding:5px;
    padding-left:0; }
  .bp3-toast > .bp3-icon{
    margin:12px;
    margin-right:0;
    color:#5c7080; }
  .bp3-toast.bp3-dark,
  .bp3-dark .bp3-toast{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
    background-color:#394b59; }
    .bp3-toast.bp3-dark > .bp3-icon,
    .bp3-dark .bp3-toast > .bp3-icon{
      color:#a7b6c2; }
  .bp3-toast[class*="bp3-intent-"] a{
    color:rgba(255, 255, 255, 0.7); }
    .bp3-toast[class*="bp3-intent-"] a:hover{
      color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] > .bp3-icon{
    color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button, .bp3-toast[class*="bp3-intent-"] .bp3-button::before,
  .bp3-toast[class*="bp3-intent-"] .bp3-button .bp3-icon, .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    color:rgba(255, 255, 255, 0.7) !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:focus{
    outline-color:rgba(255, 255, 255, 0.5); }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:hover{
    background-color:rgba(255, 255, 255, 0.15) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    background-color:rgba(255, 255, 255, 0.3) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button::after{
    background:rgba(255, 255, 255, 0.3) !important; }
  .bp3-toast.bp3-intent-primary{
    background-color:#137cbd;
    color:#ffffff; }
  .bp3-toast.bp3-intent-success{
    background-color:#0f9960;
    color:#ffffff; }
  .bp3-toast.bp3-intent-warning{
    background-color:#d9822b;
    color:#ffffff; }
  .bp3-toast.bp3-intent-danger{
    background-color:#db3737;
    color:#ffffff; }

.bp3-toast-message{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  padding:11px;
  word-break:break-word; }

.bp3-toast-container{
  display:-webkit-box !important;
  display:-ms-flexbox !important;
  display:flex !important;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  position:fixed;
  right:0;
  left:0;
  z-index:40;
  overflow:hidden;
  padding:0 20px 20px;
  pointer-events:none; }
  .bp3-toast-container.bp3-toast-container-top{
    top:0;
    bottom:auto; }
  .bp3-toast-container.bp3-toast-container-bottom{
    -webkit-box-orient:vertical;
    -webkit-box-direction:reverse;
        -ms-flex-direction:column-reverse;
            flex-direction:column-reverse;
    top:auto;
    bottom:0; }
  .bp3-toast-container.bp3-toast-container-left{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
  .bp3-toast-container.bp3-toast-container-right{
    -webkit-box-align:end;
        -ms-flex-align:end;
            align-items:flex-end; }

.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active) ~ .bp3-toast,
.bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active ~ .bp3-toast{
  -webkit-transform:translateY(60px);
          transform:translateY(60px); }
.bp3-tooltip{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1); }
  .bp3-tooltip .bp3-popover-arrow{
    position:absolute;
    width:22px;
    height:22px; }
    .bp3-tooltip .bp3-popover-arrow::before{
      margin:4px;
      width:14px;
      height:14px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip{
    margin-top:-11px;
    margin-bottom:11px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
      bottom:-8px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip{
    margin-left:11px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
      left:-8px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip{
    margin-top:11px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
      top:-8px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip{
    margin-right:11px;
    margin-left:-11px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
      right:-8px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-tooltip > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-tooltip > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
    top:-0.22183px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
    right:-0.22183px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
    left:-0.22183px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
    bottom:-0.22183px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-tooltip .bp3-popover-content{
    background:#394b59;
    color:#f5f8fa; }
  .bp3-tooltip .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-tooltip .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-tooltip .bp3-popover-arrow-fill{
    fill:#394b59; }
  .bp3-popover-enter > .bp3-tooltip, .bp3-popover-appear > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8); }
  .bp3-popover-enter-active > .bp3-tooltip, .bp3-popover-appear-active > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-popover-exit > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8);
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-tooltip .bp3-popover-content{
    padding:10px 12px; }
  .bp3-tooltip.bp3-dark,
  .bp3-dark .bp3-tooltip{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-tooltip .bp3-popover-content{
      background:#e1e8ed;
      color:#394b59; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{
      fill:#e1e8ed; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-content{
    background:#137cbd;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{
    fill:#137cbd; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-content{
    background:#0f9960;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{
    fill:#0f9960; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-content{
    background:#d9822b;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{
    fill:#d9822b; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-content{
    background:#db3737;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{
    fill:#db3737; }

.bp3-tooltip-indicator{
  border-bottom:dotted 1px;
  cursor:help; }
.bp3-tree .bp3-icon, .bp3-tree .bp3-icon-standard, .bp3-tree .bp3-icon-large{
  color:#5c7080; }
  .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-tree .bp3-icon.bp3-intent-success, .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-tree-node-list{
  margin:0;
  padding-left:0;
  list-style:none; }

.bp3-tree-root{
  position:relative;
  background-color:transparent;
  cursor:default;
  padding-left:0; }

.bp3-tree-node-content-0{
  padding-left:0px; }

.bp3-tree-node-content-1{
  padding-left:23px; }

.bp3-tree-node-content-2{
  padding-left:46px; }

.bp3-tree-node-content-3{
  padding-left:69px; }

.bp3-tree-node-content-4{
  padding-left:92px; }

.bp3-tree-node-content-5{
  padding-left:115px; }

.bp3-tree-node-content-6{
  padding-left:138px; }

.bp3-tree-node-content-7{
  padding-left:161px; }

.bp3-tree-node-content-8{
  padding-left:184px; }

.bp3-tree-node-content-9{
  padding-left:207px; }

.bp3-tree-node-content-10{
  padding-left:230px; }

.bp3-tree-node-content-11{
  padding-left:253px; }

.bp3-tree-node-content-12{
  padding-left:276px; }

.bp3-tree-node-content-13{
  padding-left:299px; }

.bp3-tree-node-content-14{
  padding-left:322px; }

.bp3-tree-node-content-15{
  padding-left:345px; }

.bp3-tree-node-content-16{
  padding-left:368px; }

.bp3-tree-node-content-17{
  padding-left:391px; }

.bp3-tree-node-content-18{
  padding-left:414px; }

.bp3-tree-node-content-19{
  padding-left:437px; }

.bp3-tree-node-content-20{
  padding-left:460px; }

.bp3-tree-node-content{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  width:100%;
  height:30px;
  padding-right:5px; }
  .bp3-tree-node-content:hover{
    background-color:rgba(191, 204, 214, 0.4); }

.bp3-tree-node-caret,
.bp3-tree-node-caret-none{
  min-width:30px; }

.bp3-tree-node-caret{
  color:#5c7080;
  -webkit-transform:rotate(0deg);
          transform:rotate(0deg);
  cursor:pointer;
  padding:7px;
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tree-node-caret:hover{
    color:#182026; }
  .bp3-dark .bp3-tree-node-caret{
    color:#a7b6c2; }
    .bp3-dark .bp3-tree-node-caret:hover{
      color:#f5f8fa; }
  .bp3-tree-node-caret.bp3-tree-node-caret-open{
    -webkit-transform:rotate(90deg);
            transform:rotate(90deg); }
  .bp3-tree-node-caret.bp3-icon-standard::before{
    content:""; }

.bp3-tree-node-icon{
  position:relative;
  margin-right:7px; }

.bp3-tree-node-label{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  position:relative;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-label span{
    display:inline; }

.bp3-tree-node-secondary-label{
  padding:0 5px;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-secondary-label .bp3-popover-wrapper,
  .bp3-tree-node-secondary-label .bp3-popover-target{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center; }

.bp3-tree-node.bp3-disabled .bp3-tree-node-content{
  background-color:inherit;
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-tree-node.bp3-disabled .bp3-tree-node-caret,
.bp3-tree-node.bp3-disabled .bp3-tree-node-icon{
  cursor:not-allowed;
  color:rgba(92, 112, 128, 0.6); }

.bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content,
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-standard, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-large{
    color:#ffffff; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret::before{
    color:rgba(255, 255, 255, 0.7); }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret:hover::before{
    color:#ffffff; }

.bp3-dark .bp3-tree-node-content:hover{
  background-color:rgba(92, 112, 128, 0.3); }

.bp3-dark .bp3-tree .bp3-icon, .bp3-dark .bp3-tree .bp3-icon-standard, .bp3-dark .bp3-tree .bp3-icon-large{
  color:#a7b6c2; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-dark .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
/*!

Copyright 2017-present Palantir Technologies, Inc. All rights reserved.
Licensed under the Apache License, Version 2.0.

*/
.bp3-omnibar{
  -webkit-filter:blur(0);
          filter:blur(0);
  opacity:1;
  top:20vh;
  left:calc(50% - 250px);
  z-index:21;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  background-color:#ffffff;
  width:500px; }
  .bp3-omnibar.bp3-overlay-enter, .bp3-omnibar.bp3-overlay-appear{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2; }
  .bp3-omnibar.bp3-overlay-enter-active, .bp3-omnibar.bp3-overlay-appear-active{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-omnibar.bp3-overlay-exit{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1; }
  .bp3-omnibar.bp3-overlay-exit-active{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
    -webkit-transition-delay:0;
            transition-delay:0; }
  .bp3-omnibar .bp3-input{
    border-radius:0;
    background-color:transparent; }
    .bp3-omnibar .bp3-input, .bp3-omnibar .bp3-input:focus{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-omnibar .bp3-menu{
    border-radius:0;
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
    background-color:transparent;
    max-height:calc(60vh - 40px);
    overflow:auto; }
    .bp3-omnibar .bp3-menu:empty{
      display:none; }
  .bp3-dark .bp3-omnibar, .bp3-omnibar.bp3-dark{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    background-color:#30404d; }

.bp3-omnibar-overlay .bp3-overlay-backdrop{
  background-color:rgba(16, 22, 26, 0.2); }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-width:400px;
  max-height:300px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }

.bp3-multi-select{
  min-width:150px; }

.bp3-multi-select-popover .bp3-menu{
  max-width:400px;
  max-height:300px;
  overflow:auto; }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-width:400px;
  max-height:300px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);
  --jp-icon-listings-info: url(data:image/svg+xml;base64,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);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMTUwIDE1MCA1NDEuOSAyOTUuMyI+CiAgPGcgY2xhc3M9ImpwLWljb24tYnJhbmQyIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxREFGQiI+CiAgICA8cGF0aCBkPSJNNjY2LjMgMjk2LjVjMC0zMi41LTQwLjctNjMuMy0xMDMuMS04Mi40IDE0LjQtNjMuNiA4LTExNC4yLTIwLjItMTMwLjQtNi41LTMuOC0xNC4xLTUuNi0yMi40LTUuNnYyMi4zYzQuNiAwIDguMy45IDExLjQgMi42IDEzLjYgNy44IDE5LjUgMzcuNSAxNC45IDc1LjctMS4xIDkuNC0yLjkgMTkuMy01LjEgMjkuNC0xOS42LTQuOC00MS04LjUtNjMuNS0xMC45LTEzLjUtMTguNS0yNy41LTM1LjMtNDEuNi01MCAzMi42LTMwLjMgNjMuMi00Ni45IDg0LTQ2LjlWNzhjLTI3LjUgMC02My41IDE5LjYtOTkuOSA1My42LTM2LjQtMzMuOC03Mi40LTUzLjItOTkuOS01My4ydjIyLjNjMjAuNyAwIDUxLjQgMTYuNSA4NCA0Ni42LTE0IDE0LjctMjggMzEuNC00MS4zIDQ5LjktMjIuNiAyLjQtNDQgNi4xLTYzLjYgMTEtMi4zLTEwLTQtMTkuNy01LjItMjktNC43LTM4LjIgMS4xLTY3LjkgMTQuNi03NS44IDMtMS44IDYuOS0yLjYgMTEuNS0yLjZWNzguNWMtOC40IDAtMTYgMS44LTIyLjYgNS42LTI4LjEgMTYuMi0zNC40IDY2LjctMTkuOSAxMzAuMS02Mi4yIDE5LjItMTAyLjcgNDkuOS0xMDIuNyA4Mi4zIDAgMzIuNSA0MC43IDYzLjMgMTAzLjEgODIuNC0xNC40IDYzLjYtOCAxMTQuMiAyMC4yIDEzMC40IDYuNSAzLjggMTQuMSA1LjYgMjIuNSA1LjYgMjcuNSAwIDYzLjUtMTkuNiA5OS45LTUzLjYgMzYuNCAzMy44IDcyLjQgNTMuMiA5OS45IDUzLjIgOC40IDAgMTYtMS44IDIyLjYtNS42IDI4LjEtMTYuMiAzNC40LTY2LjcgMTkuOS0xMzAuMSA2Mi0xOS4xIDEwMi41LTQ5LjkgMTAyLjUtODIuM3ptLTEzMC4yLTY2LjdjLTMuNyAxMi45LTguMyAyNi4yLTEzLjUgMzkuNS00LjEtOC04LjQtMTYtMTMuMS0yNC00LjYtOC05LjUtMTUuOC0xNC40LTIzLjQgMTQuMiAyLjEgMjcuOSA0LjcgNDEgNy45em0tNDUuOCAxMDYuNWMtNy44IDEzLjUtMTUuOCAyNi4zLTI0LjEgMzguMi0xNC45IDEuMy0zMCAyLTQ1LjIgMi0xNS4xIDAtMzAuMi0uNy00NS0xLjktOC4zLTExLjktMTYuNC0yNC42LTI0LjItMzgtNy42LTEzLjEtMTQuNS0yNi40LTIwLjgtMzkuOCA2LjItMTMuNCAxMy4yLTI2LjggMjAuNy0zOS45IDcuOC0xMy41IDE1LjgtMjYuMyAyNC4xLTM4LjIgMTQuOS0xLjMgMzAtMiA0NS4yLTIgMTUuMSAwIDMwLjIuNyA0NSAxLjkgOC4zIDExLjkgMTYuNCAyNC42IDI0LjIgMzggNy42IDEzLjEgMTQuNSAyNi40IDIwLjggMzkuOC02LjMgMTMuNC0xMy4yIDI2LjgtMjAuNyAzOS45em0zMi4zLTEzYzUuNCAxMy40IDEwIDI2LjggMTMuOCAzOS44LTEzLjEgMy4yLTI2LjkgNS45LTQxLjIgOCA0LjktNy43IDkuOC0xNS42IDE0LjQtMjMuNyA0LjYtOCA4LjktMTYuMSAxMy0yNC4xek00MjEuMiA0MzBjLTkuMy05LjYtMTguNi0yMC4zLTI3LjgtMzIgOSAuNCAxOC4yLjcgMjcuNS43IDkuNCAwIDE4LjctLjIgMjcuOC0uNy05IDExLjctMTguMyAyMi40LTI3LjUgMzJ6bS03NC40LTU4LjljLTE0LjItMi4xLTI3LjktNC43LTQxLTcuOSAzLjctMTIuOSA4LjMtMjYuMiAxMy41LTM5LjUgNC4xIDggOC40IDE2IDEzLjEgMjQgNC43IDggOS41IDE1LjggMTQuNCAyMy40ek00MjAuNyAxNjNjOS4zIDkuNiAxOC42IDIwLjMgMjcuOCAzMi05LS40LTE4LjItLjctMjcuNS0uNy05LjQgMC0xOC43LjItMjcuOC43IDktMTEuNyAxOC4zLTIyLjQgMjcuNS0zMnptLTc0IDU4LjljLTQuOSA3LjctOS44IDE1LjYtMTQuNCAyMy43LTQuNiA4LTguOSAxNi0xMyAyNC01LjQtMTMuNC0xMC0yNi44LTEzLjgtMzkuOCAxMy4xLTMuMSAyNi45LTUuOCA0MS4yLTcuOXptLTkwLjUgMTI1LjJjLTM1LjQtMTUuMS01OC4zLTM0LjktNTguMy01MC42IDAtMTUuNyAyMi45LTM1LjYgNTguMy01MC42IDguNi0zLjcgMTgtNyAyNy43LTEwLjEgNS43IDE5LjYgMTMuMiA0MCAyMi41IDYwLjktOS4yIDIwLjgtMTYuNiA0MS4xLTIyLjIgNjAuNi05LjktMy4xLTE5LjMtNi41LTI4LTEwLjJ6TTMxMCA0OTBjLTEzLjYtNy44LTE5LjUtMzcuNS0xNC45LTc1LjcgMS4xLTkuNCAyLjktMTkuMyA1LjEtMjkuNCAxOS42IDQuOCA0MSA4LjUgNjMuNSAxMC45IDEzLjUgMTguNSAyNy41IDM1LjMgNDEuNiA1MC0zMi42IDMwLjMtNjMuMiA0Ni45LTg0IDQ2LjktNC41LS4xLTguMy0xLTExLjMtMi43em0yMzcuMi03Ni4yYzQuNyAzOC4yLTEuMSA2Ny45LTE0LjYgNzUuOC0zIDEuOC02LjkgMi42LTExLjUgMi42LTIwLjcgMC01MS40LTE2LjUtODQtNDYuNiAxNC0xNC43IDI4LTMxLjQgNDEuMy00OS45IDIyLjYtMi40IDQ0LTYuMSA2My42LTExIDIuMyAxMC4xIDQuMSAxOS44IDUuMiAyOS4xem0zOC41LTY2LjdjLTguNiAzLjctMTggNy0yNy43IDEwLjEtNS43LTE5LjYtMTMuMi00MC0yMi41LTYwLjkgOS4yLTIwLjggMTYuNi00MS4xIDIyLjItNjAuNiA5LjkgMy4xIDE5LjMgNi41IDI4LjEgMTAuMiAzNS40IDE1LjEgNTguMyAzNC45IDU4LjMgNTAuNi0uMSAxNS43LTIzIDM1LjYtNTguNCA1MC42ek0zMjAuOCA3OC40eiIvPgogICAgPGNpcmNsZSBjeD0iNDIwLjkiIGN5PSIyOTYuNSIgcj0iNDUuNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}
.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}
.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}
.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}
.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}
.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}
.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}
.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}
.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}
.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}
.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}
.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}
.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}
.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}
.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}
.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}
.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}
.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}
.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}
.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}
.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}
.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}
.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}
.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}
.jp-FileIcon {
  background-image: var(--jp-icon-file);
}
.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}
.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}
.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}
.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}
.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}
.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}
.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}
.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}
.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}
.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}
.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}
.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}
.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}
.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}
.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}
.jp-ListIcon {
  background-image: var(--jp-icon-list);
}
.jp-ListingsInfoIcon {
  background-image: var(--jp-icon-listings-info);
}
.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}
.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}
.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}
.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}
.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}
.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}
.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}
.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}
.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}
.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}
.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}
.jp-RunIcon {
  background-image: var(--jp-icon-run);
}
.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}
.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}
.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}
.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}
.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}
.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}
.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}
.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}
.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}
.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}
.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}
.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}
.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

:root {
  --jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
}

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}
/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}
/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}
/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}
.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}
.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}
.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}
.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}
.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}
.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}
.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}
.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}
/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}
.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}
.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}
.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}
.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}
.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}
.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}
/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

/* CSS for icons in selected items in the settings editor */
#setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
#setting-editor
  .jp-PluginList
  .jp-mod-selected
  .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected tabs in the sidebar tab manager */
#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill] {
  fill: #fff;
}

#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable[fill] {
  fill: var(--jp-brand-color1);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable-inverse[fill] {
  fill: #fff;
}

/**
 * TODO: come up with non css-hack solution for showing the busy icon on top
 *  of the close icon
 * CSS for complex behavior of close icon of tabs in the sidebar tab manager
 */
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-dirty.jp-mod-active
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: #fff;
}

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) svg {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

/* Override Blueprint's _reset.scss styles */
html {
  box-sizing: unset;
}

*,
*::before,
*::after {
  box-sizing: unset;
}

body {
  color: unset;
  font-family: var(--jp-ui-font-family);
}

p {
  margin-top: unset;
  margin-bottom: unset;
}

small {
  font-size: unset;
}

strong {
  font-weight: unset;
}

/* Override Blueprint's _typography.scss styles */
a {
  text-decoration: unset;
  color: unset;
}
a:hover {
  text-decoration: unset;
  color: unset;
}

/* Override Blueprint's _accessibility.scss styles */
:focus {
  outline: unset;
  outline-offset: unset;
  -moz-outline-radius: unset;
}

/* Styles for ui-components */
.jp-Button {
  border-radius: var(--jp-border-radius);
  padding: 0px 12px;
  font-size: var(--jp-ui-font-size1);
}

/* Use our own theme for hover styles */
button.jp-Button.bp3-button.bp3-minimal:hover {
  background-color: var(--jp-layout-color2);
}
.jp-Button.minimal {
  color: unset !important;
}

.jp-Button.jp-ToolbarButtonComponent {
  text-transform: none;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color3);
}

.jp-BPIcon {
  display: inline-block;
  vertical-align: middle;
  margin: auto;
}

/* Stop blueprint futzing with our icon fills */
.bp3-icon.jp-BPIcon > svg:not([fill]) {
  fill: var(--jp-inverse-layout-color3);
}

.jp-InputGroupAction {
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

/* Use our own theme for hover and option styles */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}
select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-top: 1px solid var(--jp-border-color2);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-Collapse-header {
  padding: 1px 12px;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size2);
}

.jp-Collapse-header:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Collapse-contents {
  padding: 0px 12px 0px 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0px;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0px 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.lm-CommandPalette-wrapper::after {
  content: ' ';
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  height: 30px;
  width: 10px;
  padding: 0px 10px;
  background-image: var(--jp-icon-search-white);
  background-size: 20px;
  background-repeat: no-repeat;
  background-position: center;
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color3);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0px;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color3);
}

.lm-CommandPalette-item.lm-mod-active {
  background: var(--jp-layout-color3);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  background: var(--jp-layout-color4);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color3);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.4;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty:after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0px 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0px;
  left: 0px;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px;
  padding-bottom: 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0px;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

.jp-Dialog-header {
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0px 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

.jp-HoverBox.jp-mod-outofview {
  display: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;

  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;

  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #aa00ff;

  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;

  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;

  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;

  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;

  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;

  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;

  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;

  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;

  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;

  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ffff00;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;

  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;

  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;

  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;

  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;

  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eeeeee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;

  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent:before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent:after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0px 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  height: 28px;
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  background-color: var(--jp-layout-color1);
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0px 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  height: 32px;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 1;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px;
  margin: 0px;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px 6px;
  margin: 0px;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent span {
  padding: 0px;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ body.p-mod-override-cursor *, /* </DEPRECATED> */
body.lm-mod-override-cursor * {
  cursor: inherit !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* BASICS */

.CodeMirror {
  /* Set height, width, borders, and global font properties here */
  font-family: monospace;
  height: 300px;
  color: black;
  direction: ltr;
}

/* PADDING */

.CodeMirror-lines {
  padding: 4px 0; /* Vertical padding around content */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  padding: 0 4px; /* Horizontal padding of content */
}

.CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  background-color: white; /* The little square between H and V scrollbars */
}

/* GUTTER */

.CodeMirror-gutters {
  border-right: 1px solid #ddd;
  background-color: #f7f7f7;
  white-space: nowrap;
}
.CodeMirror-linenumbers {}
.CodeMirror-linenumber {
  padding: 0 3px 0 5px;
  min-width: 20px;
  text-align: right;
  color: #999;
  white-space: nowrap;
}

.CodeMirror-guttermarker { color: black; }
.CodeMirror-guttermarker-subtle { color: #999; }

/* CURSOR */

.CodeMirror-cursor {
  border-left: 1px solid black;
  border-right: none;
  width: 0;
}
/* Shown when moving in bi-directional text */
.CodeMirror div.CodeMirror-secondarycursor {
  border-left: 1px solid silver;
}
.cm-fat-cursor .CodeMirror-cursor {
  width: auto;
  border: 0 !important;
  background: #7e7;
}
.cm-fat-cursor div.CodeMirror-cursors {
  z-index: 1;
}
.cm-fat-cursor-mark {
  background-color: rgba(20, 255, 20, 0.5);
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
}
.cm-animate-fat-cursor {
  width: auto;
  border: 0;
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
  background-color: #7e7;
}
@-moz-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@-webkit-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}

/* Can style cursor different in overwrite (non-insert) mode */
.CodeMirror-overwrite .CodeMirror-cursor {}

.cm-tab { display: inline-block; text-decoration: inherit; }

.CodeMirror-rulers {
  position: absolute;
  left: 0; right: 0; top: -50px; bottom: 0;
  overflow: hidden;
}
.CodeMirror-ruler {
  border-left: 1px solid #ccc;
  top: 0; bottom: 0;
  position: absolute;
}

/* DEFAULT THEME */

.cm-s-default .cm-header {color: blue;}
.cm-s-default .cm-quote {color: #090;}
.cm-negative {color: #d44;}
.cm-positive {color: #292;}
.cm-header, .cm-strong {font-weight: bold;}
.cm-em {font-style: italic;}
.cm-link {text-decoration: underline;}
.cm-strikethrough {text-decoration: line-through;}

.cm-s-default .cm-keyword {color: #708;}
.cm-s-default .cm-atom {color: #219;}
.cm-s-default .cm-number {color: #164;}
.cm-s-default .cm-def {color: #00f;}
.cm-s-default .cm-variable,
.cm-s-default .cm-punctuation,
.cm-s-default .cm-property,
.cm-s-default .cm-operator {}
.cm-s-default .cm-variable-2 {color: #05a;}
.cm-s-default .cm-variable-3, .cm-s-default .cm-type {color: #085;}
.cm-s-default .cm-comment {color: #a50;}
.cm-s-default .cm-string {color: #a11;}
.cm-s-default .cm-string-2 {color: #f50;}
.cm-s-default .cm-meta {color: #555;}
.cm-s-default .cm-qualifier {color: #555;}
.cm-s-default .cm-builtin {color: #30a;}
.cm-s-default .cm-bracket {color: #997;}
.cm-s-default .cm-tag {color: #170;}
.cm-s-default .cm-attribute {color: #00c;}
.cm-s-default .cm-hr {color: #999;}
.cm-s-default .cm-link {color: #00c;}

.cm-s-default .cm-error {color: #f00;}
.cm-invalidchar {color: #f00;}

.CodeMirror-composing { border-bottom: 2px solid; }

/* Default styles for common addons */

div.CodeMirror span.CodeMirror-matchingbracket {color: #0b0;}
div.CodeMirror span.CodeMirror-nonmatchingbracket {color: #a22;}
.CodeMirror-matchingtag { background: rgba(255, 150, 0, .3); }
.CodeMirror-activeline-background {background: #e8f2ff;}

/* STOP */

/* The rest of this file contains styles related to the mechanics of
   the editor. You probably shouldn't touch them. */

.CodeMirror {
  position: relative;
  overflow: hidden;
  background: white;
}

.CodeMirror-scroll {
  overflow: scroll !important; /* Things will break if this is overridden */
  /* 30px is the magic margin used to hide the element's real scrollbars */
  /* See overflow: hidden in .CodeMirror */
  margin-bottom: -30px; margin-right: -30px;
  padding-bottom: 30px;
  height: 100%;
  outline: none; /* Prevent dragging from highlighting the element */
  position: relative;
}
.CodeMirror-sizer {
  position: relative;
  border-right: 30px solid transparent;
}

/* The fake, visible scrollbars. Used to force redraw during scrolling
   before actual scrolling happens, thus preventing shaking and
   flickering artifacts. */
.CodeMirror-vscrollbar, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  position: absolute;
  z-index: 6;
  display: none;
}
.CodeMirror-vscrollbar {
  right: 0; top: 0;
  overflow-x: hidden;
  overflow-y: scroll;
}
.CodeMirror-hscrollbar {
  bottom: 0; left: 0;
  overflow-y: hidden;
  overflow-x: scroll;
}
.CodeMirror-scrollbar-filler {
  right: 0; bottom: 0;
}
.CodeMirror-gutter-filler {
  left: 0; bottom: 0;
}

.CodeMirror-gutters {
  position: absolute; left: 0; top: 0;
  min-height: 100%;
  z-index: 3;
}
.CodeMirror-gutter {
  white-space: normal;
  height: 100%;
  display: inline-block;
  vertical-align: top;
  margin-bottom: -30px;
}
.CodeMirror-gutter-wrapper {
  position: absolute;
  z-index: 4;
  background: none !important;
  border: none !important;
}
.CodeMirror-gutter-background {
  position: absolute;
  top: 0; bottom: 0;
  z-index: 4;
}
.CodeMirror-gutter-elt {
  position: absolute;
  cursor: default;
  z-index: 4;
}
.CodeMirror-gutter-wrapper ::selection { background-color: transparent }
.CodeMirror-gutter-wrapper ::-moz-selection { background-color: transparent }

.CodeMirror-lines {
  cursor: text;
  min-height: 1px; /* prevents collapsing before first draw */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  /* Reset some styles that the rest of the page might have set */
  -moz-border-radius: 0; -webkit-border-radius: 0; border-radius: 0;
  border-width: 0;
  background: transparent;
  font-family: inherit;
  font-size: inherit;
  margin: 0;
  white-space: pre;
  word-wrap: normal;
  line-height: inherit;
  color: inherit;
  z-index: 2;
  position: relative;
  overflow: visible;
  -webkit-tap-highlight-color: transparent;
  -webkit-font-variant-ligatures: contextual;
  font-variant-ligatures: contextual;
}
.CodeMirror-wrap pre.CodeMirror-line,
.CodeMirror-wrap pre.CodeMirror-line-like {
  word-wrap: break-word;
  white-space: pre-wrap;
  word-break: normal;
}

.CodeMirror-linebackground {
  position: absolute;
  left: 0; right: 0; top: 0; bottom: 0;
  z-index: 0;
}

.CodeMirror-linewidget {
  position: relative;
  z-index: 2;
  padding: 0.1px; /* Force widget margins to stay inside of the container */
}

.CodeMirror-widget {}

.CodeMirror-rtl pre { direction: rtl; }

.CodeMirror-code {
  outline: none;
}

/* Force content-box sizing for the elements where we expect it */
.CodeMirror-scroll,
.CodeMirror-sizer,
.CodeMirror-gutter,
.CodeMirror-gutters,
.CodeMirror-linenumber {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}

.CodeMirror-measure {
  position: absolute;
  width: 100%;
  height: 0;
  overflow: hidden;
  visibility: hidden;
}

.CodeMirror-cursor {
  position: absolute;
  pointer-events: none;
}
.CodeMirror-measure pre { position: static; }

div.CodeMirror-cursors {
  visibility: hidden;
  position: relative;
  z-index: 3;
}
div.CodeMirror-dragcursors {
  visibility: visible;
}

.CodeMirror-focused div.CodeMirror-cursors {
  visibility: visible;
}

.CodeMirror-selected { background: #d9d9d9; }
.CodeMirror-focused .CodeMirror-selected { background: #d7d4f0; }
.CodeMirror-crosshair { cursor: crosshair; }
.CodeMirror-line::selection, .CodeMirror-line > span::selection, .CodeMirror-line > span > span::selection { background: #d7d4f0; }
.CodeMirror-line::-moz-selection, .CodeMirror-line > span::-moz-selection, .CodeMirror-line > span > span::-moz-selection { background: #d7d4f0; }

.cm-searching {
  background-color: #ffa;
  background-color: rgba(255, 255, 0, .4);
}

/* Used to force a border model for a node */
.cm-force-border { padding-right: .1px; }

@media print {
  /* Hide the cursor when printing */
  .CodeMirror div.CodeMirror-cursors {
    visibility: hidden;
  }
}

/* See issue #2901 */
.cm-tab-wrap-hack:after { content: ''; }

/* Help users use markselection to safely style text background */
span.CodeMirror-selectedtext { background: none; }

.CodeMirror-dialog {
  position: absolute;
  left: 0; right: 0;
  background: inherit;
  z-index: 15;
  padding: .1em .8em;
  overflow: hidden;
  color: inherit;
}

.CodeMirror-dialog-top {
  border-bottom: 1px solid #eee;
  top: 0;
}

.CodeMirror-dialog-bottom {
  border-top: 1px solid #eee;
  bottom: 0;
}

.CodeMirror-dialog input {
  border: none;
  outline: none;
  background: transparent;
  width: 20em;
  color: inherit;
  font-family: monospace;
}

.CodeMirror-dialog button {
  font-size: 70%;
}

.CodeMirror-foldmarker {
  color: blue;
  text-shadow: #b9f 1px 1px 2px, #b9f -1px -1px 2px, #b9f 1px -1px 2px, #b9f -1px 1px 2px;
  font-family: arial;
  line-height: .3;
  cursor: pointer;
}
.CodeMirror-foldgutter {
  width: .7em;
}
.CodeMirror-foldgutter-open,
.CodeMirror-foldgutter-folded {
  cursor: pointer;
}
.CodeMirror-foldgutter-open:after {
  content: "\25BE";
}
.CodeMirror-foldgutter-folded:after {
  content: "\25B8";
}

/*
  Name:       material
  Author:     Mattia Astorino (http://github.com/equinusocio)
  Website:    https://material-theme.site/
*/

.cm-s-material.CodeMirror {
  background-color: #263238;
  color: #EEFFFF;
}

.cm-s-material .CodeMirror-gutters {
  background: #263238;
  color: #546E7A;
  border: none;
}

.cm-s-material .CodeMirror-guttermarker,
.cm-s-material .CodeMirror-guttermarker-subtle,
.cm-s-material .CodeMirror-linenumber {
  color: #546E7A;
}

.cm-s-material .CodeMirror-cursor {
  border-left: 1px solid #FFCC00;
}

.cm-s-material div.CodeMirror-selected {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material.CodeMirror-focused div.CodeMirror-selected {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material .CodeMirror-line::selection,
.cm-s-material .CodeMirror-line>span::selection,
.cm-s-material .CodeMirror-line>span>span::selection {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material .CodeMirror-line::-moz-selection,
.cm-s-material .CodeMirror-line>span::-moz-selection,
.cm-s-material .CodeMirror-line>span>span::-moz-selection {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-material .CodeMirror-activeline-background {
  background: rgba(0, 0, 0, 0.5);
}

.cm-s-material .cm-keyword {
  color: #C792EA;
}

.cm-s-material .cm-operator {
  color: #89DDFF;
}

.cm-s-material .cm-variable-2 {
  color: #EEFFFF;
}

.cm-s-material .cm-variable-3,
.cm-s-material .cm-type {
  color: #f07178;
}

.cm-s-material .cm-builtin {
  color: #FFCB6B;
}

.cm-s-material .cm-atom {
  color: #F78C6C;
}

.cm-s-material .cm-number {
  color: #FF5370;
}

.cm-s-material .cm-def {
  color: #82AAFF;
}

.cm-s-material .cm-string {
  color: #C3E88D;
}

.cm-s-material .cm-string-2 {
  color: #f07178;
}

.cm-s-material .cm-comment {
  color: #546E7A;
}

.cm-s-material .cm-variable {
  color: #f07178;
}

.cm-s-material .cm-tag {
  color: #FF5370;
}

.cm-s-material .cm-meta {
  color: #FFCB6B;
}

.cm-s-material .cm-attribute {
  color: #C792EA;
}

.cm-s-material .cm-property {
  color: #C792EA;
}

.cm-s-material .cm-qualifier {
  color: #DECB6B;
}

.cm-s-material .cm-variable-3,
.cm-s-material .cm-type {
  color: #DECB6B;
}


.cm-s-material .cm-error {
  color: rgba(255, 255, 255, 1.0);
  background-color: #FF5370;
}

.cm-s-material .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: white !important;
}
/**
 * "
 *  Using Zenburn color palette from the Emacs Zenburn Theme
 *  https://github.com/bbatsov/zenburn-emacs/blob/master/zenburn-theme.el
 *
 *  Also using parts of https://github.com/xavi/coderay-lighttable-theme
 * "
 * From: https://github.com/wisenomad/zenburn-lighttable-theme/blob/master/zenburn.css
 */

.cm-s-zenburn .CodeMirror-gutters { background: #3f3f3f !important; }
.cm-s-zenburn .CodeMirror-foldgutter-open, .CodeMirror-foldgutter-folded { color: #999; }
.cm-s-zenburn .CodeMirror-cursor { border-left: 1px solid white; }
.cm-s-zenburn { background-color: #3f3f3f; color: #dcdccc; }
.cm-s-zenburn span.cm-builtin { color: #dcdccc; font-weight: bold; }
.cm-s-zenburn span.cm-comment { color: #7f9f7f; }
.cm-s-zenburn span.cm-keyword { color: #f0dfaf; font-weight: bold; }
.cm-s-zenburn span.cm-atom { color: #bfebbf; }
.cm-s-zenburn span.cm-def { color: #dcdccc; }
.cm-s-zenburn span.cm-variable { color: #dfaf8f; }
.cm-s-zenburn span.cm-variable-2 { color: #dcdccc; }
.cm-s-zenburn span.cm-string { color: #cc9393; }
.cm-s-zenburn span.cm-string-2 { color: #cc9393; }
.cm-s-zenburn span.cm-number { color: #dcdccc; }
.cm-s-zenburn span.cm-tag { color: #93e0e3; }
.cm-s-zenburn span.cm-property { color: #dfaf8f; }
.cm-s-zenburn span.cm-attribute { color: #dfaf8f; }
.cm-s-zenburn span.cm-qualifier { color: #7cb8bb; }
.cm-s-zenburn span.cm-meta { color: #f0dfaf; }
.cm-s-zenburn span.cm-header { color: #f0efd0; }
.cm-s-zenburn span.cm-operator { color: #f0efd0; }
.cm-s-zenburn span.CodeMirror-matchingbracket { box-sizing: border-box; background: transparent; border-bottom: 1px solid; }
.cm-s-zenburn span.CodeMirror-nonmatchingbracket { border-bottom: 1px solid; background: none; }
.cm-s-zenburn .CodeMirror-activeline { background: #000000; }
.cm-s-zenburn .CodeMirror-activeline-background { background: #000000; }
.cm-s-zenburn div.CodeMirror-selected { background: #545454; }
.cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected { background: #4f4f4f; }

.cm-s-abcdef.CodeMirror { background: #0f0f0f; color: #defdef; }
.cm-s-abcdef div.CodeMirror-selected { background: #515151; }
.cm-s-abcdef .CodeMirror-line::selection, .cm-s-abcdef .CodeMirror-line > span::selection, .cm-s-abcdef .CodeMirror-line > span > span::selection { background: rgba(56, 56, 56, 0.99); }
.cm-s-abcdef .CodeMirror-line::-moz-selection, .cm-s-abcdef .CodeMirror-line > span::-moz-selection, .cm-s-abcdef .CodeMirror-line > span > span::-moz-selection { background: rgba(56, 56, 56, 0.99); }
.cm-s-abcdef .CodeMirror-gutters { background: #555; border-right: 2px solid #314151; }
.cm-s-abcdef .CodeMirror-guttermarker { color: #222; }
.cm-s-abcdef .CodeMirror-guttermarker-subtle { color: azure; }
.cm-s-abcdef .CodeMirror-linenumber { color: #FFFFFF; }
.cm-s-abcdef .CodeMirror-cursor { border-left: 1px solid #00FF00; }

.cm-s-abcdef span.cm-keyword { color: darkgoldenrod; font-weight: bold; }
.cm-s-abcdef span.cm-atom { color: #77F; }
.cm-s-abcdef span.cm-number { color: violet; }
.cm-s-abcdef span.cm-def { color: #fffabc; }
.cm-s-abcdef span.cm-variable { color: #abcdef; }
.cm-s-abcdef span.cm-variable-2 { color: #cacbcc; }
.cm-s-abcdef span.cm-variable-3, .cm-s-abcdef span.cm-type { color: #def; }
.cm-s-abcdef span.cm-property { color: #fedcba; }
.cm-s-abcdef span.cm-operator { color: #ff0; }
.cm-s-abcdef span.cm-comment { color: #7a7b7c; font-style: italic;}
.cm-s-abcdef span.cm-string { color: #2b4; }
.cm-s-abcdef span.cm-meta { color: #C9F; }
.cm-s-abcdef span.cm-qualifier { color: #FFF700; }
.cm-s-abcdef span.cm-builtin { color: #30aabc; }
.cm-s-abcdef span.cm-bracket { color: #8a8a8a; }
.cm-s-abcdef span.cm-tag { color: #FFDD44; }
.cm-s-abcdef span.cm-attribute { color: #DDFF00; }
.cm-s-abcdef span.cm-error { color: #FF0000; }
.cm-s-abcdef span.cm-header { color: aquamarine; font-weight: bold; }
.cm-s-abcdef span.cm-link { color: blueviolet; }

.cm-s-abcdef .CodeMirror-activeline-background { background: #314151; }

/*

    Name:       Base16 Default Light
    Author:     Chris Kempson (http://chriskempson.com)

    CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror)
    Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16)

*/

.cm-s-base16-light.CodeMirror { background: #f5f5f5; color: #202020; }
.cm-s-base16-light div.CodeMirror-selected { background: #e0e0e0; }
.cm-s-base16-light .CodeMirror-line::selection, .cm-s-base16-light .CodeMirror-line > span::selection, .cm-s-base16-light .CodeMirror-line > span > span::selection { background: #e0e0e0; }
.cm-s-base16-light .CodeMirror-line::-moz-selection, .cm-s-base16-light .CodeMirror-line > span::-moz-selection, .cm-s-base16-light .CodeMirror-line > span > span::-moz-selection { background: #e0e0e0; }
.cm-s-base16-light .CodeMirror-gutters { background: #f5f5f5; border-right: 0px; }
.cm-s-base16-light .CodeMirror-guttermarker { color: #ac4142; }
.cm-s-base16-light .CodeMirror-guttermarker-subtle { color: #b0b0b0; }
.cm-s-base16-light .CodeMirror-linenumber { color: #b0b0b0; }
.cm-s-base16-light .CodeMirror-cursor { border-left: 1px solid #505050; }

.cm-s-base16-light span.cm-comment { color: #8f5536; }
.cm-s-base16-light span.cm-atom { color: #aa759f; }
.cm-s-base16-light span.cm-number { color: #aa759f; }

.cm-s-base16-light span.cm-property, .cm-s-base16-light span.cm-attribute { color: #90a959; }
.cm-s-base16-light span.cm-keyword { color: #ac4142; }
.cm-s-base16-light span.cm-string { color: #f4bf75; }

.cm-s-base16-light span.cm-variable { color: #90a959; }
.cm-s-base16-light span.cm-variable-2 { color: #6a9fb5; }
.cm-s-base16-light span.cm-def { color: #d28445; }
.cm-s-base16-light span.cm-bracket { color: #202020; }
.cm-s-base16-light span.cm-tag { color: #ac4142; }
.cm-s-base16-light span.cm-link { color: #aa759f; }
.cm-s-base16-light span.cm-error { background: #ac4142; color: #505050; }

.cm-s-base16-light .CodeMirror-activeline-background { background: #DDDCDC; }
.cm-s-base16-light .CodeMirror-matchingbracket { color: #f5f5f5 !important; background-color: #6A9FB5 !important}

/*

    Name:       Base16 Default Dark
    Author:     Chris Kempson (http://chriskempson.com)

    CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror)
    Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16)

*/

.cm-s-base16-dark.CodeMirror { background: #151515; color: #e0e0e0; }
.cm-s-base16-dark div.CodeMirror-selected { background: #303030; }
.cm-s-base16-dark .CodeMirror-line::selection, .cm-s-base16-dark .CodeMirror-line > span::selection, .cm-s-base16-dark .CodeMirror-line > span > span::selection { background: rgba(48, 48, 48, .99); }
.cm-s-base16-dark .CodeMirror-line::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(48, 48, 48, .99); }
.cm-s-base16-dark .CodeMirror-gutters { background: #151515; border-right: 0px; }
.cm-s-base16-dark .CodeMirror-guttermarker { color: #ac4142; }
.cm-s-base16-dark .CodeMirror-guttermarker-subtle { color: #505050; }
.cm-s-base16-dark .CodeMirror-linenumber { color: #505050; }
.cm-s-base16-dark .CodeMirror-cursor { border-left: 1px solid #b0b0b0; }

.cm-s-base16-dark span.cm-comment { color: #8f5536; }
.cm-s-base16-dark span.cm-atom { color: #aa759f; }
.cm-s-base16-dark span.cm-number { color: #aa759f; }

.cm-s-base16-dark span.cm-property, .cm-s-base16-dark span.cm-attribute { color: #90a959; }
.cm-s-base16-dark span.cm-keyword { color: #ac4142; }
.cm-s-base16-dark span.cm-string { color: #f4bf75; }

.cm-s-base16-dark span.cm-variable { color: #90a959; }
.cm-s-base16-dark span.cm-variable-2 { color: #6a9fb5; }
.cm-s-base16-dark span.cm-def { color: #d28445; }
.cm-s-base16-dark span.cm-bracket { color: #e0e0e0; }
.cm-s-base16-dark span.cm-tag { color: #ac4142; }
.cm-s-base16-dark span.cm-link { color: #aa759f; }
.cm-s-base16-dark span.cm-error { background: #ac4142; color: #b0b0b0; }

.cm-s-base16-dark .CodeMirror-activeline-background { background: #202020; }
.cm-s-base16-dark .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; }

/*

    Name:       dracula
    Author:     Michael Kaminsky (http://github.com/mkaminsky11)

    Original dracula color scheme by Zeno Rocha (https://github.com/zenorocha/dracula-theme)

*/


.cm-s-dracula.CodeMirror, .cm-s-dracula .CodeMirror-gutters {
  background-color: #282a36 !important;
  color: #f8f8f2 !important;
  border: none;
}
.cm-s-dracula .CodeMirror-gutters { color: #282a36; }
.cm-s-dracula .CodeMirror-cursor { border-left: solid thin #f8f8f0; }
.cm-s-dracula .CodeMirror-linenumber { color: #6D8A88; }
.cm-s-dracula .CodeMirror-selected { background: rgba(255, 255, 255, 0.10); }
.cm-s-dracula .CodeMirror-line::selection, .cm-s-dracula .CodeMirror-line > span::selection, .cm-s-dracula .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-dracula .CodeMirror-line::-moz-selection, .cm-s-dracula .CodeMirror-line > span::-moz-selection, .cm-s-dracula .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-dracula span.cm-comment { color: #6272a4; }
.cm-s-dracula span.cm-string, .cm-s-dracula span.cm-string-2 { color: #f1fa8c; }
.cm-s-dracula span.cm-number { color: #bd93f9; }
.cm-s-dracula span.cm-variable { color: #50fa7b; }
.cm-s-dracula span.cm-variable-2 { color: white; }
.cm-s-dracula span.cm-def { color: #50fa7b; }
.cm-s-dracula span.cm-operator { color: #ff79c6; }
.cm-s-dracula span.cm-keyword { color: #ff79c6; }
.cm-s-dracula span.cm-atom { color: #bd93f9; }
.cm-s-dracula span.cm-meta { color: #f8f8f2; }
.cm-s-dracula span.cm-tag { color: #ff79c6; }
.cm-s-dracula span.cm-attribute { color: #50fa7b; }
.cm-s-dracula span.cm-qualifier { color: #50fa7b; }
.cm-s-dracula span.cm-property { color: #66d9ef; }
.cm-s-dracula span.cm-builtin { color: #50fa7b; }
.cm-s-dracula span.cm-variable-3, .cm-s-dracula span.cm-type { color: #ffb86c; }

.cm-s-dracula .CodeMirror-activeline-background { background: rgba(255,255,255,0.1); }
.cm-s-dracula .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; }

/*

    Name:       Hopscotch
    Author:     Jan T. Sott

    CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror)
    Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16)

*/

.cm-s-hopscotch.CodeMirror {background: #322931; color: #d5d3d5;}
.cm-s-hopscotch div.CodeMirror-selected {background: #433b42 !important;}
.cm-s-hopscotch .CodeMirror-gutters {background: #322931; border-right: 0px;}
.cm-s-hopscotch .CodeMirror-linenumber {color: #797379;}
.cm-s-hopscotch .CodeMirror-cursor {border-left: 1px solid #989498 !important;}

.cm-s-hopscotch span.cm-comment {color: #b33508;}
.cm-s-hopscotch span.cm-atom {color: #c85e7c;}
.cm-s-hopscotch span.cm-number {color: #c85e7c;}

.cm-s-hopscotch span.cm-property, .cm-s-hopscotch span.cm-attribute {color: #8fc13e;}
.cm-s-hopscotch span.cm-keyword {color: #dd464c;}
.cm-s-hopscotch span.cm-string {color: #fdcc59;}

.cm-s-hopscotch span.cm-variable {color: #8fc13e;}
.cm-s-hopscotch span.cm-variable-2 {color: #1290bf;}
.cm-s-hopscotch span.cm-def {color: #fd8b19;}
.cm-s-hopscotch span.cm-error {background: #dd464c; color: #989498;}
.cm-s-hopscotch span.cm-bracket {color: #d5d3d5;}
.cm-s-hopscotch span.cm-tag {color: #dd464c;}
.cm-s-hopscotch span.cm-link {color: #c85e7c;}

.cm-s-hopscotch .CodeMirror-matchingbracket { text-decoration: underline; color: white !important;}
.cm-s-hopscotch .CodeMirror-activeline-background { background: #302020; }

/****************************************************************/
/*   Based on mbonaci's Brackets mbo theme                      */
/*   https://github.com/mbonaci/global/blob/master/Mbo.tmTheme  */
/*   Create your own: http://tmtheme-editor.herokuapp.com       */
/****************************************************************/

.cm-s-mbo.CodeMirror { background: #2c2c2c; color: #ffffec; }
.cm-s-mbo div.CodeMirror-selected { background: #716C62; }
.cm-s-mbo .CodeMirror-line::selection, .cm-s-mbo .CodeMirror-line > span::selection, .cm-s-mbo .CodeMirror-line > span > span::selection { background: rgba(113, 108, 98, .99); }
.cm-s-mbo .CodeMirror-line::-moz-selection, .cm-s-mbo .CodeMirror-line > span::-moz-selection, .cm-s-mbo .CodeMirror-line > span > span::-moz-selection { background: rgba(113, 108, 98, .99); }
.cm-s-mbo .CodeMirror-gutters { background: #4e4e4e; border-right: 0px; }
.cm-s-mbo .CodeMirror-guttermarker { color: white; }
.cm-s-mbo .CodeMirror-guttermarker-subtle { color: grey; }
.cm-s-mbo .CodeMirror-linenumber { color: #dadada; }
.cm-s-mbo .CodeMirror-cursor { border-left: 1px solid #ffffec; }

.cm-s-mbo span.cm-comment { color: #95958a; }
.cm-s-mbo span.cm-atom { color: #00a8c6; }
.cm-s-mbo span.cm-number { color: #00a8c6; }

.cm-s-mbo span.cm-property, .cm-s-mbo span.cm-attribute { color: #9ddfe9; }
.cm-s-mbo span.cm-keyword { color: #ffb928; }
.cm-s-mbo span.cm-string { color: #ffcf6c; }
.cm-s-mbo span.cm-string.cm-property { color: #ffffec; }

.cm-s-mbo span.cm-variable { color: #ffffec; }
.cm-s-mbo span.cm-variable-2 { color: #00a8c6; }
.cm-s-mbo span.cm-def { color: #ffffec; }
.cm-s-mbo span.cm-bracket { color: #fffffc; font-weight: bold; }
.cm-s-mbo span.cm-tag { color: #9ddfe9; }
.cm-s-mbo span.cm-link { color: #f54b07; }
.cm-s-mbo span.cm-error { border-bottom: #636363; color: #ffffec; }
.cm-s-mbo span.cm-qualifier { color: #ffffec; }

.cm-s-mbo .CodeMirror-activeline-background { background: #494b41; }
.cm-s-mbo .CodeMirror-matchingbracket { color: #ffb928 !important; }
.cm-s-mbo .CodeMirror-matchingtag { background: rgba(255, 255, 255, .37); }

/*
  MDN-LIKE Theme - Mozilla
  Ported to CodeMirror by Peter Kroon <plakroon@gmail.com>
  Report bugs/issues here: https://github.com/codemirror/CodeMirror/issues
  GitHub: @peterkroon

  The mdn-like theme is inspired on the displayed code examples at: https://developer.mozilla.org/en-US/docs/Web/CSS/animation

*/
.cm-s-mdn-like.CodeMirror { color: #999; background-color: #fff; }
.cm-s-mdn-like div.CodeMirror-selected { background: #cfc; }
.cm-s-mdn-like .CodeMirror-line::selection, .cm-s-mdn-like .CodeMirror-line > span::selection, .cm-s-mdn-like .CodeMirror-line > span > span::selection { background: #cfc; }
.cm-s-mdn-like .CodeMirror-line::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span > span::-moz-selection { background: #cfc; }

.cm-s-mdn-like .CodeMirror-gutters { background: #f8f8f8; border-left: 6px solid rgba(0,83,159,0.65); color: #333; }
.cm-s-mdn-like .CodeMirror-linenumber { color: #aaa; padding-left: 8px; }
.cm-s-mdn-like .CodeMirror-cursor { border-left: 2px solid #222; }

.cm-s-mdn-like .cm-keyword { color: #6262FF; }
.cm-s-mdn-like .cm-atom { color: #F90; }
.cm-s-mdn-like .cm-number { color:  #ca7841; }
.cm-s-mdn-like .cm-def { color: #8DA6CE; }
.cm-s-mdn-like span.cm-variable-2, .cm-s-mdn-like span.cm-tag { color: #690; }
.cm-s-mdn-like span.cm-variable-3, .cm-s-mdn-like span.cm-def, .cm-s-mdn-like span.cm-type { color: #07a; }

.cm-s-mdn-like .cm-variable { color: #07a; }
.cm-s-mdn-like .cm-property { color: #905; }
.cm-s-mdn-like .cm-qualifier { color: #690; }

.cm-s-mdn-like .cm-operator { color: #cda869; }
.cm-s-mdn-like .cm-comment { color:#777; font-weight:normal; }
.cm-s-mdn-like .cm-string { color:#07a; font-style:italic; }
.cm-s-mdn-like .cm-string-2 { color:#bd6b18; } /*?*/
.cm-s-mdn-like .cm-meta { color: #000; } /*?*/
.cm-s-mdn-like .cm-builtin { color: #9B7536; } /*?*/
.cm-s-mdn-like .cm-tag { color: #997643; }
.cm-s-mdn-like .cm-attribute { color: #d6bb6d; } /*?*/
.cm-s-mdn-like .cm-header { color: #FF6400; }
.cm-s-mdn-like .cm-hr { color: #AEAEAE; }
.cm-s-mdn-like .cm-link { color:#ad9361; font-style:italic; text-decoration:none; }
.cm-s-mdn-like .cm-error { border-bottom: 1px solid red; }

div.cm-s-mdn-like .CodeMirror-activeline-background { background: #efefff; }
div.cm-s-mdn-like span.CodeMirror-matchingbracket { outline:1px solid grey; color: inherit; }

.cm-s-mdn-like.CodeMirror { background-image: url(data:image/png;base64,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); }

/*

    Name:       seti
    Author:     Michael Kaminsky (http://github.com/mkaminsky11)

    Original seti color scheme by Jesse Weed (https://github.com/jesseweed/seti-syntax)

*/


.cm-s-seti.CodeMirror {
  background-color: #151718 !important;
  color: #CFD2D1 !important;
  border: none;
}
.cm-s-seti .CodeMirror-gutters {
  color: #404b53;
  background-color: #0E1112;
  border: none;
}
.cm-s-seti .CodeMirror-cursor { border-left: solid thin #f8f8f0; }
.cm-s-seti .CodeMirror-linenumber { color: #6D8A88; }
.cm-s-seti.CodeMirror-focused div.CodeMirror-selected { background: rgba(255, 255, 255, 0.10); }
.cm-s-seti .CodeMirror-line::selection, .cm-s-seti .CodeMirror-line > span::selection, .cm-s-seti .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-seti .CodeMirror-line::-moz-selection, .cm-s-seti .CodeMirror-line > span::-moz-selection, .cm-s-seti .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); }
.cm-s-seti span.cm-comment { color: #41535b; }
.cm-s-seti span.cm-string, .cm-s-seti span.cm-string-2 { color: #55b5db; }
.cm-s-seti span.cm-number { color: #cd3f45; }
.cm-s-seti span.cm-variable { color: #55b5db; }
.cm-s-seti span.cm-variable-2 { color: #a074c4; }
.cm-s-seti span.cm-def { color: #55b5db; }
.cm-s-seti span.cm-keyword { color: #ff79c6; }
.cm-s-seti span.cm-operator { color: #9fca56; }
.cm-s-seti span.cm-keyword { color: #e6cd69; }
.cm-s-seti span.cm-atom { color: #cd3f45; }
.cm-s-seti span.cm-meta { color: #55b5db; }
.cm-s-seti span.cm-tag { color: #55b5db; }
.cm-s-seti span.cm-attribute { color: #9fca56; }
.cm-s-seti span.cm-qualifier { color: #9fca56; }
.cm-s-seti span.cm-property { color: #a074c4; }
.cm-s-seti span.cm-variable-3, .cm-s-seti span.cm-type { color: #9fca56; }
.cm-s-seti span.cm-builtin { color: #9fca56; }
.cm-s-seti .CodeMirror-activeline-background { background: #101213; }
.cm-s-seti .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; }

/*
Solarized theme for code-mirror
http://ethanschoonover.com/solarized
*/

/*
Solarized color palette
http://ethanschoonover.com/solarized/img/solarized-palette.png
*/

.solarized.base03 { color: #002b36; }
.solarized.base02 { color: #073642; }
.solarized.base01 { color: #586e75; }
.solarized.base00 { color: #657b83; }
.solarized.base0 { color: #839496; }
.solarized.base1 { color: #93a1a1; }
.solarized.base2 { color: #eee8d5; }
.solarized.base3  { color: #fdf6e3; }
.solarized.solar-yellow  { color: #b58900; }
.solarized.solar-orange  { color: #cb4b16; }
.solarized.solar-red { color: #dc322f; }
.solarized.solar-magenta { color: #d33682; }
.solarized.solar-violet  { color: #6c71c4; }
.solarized.solar-blue { color: #268bd2; }
.solarized.solar-cyan { color: #2aa198; }
.solarized.solar-green { color: #859900; }

/* Color scheme for code-mirror */

.cm-s-solarized {
  line-height: 1.45em;
  color-profile: sRGB;
  rendering-intent: auto;
}
.cm-s-solarized.cm-s-dark {
  color: #839496;
  background-color: #002b36;
  text-shadow: #002b36 0 1px;
}
.cm-s-solarized.cm-s-light {
  background-color: #fdf6e3;
  color: #657b83;
  text-shadow: #eee8d5 0 1px;
}

.cm-s-solarized .CodeMirror-widget {
  text-shadow: none;
}

.cm-s-solarized .cm-header { color: #586e75; }
.cm-s-solarized .cm-quote { color: #93a1a1; }

.cm-s-solarized .cm-keyword { color: #cb4b16; }
.cm-s-solarized .cm-atom { color: #d33682; }
.cm-s-solarized .cm-number { color: #d33682; }
.cm-s-solarized .cm-def { color: #2aa198; }

.cm-s-solarized .cm-variable { color: #839496; }
.cm-s-solarized .cm-variable-2 { color: #b58900; }
.cm-s-solarized .cm-variable-3, .cm-s-solarized .cm-type { color: #6c71c4; }

.cm-s-solarized .cm-property { color: #2aa198; }
.cm-s-solarized .cm-operator { color: #6c71c4; }

.cm-s-solarized .cm-comment { color: #586e75; font-style:italic; }

.cm-s-solarized .cm-string { color: #859900; }
.cm-s-solarized .cm-string-2 { color: #b58900; }

.cm-s-solarized .cm-meta { color: #859900; }
.cm-s-solarized .cm-qualifier { color: #b58900; }
.cm-s-solarized .cm-builtin { color: #d33682; }
.cm-s-solarized .cm-bracket { color: #cb4b16; }
.cm-s-solarized .CodeMirror-matchingbracket { color: #859900; }
.cm-s-solarized .CodeMirror-nonmatchingbracket { color: #dc322f; }
.cm-s-solarized .cm-tag { color: #93a1a1; }
.cm-s-solarized .cm-attribute { color: #2aa198; }
.cm-s-solarized .cm-hr {
  color: transparent;
  border-top: 1px solid #586e75;
  display: block;
}
.cm-s-solarized .cm-link { color: #93a1a1; cursor: pointer; }
.cm-s-solarized .cm-special { color: #6c71c4; }
.cm-s-solarized .cm-em {
  color: #999;
  text-decoration: underline;
  text-decoration-style: dotted;
}
.cm-s-solarized .cm-error,
.cm-s-solarized .cm-invalidchar {
  color: #586e75;
  border-bottom: 1px dotted #dc322f;
}

.cm-s-solarized.cm-s-dark div.CodeMirror-selected { background: #073642; }
.cm-s-solarized.cm-s-dark.CodeMirror ::selection { background: rgba(7, 54, 66, 0.99); }
.cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection, .cm-s-dark .CodeMirror-line > span::-moz-selection, .cm-s-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(7, 54, 66, 0.99); }

.cm-s-solarized.cm-s-light div.CodeMirror-selected { background: #eee8d5; }
.cm-s-solarized.cm-s-light .CodeMirror-line::selection, .cm-s-light .CodeMirror-line > span::selection, .cm-s-light .CodeMirror-line > span > span::selection { background: #eee8d5; }
.cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection, .cm-s-ligh .CodeMirror-line > span::-moz-selection, .cm-s-ligh .CodeMirror-line > span > span::-moz-selection { background: #eee8d5; }

/* Editor styling */



/* Little shadow on the view-port of the buffer view */
.cm-s-solarized.CodeMirror {
  -moz-box-shadow: inset 7px 0 12px -6px #000;
  -webkit-box-shadow: inset 7px 0 12px -6px #000;
  box-shadow: inset 7px 0 12px -6px #000;
}

/* Remove gutter border */
.cm-s-solarized .CodeMirror-gutters {
  border-right: 0;
}

/* Gutter colors and line number styling based of color scheme (dark / light) */

/* Dark */
.cm-s-solarized.cm-s-dark .CodeMirror-gutters {
  background-color: #073642;
}

.cm-s-solarized.cm-s-dark .CodeMirror-linenumber {
  color: #586e75;
  text-shadow: #021014 0 -1px;
}

/* Light */
.cm-s-solarized.cm-s-light .CodeMirror-gutters {
  background-color: #eee8d5;
}

.cm-s-solarized.cm-s-light .CodeMirror-linenumber {
  color: #839496;
}

/* Common */
.cm-s-solarized .CodeMirror-linenumber {
  padding: 0 5px;
}
.cm-s-solarized .CodeMirror-guttermarker-subtle { color: #586e75; }
.cm-s-solarized.cm-s-dark .CodeMirror-guttermarker { color: #ddd; }
.cm-s-solarized.cm-s-light .CodeMirror-guttermarker { color: #cb4b16; }

.cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text {
  color: #586e75;
}

/* Cursor */
.cm-s-solarized .CodeMirror-cursor { border-left: 1px solid #819090; }

/* Fat cursor */
.cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor { background: #77ee77; }
.cm-s-solarized.cm-s-light .cm-animate-fat-cursor { background-color: #77ee77; }
.cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor { background: #586e75; }
.cm-s-solarized.cm-s-dark .cm-animate-fat-cursor { background-color: #586e75; }

/* Active line */
.cm-s-solarized.cm-s-dark .CodeMirror-activeline-background {
  background: rgba(255, 255, 255, 0.06);
}
.cm-s-solarized.cm-s-light .CodeMirror-activeline-background {
  background: rgba(0, 0, 0, 0.06);
}

.cm-s-the-matrix.CodeMirror { background: #000000; color: #00FF00; }
.cm-s-the-matrix div.CodeMirror-selected { background: #2D2D2D; }
.cm-s-the-matrix .CodeMirror-line::selection, .cm-s-the-matrix .CodeMirror-line > span::selection, .cm-s-the-matrix .CodeMirror-line > span > span::selection { background: rgba(45, 45, 45, 0.99); }
.cm-s-the-matrix .CodeMirror-line::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span > span::-moz-selection { background: rgba(45, 45, 45, 0.99); }
.cm-s-the-matrix .CodeMirror-gutters { background: #060; border-right: 2px solid #00FF00; }
.cm-s-the-matrix .CodeMirror-guttermarker { color: #0f0; }
.cm-s-the-matrix .CodeMirror-guttermarker-subtle { color: white; }
.cm-s-the-matrix .CodeMirror-linenumber { color: #FFFFFF; }
.cm-s-the-matrix .CodeMirror-cursor { border-left: 1px solid #00FF00; }

.cm-s-the-matrix span.cm-keyword { color: #008803; font-weight: bold; }
.cm-s-the-matrix span.cm-atom { color: #3FF; }
.cm-s-the-matrix span.cm-number { color: #FFB94F; }
.cm-s-the-matrix span.cm-def { color: #99C; }
.cm-s-the-matrix span.cm-variable { color: #F6C; }
.cm-s-the-matrix span.cm-variable-2 { color: #C6F; }
.cm-s-the-matrix span.cm-variable-3, .cm-s-the-matrix span.cm-type { color: #96F; }
.cm-s-the-matrix span.cm-property { color: #62FFA0; }
.cm-s-the-matrix span.cm-operator { color: #999; }
.cm-s-the-matrix span.cm-comment { color: #CCCCCC; }
.cm-s-the-matrix span.cm-string { color: #39C; }
.cm-s-the-matrix span.cm-meta { color: #C9F; }
.cm-s-the-matrix span.cm-qualifier { color: #FFF700; }
.cm-s-the-matrix span.cm-builtin { color: #30a; }
.cm-s-the-matrix span.cm-bracket { color: #cc7; }
.cm-s-the-matrix span.cm-tag { color: #FFBD40; }
.cm-s-the-matrix span.cm-attribute { color: #FFF700; }
.cm-s-the-matrix span.cm-error { color: #FF0000; }

.cm-s-the-matrix .CodeMirror-activeline-background { background: #040; }

/*
Copyright (C) 2011 by MarkLogic Corporation
Author: Mike Brevoort <mike@brevoort.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/
.cm-s-xq-light span.cm-keyword { line-height: 1em; font-weight: bold; color: #5A5CAD; }
.cm-s-xq-light span.cm-atom { color: #6C8CD5; }
.cm-s-xq-light span.cm-number { color: #164; }
.cm-s-xq-light span.cm-def { text-decoration:underline; }
.cm-s-xq-light span.cm-variable { color: black; }
.cm-s-xq-light span.cm-variable-2 { color:black; }
.cm-s-xq-light span.cm-variable-3, .cm-s-xq-light span.cm-type { color: black; }
.cm-s-xq-light span.cm-property {}
.cm-s-xq-light span.cm-operator {}
.cm-s-xq-light span.cm-comment { color: #0080FF; font-style: italic; }
.cm-s-xq-light span.cm-string { color: red; }
.cm-s-xq-light span.cm-meta { color: yellow; }
.cm-s-xq-light span.cm-qualifier { color: grey; }
.cm-s-xq-light span.cm-builtin { color: #7EA656; }
.cm-s-xq-light span.cm-bracket { color: #cc7; }
.cm-s-xq-light span.cm-tag { color: #3F7F7F; }
.cm-s-xq-light span.cm-attribute { color: #7F007F; }
.cm-s-xq-light span.cm-error { color: #f00; }

.cm-s-xq-light .CodeMirror-activeline-background { background: #e8f2ff; }
.cm-s-xq-light .CodeMirror-matchingbracket { outline:1px solid grey;color:black !important;background:yellow; }

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.CodeMirror {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;
  /* Changed to auto to autogrow */
}

.CodeMirror pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* This causes https://github.com/jupyter/jupyterlab/issues/522 */
/* May not cause it not because we changed it! */
.CodeMirror-lines {
  padding: var(--jp-code-padding) 0;
}

.CodeMirror-linenumber {
  padding: 0 8px;
}

.jp-CodeMirrorEditor-static {
  margin: var(--jp-code-padding);
}

.jp-CodeMirrorEditor,
.jp-CodeMirrorEditor-static {
  cursor: text;
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.CodeMirror.jp-mod-readOnly .CodeMirror-cursor {
  display: none;
}

.CodeMirror-gutters {
  border-right: 1px solid var(--jp-border-color2);
  background-color: var(--jp-layout-color0);
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.CodeMirror-selectedtext.cm-searching {
  background-color: var(--jp-search-selected-match-background-color) !important;
  color: var(--jp-search-selected-match-color) !important;
}

.cm-searching {
  background-color: var(
    --jp-search-unselected-match-background-color
  ) !important;
  color: var(--jp-search-unselected-match-color) !important;
}

.CodeMirror-focused .CodeMirror-selected {
  background-color: var(--jp-editor-selected-focused-background);
}

.CodeMirror-selected {
  background-color: var(--jp-editor-selected-background);
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/**
 * Here is our jupyter theme for CodeMirror syntax highlighting
 * This is used in our marked.js syntax highlighting and CodeMirror itself
 * The string "jupyter" is set in ../codemirror/widget.DEFAULT_CODEMIRROR_THEME
 * This came from the classic notebook, which came form highlight.js/GitHub
 */

/**
 * CodeMirror themes are handling the background/color in this way. This works
 * fine for CodeMirror editors outside the notebook, but the notebook styles
 * these things differently.
 */
.CodeMirror.cm-s-jupyter {
  background: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* In the notebook, we want this styling to be handled by its container */
.jp-CodeConsole .CodeMirror.cm-s-jupyter,
.jp-Notebook .CodeMirror.cm-s-jupyter {
  background: transparent;
}

.cm-s-jupyter .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}
.cm-s-jupyter span.cm-keyword {
  color: var(--jp-mirror-editor-keyword-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-atom {
  color: var(--jp-mirror-editor-atom-color);
}
.cm-s-jupyter span.cm-number {
  color: var(--jp-mirror-editor-number-color);
}
.cm-s-jupyter span.cm-def {
  color: var(--jp-mirror-editor-def-color);
}
.cm-s-jupyter span.cm-variable {
  color: var(--jp-mirror-editor-variable-color);
}
.cm-s-jupyter span.cm-variable-2 {
  color: var(--jp-mirror-editor-variable-2-color);
}
.cm-s-jupyter span.cm-variable-3 {
  color: var(--jp-mirror-editor-variable-3-color);
}
.cm-s-jupyter span.cm-punctuation {
  color: var(--jp-mirror-editor-punctuation-color);
}
.cm-s-jupyter span.cm-property {
  color: var(--jp-mirror-editor-property-color);
}
.cm-s-jupyter span.cm-operator {
  color: var(--jp-mirror-editor-operator-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-comment {
  color: var(--jp-mirror-editor-comment-color);
  font-style: italic;
}
.cm-s-jupyter span.cm-string {
  color: var(--jp-mirror-editor-string-color);
}
.cm-s-jupyter span.cm-string-2 {
  color: var(--jp-mirror-editor-string-2-color);
}
.cm-s-jupyter span.cm-meta {
  color: var(--jp-mirror-editor-meta-color);
}
.cm-s-jupyter span.cm-qualifier {
  color: var(--jp-mirror-editor-qualifier-color);
}
.cm-s-jupyter span.cm-builtin {
  color: var(--jp-mirror-editor-builtin-color);
}
.cm-s-jupyter span.cm-bracket {
  color: var(--jp-mirror-editor-bracket-color);
}
.cm-s-jupyter span.cm-tag {
  color: var(--jp-mirror-editor-tag-color);
}
.cm-s-jupyter span.cm-attribute {
  color: var(--jp-mirror-editor-attribute-color);
}
.cm-s-jupyter span.cm-header {
  color: var(--jp-mirror-editor-header-color);
}
.cm-s-jupyter span.cm-quote {
  color: var(--jp-mirror-editor-quote-color);
}
.cm-s-jupyter span.cm-link {
  color: var(--jp-mirror-editor-link-color);
}
.cm-s-jupyter span.cm-error {
  color: var(--jp-mirror-editor-error-color);
}
.cm-s-jupyter span.cm-hr {
  color: #999;
}

.cm-s-jupyter span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}

.cm-s-jupyter .CodeMirror-activeline-background,
.cm-s-jupyter .CodeMirror-gutter {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0px;
  padding: 0px;
  line-height: normal;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}
.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}
.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}
.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}
.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}
.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}
.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}
.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
}
.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
}
.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
}
.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
}
.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
}
.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
}
.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
}
.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}
.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}
.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}
.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}
.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}
.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}
.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}
.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
}
.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
}
.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
}
.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
}
.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
}
.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
}
.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
}
.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}
.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}
.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0em;
}

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: 12px;
  table-layout: fixed;
  margin-left: auto;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon table {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0px;
}

.jp-RenderedHTMLCommon p {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}
/* ...or leave it untouched if they don't */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-dark-background {
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-light-background {
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}
.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}
.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}
.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}
.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}
.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}
.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}
.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}
.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: 0.8em;
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser {
  display: flex;
  flex-direction: column;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  border-bottom: none;
  height: auto;
  margin: var(--jp-toolbar-header-margin);
  box-shadow: none;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 4px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0px 2px;
  padding: 0px 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0px;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar.jp-Toolbar {
  padding: 0px;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  justify-content: space-evenly;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item {
  flex: 1;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent {
  width: 100%;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px 12px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-item.jp-mod-selected {
  color: white;
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before {
  color: limegreen;
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0px;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-DirListing-deadSpace {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

.jp-FileDialog.jp-mod-conflict input {
  color: red;
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
}

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: flex;
  flex-direction: row;
}

.jp-OutputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-output {
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea-child .jp-OutputArea-output {
  flex-grow: 1;
  flex-shrink: 1;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0px;
  padding: 0px;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0px;
  flex: 1 1 auto;
}

.jp-OutputArea-executeResult.jp-RenderedText {
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-OutputArea-stdin {
  line-height: var(--jp-code-line-height);
  padding-top: var(--jp-code-padding);
  display: flex;
}

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;
  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0px;
  bottom: 0px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0px;
  width: 100%;
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: flex;
  flex-direction: row;
}

.jp-InputArea-editor {
  flex: 1 1 auto;
}

.jp-InputArea-editor {
  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0px;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: flex;
  flex-direction: row;
  flex: 1 1 auto;
}

.jp-Placeholder-prompt {
  box-sizing: border-box;
}

.jp-Placeholder-content {
  flex: 1 1 auto;
  border: none;
  background: transparent;
  height: 20px;
  box-sizing: border-box;
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0px;
  margin: 0px;
  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 200px;
  box-shadow: inset 0 0 6px 2px rgba(0, 0, 0, 0.3);
  margin-left: var(--jp-private-cell-scrolling-output-offset);
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  flex: 0 0
    calc(
      var(--jp-cell-prompt-width) -
        var(--jp-private-cell-scrolling-output-offset)
    );
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  flex: 1 1 auto;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: 2px;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: flex;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0px;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-NotebookTools-tool {
  padding: 0px 12px 0 12px;
}

.jp-ActiveCellTool {
  padding: 12px;
  background-color: var(--jp-layout-color1);
  border-top: none !important;
}

.jp-ActiveCellTool .jp-InputArea-prompt {
  flex: 0 0 auto;
  padding-left: 0px;
}

.jp-ActiveCellTool .jp-InputArea-editor {
  flex: 1 1 auto;
  background: var(--jp-cell-editor-background);
  border-color: var(--jp-cell-editor-border-color);
}

.jp-ActiveCellTool .jp-InputArea-editor .CodeMirror {
  background: transparent;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0px 12px 0px;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label {
  line-height: 1.4;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

</style>

    <style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color),
    0px 1px 1px 0px var(--jp-shadow-penumbra-color),
    0px 1px 3px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color),
    0px 2px 2px 0px var(--jp-shadow-penumbra-color),
    0px 1px 5px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color),
    0px 4px 5px 0px var(--jp-shadow-penumbra-color),
    0px 1px 10px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color),
    0px 6px 10px 0px var(--jp-shadow-penumbra-color),
    0px 1px 18px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color),
    0px 8px 10px 1px var(--jp-shadow-penumbra-color),
    0px 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color),
    0px 12px 17px 2px var(--jp-shadow-penumbra-color),
    0px 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color),
    0px 16px 24px 2px var(--jp-shadow-penumbra-color),
    0px 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color),
    0px 20px 31px 3px var(--jp-shadow-penumbra-color),
    0px 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color),
    0px 24px 38px 3px var(--jp-shadow-penumbra-color),
    0px 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;

  --jp-ui-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica,
    Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;

  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);

  --jp-content-link-color: var(--md-blue-700);

  --jp-content-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI',
    Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: Menlo, Consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-700);
  --jp-brand-color1: var(--md-blue-500);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);

  --jp-accent-color0: var(--md-green-700);
  --jp-accent-color1: var(--md-green-500);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-700);
  --jp-warn-color1: var(--md-orange-500);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);

  --jp-error-color0: var(--md-red-700);
  --jp-error-color1: var(--md-red-500);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);

  --jp-success-color0: var(--md-green-700);
  --jp-success-color1: var(--md-green-500);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);

  --jp-info-color0: var(--md-cyan-700);
  --jp-info-color1: var(--md-cyan-500);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;

  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;

  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);

  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: 'Source Code Pro', monospace;
  --jp-cell-prompt-letter-spacing: 0px;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);
  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;
  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0px 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-border-color1);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: #05a;
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #aa22ff;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #aa22ff;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 180px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);
}
</style>

<style type="text/css">
a.anchor-link {
   display: none;
}
.highlight  {
    margin: 0.4em;
}

/* Input area styling */
.jp-InputArea {
    overflow: hidden;
}

.jp-InputArea-editor {
    overflow: hidden;
}

@media print {
  body {
    margin: 0;
  }
}
</style>



<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML-full,Safe"> </script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: { 
                    automatic: true 
                    }
                },
                "HTML-CSS": {
                    linebreaks: { 
                    automatic: true 
                    }
                }
            });
        
            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
    <!-- End of mathjax configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">

<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="Cercador-model-&#242;ptim-de-LSTM-i-GRU-de-una-capa">Cercador model &#242;ptim de LSTM i GRU de una capa<a class="anchor-link" href="#Cercador-model-&#242;ptim-de-LSTM-i-GRU-de-una-capa">&#182;</a></h1>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow.compat.v2</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">enable_v2_behavior</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">GRU</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">TimeDistributed</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">backend</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>C:\Users\dj_kr\anaconda3\lib\site-packages\statsmodels\tools\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
  import pandas.util.testing as tm
</pre>
</div>
</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="C&#224;rrega-de-les-dades">C&#224;rrega de les dades<a class="anchor-link" href="#C&#224;rrega-de-les-dades">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/SentDATA.csv&#39;</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Time&#39;</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Transformaci&#243;-de-dades">Transformaci&#243; de dades<a class="anchor-link" href="#Transformaci&#243;-de-dades">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PM1&#39;</span><span class="p">,</span><span class="s1">&#39;PM25&#39;</span><span class="p">,</span><span class="s1">&#39;PM10&#39;</span><span class="p">,</span><span class="s1">&#39;PM1ATM&#39;</span><span class="p">,</span><span class="s1">&#39;PM25ATM&#39;</span><span class="p">,</span><span class="s1">&#39;PM10ATM&#39;</span><span class="p">]</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">();</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;PM 1&quot;</span><span class="p">:</span><span class="s2">&quot;PM1&quot;</span><span class="p">,</span><span class="s2">&quot;PM 2.5&quot;</span><span class="p">:</span><span class="s2">&quot;PM25&quot;</span><span class="p">,</span><span class="s2">&quot;PM 10&quot;</span><span class="p">:</span><span class="s2">&quot;PM10&quot;</span><span class="p">,</span><span class="s2">&quot;PM 1 ATM&quot;</span><span class="p">:</span><span class="s2">&quot;PM1ATM&quot;</span><span class="p">,</span><span class="s2">&quot;PM 2.5 ATM&quot;</span><span class="p">:</span><span class="s2">&quot;PM25ATM&quot;</span><span class="p">,</span><span class="s2">&quot;PM 10 ATM&quot;</span><span class="p">:</span><span class="s2">&quot;PM10ATM&quot;</span><span class="p">})</span>

<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM25&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM10&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 10&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM1ATM&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 1 ATM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM25ATM&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 2.5 ATM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;PM10ATM&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;PM 10 ATM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df2</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Crear-dades-d'entrenament-i-de-test">Crear dades d'entrenament i de test<a class="anchor-link" href="#Crear-dades-d'entrenament-i-de-test">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">train_size</span><span class="p">],</span> <span class="n">df2</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_size</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">df2</span><span class="p">)]</span>
<span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[12]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>((3991, 7), (998, 7))</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Normalitzar-les-dades-d'entrenament">Normalitzar les dades d'entrenament<a class="anchor-link" href="#Normalitzar-les-dades-d'entrenament">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Standardize the data</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">train</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">col</span><span class="p">]])</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>&lt;ipython-input-13-ad7c79e4e223&gt;:4: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

&lt;ipython-input-13-ad7c79e4e223&gt;:4: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

&lt;ipython-input-13-ad7c79e4e223&gt;:4: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

&lt;ipython-input-13-ad7c79e4e223&gt;:4: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

&lt;ipython-input-13-ad7c79e4e223&gt;:4: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

&lt;ipython-input-13-ad7c79e4e223&gt;:4: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

</pre>
</div>
</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Crear-finestra-de-temps-PM-2.5">Crear finestra de temps PM 2.5<a class="anchor-link" href="#Crear-finestra-de-temps-PM-2.5">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">TIME_STEPS</span><span class="o">=</span><span class="mi">144</span> <span class="c1">#6 registres hora x 24h x 3 --&gt; equival a una finestra d&#39;un dia</span>

<span class="k">def</span> <span class="nf">create_sequences</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">time_steps</span><span class="o">=</span><span class="n">TIME_STEPS</span><span class="p">):</span>
    <span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">-</span><span class="n">time_steps</span><span class="p">):</span>
        <span class="n">Xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="n">time_steps</span><span class="p">)]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">time_steps</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Xs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

<span class="n">X_train1h</span><span class="p">,</span> <span class="n">y_train1h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">6</span><span class="p">)</span> <span class="c1">#1 hour</span>

<span class="n">X_train3h</span><span class="p">,</span> <span class="n">y_train3h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">18</span><span class="p">)</span> <span class="c1">#3 hours</span>

<span class="n">X_train6h</span><span class="p">,</span> <span class="n">y_train6h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">36</span><span class="p">)</span> <span class="c1">#6 hours</span>

<span class="n">X_train12h</span><span class="p">,</span> <span class="n">y_train12h</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">72</span><span class="p">)</span> <span class="c1">#12 hours</span>

<span class="n">X_train1d</span><span class="p">,</span> <span class="n">y_train1d</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">144</span><span class="p">)</span> <span class="c1">#1 day</span>

<span class="n">X_train3d</span><span class="p">,</span> <span class="n">y_train3d</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">432</span><span class="p">)</span> <span class="c1">#3 days</span>

<span class="n">X_train7d</span><span class="p">,</span> <span class="n">y_train7d</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">train</span><span class="p">[</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">1008</span><span class="p">)</span> <span class="c1">#7 days</span>
<span class="c1">#X_test, y_test = create_sequences(test[[columns[1]]], test[columns[1]])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train1h shape: </span><span class="si">{</span><span class="n">X_train1d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train3d shape: </span><span class="si">{</span><span class="n">X_train3h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train6h shape: </span><span class="si">{</span><span class="n">X_train6h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train12h shape: </span><span class="si">{</span><span class="n">X_train12h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train1d shape: </span><span class="si">{</span><span class="n">X_train1d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train3d shape: </span><span class="si">{</span><span class="n">X_train3d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train7d shape: </span><span class="si">{</span><span class="n">X_train7d</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>X_train1h shape: (3847, 144, 1)
X_train3d shape: (3973, 18, 1)
X_train6h shape: (3955, 36, 1)
X_train12h shape: (3919, 72, 1)
X_train1d shape: (3847, 144, 1)
X_train3d shape: (3559, 432, 1)
X_train7d shape: (2983, 1008, 1)
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_prediction</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">actual</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">model_name</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean Absolute Error: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mae</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Root Mean Square Error: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean Square Error: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mse</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mae</span><span class="p">,</span><span class="n">rmse</span><span class="p">,</span><span class="n">mse</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Cerca-dels-models-&#242;ptims">Cerca dels models &#242;ptims<a class="anchor-link" href="#Cerca-dels-models-&#242;ptims">&#182;</a></h2>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">_model</span><span class="p">(</span><span class="n">cmodel</span><span class="p">,</span><span class="n">units</span><span class="p">,</span><span class="n">activationDense</span><span class="p">,</span><span class="n">dropout1</span><span class="p">,</span><span class="n">optimizer</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">cmodel</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout1</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="n">activationDense</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mae&#39;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">LSTM</span><span class="p">,</span><span class="n">GRU</span><span class="p">]</span>
<span class="n">nmodels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;LSTM&quot;</span><span class="p">,</span><span class="s2">&quot;GRU&quot;</span><span class="p">]</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1h&quot;</span><span class="p">,</span><span class="s2">&quot;3h&quot;</span><span class="p">,</span><span class="s2">&quot;6h&quot;</span><span class="p">,</span><span class="s2">&quot;12h&quot;</span><span class="p">,</span><span class="s2">&quot;1d&quot;</span><span class="p">,</span> <span class="s2">&quot;3d&quot;</span><span class="p">,</span> <span class="s2">&quot;7d&quot;</span><span class="p">]</span>
<span class="n">X_trains</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_train1h</span><span class="p">,</span><span class="n">X_train3h</span><span class="p">,</span> <span class="n">X_train6h</span><span class="p">,</span> <span class="n">X_train12h</span><span class="p">,</span><span class="n">X_train1d</span><span class="p">,</span><span class="n">X_train3d</span><span class="p">,</span> <span class="n">X_train7d</span><span class="p">]</span>
<span class="n">y_trains</span><span class="o">=</span> <span class="p">[</span><span class="n">y_train1h</span><span class="p">,</span><span class="n">y_train3h</span><span class="p">,</span> <span class="n">y_train6h</span><span class="p">,</span><span class="n">y_train12h</span><span class="p">,</span><span class="n">y_train1d</span><span class="p">,</span><span class="n">y_train3d</span><span class="p">,</span> <span class="n">y_train7d</span><span class="p">]</span>
<span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;relu&#39;</span><span class="p">]</span>
<span class="n">activationsDense</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">]</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span><span class="s1">&#39;adadelta&#39;</span><span class="p">,</span><span class="s1">&#39;adamax&#39;</span><span class="p">]</span>
<span class="n">list_validationSplit</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">]</span>
<span class="n">list_dropout1</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>   
<span class="n">list_units</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> 
<span class="n">list_epochs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>   
<span class="n">list_batchsize</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>     

<span class="n">list_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="k">for</span> <span class="n">cmodel</span><span class="p">,</span><span class="n">nmodel</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span><span class="n">nmodels</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">sequence</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_trains</span><span class="p">,</span><span class="n">y_trains</span><span class="p">,</span><span class="n">sequences</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
            <span class="c1">#for activation in activations:</span>
                <span class="k">for</span> <span class="n">activationDense</span> <span class="ow">in</span> <span class="n">activationsDense</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">units</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">batchsize</span><span class="p">,</span><span class="n">dropout1</span><span class="p">,</span><span class="n">validationsplit</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">list_units</span><span class="p">,</span><span class="n">list_epochs</span><span class="p">,</span><span class="n">list_batchsize</span><span class="p">,</span><span class="n">list_dropout1</span><span class="p">,</span><span class="n">list_validationSplit</span><span class="p">):</span> 
                        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;###########################</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MODEL: &quot;</span><span class="p">,</span> <span class="n">nmodel</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence: &#39;</span><span class="p">,</span><span class="n">sequence</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;units: &#39;</span><span class="p">,</span><span class="n">units</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dropout1: &#39;</span><span class="p">,</span><span class="n">dropout1</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer:&#39;</span><span class="p">,</span><span class="n">optimizer</span><span class="p">)</span>
                        <span class="c1">#print(&#39;activation:&#39;,activation)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;activationDense:&#39;</span><span class="p">,</span><span class="n">activationDense</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epochs:&#39;</span><span class="p">,</span><span class="n">epochs</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;batchsize:&#39;</span><span class="p">,</span><span class="n">batchsize</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;validation_split:&#39;</span><span class="p">,</span><span class="n">validationsplit</span><span class="p">)</span>
                    
                        <span class="n">model</span> <span class="o">=</span> <span class="n">_model</span><span class="p">(</span><span class="n">cmodel</span><span class="p">,</span><span class="n">units</span><span class="p">,</span><span class="n">activationDense</span><span class="p">,</span><span class="n">dropout1</span><span class="p">,</span><span class="n">optimizer</span><span class="p">)</span>
                        <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="n">validationsplit</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">)],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
                        <span class="n">totalTime</span> <span class="o">=</span> <span class="n">end</span><span class="o">-</span><span class="n">start</span>
                        <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Execution time: &#39;</span><span class="p">,</span><span class="n">totalTime</span><span class="p">)</span>

                        <span class="n">X_train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                        
                        <span class="n">mae</span><span class="p">,</span><span class="n">rmse</span><span class="p">,</span><span class="n">mse</span> <span class="o">=</span> <span class="n">evaluate_prediction</span><span class="p">(</span><span class="n">X_train_pred</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span><span class="n">nmodel</span><span class="p">)</span>
                        
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train RMSE: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rmse</span><span class="p">);</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train MSE: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mse</span><span class="p">);</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train MAE: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mae</span><span class="p">);</span>
                        
                        <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
                                               <span class="c1">#&#39;activation&#39;:[activation],</span>
                                               <span class="s1">&#39;model&#39;</span><span class="p">:[</span><span class="n">nmodel</span><span class="p">],</span>
                                               <span class="s1">&#39;sequence&#39;</span><span class="p">:[</span><span class="n">sequence</span><span class="p">],</span>
                                               <span class="s1">&#39;activationDense&#39;</span><span class="p">:[</span><span class="n">activationDense</span><span class="p">],</span>
                                               <span class="s1">&#39;optimizer&#39;</span><span class="p">:[</span><span class="n">optimizer</span><span class="p">],</span>
                                               <span class="s1">&#39;dropout1&#39;</span><span class="p">:[</span><span class="n">dropout1</span><span class="p">],</span>
                                               <span class="s1">&#39;units&#39;</span><span class="p">:[</span><span class="n">units</span><span class="p">],</span>
                                               <span class="s1">&#39;epochs&#39;</span><span class="p">:[</span><span class="n">epochs</span><span class="p">],</span>
                                               <span class="s1">&#39;batchsize&#39;</span><span class="p">:[</span><span class="n">batchsize</span><span class="p">],</span>
                                               <span class="s1">&#39;validation_split&#39;</span><span class="p">:[</span><span class="n">validationsplit</span><span class="p">],</span>
                                               
                                               <span class="s1">&#39;RMSE&#39;</span><span class="p">:[</span><span class="n">rmse</span><span class="p">],</span>
                                               <span class="s1">&#39;MSE&#39;</span><span class="p">:[</span><span class="n">mse</span><span class="p">],</span>
                                               <span class="s1">&#39;MAE&#39;</span><span class="p">:[</span><span class="n">mae</span><span class="p">],</span>                            
                                               <span class="s1">&#39;Time&#39;</span><span class="p">:[</span><span class="n">totalTime</span><span class="p">]})</span>
                        <span class="n">list_results</span> <span class="o">=</span> <span class="n">list_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> 
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>###########################

MODEL:  LSTM
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm (LSTM)                  (None, 6, 40)             6720      
_________________________________________________________________
dropout (Dropout)            (None, 6, 40)             0         
_________________________________________________________________
time_distributed (TimeDistri (None, 6, 1)              41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 11ms/step - loss: 0.5607 - val_loss: 0.3392
Epoch 2/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3897 - val_loss: 0.3060
Epoch 3/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3470 - val_loss: 0.2439
Epoch 4/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3207 - val_loss: 0.2145
Epoch 5/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3083 - val_loss: 0.1999
Epoch 6/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2997 - val_loss: 0.1878
Epoch 7/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2932 - val_loss: 0.1743
Epoch 8/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2871 - val_loss: 0.1666
Epoch 9/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2831 - val_loss: 0.1602
Epoch 10/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2797 - val_loss: 0.1566
Epoch 11/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2774 - val_loss: 0.1505
Epoch 12/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2761 - val_loss: 0.1522
Epoch 13/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2759 - val_loss: 0.1484
Epoch 14/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2744 - val_loss: 0.1477
Epoch 15/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2737 - val_loss: 0.1459
Epoch 16/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2734 - val_loss: 0.1442
Epoch 17/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2729 - val_loss: 0.1432
Epoch 18/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2723 - val_loss: 0.1418
Epoch 19/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2723 - val_loss: 0.1429
Epoch 20/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2722 - val_loss: 0.1414
Epoch 21/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2722 - val_loss: 0.1408
Epoch 22/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2713 - val_loss: 0.1382
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2719 - val_loss: 0.1373
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2711 - val_loss: 0.1376
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2707 - val_loss: 0.1393
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2706 - val_loss: 0.1385
Epoch 27/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2708 - val_loss: 0.1372
Epoch 28/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2707 - val_loss: 0.1344
Epoch 29/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2697 - val_loss: 0.1372
Epoch 30/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 0.1331
Epoch 31/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2700 - val_loss: 0.1343
Epoch 32/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.1345
Epoch 33/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2697 - val_loss: 0.1346
Epoch 34/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2691 - val_loss: 0.1342
Epoch 35/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2692 - val_loss: 0.1324
Epoch 36/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2694 - val_loss: 0.1301
Epoch 37/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2688 - val_loss: 0.1310
Epoch 38/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2686 - val_loss: 0.1305
Epoch 39/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2684 - val_loss: 0.1274
Epoch 40/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2684 - val_loss: 0.1280
Epoch 41/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2679 - val_loss: 0.1250
Epoch 42/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2676 - val_loss: 0.1278
Epoch 43/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2674 - val_loss: 0.1245
Epoch 44/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2670 - val_loss: 0.1267
Epoch 45/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2673 - val_loss: 0.1233
Epoch 46/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2666 - val_loss: 0.1192
Epoch 47/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2669 - val_loss: 0.1214
Epoch 48/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2669 - val_loss: 0.1239
Epoch 49/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2666 - val_loss: 0.1187
Epoch 50/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2662 - val_loss: 0.1213
Epoch 51/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2658 - val_loss: 0.1182
Epoch 52/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2657 - val_loss: 0.1202
Epoch 53/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2654 - val_loss: 0.1169
Epoch 54/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2656 - val_loss: 0.1139
Epoch 55/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2650 - val_loss: 0.1136
Epoch 56/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2648 - val_loss: 0.1155
Epoch 57/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2644 - val_loss: 0.1138
Epoch 58/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2644 - val_loss: 0.1169
Epoch 59/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2648 - val_loss: 0.1144
Epoch 60/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2643 - val_loss: 0.1115
Epoch 61/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2639 - val_loss: 0.1131
Epoch 62/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2642 - val_loss: 0.1099
Epoch 63/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2636 - val_loss: 0.1092
Epoch 64/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2636 - val_loss: 0.1069
Epoch 65/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 0.1059
Epoch 66/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 0.1104
Epoch 67/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2632 - val_loss: 0.1086
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2629 - val_loss: 0.1084
Execution time:  27.412817239761353
LSTM:
Mean Absolute Error: 0.1593
Root Mean Square Error: 0.5818
Mean Square Error: 0.3385

Train RMSE: 0.582
Train MSE: 0.338
Train MAE: 0.159
###########################

MODEL:  LSTM
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 6, 55)             12540     
_________________________________________________________________
dropout_1 (Dropout)          (None, 6, 55)             0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 6, 1)              56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 6ms/step - loss: 0.3993 - val_loss: 0.2529
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2935 - val_loss: 0.2201
Epoch 3/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2740 - val_loss: 0.2121
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2690 - val_loss: 0.2104
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2678 - val_loss: 0.2091
Epoch 6/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2673 - val_loss: 0.2083
Epoch 7/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2670 - val_loss: 0.2075
Epoch 8/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2667 - val_loss: 0.2064
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2662 - val_loss: 0.2054
Epoch 10/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2659 - val_loss: 0.2051
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2654 - val_loss: 0.2026
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2002
Epoch 13/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2644 - val_loss: 0.1984
Epoch 14/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.1973
Epoch 15/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2630 - val_loss: 0.1934
Epoch 16/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2624 - val_loss: 0.1915
Epoch 17/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2613 - val_loss: 0.1889
Epoch 18/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2610 - val_loss: 0.1872
Epoch 19/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2605 - val_loss: 0.1875
Epoch 20/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2598 - val_loss: 0.1867
Epoch 21/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1840
Epoch 22/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2592 - val_loss: 0.1835
Epoch 23/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2589 - val_loss: 0.1808
Epoch 24/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2585 - val_loss: 0.1827
Epoch 25/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2583 - val_loss: 0.1829
Epoch 26/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2580 - val_loss: 0.1816
Epoch 27/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2576 - val_loss: 0.1804
Epoch 28/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2578 - val_loss: 0.1793
Epoch 29/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2572 - val_loss: 0.1785
Epoch 30/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2571 - val_loss: 0.1803
Epoch 31/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2569 - val_loss: 0.1778
Epoch 32/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2570 - val_loss: 0.1786
Epoch 33/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2568 - val_loss: 0.1758
Epoch 34/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2566 - val_loss: 0.1780
Epoch 35/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2564 - val_loss: 0.1779
Epoch 36/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2567 - val_loss: 0.1775
Execution time:  51.12305450439453
LSTM:
Mean Absolute Error: 0.1511
Root Mean Square Error: 0.5819
Mean Square Error: 0.3386

Train RMSE: 0.582
Train MSE: 0.339
Train MAE: 0.151
###########################

MODEL:  LSTM
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_2&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_2 (LSTM)                (None, 6, 40)             6720      
_________________________________________________________________
dropout_2 (Dropout)          (None, 6, 40)             0         
_________________________________________________________________
time_distributed_2 (TimeDist (None, 6, 1)              41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 11ms/step - loss: 0.8414 - val_loss: 1.0555
Epoch 2/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6386 - val_loss: 0.8866
Epoch 3/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5859 - val_loss: 0.8566
Epoch 4/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5654 - val_loss: 0.8405
Epoch 5/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5545 - val_loss: 0.8302
Epoch 6/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5469 - val_loss: 0.8226
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5412 - val_loss: 0.8167
Epoch 8/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5362 - val_loss: 0.8120
Epoch 9/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5314 - val_loss: 0.8082
Epoch 10/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5243 - val_loss: 0.8049
Epoch 11/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5164 - val_loss: 0.8022
Epoch 12/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5111 - val_loss: 0.8002
Epoch 13/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5073 - val_loss: 0.7985
Epoch 14/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5042 - val_loss: 0.7972
Epoch 15/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5023 - val_loss: 0.7962
Epoch 16/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5003 - val_loss: 0.7955
Epoch 17/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4990 - val_loss: 0.7949
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4980 - val_loss: 0.7945
Epoch 19/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4972 - val_loss: 0.7941
Epoch 20/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4966 - val_loss: 0.7939
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4961 - val_loss: 0.7937
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7935
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7934
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 0.7933
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4945 - val_loss: 0.7932
Epoch 26/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4944 - val_loss: 0.7931
Epoch 27/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7931
Epoch 28/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4939 - val_loss: 0.7930
Epoch 29/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4938 - val_loss: 0.7930
Epoch 30/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4936 - val_loss: 0.7929
Epoch 31/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4937 - val_loss: 0.7929
Epoch 32/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4935 - val_loss: 0.7929
Epoch 33/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4933 - val_loss: 0.7929
Epoch 34/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4934 - val_loss: 0.7928
Epoch 35/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4931 - val_loss: 0.7928
Epoch 36/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4933 - val_loss: 0.7928
Epoch 37/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4931 - val_loss: 0.7928
Epoch 38/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4929 - val_loss: 0.7928
Epoch 39/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4932 - val_loss: 0.7928
Epoch 40/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4929 - val_loss: 0.7927
Epoch 41/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927
Epoch 42/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.7927
Epoch 43/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927
Epoch 44/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4928 - val_loss: 0.7927
Epoch 45/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.7927
Epoch 46/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4928 - val_loss: 0.7927
Epoch 47/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927
Epoch 48/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927
Epoch 49/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.7927
Epoch 50/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927
Epoch 51/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7927
Epoch 52/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7927
Epoch 53/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4922 - val_loss: 0.7927
Epoch 54/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.7927
Epoch 55/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7927
Epoch 56/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926
Epoch 57/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4924 - val_loss: 0.7926
Epoch 58/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4924 - val_loss: 0.7926
Epoch 59/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926
Epoch 60/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926
Epoch 61/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4922 - val_loss: 0.7926
Epoch 62/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4925 - val_loss: 0.7926
Epoch 63/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.7926
Epoch 64/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4921 - val_loss: 0.7926
Epoch 65/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4921 - val_loss: 0.7926
Epoch 66/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4921 - val_loss: 0.7926
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4922 - val_loss: 0.7926
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4922 - val_loss: 0.7926
Execution time:  28.099231719970703
LSTM:
Mean Absolute Error: 0.4837
Root Mean Square Error: 0.7525
Mean Square Error: 0.5663

Train RMSE: 0.753
Train MSE: 0.566
Train MAE: 0.484
###########################

MODEL:  LSTM
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_3&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_3 (LSTM)                (None, 6, 55)             12540     
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 55)             0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 6, 1)              56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 5ms/step - loss: 0.6488 - val_loss: 0.7089
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5468 - val_loss: 0.6836
Epoch 3/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5270 - val_loss: 0.6669
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5125 - val_loss: 0.6599
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5022 - val_loss: 0.6556
Epoch 6/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4968 - val_loss: 0.6533
Epoch 7/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4946 - val_loss: 0.6523
Epoch 8/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4934 - val_loss: 0.6516
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4928 - val_loss: 0.6510
Epoch 10/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4923 - val_loss: 0.6514
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4920 - val_loss: 0.6510
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4917 - val_loss: 0.6508
Epoch 13/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.6510
Epoch 14/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4914 - val_loss: 0.6506
Epoch 15/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4913 - val_loss: 0.6510
Epoch 16/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4911 - val_loss: 0.6511
Epoch 17/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4909 - val_loss: 0.6513
Epoch 18/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4908 - val_loss: 0.6507
Epoch 19/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4904 - val_loss: 0.6510
Epoch 20/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4902 - val_loss: 0.6514
Epoch 21/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4904 - val_loss: 0.6516
Epoch 22/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4902 - val_loss: 0.6511
Epoch 23/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4902 - val_loss: 0.6512
Epoch 24/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4900 - val_loss: 0.6510
Execution time:  35.80128622055054
LSTM:
Mean Absolute Error: 0.4832
Root Mean Square Error: 0.7581
Mean Square Error: 0.5746

Train RMSE: 0.758
Train MSE: 0.575
Train MAE: 0.483
###########################

MODEL:  LSTM
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_4&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_4 (LSTM)                (None, 6, 40)             6720      
_________________________________________________________________
dropout_4 (Dropout)          (None, 6, 40)             0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 6, 1)              41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6788 - val_loss: 0.7690
Epoch 2/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6787 - val_loss: 0.7687
Epoch 3/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6784 - val_loss: 0.7684
Epoch 4/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6784 - val_loss: 0.7680
Epoch 5/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6778 - val_loss: 0.7677
Epoch 6/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6777 - val_loss: 0.7674
Epoch 7/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6776 - val_loss: 0.7670
Epoch 8/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6774 - val_loss: 0.7667
Epoch 9/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6770 - val_loss: 0.7663
Epoch 10/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6769 - val_loss: 0.7660
Epoch 11/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6767 - val_loss: 0.7656
Epoch 12/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6763 - val_loss: 0.7653
Epoch 13/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6763 - val_loss: 0.7649
Epoch 14/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6758 - val_loss: 0.7646
Epoch 15/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6757 - val_loss: 0.7642
Epoch 16/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6755 - val_loss: 0.7638
Epoch 17/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6754 - val_loss: 0.7635
Epoch 18/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6751 - val_loss: 0.7631
Epoch 19/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6748 - val_loss: 0.7628
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6743 - val_loss: 0.7624
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6744 - val_loss: 0.7620
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6743 - val_loss: 0.7616
Epoch 23/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6740 - val_loss: 0.7613
Epoch 24/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6737 - val_loss: 0.7609
Epoch 25/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6734 - val_loss: 0.7605
Epoch 26/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6731 - val_loss: 0.7601
Epoch 27/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6729 - val_loss: 0.7598
Epoch 28/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6729 - val_loss: 0.7594
Epoch 29/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6725 - val_loss: 0.7590
Epoch 30/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6723 - val_loss: 0.7586
Epoch 31/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6721 - val_loss: 0.7582
Epoch 32/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6716 - val_loss: 0.7579
Epoch 33/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6717 - val_loss: 0.7575
Epoch 34/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6713 - val_loss: 0.7571
Epoch 35/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6709 - val_loss: 0.7567
Epoch 36/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6709 - val_loss: 0.7563
Epoch 37/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6706 - val_loss: 0.7559
Epoch 38/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6703 - val_loss: 0.7556
Epoch 39/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6701 - val_loss: 0.7552
Epoch 40/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6698 - val_loss: 0.7548
Epoch 41/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6696 - val_loss: 0.7544
Epoch 42/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6695 - val_loss: 0.7540
Epoch 43/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6691 - val_loss: 0.7536
Epoch 44/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6690 - val_loss: 0.7532
Epoch 45/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6685 - val_loss: 0.7528
Epoch 46/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6683 - val_loss: 0.7524
Epoch 47/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6683 - val_loss: 0.7520
Epoch 48/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6680 - val_loss: 0.7516
Epoch 49/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6677 - val_loss: 0.7512
Epoch 50/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6676 - val_loss: 0.7508
Epoch 51/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6672 - val_loss: 0.7504
Epoch 52/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6670 - val_loss: 0.7500
Epoch 53/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6669 - val_loss: 0.7496
Epoch 54/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6667 - val_loss: 0.7492
Epoch 55/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6662 - val_loss: 0.7488
Epoch 56/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6662 - val_loss: 0.7484
Epoch 57/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6656 - val_loss: 0.7480
Epoch 58/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6655 - val_loss: 0.7476
Epoch 59/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6652 - val_loss: 0.7472
Epoch 60/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6651 - val_loss: 0.7468
Epoch 61/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6649 - val_loss: 0.7464
Epoch 62/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6645 - val_loss: 0.7460
Epoch 63/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6646 - val_loss: 0.7456
Epoch 64/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6639 - val_loss: 0.7452
Epoch 65/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6639 - val_loss: 0.7448
Epoch 66/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6636 - val_loss: 0.7444
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6634 - val_loss: 0.7440
Epoch 68/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6629 - val_loss: 0.7436
Execution time:  27.464905261993408
LSTM:
Mean Absolute Error: 0.6691
Root Mean Square Error: 0.9589
Mean Square Error: 0.9195

Train RMSE: 0.959
Train MSE: 0.919
Train MAE: 0.669
###########################

MODEL:  LSTM
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_5&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_5 (LSTM)                (None, 6, 55)             12540     
_________________________________________________________________
dropout_5 (Dropout)          (None, 6, 55)             0         
_________________________________________________________________
time_distributed_5 (TimeDist (None, 6, 1)              56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 5ms/step - loss: 0.7034 - val_loss: 0.6855
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7019 - val_loss: 0.6838
Epoch 3/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7005 - val_loss: 0.6821
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6992 - val_loss: 0.6804
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6979 - val_loss: 0.6786
Epoch 6/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6965 - val_loss: 0.6768
Epoch 7/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6951 - val_loss: 0.6750
Epoch 8/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6938 - val_loss: 0.6731
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6924 - val_loss: 0.6713
Epoch 10/36
355/355 [==============================] - ETA: 0s - loss: 0.693 - 1s 4ms/step - loss: 0.6910 - val_loss: 0.6694
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6895 - val_loss: 0.6674
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6880 - val_loss: 0.6655
Epoch 13/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6867 - val_loss: 0.6635
Epoch 14/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6852 - val_loss: 0.6616
Epoch 15/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6838 - val_loss: 0.6595
Epoch 16/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6823 - val_loss: 0.6575
Epoch 17/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6808 - val_loss: 0.6555
Epoch 18/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6794 - val_loss: 0.6534
Epoch 19/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6778 - val_loss: 0.6514
Epoch 20/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6763 - val_loss: 0.6493
Epoch 21/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6747 - val_loss: 0.6472
Epoch 22/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6733 - val_loss: 0.6450
Epoch 23/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6716 - val_loss: 0.6429
Epoch 24/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6704 - val_loss: 0.6407
Epoch 25/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6688 - val_loss: 0.6386
Epoch 26/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6671 - val_loss: 0.6364
Epoch 27/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6655 - val_loss: 0.6342
Epoch 28/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6639 - val_loss: 0.6320
Epoch 29/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6623 - val_loss: 0.6297
Epoch 30/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6607 - val_loss: 0.6275
Epoch 31/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6591 - val_loss: 0.6253
Epoch 32/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6575 - val_loss: 0.6230
Epoch 33/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6559 - val_loss: 0.6207
Epoch 34/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6543 - val_loss: 0.6184
Epoch 35/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6526 - val_loss: 0.6161
Epoch 36/36
355/355 [==============================] - 1s 4ms/step - loss: 0.6510 - val_loss: 0.6138
Execution time:  51.92017316818237
LSTM:
Mean Absolute Error: 0.6398
Root Mean Square Error: 0.9327
Mean Square Error: 0.8699

Train RMSE: 0.933
Train MSE: 0.870
Train MAE: 0.640
###########################

MODEL:  LSTM
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_6&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_6 (LSTM)                (None, 6, 40)             6720      
_________________________________________________________________
dropout_6 (Dropout)          (None, 6, 40)             0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 6, 1)              41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8879 - val_loss: 1.2982
Epoch 2/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8878 - val_loss: 1.2982
Epoch 3/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8877 - val_loss: 1.2981
Epoch 4/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8878 - val_loss: 1.2980
Epoch 5/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8876 - val_loss: 1.2979
Epoch 6/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8876 - val_loss: 1.2978
Epoch 7/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8876 - val_loss: 1.2977
Epoch 8/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8875 - val_loss: 1.2976
Epoch 9/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8874 - val_loss: 1.2975
Epoch 10/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8874 - val_loss: 1.2974
Epoch 11/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8873 - val_loss: 1.2974
Epoch 12/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8873 - val_loss: 1.2973
Epoch 13/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8872 - val_loss: 1.2972
Epoch 14/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8872 - val_loss: 1.2971
Epoch 15/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8870 - val_loss: 1.2970
Epoch 16/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8870 - val_loss: 1.2969
Epoch 17/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8870 - val_loss: 1.2968
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8869 - val_loss: 1.2967
Epoch 19/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8868 - val_loss: 1.2966
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8868 - val_loss: 1.2965
Epoch 21/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8867 - val_loss: 1.2964
Epoch 22/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8867 - val_loss: 1.2963
Epoch 23/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8866 - val_loss: 1.2962
Epoch 24/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8866 - val_loss: 1.2961
Epoch 25/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8865 - val_loss: 1.2960
Epoch 26/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8865 - val_loss: 1.2959
Epoch 27/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8864 - val_loss: 1.2958
Epoch 28/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8863 - val_loss: 1.2957
Epoch 29/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8862 - val_loss: 1.2956
Epoch 30/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8863 - val_loss: 1.2955
Epoch 31/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8862 - val_loss: 1.2954
Epoch 32/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8861 - val_loss: 1.2953
Epoch 33/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8860 - val_loss: 1.2952
Epoch 34/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8860 - val_loss: 1.2951
Epoch 35/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8859 - val_loss: 1.2950
Epoch 36/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8859 - val_loss: 1.2949
Epoch 37/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8858 - val_loss: 1.2948
Epoch 38/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8857 - val_loss: 1.2947
Epoch 39/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8857 - val_loss: 1.2946
Epoch 40/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8856 - val_loss: 1.2945
Epoch 41/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8856 - val_loss: 1.2944
Epoch 42/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8856 - val_loss: 1.2943
Epoch 43/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8855 - val_loss: 1.2942
Epoch 44/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8854 - val_loss: 1.2941
Epoch 45/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8853 - val_loss: 1.2940
Epoch 46/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8853 - val_loss: 1.2939
Epoch 47/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8852 - val_loss: 1.2938
Epoch 48/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8852 - val_loss: 1.2937
Epoch 49/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8851 - val_loss: 1.2936
Epoch 50/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8851 - val_loss: 1.2935
Epoch 51/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8850 - val_loss: 1.2934
Epoch 52/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8849 - val_loss: 1.2933
Epoch 53/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8848 - val_loss: 1.2932
Epoch 54/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8847 - val_loss: 1.2931
Epoch 55/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8848 - val_loss: 1.2930
Epoch 56/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8847 - val_loss: 1.2929
Epoch 57/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8846 - val_loss: 1.2928
Epoch 58/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8846 - val_loss: 1.2927
Epoch 59/68
80/80 [==============================] - 0s 6ms/step - loss: 0.8845 - val_loss: 1.2926
Epoch 60/68
80/80 [==============================] - 1s 7ms/step - loss: 0.8844 - val_loss: 1.2925
Epoch 61/68
80/80 [==============================] - 1s 6ms/step - loss: 0.8844 - val_loss: 1.2924
Epoch 62/68
80/80 [==============================] - 1s 7ms/step - loss: 0.8843 - val_loss: 1.2923
Epoch 63/68
80/80 [==============================] - 1s 7ms/step - loss: 0.8843 - val_loss: 1.2922
Epoch 64/68
80/80 [==============================] - 0s 6ms/step - loss: 0.8842 - val_loss: 1.2921
Epoch 65/68
80/80 [==============================] - 0s 6ms/step - loss: 0.8841 - val_loss: 1.2920
Epoch 66/68
80/80 [==============================] - 0s 6ms/step - loss: 0.8841 - val_loss: 1.2919
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8840 - val_loss: 1.2918
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8839 - val_loss: 1.2917
Execution time:  29.166857481002808
LSTM:
Mean Absolute Error: 0.9237
Root Mean Square Error: 1.1187
Mean Square Error: 1.2514

Train RMSE: 1.119
Train MSE: 1.251
Train MAE: 0.924
###########################

MODEL:  LSTM
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_7&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_7 (LSTM)                (None, 6, 55)             12540     
_________________________________________________________________
dropout_7 (Dropout)          (None, 6, 55)             0         
_________________________________________________________________
time_distributed_7 (TimeDist (None, 6, 1)              56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 7ms/step - loss: 0.8733 - val_loss: 1.1280
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8730 - val_loss: 1.1276
Epoch 3/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8727 - val_loss: 1.1271
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8724 - val_loss: 1.1267
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8721 - val_loss: 1.1262
Epoch 6/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8718 - val_loss: 1.1258
Epoch 7/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8715 - val_loss: 1.1253
Epoch 8/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8712 - val_loss: 1.1248
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8709 - val_loss: 1.1243
Epoch 10/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8705 - val_loss: 1.1238
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8702 - val_loss: 1.1233
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8699 - val_loss: 1.1228
Epoch 13/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8696 - val_loss: 1.1222
Epoch 14/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8692 - val_loss: 1.1217
Epoch 15/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8689 - val_loss: 1.1212
Epoch 16/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8685 - val_loss: 1.1206
Epoch 17/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8682 - val_loss: 1.1201
Epoch 18/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8678 - val_loss: 1.1195
Epoch 19/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8674 - val_loss: 1.1190
Epoch 20/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8671 - val_loss: 1.1184
Epoch 21/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8667 - val_loss: 1.1179
Epoch 22/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8664 - val_loss: 1.1173
Epoch 23/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8661 - val_loss: 1.1167
Epoch 24/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8657 - val_loss: 1.1162
Epoch 25/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8653 - val_loss: 1.1156
Epoch 26/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8649 - val_loss: 1.1150
Epoch 27/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8646 - val_loss: 1.1144
Epoch 28/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8642 - val_loss: 1.1138
Epoch 29/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8638 - val_loss: 1.1133
Epoch 30/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8634 - val_loss: 1.1127
Epoch 31/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8631 - val_loss: 1.1121
Epoch 32/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8627 - val_loss: 1.1115
Epoch 33/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8623 - val_loss: 1.1109
Epoch 34/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8619 - val_loss: 1.1102
Epoch 35/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8616 - val_loss: 1.1096
Epoch 36/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8612 - val_loss: 1.1090
Execution time:  52.62877559661865
LSTM:
Mean Absolute Error: 0.9090
Root Mean Square Error: 1.1032
Mean Square Error: 1.2170

Train RMSE: 1.103
Train MSE: 1.217
Train MAE: 0.909
###########################

MODEL:  LSTM
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_8&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_8 (LSTM)                (None, 6, 40)             6720      
_________________________________________________________________
dropout_8 (Dropout)          (None, 6, 40)             0         
_________________________________________________________________
time_distributed_8 (TimeDist (None, 6, 1)              41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 11ms/step - loss: 0.6451 - val_loss: 0.6118
Epoch 2/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.3694
Epoch 3/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4148 - val_loss: 0.3375
Epoch 4/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3864 - val_loss: 0.3173
Epoch 5/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3664 - val_loss: 0.2920
Epoch 6/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3493 - val_loss: 0.2645
Epoch 7/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3356 - val_loss: 0.2392
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3236 - val_loss: 0.2228
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3162 - val_loss: 0.2104
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3105 - val_loss: 0.2025
Epoch 11/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3052 - val_loss: 0.1949
Epoch 12/68
80/80 [==============================] - 0s 4ms/step - loss: 0.3013 - val_loss: 0.1914
Epoch 13/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2980 - val_loss: 0.1857
Epoch 14/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2938 - val_loss: 0.1820
Epoch 15/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2912 - val_loss: 0.1771
Epoch 16/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2883 - val_loss: 0.1739
Epoch 17/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2862 - val_loss: 0.1724
Epoch 18/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2839 - val_loss: 0.1654
Epoch 19/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2823 - val_loss: 0.1638
Epoch 20/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2808 - val_loss: 0.1631
Epoch 21/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2793 - val_loss: 0.1598
Epoch 22/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2777 - val_loss: 0.1578
Epoch 23/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2772 - val_loss: 0.1560
Epoch 24/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2765 - val_loss: 0.1553
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2756 - val_loss: 0.1552
Epoch 26/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2754 - val_loss: 0.1553
Epoch 27/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2751 - val_loss: 0.1556
Epoch 28/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1532
Epoch 29/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2739 - val_loss: 0.1545
Epoch 30/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2743 - val_loss: 0.1517
Epoch 31/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2741 - val_loss: 0.1505
Epoch 32/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2733 - val_loss: 0.1515
Epoch 33/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2732 - val_loss: 0.1510
Epoch 34/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2726 - val_loss: 0.1504
Epoch 35/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2728 - val_loss: 0.1515
Epoch 36/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2727 - val_loss: 0.1488
Epoch 37/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2729 - val_loss: 0.1503
Epoch 38/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2721 - val_loss: 0.1484
Epoch 39/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2723 - val_loss: 0.1489
Epoch 40/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2726 - val_loss: 0.1486
Epoch 41/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2717 - val_loss: 0.1483
Epoch 42/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2718 - val_loss: 0.1477
Epoch 43/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2716 - val_loss: 0.1462
Epoch 44/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2718 - val_loss: 0.1462
Epoch 45/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2714 - val_loss: 0.1462
Epoch 46/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2711 - val_loss: 0.1463
Epoch 47/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2716 - val_loss: 0.1461
Epoch 48/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2715 - val_loss: 0.1460
Epoch 49/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2714 - val_loss: 0.1445
Epoch 50/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2712 - val_loss: 0.1433
Epoch 51/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2709 - val_loss: 0.1442
Epoch 52/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2708 - val_loss: 0.1438
Epoch 53/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2706 - val_loss: 0.1458
Epoch 54/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2708 - val_loss: 0.1422
Epoch 55/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2704 - val_loss: 0.1425
Epoch 56/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2705 - val_loss: 0.1417
Epoch 57/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 0.1419
Epoch 58/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 0.1416
Epoch 59/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2704 - val_loss: 0.1397
Epoch 60/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2703 - val_loss: 0.1416
Epoch 61/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2701 - val_loss: 0.1416
Epoch 62/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2700 - val_loss: 0.1398
Epoch 63/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2701 - val_loss: 0.1394
Epoch 64/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2703 - val_loss: 0.1391
Epoch 65/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.1371
Epoch 66/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2698 - val_loss: 0.1381
Epoch 67/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2698 - val_loss: 0.1382
Epoch 68/68
80/80 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.1380
Execution time:  27.472850561141968
LSTM:
Mean Absolute Error: 0.1616
Root Mean Square Error: 0.5849
Mean Square Error: 0.3421

Train RMSE: 0.585
Train MSE: 0.342
Train MAE: 0.162
###########################

MODEL:  LSTM
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_9&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_9 (LSTM)                (None, 6, 55)             12540     
_________________________________________________________________
dropout_9 (Dropout)          (None, 6, 55)             0         
_________________________________________________________________
time_distributed_9 (TimeDist (None, 6, 1)              56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4727 - val_loss: 0.3076
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.3310 - val_loss: 0.2575
Epoch 3/36
355/355 [==============================] - 1s 4ms/step - loss: 0.3028 - val_loss: 0.2365
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2874 - val_loss: 0.2255
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2788 - val_loss: 0.2181
Epoch 6/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2736 - val_loss: 0.2146
Epoch 7/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2703 - val_loss: 0.2120
Epoch 8/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2686 - val_loss: 0.2102
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2671 - val_loss: 0.2097
Epoch 10/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2664 - val_loss: 0.2089
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2662 - val_loss: 0.2084
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2660 - val_loss: 0.2078
Epoch 13/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2656 - val_loss: 0.2068
Epoch 14/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2654 - val_loss: 0.2064
Epoch 15/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2060
Epoch 16/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2055
Epoch 17/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2645 - val_loss: 0.2044
Epoch 18/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2642 - val_loss: 0.2044
Epoch 19/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2640 - val_loss: 0.2039
Epoch 20/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.2020
Epoch 21/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2636 - val_loss: 0.2019
Epoch 22/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2632 - val_loss: 0.2015
Epoch 23/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2629 - val_loss: 0.2006
Epoch 24/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2626 - val_loss: 0.1999
Epoch 25/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2624 - val_loss: 0.1998
Epoch 26/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2622 - val_loss: 0.1981
Epoch 27/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2617 - val_loss: 0.1977
Epoch 28/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2615 - val_loss: 0.1966
Epoch 29/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2612 - val_loss: 0.1952
Epoch 30/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2611 - val_loss: 0.1949
Epoch 31/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2607 - val_loss: 0.1946
Epoch 32/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2606 - val_loss: 0.1932
Epoch 33/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2606 - val_loss: 0.1932
Epoch 34/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1919
Epoch 35/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1912
Epoch 36/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.1907
Execution time:  51.26910877227783
LSTM:
Mean Absolute Error: 0.1536
Root Mean Square Error: 0.5828
Mean Square Error: 0.3397

Train RMSE: 0.583
Train MSE: 0.340
Train MAE: 0.154
###########################

MODEL:  LSTM
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_10&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_10 (LSTM)               (None, 6, 40)             6720      
_________________________________________________________________
dropout_10 (Dropout)         (None, 6, 40)             0         
_________________________________________________________________
time_distributed_10 (TimeDis (None, 6, 1)              41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 11ms/step - loss: 0.8701 - val_loss: 1.2355
Epoch 2/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8039 - val_loss: 1.0838
Epoch 3/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6868 - val_loss: 0.9458
Epoch 4/68
80/80 [==============================] - 0s 4ms/step - loss: 0.6250 - val_loss: 0.8996
Epoch 5/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6000 - val_loss: 0.8773
Epoch 6/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5847 - val_loss: 0.8631
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5742 - val_loss: 0.8530
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5663 - val_loss: 0.8451
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5601 - val_loss: 0.8389
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5552 - val_loss: 0.8336
Epoch 11/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5506 - val_loss: 0.8289
Epoch 12/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5471 - val_loss: 0.8250
Epoch 13/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5438 - val_loss: 0.8217
Epoch 14/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5410 - val_loss: 0.8187
Epoch 15/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5384 - val_loss: 0.8160
Epoch 16/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5361 - val_loss: 0.8135
Epoch 17/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5339 - val_loss: 0.8112
Epoch 18/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5318 - val_loss: 0.8091
Epoch 19/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5298 - val_loss: 0.8073
Epoch 20/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5279 - val_loss: 0.8056
Epoch 21/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5259 - val_loss: 0.8041
Epoch 22/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5234 - val_loss: 0.8028
Epoch 23/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5211 - val_loss: 0.8017
Epoch 24/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5186 - val_loss: 0.8008
Epoch 25/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5159 - val_loss: 0.7999
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5135 - val_loss: 0.7992
Epoch 27/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5121 - val_loss: 0.7984
Epoch 28/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5108 - val_loss: 0.7978
Epoch 29/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5098 - val_loss: 0.7973
Epoch 30/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5089 - val_loss: 0.7968
Epoch 31/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5079 - val_loss: 0.7963
Epoch 32/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5069 - val_loss: 0.7959
Epoch 33/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5061 - val_loss: 0.7956
Epoch 34/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5050 - val_loss: 0.7953
Epoch 35/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5038 - val_loss: 0.7950
Epoch 36/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5031 - val_loss: 0.7948
Epoch 37/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5021 - val_loss: 0.7946
Epoch 38/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5013 - val_loss: 0.7944
Epoch 39/68
80/80 [==============================] - 0s 4ms/step - loss: 0.5004 - val_loss: 0.7942
Epoch 40/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4996 - val_loss: 0.7941
Epoch 41/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4989 - val_loss: 0.7940
Epoch 42/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4982 - val_loss: 0.7938
Epoch 43/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4977 - val_loss: 0.7937
Epoch 44/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4974 - val_loss: 0.7936
Epoch 45/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4968 - val_loss: 0.7935
Epoch 46/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4966 - val_loss: 0.7935
Epoch 47/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4964 - val_loss: 0.7934
Epoch 48/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4962 - val_loss: 0.7933
Epoch 49/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4958 - val_loss: 0.7933
Epoch 50/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4957 - val_loss: 0.7932
Epoch 51/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4955 - val_loss: 0.7932
Epoch 52/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7932
Epoch 53/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4951 - val_loss: 0.7931
Epoch 54/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4952 - val_loss: 0.7931
Epoch 55/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4950 - val_loss: 0.7931
Epoch 56/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4949 - val_loss: 0.7930
Epoch 57/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4947 - val_loss: 0.7930
Epoch 58/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4948 - val_loss: 0.7930
Epoch 59/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4945 - val_loss: 0.7930
Epoch 60/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4945 - val_loss: 0.7929
Epoch 61/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4944 - val_loss: 0.7929
Epoch 62/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4945 - val_loss: 0.7929
Epoch 63/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4943 - val_loss: 0.7929
Epoch 64/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7929
Epoch 65/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7929
Epoch 66/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.7929
Epoch 67/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4941 - val_loss: 0.7929
Epoch 68/68
80/80 [==============================] - 0s 4ms/step - loss: 0.4940 - val_loss: 0.7928
Execution time:  28.038204193115234
LSTM:
Mean Absolute Error: 0.4847
Root Mean Square Error: 0.7526
Mean Square Error: 0.5665

Train RMSE: 0.753
Train MSE: 0.566
Train MAE: 0.485
###########################

MODEL:  LSTM
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_11&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_11 (LSTM)               (None, 6, 55)             12540     
_________________________________________________________________
dropout_11 (Dropout)         (None, 6, 55)             0         
_________________________________________________________________
time_distributed_11 (TimeDis (None, 6, 1)              56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 5ms/step - loss: 0.7271 - val_loss: 0.7620
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5800 - val_loss: 0.7183
Epoch 3/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5584 - val_loss: 0.7008
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5478 - val_loss: 0.6892
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5404 - val_loss: 0.6806
Epoch 6/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5346 - val_loss: 0.6744
Epoch 7/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5303 - val_loss: 0.6696
Epoch 8/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5265 - val_loss: 0.6660
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5237 - val_loss: 0.6634
Epoch 10/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5217 - val_loss: 0.6613
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5200 - val_loss: 0.6598
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5187 - val_loss: 0.6589
Epoch 13/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5177 - val_loss: 0.6579
Epoch 14/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5165 - val_loss: 0.6569
Epoch 15/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5147 - val_loss: 0.6561
Epoch 16/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5118 - val_loss: 0.6548
Epoch 17/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5080 - val_loss: 0.6539
Epoch 18/36
355/355 [==============================] - 1s 4ms/step - loss: 0.5035 - val_loss: 0.6527
Epoch 19/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4994 - val_loss: 0.6521
Epoch 20/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4967 - val_loss: 0.6520
Epoch 21/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4951 - val_loss: 0.6517
Epoch 22/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4940 - val_loss: 0.6513
Epoch 23/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4934 - val_loss: 0.6511
Epoch 24/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4930 - val_loss: 0.6509
Epoch 25/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4927 - val_loss: 0.6508
Epoch 26/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4924 - val_loss: 0.6507
Epoch 27/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4921 - val_loss: 0.6506
Epoch 28/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4921 - val_loss: 0.6505
Epoch 29/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4920 - val_loss: 0.6504
Epoch 30/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4918 - val_loss: 0.6504
Epoch 31/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.6503
Epoch 32/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4917 - val_loss: 0.6503
Epoch 33/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4914 - val_loss: 0.6502
Epoch 34/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4915 - val_loss: 0.6502
Epoch 35/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4913 - val_loss: 0.6502
Epoch 36/36
355/355 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.6502
Execution time:  52.554423809051514
LSTM:
Mean Absolute Error: 0.4827
Root Mean Square Error: 0.7519
Mean Square Error: 0.5653

Train RMSE: 0.752
Train MSE: 0.565
Train MAE: 0.483
###########################

MODEL:  LSTM
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_12&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_12 (LSTM)               (None, 18, 40)            6720      
_________________________________________________________________
dropout_12 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_12 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 19ms/step - loss: 0.5428 - val_loss: 0.3229
Epoch 2/68
80/80 [==============================] - 1s 8ms/step - loss: 0.4200 - val_loss: 0.2475
Epoch 3/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3916 - val_loss: 0.2248
Epoch 4/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3778 - val_loss: 0.2098
Epoch 5/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3680 - val_loss: 0.1974
Epoch 6/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3617 - val_loss: 0.1904
Epoch 7/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3571 - val_loss: 0.1847
Epoch 8/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3538 - val_loss: 0.1804
Epoch 9/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3509 - val_loss: 0.1786
Epoch 10/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3490 - val_loss: 0.1762
Epoch 11/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3468 - val_loss: 0.1723
Epoch 12/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3449 - val_loss: 0.1685
Epoch 13/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3437 - val_loss: 0.1688
Epoch 14/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3422 - val_loss: 0.1660
Epoch 15/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3415 - val_loss: 0.1640
Epoch 16/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3404 - val_loss: 0.1632
Epoch 17/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3395 - val_loss: 0.1623
Epoch 18/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3388 - val_loss: 0.1609
Epoch 19/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3381 - val_loss: 0.1598
Epoch 20/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3377 - val_loss: 0.1587
Epoch 21/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3371 - val_loss: 0.1576
Epoch 22/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3365 - val_loss: 0.1577
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.1583
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.1581
Epoch 25/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3352 - val_loss: 0.1572
Epoch 26/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3347 - val_loss: 0.1578
Epoch 27/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3342 - val_loss: 0.1564
Epoch 28/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3344 - val_loss: 0.1565
Epoch 29/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3339 - val_loss: 0.1558
Epoch 30/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3339 - val_loss: 0.1559
Epoch 31/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3332 - val_loss: 0.1558
Epoch 32/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3334 - val_loss: 0.1545
Epoch 33/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3330 - val_loss: 0.1555
Epoch 34/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3334 - val_loss: 0.1544
Epoch 35/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3325 - val_loss: 0.1540
Epoch 36/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3324 - val_loss: 0.1558
Epoch 37/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3326 - val_loss: 0.1557
Epoch 38/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3325 - val_loss: 0.1557
Epoch 39/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3319 - val_loss: 0.1551
Epoch 40/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3323 - val_loss: 0.1556
Epoch 41/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3321 - val_loss: 0.1557
Epoch 42/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3321 - val_loss: 0.1550
Epoch 43/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3322 - val_loss: 0.1551
Epoch 44/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3319 - val_loss: 0.1550
Epoch 45/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3314 - val_loss: 0.1567
Execution time:  34.46935534477234
LSTM:
Mean Absolute Error: 0.1715
Root Mean Square Error: 0.5867
Mean Square Error: 0.3442

Train RMSE: 0.587
Train MSE: 0.344
Train MAE: 0.171
###########################

MODEL:  LSTM
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_13&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_13 (LSTM)               (None, 18, 55)            12540     
_________________________________________________________________
dropout_13 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_13 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 3s 9ms/step - loss: 0.4433 - val_loss: 0.2909
Epoch 2/36
354/354 [==============================] - 3s 7ms/step - loss: 0.3632 - val_loss: 0.2717
Epoch 3/36
354/354 [==============================] - 3s 7ms/step - loss: 0.3469 - val_loss: 0.2630
Epoch 4/36
354/354 [==============================] - 3s 7ms/step - loss: 0.3401 - val_loss: 0.2583
Epoch 5/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3361 - val_loss: 0.2559
Epoch 6/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3335 - val_loss: 0.2545
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3318 - val_loss: 0.2551
Epoch 8/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3311 - val_loss: 0.2539
Epoch 9/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3306 - val_loss: 0.2543
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2540
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3300 - val_loss: 0.2534
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3294 - val_loss: 0.2542
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3296 - val_loss: 0.2541
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3295 - val_loss: 0.2539
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3291 - val_loss: 0.2539
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3289 - val_loss: 0.2542
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3287 - val_loss: 0.2526
Epoch 18/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3285 - val_loss: 0.2525
Epoch 19/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3283 - val_loss: 0.2521
Epoch 20/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3281 - val_loss: 0.2519
Epoch 21/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3280 - val_loss: 0.2523
Epoch 22/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3276 - val_loss: 0.2510
Epoch 23/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3274 - val_loss: 0.2494
Epoch 24/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3272 - val_loss: 0.2498
Epoch 25/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3271 - val_loss: 0.2485
Epoch 26/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3269 - val_loss: 0.2489
Epoch 27/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3266 - val_loss: 0.2480
Epoch 28/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3265 - val_loss: 0.2471
Epoch 29/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3264 - val_loss: 0.2454
Epoch 30/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3262 - val_loss: 0.2457
Epoch 31/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3259 - val_loss: 0.2440
Epoch 32/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3261 - val_loss: 0.2437
Epoch 33/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3260 - val_loss: 0.2434
Epoch 34/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3253 - val_loss: 0.2416
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3254 - val_loss: 0.2408
Epoch 36/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2404
Execution time:  103.10356855392456
LSTM:
Mean Absolute Error: 0.1657
Root Mean Square Error: 0.5856
Mean Square Error: 0.3429

Train RMSE: 0.586
Train MSE: 0.343
Train MAE: 0.166
###########################

MODEL:  LSTM
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_14&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_14 (LSTM)               (None, 18, 40)            6720      
_________________________________________________________________
dropout_14 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_14 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8012 - val_loss: 0.9284
Epoch 2/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6322 - val_loss: 0.8511
Epoch 3/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6136 - val_loss: 0.8369
Epoch 4/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6012 - val_loss: 0.8284
Epoch 5/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5904 - val_loss: 0.8228
Epoch 6/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5821 - val_loss: 0.8178
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5756 - val_loss: 0.8145
Epoch 8/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5690 - val_loss: 0.8117
Epoch 9/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5650 - val_loss: 0.8091
Epoch 10/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5594 - val_loss: 0.8071
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5547 - val_loss: 0.8050
Epoch 12/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5518 - val_loss: 0.8033
Epoch 13/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5491 - val_loss: 0.8019
Epoch 14/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5469 - val_loss: 0.8006
Epoch 15/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5456 - val_loss: 0.7996
Epoch 16/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5440 - val_loss: 0.7987
Epoch 17/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5430 - val_loss: 0.7979
Epoch 18/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5418 - val_loss: 0.7972
Epoch 19/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5407 - val_loss: 0.7966
Epoch 20/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5401 - val_loss: 0.7961
Epoch 21/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5391 - val_loss: 0.7956
Epoch 22/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5383 - val_loss: 0.7952
Epoch 23/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5377 - val_loss: 0.7949
Epoch 24/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5374 - val_loss: 0.7946
Epoch 25/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5368 - val_loss: 0.7943
Epoch 26/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5362 - val_loss: 0.7941
Epoch 27/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5358 - val_loss: 0.7939
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7937
Epoch 29/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5352 - val_loss: 0.7936
Epoch 30/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5349 - val_loss: 0.7935
Epoch 31/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5344 - val_loss: 0.7933
Epoch 32/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5343 - val_loss: 0.7932
Epoch 33/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5338 - val_loss: 0.7931
Epoch 34/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5338 - val_loss: 0.7931
Epoch 35/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5336 - val_loss: 0.7930
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7929
Epoch 37/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5332 - val_loss: 0.7929
Epoch 38/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5329 - val_loss: 0.7928
Epoch 39/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5329 - val_loss: 0.7928
Epoch 40/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5329 - val_loss: 0.7927
Epoch 41/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5327 - val_loss: 0.7927
Epoch 42/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5324 - val_loss: 0.7926
Epoch 43/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5325 - val_loss: 0.7926
Epoch 44/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5323 - val_loss: 0.7926
Epoch 45/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5323 - val_loss: 0.7926
Epoch 46/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5321 - val_loss: 0.7925
Epoch 47/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5322 - val_loss: 0.7925
Epoch 48/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5320 - val_loss: 0.7925
Epoch 49/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5319 - val_loss: 0.7925
Epoch 50/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5317 - val_loss: 0.7925
Epoch 51/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5317 - val_loss: 0.7925
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7924
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7924
Epoch 54/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7924
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7924
Epoch 56/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5312 - val_loss: 0.7924
Epoch 57/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5314 - val_loss: 0.7924
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5314 - val_loss: 0.7924
Epoch 59/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5312 - val_loss: 0.7924
Epoch 60/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5310 - val_loss: 0.7924
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5310 - val_loss: 0.7924
Epoch 62/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5311 - val_loss: 0.7924
Epoch 63/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5309 - val_loss: 0.7924
Epoch 64/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5311 - val_loss: 0.7923
Epoch 65/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5309 - val_loss: 0.7923
Epoch 66/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5308 - val_loss: 0.7923
Epoch 67/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5305 - val_loss: 0.7923
Epoch 68/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5307 - val_loss: 0.7923
Execution time:  50.67752957344055
LSTM:
Mean Absolute Error: 0.4956
Root Mean Square Error: 0.7571
Mean Square Error: 0.5732

Train RMSE: 0.757
Train MSE: 0.573
Train MAE: 0.496
###########################

MODEL:  LSTM
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_15&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_15 (LSTM)               (None, 18, 55)            12540     
_________________________________________________________________
dropout_15 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_15 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 4s 11ms/step - loss: 0.6584 - val_loss: 0.7140
Epoch 2/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5911 - val_loss: 0.6992
Epoch 3/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5688 - val_loss: 0.6932
Epoch 4/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5558 - val_loss: 0.6841
Epoch 5/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5472 - val_loss: 0.6802
Epoch 6/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5421 - val_loss: 0.6754
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5387 - val_loss: 0.6735
Epoch 8/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5366 - val_loss: 0.6718
Epoch 9/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5352 - val_loss: 0.6703
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5344 - val_loss: 0.6694
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5337 - val_loss: 0.6686
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5334 - val_loss: 0.6683
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5329 - val_loss: 0.6678
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5326 - val_loss: 0.6674
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5323 - val_loss: 0.6670
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5323 - val_loss: 0.6667
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5318 - val_loss: 0.6668
Epoch 18/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5316 - val_loss: 0.6665
Epoch 19/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5314 - val_loss: 0.6667
Epoch 20/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5312 - val_loss: 0.6665
Epoch 21/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5310 - val_loss: 0.6663
Epoch 22/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5311 - val_loss: 0.6659
Epoch 23/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5310 - val_loss: 0.6662
Epoch 24/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5306 - val_loss: 0.6662
Epoch 25/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5305 - val_loss: 0.6661
Epoch 26/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5303 - val_loss: 0.6657
Epoch 27/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5302 - val_loss: 0.6659
Epoch 28/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5301 - val_loss: 0.6655
Epoch 29/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5300 - val_loss: 0.6660
Epoch 30/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5298 - val_loss: 0.6658
Epoch 31/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5298 - val_loss: 0.6659
Epoch 32/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5296 - val_loss: 0.6660
Epoch 33/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5294 - val_loss: 0.6658
Epoch 34/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5293 - val_loss: 0.6661
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5294 - val_loss: 0.6656
Epoch 36/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5292 - val_loss: 0.6660
Execution time:  104.57192826271057
LSTM:
Mean Absolute Error: 0.4969
Root Mean Square Error: 0.7660
Mean Square Error: 0.5868

Train RMSE: 0.766
Train MSE: 0.587
Train MAE: 0.497
###########################

MODEL:  LSTM
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_16&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_16 (LSTM)               (None, 18, 40)            6720      
_________________________________________________________________
dropout_16 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_16 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6935 - val_loss: 0.7820
Epoch 2/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6931 - val_loss: 0.7815
Epoch 3/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6929 - val_loss: 0.7810
Epoch 4/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6925 - val_loss: 0.7804
Epoch 5/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6921 - val_loss: 0.7799
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6918 - val_loss: 0.7793
Epoch 7/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6915 - val_loss: 0.7788
Epoch 8/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6911 - val_loss: 0.7782
Epoch 9/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6907 - val_loss: 0.7777
Epoch 10/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6903 - val_loss: 0.7771
Epoch 11/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6901 - val_loss: 0.7765
Epoch 12/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6897 - val_loss: 0.7759
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6893 - val_loss: 0.7753
Epoch 14/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6890 - val_loss: 0.7747
Epoch 15/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6886 - val_loss: 0.7741
Epoch 16/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6883 - val_loss: 0.7735
Epoch 17/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6878 - val_loss: 0.7729
Epoch 18/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6873 - val_loss: 0.7723
Epoch 19/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6869 - val_loss: 0.7717
Epoch 20/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6867 - val_loss: 0.7711
Epoch 21/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6862 - val_loss: 0.7705
Epoch 22/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6859 - val_loss: 0.7699
Epoch 23/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6856 - val_loss: 0.7693
Epoch 24/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6852 - val_loss: 0.7687
Epoch 25/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6847 - val_loss: 0.7680
Epoch 26/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6844 - val_loss: 0.7674
Epoch 27/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6841 - val_loss: 0.7668
Epoch 28/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6835 - val_loss: 0.7662
Epoch 29/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6833 - val_loss: 0.7655
Epoch 30/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6828 - val_loss: 0.7649
Epoch 31/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6825 - val_loss: 0.7643
Epoch 32/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6822 - val_loss: 0.7636
Epoch 33/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6819 - val_loss: 0.7630
Epoch 34/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6814 - val_loss: 0.7623
Epoch 35/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6809 - val_loss: 0.7617
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6805 - val_loss: 0.7610
Epoch 37/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6802 - val_loss: 0.7604
Epoch 38/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6796 - val_loss: 0.7597
Epoch 39/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6793 - val_loss: 0.7591
Epoch 40/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6789 - val_loss: 0.7584
Epoch 41/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6786 - val_loss: 0.7578
Epoch 42/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6780 - val_loss: 0.7571
Epoch 43/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6775 - val_loss: 0.7565
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6772 - val_loss: 0.7558
Epoch 45/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6769 - val_loss: 0.7551
Epoch 46/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6765 - val_loss: 0.7545
Epoch 47/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6760 - val_loss: 0.7538
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6757 - val_loss: 0.7531
Epoch 49/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6753 - val_loss: 0.7525
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6747 - val_loss: 0.7518
Epoch 51/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6744 - val_loss: 0.7511
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6741 - val_loss: 0.7504
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6737 - val_loss: 0.7497
Epoch 54/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6732 - val_loss: 0.7491
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6729 - val_loss: 0.7484
Epoch 56/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6724 - val_loss: 0.7477
Epoch 57/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6720 - val_loss: 0.7470
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6715 - val_loss: 0.7463
Epoch 59/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6710 - val_loss: 0.7456
Epoch 60/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6706 - val_loss: 0.7449
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6704 - val_loss: 0.7442
Epoch 62/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6698 - val_loss: 0.7435
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6695 - val_loss: 0.7428
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6689 - val_loss: 0.7421
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6687 - val_loss: 0.7414
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6681 - val_loss: 0.7407
Epoch 67/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6678 - val_loss: 0.7400
Epoch 68/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6673 - val_loss: 0.7392
Execution time:  50.471102476119995
LSTM:
Mean Absolute Error: 0.6701
Root Mean Square Error: 0.9662
Mean Square Error: 0.9336

Train RMSE: 0.966
Train MSE: 0.934
Train MAE: 0.670
###########################

MODEL:  LSTM
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_17&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_17 (LSTM)               (None, 18, 55)            12540     
_________________________________________________________________
dropout_17 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_17 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6988 - val_loss: 0.6786
Epoch 2/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6967 - val_loss: 0.6762
Epoch 3/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6947 - val_loss: 0.6737
Epoch 4/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6927 - val_loss: 0.6713
Epoch 5/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6907 - val_loss: 0.6687
Epoch 6/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6887 - val_loss: 0.6662
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6868 - val_loss: 0.6636
Epoch 8/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6848 - val_loss: 0.6610
Epoch 9/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6828 - val_loss: 0.6583
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6807 - val_loss: 0.6557
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6785 - val_loss: 0.6529
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6765 - val_loss: 0.6502
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6744 - val_loss: 0.6474
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6722 - val_loss: 0.6445
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6701 - val_loss: 0.6416
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6679 - val_loss: 0.6387
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6656 - val_loss: 0.6358
Epoch 18/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6634 - val_loss: 0.6328
Epoch 19/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6612 - val_loss: 0.6297
Epoch 20/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6588 - val_loss: 0.6267
Epoch 21/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6565 - val_loss: 0.6236
Epoch 22/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6541 - val_loss: 0.6204
Epoch 23/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6517 - val_loss: 0.6172
Epoch 24/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6493 - val_loss: 0.6140
Epoch 25/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6468 - val_loss: 0.6107
Epoch 26/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6443 - val_loss: 0.6074
Epoch 27/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6418 - val_loss: 0.6040
Epoch 28/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6391 - val_loss: 0.6005
Epoch 29/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6364 - val_loss: 0.5970
Epoch 30/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6337 - val_loss: 0.5934
Epoch 31/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6309 - val_loss: 0.5897
Epoch 32/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6280 - val_loss: 0.5860
Epoch 33/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6251 - val_loss: 0.5821
Epoch 34/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6221 - val_loss: 0.5782
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6189 - val_loss: 0.5742
Epoch 36/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6158 - val_loss: 0.5701
Execution time:  103.2042829990387
LSTM:
Mean Absolute Error: 0.5930
Root Mean Square Error: 0.8943
Mean Square Error: 0.7998

Train RMSE: 0.894
Train MSE: 0.800
Train MAE: 0.593
###########################

MODEL:  LSTM
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_18&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_18 (LSTM)               (None, 18, 40)            6720      
_________________________________________________________________
dropout_18 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_18 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8942 - val_loss: 1.3055
Epoch 2/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8941 - val_loss: 1.3053
Epoch 3/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8940 - val_loss: 1.3052
Epoch 4/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8939 - val_loss: 1.3050
Epoch 5/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8938 - val_loss: 1.3048
Epoch 6/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8937 - val_loss: 1.3046
Epoch 7/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8936 - val_loss: 1.3044
Epoch 8/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8935 - val_loss: 1.3042
Epoch 9/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8934 - val_loss: 1.3040
Epoch 10/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8933 - val_loss: 1.3038
Epoch 11/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8932 - val_loss: 1.3036
Epoch 12/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8931 - val_loss: 1.3034
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8929 - val_loss: 1.3032
Epoch 14/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8929 - val_loss: 1.3030
Epoch 15/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8927 - val_loss: 1.3028
Epoch 16/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8926 - val_loss: 1.3026
Epoch 17/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8925 - val_loss: 1.3024
Epoch 18/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8924 - val_loss: 1.3022
Epoch 19/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8923 - val_loss: 1.3020
Epoch 20/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8922 - val_loss: 1.3018
Epoch 21/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8921 - val_loss: 1.3016
Epoch 22/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8919 - val_loss: 1.3014
Epoch 23/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8918 - val_loss: 1.3012
Epoch 24/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8918 - val_loss: 1.3010
Epoch 25/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8916 - val_loss: 1.3007
Epoch 26/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8915 - val_loss: 1.3005
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8914 - val_loss: 1.3003
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8913 - val_loss: 1.3001
Epoch 29/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8911 - val_loss: 1.2999
Epoch 30/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8910 - val_loss: 1.2997
Epoch 31/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8909 - val_loss: 1.2995
Epoch 32/68
80/80 [==============================] - ETA: 0s - loss: 0.894 - 1s 8ms/step - loss: 0.8908 - val_loss: 1.2993
Epoch 33/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8907 - val_loss: 1.2991
Epoch 34/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8906 - val_loss: 1.2989
Epoch 35/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8905 - val_loss: 1.2987
Epoch 36/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8904 - val_loss: 1.2985
Epoch 37/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8903 - val_loss: 1.2983
Epoch 38/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8902 - val_loss: 1.2980
Epoch 39/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8900 - val_loss: 1.2978
Epoch 40/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8899 - val_loss: 1.2976
Epoch 41/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8898 - val_loss: 1.2974
Epoch 42/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8897 - val_loss: 1.2972
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8896 - val_loss: 1.2970
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8895 - val_loss: 1.2968
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8894 - val_loss: 1.2966
Epoch 46/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8892 - val_loss: 1.2964
Epoch 47/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8891 - val_loss: 1.2962
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8890 - val_loss: 1.2960
Epoch 49/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8889 - val_loss: 1.2957
Epoch 50/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8888 - val_loss: 1.2955
Epoch 51/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8887 - val_loss: 1.2953
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8886 - val_loss: 1.2951
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8884 - val_loss: 1.2949
Epoch 54/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8883 - val_loss: 1.2947
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8882 - val_loss: 1.2945
Epoch 56/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8881 - val_loss: 1.2943
Epoch 57/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8880 - val_loss: 1.2941
Epoch 58/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8879 - val_loss: 1.2939
Epoch 59/68
80/80 [==============================] - 1s 8ms/step - loss: 0.8878 - val_loss: 1.2936
Epoch 60/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8877 - val_loss: 1.2934
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8875 - val_loss: 1.2932
Epoch 62/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8874 - val_loss: 1.2930
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8873 - val_loss: 1.2928
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8872 - val_loss: 1.2926
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8870 - val_loss: 1.2924
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8869 - val_loss: 1.2922
Epoch 67/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8868 - val_loss: 1.2919
Epoch 68/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8867 - val_loss: 1.2917
Execution time:  51.19877076148987
LSTM:
Mean Absolute Error: 0.9248
Root Mean Square Error: 1.1194
Mean Square Error: 1.2530

Train RMSE: 1.119
Train MSE: 1.253
Train MAE: 0.925
###########################

MODEL:  LSTM
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_19&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_19 (LSTM)               (None, 18, 55)            12540     
_________________________________________________________________
dropout_19 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_19 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8759 - val_loss: 1.1276
Epoch 2/36
354/354 [==============================] - 3s 7ms/step - loss: 0.8755 - val_loss: 1.1270
Epoch 3/36
354/354 [==============================] - 3s 7ms/step - loss: 0.8750 - val_loss: 1.1264
Epoch 4/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8746 - val_loss: 1.1257
Epoch 5/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8741 - val_loss: 1.1250
Epoch 6/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8736 - val_loss: 1.1242
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8732 - val_loss: 1.1235
Epoch 8/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8727 - val_loss: 1.1227
Epoch 9/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8722 - val_loss: 1.1220
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8717 - val_loss: 1.1212
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8712 - val_loss: 1.1204
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8707 - val_loss: 1.1196
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8701 - val_loss: 1.1187
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8696 - val_loss: 1.1179
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8691 - val_loss: 1.1171
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8685 - val_loss: 1.1162
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8680 - val_loss: 1.1153
Epoch 18/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8674 - val_loss: 1.1145
Epoch 19/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8668 - val_loss: 1.1136
Epoch 20/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8663 - val_loss: 1.1127
Epoch 21/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8657 - val_loss: 1.1118
Epoch 22/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8651 - val_loss: 1.1108
Epoch 23/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8645 - val_loss: 1.1099
Epoch 24/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8639 - val_loss: 1.1089
Epoch 25/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8633 - val_loss: 1.1080
Epoch 26/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8627 - val_loss: 1.1070
Epoch 27/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8621 - val_loss: 1.1060
Epoch 28/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8614 - val_loss: 1.1050
Epoch 29/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8608 - val_loss: 1.1040
Epoch 30/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8601 - val_loss: 1.1030
Epoch 31/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8595 - val_loss: 1.1019
Epoch 32/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8588 - val_loss: 1.1008
Epoch 33/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8582 - val_loss: 1.0998
Epoch 34/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8574 - val_loss: 1.0987
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8567 - val_loss: 1.0975
Epoch 36/36
354/354 [==============================] - 3s 8ms/step - loss: 0.8560 - val_loss: 1.0964
Execution time:  102.7319438457489
LSTM:
Mean Absolute Error: 0.8990
Root Mean Square Error: 1.0935
Mean Square Error: 1.1957

Train RMSE: 1.093
Train MSE: 1.196
Train MAE: 0.899
###########################

MODEL:  LSTM
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_20&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_20 (LSTM)               (None, 18, 40)            6720      
_________________________________________________________________
dropout_20 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_20 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5782 - val_loss: 0.3338
Epoch 2/68
80/80 [==============================] - 1s 9ms/step - loss: 0.4444 - val_loss: 0.2973
Epoch 3/68
80/80 [==============================] - 1s 9ms/step - loss: 0.4154 - val_loss: 0.2611
Epoch 4/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3995 - val_loss: 0.2405
Epoch 5/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3904 - val_loss: 0.2291
Epoch 6/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3832 - val_loss: 0.2204
Epoch 7/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3778 - val_loss: 0.2129
Epoch 8/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3727 - val_loss: 0.2076
Epoch 9/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3684 - val_loss: 0.2021
Epoch 10/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3649 - val_loss: 0.1980
Epoch 11/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3615 - val_loss: 0.1936
Epoch 12/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3585 - val_loss: 0.1909
Epoch 13/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3565 - val_loss: 0.1880
Epoch 14/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3541 - val_loss: 0.1861
Epoch 15/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3528 - val_loss: 0.1838
Epoch 16/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3511 - val_loss: 0.1826
Epoch 17/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3499 - val_loss: 0.1807
Epoch 18/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3488 - val_loss: 0.1790
Epoch 19/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3478 - val_loss: 0.1775
Epoch 20/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3466 - val_loss: 0.1767
Epoch 21/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3458 - val_loss: 0.1755
Epoch 22/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3449 - val_loss: 0.1747
Epoch 23/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3440 - val_loss: 0.1742
Epoch 24/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3438 - val_loss: 0.1733
Epoch 25/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3428 - val_loss: 0.1713
Epoch 26/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3422 - val_loss: 0.1712
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3416 - val_loss: 0.1711
Epoch 28/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3411 - val_loss: 0.1700
Epoch 29/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3405 - val_loss: 0.1691
Epoch 30/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3400 - val_loss: 0.1693
Epoch 31/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3393 - val_loss: 0.1681
Epoch 32/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3393 - val_loss: 0.1673
Epoch 33/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3387 - val_loss: 0.1680
Epoch 34/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3387 - val_loss: 0.1664
Epoch 35/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3380 - val_loss: 0.1651
Epoch 36/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3375 - val_loss: 0.1659
Epoch 37/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3373 - val_loss: 0.1645
Epoch 38/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3373 - val_loss: 0.1643
Epoch 39/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3364 - val_loss: 0.1639
Epoch 40/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3367 - val_loss: 0.1634
Epoch 41/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3365 - val_loss: 0.1637
Epoch 42/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3362 - val_loss: 0.1624
Epoch 43/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3360 - val_loss: 0.1618
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.1617
Epoch 45/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3353 - val_loss: 0.1617
Epoch 46/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3351 - val_loss: 0.1608
Epoch 47/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3350 - val_loss: 0.1599
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3347 - val_loss: 0.1603
Epoch 49/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3347 - val_loss: 0.1596
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3345 - val_loss: 0.1589
Epoch 51/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3343 - val_loss: 0.1597
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3340 - val_loss: 0.1590
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3340 - val_loss: 0.1585
Epoch 54/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3339 - val_loss: 0.1579
Epoch 55/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3336 - val_loss: 0.1580
Epoch 56/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3335 - val_loss: 0.1571
Epoch 57/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3331 - val_loss: 0.1567
Epoch 58/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3336 - val_loss: 0.1561
Epoch 59/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3332 - val_loss: 0.1564
Epoch 60/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3329 - val_loss: 0.1559
Epoch 61/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3328 - val_loss: 0.1564
Epoch 62/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3327 - val_loss: 0.1557
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3326 - val_loss: 0.1555
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3323 - val_loss: 0.1554
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3321 - val_loss: 0.1550
Epoch 66/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3324 - val_loss: 0.1544
Epoch 67/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3320 - val_loss: 0.1547
Epoch 68/68
80/80 [==============================] - 1s 8ms/step - loss: 0.3322 - val_loss: 0.1540
Execution time:  50.554858684539795
LSTM:
Mean Absolute Error: 0.1753
Root Mean Square Error: 0.5879
Mean Square Error: 0.3456

Train RMSE: 0.588
Train MSE: 0.346
Train MAE: 0.175
###########################

MODEL:  LSTM
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_21&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_21 (LSTM)               (None, 18, 55)            12540     
_________________________________________________________________
dropout_21 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_21 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 3s 9ms/step - loss: 0.4806 - val_loss: 0.3197
Epoch 2/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3831 - val_loss: 0.2917
Epoch 3/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3632 - val_loss: 0.2785
Epoch 4/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3533 - val_loss: 0.2720
Epoch 5/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3472 - val_loss: 0.2672
Epoch 6/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3431 - val_loss: 0.2638
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3404 - val_loss: 0.2614
Epoch 8/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3381 - val_loss: 0.2596
Epoch 9/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3365 - val_loss: 0.2580
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3349 - val_loss: 0.2571
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3337 - val_loss: 0.2565
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3327 - val_loss: 0.2558
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3320 - val_loss: 0.2555
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3313 - val_loss: 0.2549
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3309 - val_loss: 0.2544
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2540
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3296 - val_loss: 0.2537
Epoch 18/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3292 - val_loss: 0.2532
Epoch 19/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3289 - val_loss: 0.2528
Epoch 20/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3284 - val_loss: 0.2527
Epoch 21/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3279 - val_loss: 0.2520
Epoch 22/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3276 - val_loss: 0.2515
Epoch 23/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3271 - val_loss: 0.2512
Epoch 24/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3268 - val_loss: 0.2505
Epoch 25/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3266 - val_loss: 0.2502
Epoch 26/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3263 - val_loss: 0.2500
Epoch 27/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3260 - val_loss: 0.2496
Epoch 28/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3260 - val_loss: 0.2492
Epoch 29/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3257 - val_loss: 0.2486
Epoch 30/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3255 - val_loss: 0.2486
Epoch 31/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3255 - val_loss: 0.2482
Epoch 32/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3255 - val_loss: 0.2482
Epoch 33/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2480
Epoch 34/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2472
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3251 - val_loss: 0.2471
Epoch 36/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3250 - val_loss: 0.2470
Execution time:  105.72694277763367
LSTM:
Mean Absolute Error: 0.1691
Root Mean Square Error: 0.5855
Mean Square Error: 0.3428

Train RMSE: 0.586
Train MSE: 0.343
Train MAE: 0.169
###########################

MODEL:  LSTM
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_22&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_22 (LSTM)               (None, 18, 40)            6720      
_________________________________________________________________
dropout_22 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_22 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 20ms/step - loss: 0.8514 - val_loss: 1.1396
Epoch 2/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7094 - val_loss: 0.9193
Epoch 3/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6436 - val_loss: 0.8747
Epoch 4/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6275 - val_loss: 0.8582
Epoch 5/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6188 - val_loss: 0.8487
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6130 - val_loss: 0.8421
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6080 - val_loss: 0.8376
Epoch 8/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6042 - val_loss: 0.8342
Epoch 9/68
80/80 [==============================] - 1s 8ms/step - loss: 0.6009 - val_loss: 0.8314
Epoch 10/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5980 - val_loss: 0.8288
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5949 - val_loss: 0.8267
Epoch 12/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5918 - val_loss: 0.8246
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5882 - val_loss: 0.8226
Epoch 14/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5849 - val_loss: 0.8209
Epoch 15/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5816 - val_loss: 0.8192
Epoch 16/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5791 - val_loss: 0.8176
Epoch 17/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5767 - val_loss: 0.8161
Epoch 18/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5748 - val_loss: 0.8147
Epoch 19/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5730 - val_loss: 0.8133
Epoch 20/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5715 - val_loss: 0.8121
Epoch 21/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5696 - val_loss: 0.8109
Epoch 22/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5684 - val_loss: 0.8098
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5667 - val_loss: 0.8088
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5649 - val_loss: 0.8079
Epoch 25/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5630 - val_loss: 0.8069
Epoch 26/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5610 - val_loss: 0.8060
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5588 - val_loss: 0.8052
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5561 - val_loss: 0.8043
Epoch 29/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5536 - val_loss: 0.8035
Epoch 30/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5520 - val_loss: 0.8027
Epoch 31/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5504 - val_loss: 0.8019
Epoch 32/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5493 - val_loss: 0.8012
Epoch 33/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5482 - val_loss: 0.8006
Epoch 34/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5472 - val_loss: 0.7999
Epoch 35/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5462 - val_loss: 0.7994
Epoch 36/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5453 - val_loss: 0.7988
Epoch 37/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5445 - val_loss: 0.7983
Epoch 38/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5438 - val_loss: 0.7979
Epoch 39/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5430 - val_loss: 0.7975
Epoch 40/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5423 - val_loss: 0.7971
Epoch 41/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5416 - val_loss: 0.7967
Epoch 42/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5411 - val_loss: 0.7964
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5407 - val_loss: 0.7961
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5402 - val_loss: 0.7958
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5398 - val_loss: 0.7956
Epoch 46/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5392 - val_loss: 0.7954
Epoch 47/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5389 - val_loss: 0.7952
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5383 - val_loss: 0.7950
Epoch 49/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5381 - val_loss: 0.7948
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5376 - val_loss: 0.7946
Epoch 51/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5374 - val_loss: 0.7944
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5371 - val_loss: 0.7943
Epoch 53/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5368 - val_loss: 0.7942
Epoch 54/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5366 - val_loss: 0.7941
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5362 - val_loss: 0.7939
Epoch 56/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5361 - val_loss: 0.7938
Epoch 57/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5356 - val_loss: 0.7937
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5354 - val_loss: 0.7937
Epoch 59/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7936
Epoch 60/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5351 - val_loss: 0.7935
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5351 - val_loss: 0.7934
Epoch 62/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5347 - val_loss: 0.7934
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7933
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7932
Epoch 65/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5344 - val_loss: 0.7932
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5342 - val_loss: 0.7931
Epoch 67/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5340 - val_loss: 0.7931
Epoch 68/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5340 - val_loss: 0.7931
Execution time:  51.192992210388184
LSTM:
Mean Absolute Error: 0.4960
Root Mean Square Error: 0.7579
Mean Square Error: 0.5745

Train RMSE: 0.758
Train MSE: 0.574
Train MAE: 0.496
###########################

MODEL:  LSTM
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_23&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_23 (LSTM)               (None, 18, 55)            12540     
_________________________________________________________________
dropout_23 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_23 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 3s 9ms/step - loss: 0.7157 - val_loss: 0.7534
Epoch 2/36
354/354 [==============================] - 3s 7ms/step - loss: 0.6180 - val_loss: 0.7329
Epoch 3/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6036 - val_loss: 0.7206
Epoch 4/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5895 - val_loss: 0.7102
Epoch 5/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5811 - val_loss: 0.7042
Epoch 6/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5754 - val_loss: 0.6996
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5706 - val_loss: 0.6955
Epoch 8/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5664 - val_loss: 0.6920
Epoch 9/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5626 - val_loss: 0.6891
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5592 - val_loss: 0.6864
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5556 - val_loss: 0.6841
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5524 - val_loss: 0.6820
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5493 - val_loss: 0.6802
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5468 - val_loss: 0.6787
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5451 - val_loss: 0.6773
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5436 - val_loss: 0.6760
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5420 - val_loss: 0.6751
Epoch 18/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5407 - val_loss: 0.6744
Epoch 19/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5396 - val_loss: 0.6734
Epoch 20/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5387 - val_loss: 0.6730
Epoch 21/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5380 - val_loss: 0.6724
Epoch 22/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5374 - val_loss: 0.6718
Epoch 23/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5369 - val_loss: 0.6710
Epoch 24/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5363 - val_loss: 0.6707
Epoch 25/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5358 - val_loss: 0.6702
Epoch 26/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5353 - val_loss: 0.6697
Epoch 27/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5349 - val_loss: 0.6690
Epoch 28/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5346 - val_loss: 0.6686
Epoch 29/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5342 - val_loss: 0.6683
Epoch 30/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5339 - val_loss: 0.6684
Epoch 31/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5334 - val_loss: 0.6682
Epoch 32/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5333 - val_loss: 0.6676
Epoch 33/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5331 - val_loss: 0.6676
Epoch 34/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5327 - val_loss: 0.6676
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5326 - val_loss: 0.6673
Epoch 36/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5324 - val_loss: 0.6674
Execution time:  102.66195869445801
LSTM:
Mean Absolute Error: 0.4942
Root Mean Square Error: 0.7569
Mean Square Error: 0.5729

Train RMSE: 0.757
Train MSE: 0.573
Train MAE: 0.494
###########################

MODEL:  LSTM
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_24&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_24 (LSTM)               (None, 36, 40)            6720      
_________________________________________________________________
dropout_24 (Dropout)         (None, 36, 40)            0         
_________________________________________________________________
time_distributed_24 (TimeDis (None, 36, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.5578 - val_loss: 0.2927
Epoch 2/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4657 - val_loss: 0.2469
Epoch 3/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4475 - val_loss: 0.2243
Epoch 4/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4379 - val_loss: 0.2104
Epoch 5/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4307 - val_loss: 0.2036
Epoch 6/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4255 - val_loss: 0.1971
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4220 - val_loss: 0.1915
Epoch 8/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4194 - val_loss: 0.1899
Epoch 9/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4170 - val_loss: 0.1875
Epoch 10/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4154 - val_loss: 0.1867
Epoch 11/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4138 - val_loss: 0.1843
Epoch 12/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4126 - val_loss: 0.1837
Epoch 13/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4114 - val_loss: 0.1828
Epoch 14/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4106 - val_loss: 0.1816
Epoch 15/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4097 - val_loss: 0.1820
Epoch 16/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4090 - val_loss: 0.1805
Epoch 17/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4085 - val_loss: 0.1800
Epoch 18/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4080 - val_loss: 0.1797
Epoch 19/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4072 - val_loss: 0.1788
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4070 - val_loss: 0.1792
Epoch 21/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4062 - val_loss: 0.1792
Epoch 22/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4058 - val_loss: 0.1780
Epoch 23/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4052 - val_loss: 0.1781
Epoch 24/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4047 - val_loss: 0.1781
Epoch 25/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4045 - val_loss: 0.1781
Epoch 26/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4041 - val_loss: 0.1775
Epoch 27/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4037 - val_loss: 0.1778
Epoch 28/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4034 - val_loss: 0.1774
Epoch 29/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4030 - val_loss: 0.1770
Epoch 30/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4030 - val_loss: 0.1771
Epoch 31/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4028 - val_loss: 0.1771
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4026 - val_loss: 0.1768
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4023 - val_loss: 0.1780
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4021 - val_loss: 0.1770
Epoch 35/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4016 - val_loss: 0.1776
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4015 - val_loss: 0.1773
Epoch 37/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4013 - val_loss: 0.1776
Epoch 38/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4012 - val_loss: 0.1774
Epoch 39/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4007 - val_loss: 0.1771
Epoch 40/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4007 - val_loss: 0.1774
Epoch 41/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4005 - val_loss: 0.1767
Epoch 42/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4009 - val_loss: 0.1778
Epoch 43/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4006 - val_loss: 0.1777
Epoch 44/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4000 - val_loss: 0.1777
Epoch 45/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4001 - val_loss: 0.1769
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4002 - val_loss: 0.1785
Epoch 47/68
80/80 [==============================] - 1s 15ms/step - loss: 0.3998 - val_loss: 0.1775
Epoch 48/68
80/80 [==============================] - 1s 15ms/step - loss: 0.3998 - val_loss: 0.1777
Epoch 49/68
80/80 [==============================] - 1s 15ms/step - loss: 0.3996 - val_loss: 0.1777
Epoch 50/68
80/80 [==============================] - 1s 16ms/step - loss: 0.3991 - val_loss: 0.1781
Epoch 51/68
80/80 [==============================] - 1s 15ms/step - loss: 0.3991 - val_loss: 0.1782
Execution time:  67.7232837677002
LSTM:
Mean Absolute Error: 0.1886
Root Mean Square Error: 0.6016
Mean Square Error: 0.3620

Train RMSE: 0.602
Train MSE: 0.362
Train MAE: 0.189
###########################

MODEL:  LSTM
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_25&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_25 (LSTM)               (None, 36, 55)            12540     
_________________________________________________________________
dropout_25 (Dropout)         (None, 36, 55)            0         
_________________________________________________________________
time_distributed_25 (TimeDis (None, 36, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4842 - val_loss: 0.3335
Epoch 2/36
352/352 [==============================] - 4s 13ms/step - loss: 0.4217 - val_loss: 0.3220
Epoch 3/36
352/352 [==============================] - 4s 13ms/step - loss: 0.4111 - val_loss: 0.3180
Epoch 4/36
352/352 [==============================] - 4s 13ms/step - loss: 0.4062 - val_loss: 0.3151
Epoch 5/36
352/352 [==============================] - 5s 14ms/step - loss: 0.4031 - val_loss: 0.3124
Epoch 6/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4007 - val_loss: 0.3117
Epoch 7/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3989 - val_loss: 0.3120
Epoch 8/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3978 - val_loss: 0.3113
Epoch 9/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3975 - val_loss: 0.3117
Epoch 10/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3968 - val_loss: 0.3123
Epoch 11/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3965 - val_loss: 0.3128
Epoch 12/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3961 - val_loss: 0.3127
Epoch 13/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3958 - val_loss: 0.3121
Epoch 14/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3962 - val_loss: 0.3116
Epoch 15/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3960 - val_loss: 0.3125
Epoch 16/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3954 - val_loss: 0.3122
Epoch 17/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3954 - val_loss: 0.3123
Epoch 18/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3948 - val_loss: 0.3131
Execution time:  87.66285276412964
LSTM:
Mean Absolute Error: 0.1884
Root Mean Square Error: 0.6122
Mean Square Error: 0.3748

Train RMSE: 0.612
Train MSE: 0.375
Train MAE: 0.188
###########################

MODEL:  LSTM
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_26&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_26 (LSTM)               (None, 36, 40)            6720      
_________________________________________________________________
dropout_26 (Dropout)         (None, 36, 40)            0         
_________________________________________________________________
time_distributed_26 (TimeDis (None, 36, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.8001 - val_loss: 0.9025
Epoch 2/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6505 - val_loss: 0.8378
Epoch 3/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6324 - val_loss: 0.8215
Epoch 4/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6271 - val_loss: 0.8158
Epoch 5/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6221 - val_loss: 0.8119
Epoch 6/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6197 - val_loss: 0.8094
Epoch 7/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6173 - val_loss: 0.8076
Epoch 8/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6111 - val_loss: 0.8064
Epoch 9/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6090 - val_loss: 0.8043
Epoch 10/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6078 - val_loss: 0.8032
Epoch 11/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6032 - val_loss: 0.8026
Epoch 12/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6010 - val_loss: 0.8011
Epoch 13/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5942 - val_loss: 0.8004
Epoch 14/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5904 - val_loss: 0.7996
Epoch 15/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5878 - val_loss: 0.7989
Epoch 16/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5865 - val_loss: 0.7983
Epoch 17/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5852 - val_loss: 0.7977
Epoch 18/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5842 - val_loss: 0.7972
Epoch 19/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5833 - val_loss: 0.7968
Epoch 20/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5825 - val_loss: 0.7964
Epoch 21/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5816 - val_loss: 0.7960
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5809 - val_loss: 0.7957
Epoch 23/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5805 - val_loss: 0.7954
Epoch 24/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5796 - val_loss: 0.7952
Epoch 25/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5794 - val_loss: 0.7949
Epoch 26/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5791 - val_loss: 0.7947
Epoch 27/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5783 - val_loss: 0.7945
Epoch 28/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5782 - val_loss: 0.7943
Epoch 29/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5777 - val_loss: 0.7941
Epoch 30/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5773 - val_loss: 0.7939
Epoch 31/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5772 - val_loss: 0.7938
Epoch 32/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5768 - val_loss: 0.7936
Epoch 33/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5764 - val_loss: 0.7935
Epoch 34/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5761 - val_loss: 0.7934
Epoch 35/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5761 - val_loss: 0.7933
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5757 - val_loss: 0.7932
Epoch 37/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5757 - val_loss: 0.7931
Epoch 38/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5752 - val_loss: 0.7930
Epoch 39/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5751 - val_loss: 0.7929
Epoch 40/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5747 - val_loss: 0.7928
Epoch 41/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5746 - val_loss: 0.7927
Epoch 42/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5745 - val_loss: 0.7927
Epoch 43/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5745 - val_loss: 0.7926
Epoch 44/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5741 - val_loss: 0.7925
Epoch 45/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5741 - val_loss: 0.7925
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5739 - val_loss: 0.7924
Epoch 47/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5740 - val_loss: 0.7924
Epoch 48/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5735 - val_loss: 0.7923
Epoch 49/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5735 - val_loss: 0.7923
Epoch 50/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5730 - val_loss: 0.7922
Epoch 51/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5731 - val_loss: 0.7922
Epoch 52/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5730 - val_loss: 0.7922
Epoch 53/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5729 - val_loss: 0.7921
Epoch 54/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5728 - val_loss: 0.7921
Epoch 55/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5725 - val_loss: 0.7921
Epoch 56/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.7921
Epoch 57/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.7920
Epoch 58/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5722 - val_loss: 0.7920
Epoch 59/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5721 - val_loss: 0.7920
Epoch 60/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5717 - val_loss: 0.7920
Epoch 61/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5718 - val_loss: 0.7919
Epoch 62/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5718 - val_loss: 0.7919
Epoch 63/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5715 - val_loss: 0.7919
Epoch 64/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5714 - val_loss: 0.7919
Epoch 65/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5714 - val_loss: 0.7919
Epoch 66/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5713 - val_loss: 0.7919
Epoch 67/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5710 - val_loss: 0.7919
Epoch 68/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5708 - val_loss: 0.7918
Execution time:  87.32253789901733
LSTM:
Mean Absolute Error: 0.5057
Root Mean Square Error: 0.7654
Mean Square Error: 0.5858

Train RMSE: 0.765
Train MSE: 0.586
Train MAE: 0.506
###########################

MODEL:  LSTM
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_27&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_27 (LSTM)               (None, 36, 55)            12540     
_________________________________________________________________
dropout_27 (Dropout)         (None, 36, 55)            0         
_________________________________________________________________
time_distributed_27 (TimeDis (None, 36, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
  1/352 [..............................] - ETA: 0s - loss: 0.3995WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.
352/352 [==============================] - 5s 15ms/step - loss: 0.6817 - val_loss: 0.7408
Epoch 2/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6186 - val_loss: 0.7273
Epoch 3/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6109 - val_loss: 0.7218
Epoch 4/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6131 - val_loss: 0.7188
Epoch 5/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6108 - val_loss: 0.7176
Epoch 6/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6069 - val_loss: 0.7164
Epoch 7/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6065 - val_loss: 0.7156
Epoch 8/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6031 - val_loss: 0.7145
Epoch 9/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6064 - val_loss: 0.7128
Epoch 10/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6028 - val_loss: 0.7123
Epoch 11/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6010 - val_loss: 0.7110
Epoch 12/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6009 - val_loss: 0.7096
Epoch 13/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5988 - val_loss: 0.7031
Epoch 14/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5906 - val_loss: 0.6931ss: 0.59
Epoch 15/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5789 - val_loss: 0.6925
Epoch 16/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5754 - val_loss: 0.6921
Epoch 17/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5748 - val_loss: 0.6910
Epoch 18/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5736 - val_loss: 0.6909
Epoch 19/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5737 - val_loss: 0.6904
Epoch 20/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5732 - val_loss: 0.6902
Epoch 21/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5728 - val_loss: 0.6899
Epoch 22/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5723 - val_loss: 0.6895
Epoch 23/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5725 - val_loss: 0.6899
Epoch 24/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5717 - val_loss: 0.6891
Epoch 25/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5715 - val_loss: 0.6895
Epoch 26/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5717 - val_loss: 0.6889
Epoch 27/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5713 - val_loss: 0.6886
Epoch 28/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5709 - val_loss: 0.6893
Epoch 29/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5705 - val_loss: 0.6887
Epoch 30/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5702 - val_loss: 0.6898
Epoch 31/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5701 - val_loss: 0.6889
Epoch 32/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5700 - val_loss: 0.6888
Epoch 33/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5698 - val_loss: 0.6890
Epoch 34/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5697 - val_loss: 0.6889
Epoch 35/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5694 - val_loss: 0.6887
Epoch 36/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5693 - val_loss: 0.6887
Execution time:  173.4124629497528
LSTM:
Mean Absolute Error: 0.5046
Root Mean Square Error: 0.7699
Mean Square Error: 0.5928

Train RMSE: 0.770
Train MSE: 0.593
Train MAE: 0.505
###########################

MODEL:  LSTM
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_28&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_28 (LSTM)               (None, 36, 40)            6720      
_________________________________________________________________
dropout_28 (Dropout)         (None, 36, 40)            0         
_________________________________________________________________
time_distributed_28 (TimeDis (None, 36, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.7075 - val_loss: 0.8010
Epoch 2/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7072 - val_loss: 0.8004
Epoch 3/68
80/80 [==============================] - 1s 16ms/step - loss: 0.7069 - val_loss: 0.7999
Epoch 4/68
80/80 [==============================] - 1s 16ms/step - loss: 0.7067 - val_loss: 0.7993
Epoch 5/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7063 - val_loss: 0.7987
Epoch 6/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7060 - val_loss: 0.7980
Epoch 7/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7057 - val_loss: 0.7974
Epoch 8/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7053 - val_loss: 0.7968
Epoch 9/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7050 - val_loss: 0.7961
Epoch 10/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7047 - val_loss: 0.7955
Epoch 11/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7043 - val_loss: 0.7948
Epoch 12/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7039 - val_loss: 0.7941
Epoch 13/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7036 - val_loss: 0.7935
Epoch 14/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7032 - val_loss: 0.7928
Epoch 15/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7029 - val_loss: 0.7921
Epoch 16/68
80/80 [==============================] - 1s 16ms/step - loss: 0.7026 - val_loss: 0.7914
Epoch 17/68
80/80 [==============================] - 1s 16ms/step - loss: 0.7022 - val_loss: 0.7907
Epoch 18/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7018 - val_loss: 0.7900
Epoch 19/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7014 - val_loss: 0.7893
Epoch 20/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7012 - val_loss: 0.7886
Epoch 21/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7008 - val_loss: 0.7879
Epoch 22/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7004 - val_loss: 0.7872
Epoch 23/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7001 - val_loss: 0.7865
Epoch 24/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6997 - val_loss: 0.7858
Epoch 25/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6993 - val_loss: 0.7851
Epoch 26/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6990 - val_loss: 0.7844
Epoch 27/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6986 - val_loss: 0.7837
Epoch 28/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6983 - val_loss: 0.7830
Epoch 29/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6979 - val_loss: 0.7822
Epoch 30/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6976 - val_loss: 0.7815
Epoch 31/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6972 - val_loss: 0.7808
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6968 - val_loss: 0.7801
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6965 - val_loss: 0.7793
Epoch 34/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6961 - val_loss: 0.7786
Epoch 35/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6958 - val_loss: 0.7779
Epoch 36/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6954 - val_loss: 0.7771
Epoch 37/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6950 - val_loss: 0.7764
Epoch 38/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6946 - val_loss: 0.7756
Epoch 39/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6943 - val_loss: 0.7749
Epoch 40/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6939 - val_loss: 0.7741
Epoch 41/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6935 - val_loss: 0.7734
Epoch 42/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6932 - val_loss: 0.7726
Epoch 43/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6927 - val_loss: 0.7719
Epoch 44/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6923 - val_loss: 0.7711
Epoch 45/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6919 - val_loss: 0.7703
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6916 - val_loss: 0.7696
Epoch 47/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6912 - val_loss: 0.7688
Epoch 48/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6909 - val_loss: 0.7680
Epoch 49/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6905 - val_loss: 0.7673
Epoch 50/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6901 - val_loss: 0.7665
Epoch 51/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6898 - val_loss: 0.7657
Epoch 52/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6893 - val_loss: 0.7649
Epoch 53/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6888 - val_loss: 0.7641
Epoch 54/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6885 - val_loss: 0.7634
Epoch 55/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6882 - val_loss: 0.7626
Epoch 56/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6878 - val_loss: 0.7618
Epoch 57/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6874 - val_loss: 0.7610
Epoch 58/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6869 - val_loss: 0.7602
Epoch 59/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6866 - val_loss: 0.7594
Epoch 60/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6861 - val_loss: 0.7586
Epoch 61/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6858 - val_loss: 0.7578
Epoch 62/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6854 - val_loss: 0.7570
Epoch 63/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6850 - val_loss: 0.7562
Epoch 64/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6846 - val_loss: 0.7553
Epoch 65/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6841 - val_loss: 0.7545
Epoch 66/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6838 - val_loss: 0.7537
Epoch 67/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6833 - val_loss: 0.7529
Epoch 68/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6829 - val_loss: 0.7520
Execution time:  89.1095700263977
LSTM:
Mean Absolute Error: 0.6842
Root Mean Square Error: 0.9809
Mean Square Error: 0.9621

Train RMSE: 0.981
Train MSE: 0.962
Train MAE: 0.684
###########################

MODEL:  LSTM
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_29&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_29 (LSTM)               (None, 36, 55)            12540     
_________________________________________________________________
dropout_29 (Dropout)         (None, 36, 55)            0         
_________________________________________________________________
time_distributed_29 (TimeDis (None, 36, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 5s 14ms/step - loss: 0.7094 - val_loss: 0.6888
Epoch 2/36
352/352 [==============================] - 5s 13ms/step - loss: 0.7059 - val_loss: 0.6844
Epoch 3/36
352/352 [==============================] - 5s 13ms/step - loss: 0.7025 - val_loss: 0.6799
Epoch 4/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6989 - val_loss: 0.6751
Epoch 5/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6952 - val_loss: 0.6703
Epoch 6/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6915 - val_loss: 0.6653
Epoch 7/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6877 - val_loss: 0.6601
Epoch 8/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6838 - val_loss: 0.6548
Epoch 9/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6798 - val_loss: 0.6495
Epoch 10/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6758 - val_loss: 0.6440
Epoch 11/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6716 - val_loss: 0.6384
Epoch 12/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6674 - val_loss: 0.6327
Epoch 13/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6631 - val_loss: 0.6268
Epoch 14/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6587 - val_loss: 0.6209
Epoch 15/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6541 - val_loss: 0.6147
Epoch 16/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6496 - val_loss: 0.6085
Epoch 17/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6449 - val_loss: 0.6020
Epoch 18/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6401 - val_loss: 0.5954
Epoch 19/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6349 - val_loss: 0.5885
Epoch 20/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6298 - val_loss: 0.5815
Epoch 21/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6243 - val_loss: 0.5742
Epoch 22/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6188 - val_loss: 0.5666
Epoch 23/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6132 - val_loss: 0.5589
Epoch 24/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6074 - val_loss: 0.5510
Epoch 25/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6015 - val_loss: 0.5430
Epoch 26/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5955 - val_loss: 0.5348
Epoch 27/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5895 - val_loss: 0.5266
Epoch 28/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5835 - val_loss: 0.5184
Epoch 29/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5776 - val_loss: 0.5103
Epoch 30/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5719 - val_loss: 0.5024
Epoch 31/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5662 - val_loss: 0.4948
Epoch 32/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5609 - val_loss: 0.4875
Epoch 33/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5558 - val_loss: 0.4807
Epoch 34/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5512 - val_loss: 0.4742
Epoch 35/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5465 - val_loss: 0.4680
Epoch 36/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5424 - val_loss: 0.4621
Execution time:  173.8960039615631
LSTM:
Mean Absolute Error: 0.4821
Root Mean Square Error: 0.8295
Mean Square Error: 0.6881

Train RMSE: 0.830
Train MSE: 0.688
Train MAE: 0.482
###########################

MODEL:  LSTM
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_30&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_30 (LSTM)               (None, 36, 40)            6720      
_________________________________________________________________
dropout_30 (Dropout)         (None, 36, 40)            0         
_________________________________________________________________
time_distributed_30 (TimeDis (None, 36, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 27ms/step - loss: 0.8916 - val_loss: 1.2948
Epoch 2/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8915 - val_loss: 1.2946
Epoch 3/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8914 - val_loss: 1.2945
Epoch 4/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8913 - val_loss: 1.2944
Epoch 5/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8913 - val_loss: 1.2943
Epoch 6/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8912 - val_loss: 1.2941
Epoch 7/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8912 - val_loss: 1.2940
Epoch 8/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8911 - val_loss: 1.2939
Epoch 9/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8910 - val_loss: 1.2937
Epoch 10/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8909 - val_loss: 1.2936
Epoch 11/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8909 - val_loss: 1.2935
Epoch 12/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8908 - val_loss: 1.2933
Epoch 13/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8907 - val_loss: 1.2932
Epoch 14/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8907 - val_loss: 1.2930
Epoch 15/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8906 - val_loss: 1.2929
Epoch 16/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8905 - val_loss: 1.2928
Epoch 17/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8905 - val_loss: 1.2926
Epoch 18/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8904 - val_loss: 1.2925
Epoch 19/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8903 - val_loss: 1.2923
Epoch 20/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8902 - val_loss: 1.2922
Epoch 21/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8901 - val_loss: 1.2921
Epoch 22/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8901 - val_loss: 1.2919
Epoch 23/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8900 - val_loss: 1.2918
Epoch 24/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8899 - val_loss: 1.2916
Epoch 25/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8898 - val_loss: 1.2915
Epoch 26/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8898 - val_loss: 1.2913
Epoch 27/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8897 - val_loss: 1.2912 0s - los
Epoch 28/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8896 - val_loss: 1.2910
Epoch 29/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8895 - val_loss: 1.2909
Epoch 30/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8895 - val_loss: 1.2907
Epoch 31/68
80/80 [==============================] - ETA: 0s - loss: 0.889 - 1s 15ms/step - loss: 0.8894 - val_loss: 1.2906
Epoch 32/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8893 - val_loss: 1.2904
Epoch 33/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8893 - val_loss: 1.2903
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8891 - val_loss: 1.2901
Epoch 35/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8891 - val_loss: 1.2900
Epoch 36/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8890 - val_loss: 1.2898
Epoch 37/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8890 - val_loss: 1.2897
Epoch 38/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8889 - val_loss: 1.2895
Epoch 39/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8888 - val_loss: 1.2894
Epoch 40/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8887 - val_loss: 1.2892
Epoch 41/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8886 - val_loss: 1.2891
Epoch 42/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8886 - val_loss: 1.2889
Epoch 43/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8885 - val_loss: 1.2888
Epoch 44/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8884 - val_loss: 1.2886
Epoch 45/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8884 - val_loss: 1.2885
Epoch 46/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8883 - val_loss: 1.2883
Epoch 47/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8882 - val_loss: 1.2882
Epoch 48/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8881 - val_loss: 1.2880
Epoch 49/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8880 - val_loss: 1.2879
Epoch 50/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8879 - val_loss: 1.2877
Epoch 51/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8879 - val_loss: 1.2876
Epoch 52/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8878 - val_loss: 1.2874
Epoch 53/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8877 - val_loss: 1.2873
Epoch 54/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8876 - val_loss: 1.2871
Epoch 55/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8875 - val_loss: 1.2870
Epoch 56/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8875 - val_loss: 1.2868
Epoch 57/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8874 - val_loss: 1.2867
Epoch 58/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8873 - val_loss: 1.2865
Epoch 59/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8872 - val_loss: 1.2863
Epoch 60/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8872 - val_loss: 1.2862
Epoch 61/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8871 - val_loss: 1.2860
Epoch 62/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8870 - val_loss: 1.2859
Epoch 63/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8869 - val_loss: 1.2857
Epoch 64/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8869 - val_loss: 1.2856
Epoch 65/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8868 - val_loss: 1.2854
Epoch 66/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8867 - val_loss: 1.2853
Epoch 67/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8866 - val_loss: 1.2851
Epoch 68/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8866 - val_loss: 1.2849
Execution time:  88.27953171730042
LSTM:
Mean Absolute Error: 0.9210
Root Mean Square Error: 1.1156
Mean Square Error: 1.2446

Train RMSE: 1.116
Train MSE: 1.245
Train MAE: 0.921
###########################

MODEL:  LSTM
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_31&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_31 (LSTM)               (None, 36, 55)            12540     
_________________________________________________________________
dropout_31 (Dropout)         (None, 36, 55)            0         
_________________________________________________________________
time_distributed_31 (TimeDis (None, 36, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8758 - val_loss: 1.1230
Epoch 2/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8753 - val_loss: 1.1222
Epoch 3/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8747 - val_loss: 1.1213
Epoch 4/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8741 - val_loss: 1.1205
Epoch 5/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8736 - val_loss: 1.1196
Epoch 6/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8730 - val_loss: 1.1186
Epoch 7/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8723 - val_loss: 1.1177
Epoch 8/36
352/352 [==============================] - 5s 14ms/step - loss: 0.8717 - val_loss: 1.1167
Epoch 9/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8711 - val_loss: 1.1157
Epoch 10/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8704 - val_loss: 1.1147
Epoch 11/36
352/352 [==============================] - 5s 14ms/step - loss: 0.8698 - val_loss: 1.1137
Epoch 12/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8691 - val_loss: 1.1126
Epoch 13/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8684 - val_loss: 1.1115
Epoch 14/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8677 - val_loss: 1.1104
Epoch 15/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8670 - val_loss: 1.1093
Epoch 16/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8663 - val_loss: 1.1082
Epoch 17/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8655 - val_loss: 1.1070
Epoch 18/36
352/352 [==============================] - 5s 14ms/step - loss: 0.8648 - val_loss: 1.1058
Epoch 19/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8640 - val_loss: 1.1046
Epoch 20/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8632 - val_loss: 1.1034
Epoch 21/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8624 - val_loss: 1.1021
Epoch 22/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8616 - val_loss: 1.1008
Epoch 23/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8607 - val_loss: 1.0995
Epoch 24/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8599 - val_loss: 1.0981
Epoch 25/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8590 - val_loss: 1.0967
Epoch 26/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8581 - val_loss: 1.0953
Epoch 27/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8572 - val_loss: 1.0938
Epoch 28/36
352/352 [==============================] - 5s 14ms/step - loss: 0.8562 - val_loss: 1.0923
Epoch 29/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8553 - val_loss: 1.0908
Epoch 30/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8543 - val_loss: 1.0892
Epoch 31/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8532 - val_loss: 1.0876
Epoch 32/36
352/352 [==============================] - 5s 14ms/step - loss: 0.8522 - val_loss: 1.0859
Epoch 33/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8511 - val_loss: 1.0842
Epoch 34/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8499 - val_loss: 1.0824
Epoch 35/36
352/352 [==============================] - 5s 14ms/step - loss: 0.8488 - val_loss: 1.0806
Epoch 36/36
352/352 [==============================] - 5s 13ms/step - loss: 0.8476 - val_loss: 1.0787
Execution time:  171.20195198059082
LSTM:
Mean Absolute Error: 0.8821
Root Mean Square Error: 1.0774
Mean Square Error: 1.1609

Train RMSE: 1.077
Train MSE: 1.161
Train MAE: 0.882
###########################

MODEL:  LSTM
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_32&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_32 (LSTM)               (None, 36, 40)            6720      
_________________________________________________________________
dropout_32 (Dropout)         (None, 36, 40)            0         
_________________________________________________________________
time_distributed_32 (TimeDis (None, 36, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.5675 - val_loss: 0.2891
Epoch 2/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4786 - val_loss: 0.2690
Epoch 3/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4616 - val_loss: 0.2483
Epoch 4/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4529 - val_loss: 0.2355
Epoch 5/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4468 - val_loss: 0.2278
Epoch 6/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4416 - val_loss: 0.2208
Epoch 7/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4374 - val_loss: 0.2145
Epoch 8/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4336 - val_loss: 0.2116
Epoch 9/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4302 - val_loss: 0.2076
Epoch 10/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4276 - val_loss: 0.2053
Epoch 11/68
80/80 [==============================] - 1s 14ms/step - loss: 0.4247 - val_loss: 0.2021
Epoch 12/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4226 - val_loss: 0.1996
Epoch 13/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4207 - val_loss: 0.1979
Epoch 14/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4189 - val_loss: 0.1971
Epoch 15/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4177 - val_loss: 0.1959
Epoch 16/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4165 - val_loss: 0.1936
Epoch 17/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4155 - val_loss: 0.1924
Epoch 18/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4144 - val_loss: 0.1916
Epoch 19/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4134 - val_loss: 0.1913
Epoch 20/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4129 - val_loss: 0.1912
Epoch 21/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4118 - val_loss: 0.1916
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4113 - val_loss: 0.1903
Epoch 23/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4105 - val_loss: 0.1902
Epoch 24/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4100 - val_loss: 0.1901
Epoch 25/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4097 - val_loss: 0.1891
Epoch 26/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4093 - val_loss: 0.1890
Epoch 27/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4085 - val_loss: 0.1897
Epoch 28/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4082 - val_loss: 0.1890
Epoch 29/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4078 - val_loss: 0.1887
Epoch 30/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4074 - val_loss: 0.1884
Epoch 31/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4072 - val_loss: 0.1886
Epoch 32/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4070 - val_loss: 0.1881
Epoch 33/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4067 - val_loss: 0.1887
Epoch 34/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4061 - val_loss: 0.1876
Epoch 35/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4059 - val_loss: 0.1881
Epoch 36/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4057 - val_loss: 0.1878
Epoch 37/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4052 - val_loss: 0.1877
Epoch 38/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4050 - val_loss: 0.1871
Epoch 39/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4046 - val_loss: 0.1870
Epoch 40/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4046 - val_loss: 0.1873
Epoch 41/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4043 - val_loss: 0.1865
Epoch 42/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4043 - val_loss: 0.1874
Epoch 43/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4042 - val_loss: 0.1868
Epoch 44/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4037 - val_loss: 0.1863
Epoch 45/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4036 - val_loss: 0.1858
Epoch 46/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4035 - val_loss: 0.1864
Epoch 47/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4035 - val_loss: 0.1860
Epoch 48/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4032 - val_loss: 0.1861
Epoch 49/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4029 - val_loss: 0.1860
Epoch 50/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4025 - val_loss: 0.1859
Epoch 51/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4026 - val_loss: 0.1857
Epoch 52/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4025 - val_loss: 0.1854
Epoch 53/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4025 - val_loss: 0.1851
Epoch 54/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4023 - val_loss: 0.1852
Epoch 55/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4019 - val_loss: 0.1846
Epoch 56/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4021 - val_loss: 0.1851
Epoch 57/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4019 - val_loss: 0.1847
Epoch 58/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4017 - val_loss: 0.1848
Epoch 59/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4015 - val_loss: 0.1845
Epoch 60/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4014 - val_loss: 0.1842
Epoch 61/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4013 - val_loss: 0.1844
Epoch 62/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4014 - val_loss: 0.1838
Epoch 63/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4015 - val_loss: 0.1843
Epoch 64/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4010 - val_loss: 0.1842
Epoch 65/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4010 - val_loss: 0.1832
Epoch 66/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4010 - val_loss: 0.1838
Epoch 67/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4007 - val_loss: 0.1839
Epoch 68/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4004 - val_loss: 0.1836
Execution time:  88.11387300491333
LSTM:
Mean Absolute Error: 0.1945
Root Mean Square Error: 0.5975
Mean Square Error: 0.3570

Train RMSE: 0.598
Train MSE: 0.357
Train MAE: 0.194
###########################

MODEL:  LSTM
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_33&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_33 (LSTM)               (None, 36, 55)            12540     
_________________________________________________________________
dropout_33 (Dropout)         (None, 36, 55)            0         
_________________________________________________________________
time_distributed_33 (TimeDis (None, 36, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 5s 14ms/step - loss: 0.4970 - val_loss: 0.3536
Epoch 2/36
352/352 [==============================] - 4s 12ms/step - loss: 0.4343 - val_loss: 0.3346
Epoch 3/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4207 - val_loss: 0.3275
Epoch 4/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4140 - val_loss: 0.3238
Epoch 5/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4095 - val_loss: 0.3216
Epoch 6/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4067 - val_loss: 0.3197
Epoch 7/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4045 - val_loss: 0.3185
Epoch 8/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4025 - val_loss: 0.3175
Epoch 9/36
352/352 [==============================] - 5s 13ms/step - loss: 0.4010 - val_loss: 0.3164
Epoch 10/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3994 - val_loss: 0.3160
Epoch 11/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3984 - val_loss: 0.3154
Epoch 12/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3975 - val_loss: 0.3153
Epoch 13/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3969 - val_loss: 0.3148
Epoch 14/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3963 - val_loss: 0.3143
Epoch 15/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3956 - val_loss: 0.3141
Epoch 16/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3952 - val_loss: 0.3137
Epoch 17/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3948 - val_loss: 0.3137
Epoch 18/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3944 - val_loss: 0.3137
Epoch 19/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3941 - val_loss: 0.3134
Epoch 20/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3938 - val_loss: 0.3135
Epoch 21/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3934 - val_loss: 0.3130
Epoch 22/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3932 - val_loss: 0.3132
Epoch 23/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3929 - val_loss: 0.3133
Epoch 24/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3927 - val_loss: 0.3135
Epoch 25/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3924 - val_loss: 0.3134
Epoch 26/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3920 - val_loss: 0.3133
Epoch 27/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3916 - val_loss: 0.3137
Epoch 28/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3913 - val_loss: 0.3135ETA
Epoch 29/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3911 - val_loss: 0.3137
Epoch 30/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3909 - val_loss: 0.3138
Epoch 31/36
352/352 [==============================] - 5s 13ms/step - loss: 0.3909 - val_loss: 0.3140
Execution time:  146.72795796394348
LSTM:
Mean Absolute Error: 0.1893
Root Mean Square Error: 0.5927
Mean Square Error: 0.3513

Train RMSE: 0.593
Train MSE: 0.351
Train MAE: 0.189
###########################

MODEL:  LSTM
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_34&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_34 (LSTM)               (None, 36, 40)            6720      
_________________________________________________________________
dropout_34 (Dropout)         (None, 36, 40)            0         
_________________________________________________________________
time_distributed_34 (TimeDis (None, 36, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 22ms/step - loss: 0.8534 - val_loss: 1.0939
Epoch 2/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7103 - val_loss: 0.8974
Epoch 3/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6594 - val_loss: 0.8561
Epoch 4/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6467 - val_loss: 0.8423
Epoch 5/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6407 - val_loss: 0.8356
Epoch 6/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6360 - val_loss: 0.8307
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6324 - val_loss: 0.8269
Epoch 8/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6291 - val_loss: 0.8238
Epoch 9/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6264 - val_loss: 0.8217
Epoch 10/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6240 - val_loss: 0.8201
Epoch 11/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6220 - val_loss: 0.8183
Epoch 12/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6200 - val_loss: 0.8167
Epoch 13/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6180 - val_loss: 0.8156
Epoch 14/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6160 - val_loss: 0.8144
Epoch 15/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6137 - val_loss: 0.8133
Epoch 16/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6111 - val_loss: 0.8122
Epoch 17/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6091 - val_loss: 0.8110
Epoch 18/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6068 - val_loss: 0.8100
Epoch 19/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6048 - val_loss: 0.8090
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6032 - val_loss: 0.8081
Epoch 21/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6015 - val_loss: 0.8072
Epoch 22/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6002 - val_loss: 0.8063
Epoch 23/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5988 - val_loss: 0.8055
Epoch 24/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5975 - val_loss: 0.8048
Epoch 25/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5962 - val_loss: 0.8041
Epoch 26/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5952 - val_loss: 0.8035
Epoch 27/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5940 - val_loss: 0.8029
Epoch 28/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5929 - val_loss: 0.8023
Epoch 29/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5919 - val_loss: 0.8018
Epoch 30/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5911 - val_loss: 0.8013
Epoch 31/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5901 - val_loss: 0.8008
Epoch 32/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5895 - val_loss: 0.8004
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5887 - val_loss: 0.7999
Epoch 34/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5879 - val_loss: 0.7996
Epoch 35/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5874 - val_loss: 0.7992
Epoch 36/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5866 - val_loss: 0.7989TA: 0s - loss: 0.60
Epoch 37/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5860 - val_loss: 0.7986
Epoch 38/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5855 - val_loss: 0.7983
Epoch 39/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5848 - val_loss: 0.7980
Epoch 40/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5844 - val_loss: 0.7977
Epoch 41/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5839 - val_loss: 0.7975
Epoch 42/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5835 - val_loss: 0.7972
Epoch 43/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5830 - val_loss: 0.7970
Epoch 44/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.7968
Epoch 45/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5822 - val_loss: 0.7966
Epoch 46/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5819 - val_loss: 0.7964
Epoch 47/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5816 - val_loss: 0.7962
Epoch 48/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5811 - val_loss: 0.7960
Epoch 49/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5808 - val_loss: 0.7959
Epoch 50/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5803 - val_loss: 0.7957- ETA: 0s - loss:
Epoch 51/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5802 - val_loss: 0.7956
Epoch 52/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5799 - val_loss: 0.7954
Epoch 53/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5796 - val_loss: 0.7953
Epoch 54/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5794 - val_loss: 0.7952
Epoch 55/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5791 - val_loss: 0.7950
Epoch 56/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5789 - val_loss: 0.7949
Epoch 57/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5786 - val_loss: 0.7948
Epoch 58/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5783 - val_loss: 0.7947
Epoch 59/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5782 - val_loss: 0.7946
Epoch 60/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5780 - val_loss: 0.7945
Epoch 61/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5778 - val_loss: 0.7944
Epoch 62/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5776 - val_loss: 0.7943
Epoch 63/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5774 - val_loss: 0.7942
Epoch 64/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5772 - val_loss: 0.7941
Epoch 65/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5771 - val_loss: 0.7940
Epoch 66/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5769 - val_loss: 0.7940
Epoch 67/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5767 - val_loss: 0.7939
Epoch 68/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5765 - val_loss: 0.7938
Execution time:  88.48160099983215
LSTM:
Mean Absolute Error: 0.5115
Root Mean Square Error: 0.7669
Mean Square Error: 0.5882

Train RMSE: 0.767
Train MSE: 0.588
Train MAE: 0.511
###########################

MODEL:  LSTM
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_35&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_35 (LSTM)               (None, 36, 55)            12540     
_________________________________________________________________
dropout_35 (Dropout)         (None, 36, 55)            0         
_________________________________________________________________
time_distributed_35 (TimeDis (None, 36, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 5s 14ms/step - loss: 0.7059 - val_loss: 0.7600
Epoch 2/36
352/352 [==============================] - 4s 13ms/step - loss: 0.6331 - val_loss: 0.7443
Epoch 3/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6238 - val_loss: 0.7372
Epoch 4/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6166 - val_loss: 0.7337
Epoch 5/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6125 - val_loss: 0.7309
Epoch 6/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6094 - val_loss: 0.7285
Epoch 7/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6066 - val_loss: 0.7258
Epoch 8/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6038 - val_loss: 0.7239
Epoch 9/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6018 - val_loss: 0.7220
Epoch 10/36
352/352 [==============================] - 5s 13ms/step - loss: 0.6002 - val_loss: 0.7191
Epoch 11/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5976 - val_loss: 0.7163
Epoch 12/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5951 - val_loss: 0.7119
Epoch 13/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5911 - val_loss: 0.7081
Epoch 14/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5873 - val_loss: 0.7044
Epoch 15/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5851 - val_loss: 0.7027
Epoch 16/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5837 - val_loss: 0.7013
Epoch 17/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5824 - val_loss: 0.6999
Epoch 18/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5812 - val_loss: 0.6989
Epoch 19/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5803 - val_loss: 0.6981
Epoch 20/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5795 - val_loss: 0.6972
Epoch 21/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5787 - val_loss: 0.6966
Epoch 22/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5779 - val_loss: 0.6960
Epoch 23/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5773 - val_loss: 0.6957
Epoch 24/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5767 - val_loss: 0.6951
Epoch 25/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5760 - val_loss: 0.6948
Epoch 26/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5754 - val_loss: 0.6955
Epoch 27/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5751 - val_loss: 0.6954
Epoch 28/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5748 - val_loss: 0.6954
Epoch 29/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5742 - val_loss: 0.6952
Epoch 30/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5740 - val_loss: 0.6952
Epoch 31/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5737 - val_loss: 0.6951
Epoch 32/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5733 - val_loss: 0.6950
Epoch 33/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5730 - val_loss: 0.6949
Epoch 34/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5727 - val_loss: 0.6948
Epoch 35/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5727 - val_loss: 0.6946
Epoch 36/36
352/352 [==============================] - 5s 13ms/step - loss: 0.5725 - val_loss: 0.6944
Execution time:  170.76853561401367
LSTM:
Mean Absolute Error: 0.5091
Root Mean Square Error: 0.7645
Mean Square Error: 0.5844

Train RMSE: 0.764
Train MSE: 0.584
Train MAE: 0.509
###########################

MODEL:  LSTM
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_36&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_36 (LSTM)               (None, 72, 40)            6720      
_________________________________________________________________
dropout_36 (Dropout)         (None, 72, 40)            0         
_________________________________________________________________
time_distributed_36 (TimeDis (None, 72, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 37ms/step - loss: 0.6086 - val_loss: 0.2800
Epoch 2/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5312 - val_loss: 0.2414
Epoch 3/68
79/79 [==============================] - 2s 27ms/step - loss: 0.5193 - val_loss: 0.2284
Epoch 4/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5139 - val_loss: 0.2201
Epoch 5/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5108 - val_loss: 0.2167
Epoch 6/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5083 - val_loss: 0.2138
Epoch 7/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5058 - val_loss: 0.2129
Epoch 8/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5043 - val_loss: 0.2115
Epoch 9/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5024 - val_loss: 0.2111
Epoch 10/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5009 - val_loss: 0.2106
Epoch 11/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4997 - val_loss: 0.2106
Epoch 12/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4985 - val_loss: 0.2101
Epoch 13/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4974 - val_loss: 0.2102
Epoch 14/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4968 - val_loss: 0.2100
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4951 - val_loss: 0.2106
Epoch 16/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4974 - val_loss: 0.2098
Epoch 17/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4917 - val_loss: 0.2101
Epoch 18/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4963 - val_loss: 0.2098
Epoch 19/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4882 - val_loss: 0.2094
Epoch 20/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4878 - val_loss: 0.2102
Epoch 21/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4778 - val_loss: 0.2092
Epoch 22/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4952 - val_loss: 0.2108
Epoch 23/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4932 - val_loss: 0.2105
Epoch 24/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4919 - val_loss: 0.2097
Epoch 25/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4887 - val_loss: 0.2085
Epoch 26/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4784 - val_loss: 0.2093
Epoch 27/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4765 - val_loss: 0.2063
Epoch 28/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4952 - val_loss: 0.2096
Epoch 29/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4855 - val_loss: 0.2080
Epoch 30/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4776 - val_loss: 0.2091
Epoch 31/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4762 - val_loss: 0.2061
Epoch 32/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4735 - val_loss: 0.2066
Epoch 33/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4719 - val_loss: 0.2074
Epoch 34/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4704 - val_loss: 0.2079
Epoch 35/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4694 - val_loss: 0.2079
Epoch 36/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4690 - val_loss: 0.2085
Epoch 37/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4707 - val_loss: 0.2080
Epoch 38/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4691 - val_loss: 0.2080
Epoch 39/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4707 - val_loss: 0.2078
Epoch 40/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4689 - val_loss: 0.2082
Epoch 41/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4680 - val_loss: 0.2082
Execution time:  97.28828144073486
LSTM:
Mean Absolute Error: 0.2927
Root Mean Square Error: 0.7776
Mean Square Error: 0.6047

Train RMSE: 0.778
Train MSE: 0.605
Train MAE: 0.293
###########################

MODEL:  LSTM
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_37&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_37 (LSTM)               (None, 72, 55)            12540     
_________________________________________________________________
dropout_37 (Dropout)         (None, 72, 55)            0         
_________________________________________________________________
time_distributed_37 (TimeDis (None, 72, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 9s 26ms/step - loss: 0.5565 - val_loss: 0.3951
Epoch 2/36
349/349 [==============================] - 9s 26ms/step - loss: 0.5028 - val_loss: 0.3865
Epoch 3/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4961 - val_loss: 0.3850
Epoch 4/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4923 - val_loss: 0.3843
Epoch 5/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4893 - val_loss: 0.3835
Epoch 6/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4871 - val_loss: 0.3829
Epoch 7/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4860 - val_loss: 0.3828
Epoch 8/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4846 - val_loss: 0.3811
Epoch 9/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4839 - val_loss: 0.3815
Epoch 10/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4815 - val_loss: 0.3800
Epoch 11/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4805 - val_loss: 0.3797
Epoch 12/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4882 - val_loss: 0.3864
Epoch 13/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4811 - val_loss: 0.3784
Epoch 14/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4780 - val_loss: 0.3792
Epoch 15/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4739 - val_loss: 0.3762
Epoch 16/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4737 - val_loss: 0.3722
Epoch 17/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4699 - val_loss: 0.3695
Epoch 18/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4670 - val_loss: 0.3678
Epoch 19/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4648 - val_loss: 0.3663
Epoch 20/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4735 - val_loss: 0.3720
Epoch 21/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4683 - val_loss: 0.3631
Epoch 22/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4611 - val_loss: 0.3642
Epoch 23/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4621 - val_loss: 0.3643
Epoch 24/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4617 - val_loss: 0.3656
Epoch 25/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4599 - val_loss: 0.3667
Epoch 26/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4570 - val_loss: 0.3661
Epoch 27/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4591 - val_loss: 0.3665
Epoch 28/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4560 - val_loss: 0.3667
Epoch 29/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4697 - val_loss: 0.3635
Epoch 30/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4611 - val_loss: 0.3657
Epoch 31/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4583 - val_loss: 0.3677
Execution time:  278.48469734191895
LSTM:
Mean Absolute Error: 0.2832
Root Mean Square Error: 0.7731
Mean Square Error: 0.5976

Train RMSE: 0.773
Train MSE: 0.598
Train MAE: 0.283
###########################

MODEL:  LSTM
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_38&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_38 (LSTM)               (None, 72, 40)            6720      
_________________________________________________________________
dropout_38 (Dropout)         (None, 72, 40)            0         
_________________________________________________________________
time_distributed_38 (TimeDis (None, 72, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 33ms/step - loss: 0.8061 - val_loss: 0.8870
Epoch 2/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6826 - val_loss: 0.8276
Epoch 3/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6697 - val_loss: 0.8162
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6634 - val_loss: 0.8115
Epoch 5/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6601 - val_loss: 0.8081
Epoch 6/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6589 - val_loss: 0.8059
Epoch 7/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6570 - val_loss: 0.8042
Epoch 8/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6555 - val_loss: 0.8028
Epoch 9/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6545 - val_loss: 0.8016
Epoch 10/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6538 - val_loss: 0.8008
Epoch 11/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6530 - val_loss: 0.7999
Epoch 12/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6523 - val_loss: 0.7993
Epoch 13/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6520 - val_loss: 0.7987
Epoch 14/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6513 - val_loss: 0.7983
Epoch 15/68
79/79 [==============================] - ETA: 0s - loss: 0.651 - 2s 27ms/step - loss: 0.6509 - val_loss: 0.7976
Epoch 16/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6515 - val_loss: 0.7974
Epoch 17/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6501 - val_loss: 0.7968
Epoch 18/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6504 - val_loss: 0.7966
Epoch 19/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6500 - val_loss: 0.7963
Epoch 20/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6497 - val_loss: 0.7961
Epoch 21/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6494 - val_loss: 0.7958
Epoch 22/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6493 - val_loss: 0.7955
Epoch 23/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6504 - val_loss: 0.7953
Epoch 24/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6487 - val_loss: 0.7951
Epoch 25/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6491 - val_loss: 0.7949
Epoch 26/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6483 - val_loss: 0.7947
Epoch 27/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6481 - val_loss: 0.7946
Epoch 28/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6488 - val_loss: 0.7943
Epoch 29/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6506 - val_loss: 0.7942
Epoch 30/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6491 - val_loss: 0.7941
Epoch 31/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6478 - val_loss: 0.7940
Epoch 32/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6473 - val_loss: 0.7938
Epoch 33/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6476 - val_loss: 0.7938
Epoch 34/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6471 - val_loss: 0.7936
Epoch 35/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6467 - val_loss: 0.7932
Epoch 36/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6562 - val_loss: 0.7932
Epoch 37/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6512 - val_loss: 0.7931
Epoch 38/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6533 - val_loss: 0.7931
Epoch 39/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6494 - val_loss: 0.7930
Epoch 40/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6491 - val_loss: 0.7930
Epoch 41/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6495 - val_loss: 0.7929
Epoch 42/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6493 - val_loss: 0.7928
Epoch 43/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6490 - val_loss: 0.7927
Epoch 44/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6491 - val_loss: 0.7927
Epoch 45/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6484 - val_loss: 0.7926
Epoch 46/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6473 - val_loss: 0.7926
Epoch 47/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6465 - val_loss: 0.7926
Epoch 48/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6462 - val_loss: 0.7925
Epoch 49/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6454 - val_loss: 0.7924
Epoch 50/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6448 - val_loss: 0.7924
Epoch 51/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6440 - val_loss: 0.7923
Epoch 52/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6435 - val_loss: 0.7923
Epoch 53/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6430 - val_loss: 0.7922
Epoch 54/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6424 - val_loss: 0.7923
Epoch 55/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6411 - val_loss: 0.7923
Epoch 56/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6394 - val_loss: 0.7922
Epoch 57/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6393 - val_loss: 0.7922
Epoch 58/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6389 - val_loss: 0.7921
Epoch 59/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6401 - val_loss: 0.7921
Epoch 60/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6377 - val_loss: 0.7920
Epoch 61/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6383 - val_loss: 0.7920
Epoch 62/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6382 - val_loss: 0.7919
Epoch 63/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6375 - val_loss: 0.7919
Epoch 64/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6369 - val_loss: 0.7919
Epoch 65/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6384 - val_loss: 0.7919
Epoch 66/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6354 - val_loss: 0.7918
Epoch 67/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6349 - val_loss: 0.7918
Epoch 68/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6355 - val_loss: 0.7917
Execution time:  156.2325563430786
LSTM:
Mean Absolute Error: 0.6051
Root Mean Square Error: 0.9010
Mean Square Error: 0.8119

Train RMSE: 0.901
Train MSE: 0.812
Train MAE: 0.605
###########################

MODEL:  LSTM
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_39&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_39 (LSTM)               (None, 72, 55)            12540     
_________________________________________________________________
dropout_39 (Dropout)         (None, 72, 55)            0         
_________________________________________________________________
time_distributed_39 (TimeDis (None, 72, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 9s 26ms/step - loss: 0.7196 - val_loss: 0.7421
Epoch 2/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6585 - val_loss: 0.7309
Epoch 3/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6561 - val_loss: 0.7255
Epoch 4/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6539 - val_loss: 0.7243
Epoch 5/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6534 - val_loss: 0.7231
Epoch 6/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6515 - val_loss: 0.7209
Epoch 7/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6558 - val_loss: 0.7191
Epoch 8/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6537 - val_loss: 0.7195
Epoch 9/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6512 - val_loss: 0.7238
Epoch 10/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6501 - val_loss: 0.7215
Epoch 11/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6478 - val_loss: 0.7185
Epoch 12/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6487 - val_loss: 0.7185
Epoch 13/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6476 - val_loss: 0.7170
Epoch 14/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6471 - val_loss: 0.7163
Epoch 15/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6469 - val_loss: 0.7160
Epoch 16/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6477 - val_loss: 0.7172
Epoch 17/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6473 - val_loss: 0.7195
Epoch 18/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6460 - val_loss: 0.7170
Epoch 19/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6457 - val_loss: 0.7153
Epoch 20/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6457 - val_loss: 0.7155
Epoch 21/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6454 - val_loss: 0.7162
Epoch 22/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6500 - val_loss: 0.7141
Epoch 23/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7135
Epoch 24/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6446 - val_loss: 0.7131
Epoch 25/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6568 - val_loss: 0.7118
Epoch 26/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6508 - val_loss: 0.7161
Epoch 27/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6369 - val_loss: 0.7144
Epoch 28/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6243 - val_loss: 0.7131
Epoch 29/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6223 - val_loss: 0.7115
Epoch 30/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6176 - val_loss: 0.7107
Epoch 31/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6156 - val_loss: 0.7089
Epoch 32/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6129 - val_loss: 0.7079
Epoch 33/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6120 - val_loss: 0.7091
Epoch 34/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6077 - val_loss: 0.7068
Epoch 35/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6159 - val_loss: 0.7079
Epoch 36/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6051 - val_loss: 0.7051
Execution time:  321.54596757888794
LSTM:
Mean Absolute Error: 0.5525
Root Mean Square Error: 0.8496
Mean Square Error: 0.7219

Train RMSE: 0.850
Train MSE: 0.722
Train MAE: 0.552
###########################

MODEL:  LSTM
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_40&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_40 (LSTM)               (None, 72, 40)            6720      
_________________________________________________________________
dropout_40 (Dropout)         (None, 72, 40)            0         
_________________________________________________________________
time_distributed_40 (TimeDis (None, 72, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 34ms/step - loss: 0.6976 - val_loss: 0.7943
Epoch 2/68
79/79 [==============================] - 2s 27ms/step - loss: 0.6975 - val_loss: 0.7937
Epoch 3/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6972 - val_loss: 0.7931
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6969 - val_loss: 0.7925
Epoch 5/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6967 - val_loss: 0.7919
Epoch 6/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6964 - val_loss: 0.7913
Epoch 7/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6960 - val_loss: 0.7906
Epoch 8/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6958 - val_loss: 0.7900
Epoch 9/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6955 - val_loss: 0.7893
Epoch 10/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6952 - val_loss: 0.7886
Epoch 11/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6948 - val_loss: 0.7879
Epoch 12/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6947 - val_loss: 0.7872
Epoch 13/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6942 - val_loss: 0.7865
Epoch 14/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6939 - val_loss: 0.7858
Epoch 15/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6936 - val_loss: 0.7851
Epoch 16/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6933 - val_loss: 0.7844
Epoch 17/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6929 - val_loss: 0.7837
Epoch 18/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6926 - val_loss: 0.7830
Epoch 19/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6923 - val_loss: 0.7823
Epoch 20/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6920 - val_loss: 0.7815
Epoch 21/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6917 - val_loss: 0.7808
Epoch 22/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6914 - val_loss: 0.7801
Epoch 23/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6910 - val_loss: 0.7793
Epoch 24/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6907 - val_loss: 0.7786
Epoch 25/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6904 - val_loss: 0.7779
Epoch 26/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6900 - val_loss: 0.7771
Epoch 27/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6897 - val_loss: 0.7764
Epoch 28/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6893 - val_loss: 0.7757
Epoch 29/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6890 - val_loss: 0.7749
Epoch 30/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6888 - val_loss: 0.7742
Epoch 31/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6883 - val_loss: 0.7734
Epoch 32/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6880 - val_loss: 0.7727
Epoch 33/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6877 - val_loss: 0.7719
Epoch 34/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6874 - val_loss: 0.7712
Epoch 35/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6870 - val_loss: 0.7704
Epoch 36/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6867 - val_loss: 0.7697
Epoch 37/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6864 - val_loss: 0.7689
Epoch 38/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6860 - val_loss: 0.7681
Epoch 39/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6858 - val_loss: 0.7674
Epoch 40/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6854 - val_loss: 0.7666
Epoch 41/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6850 - val_loss: 0.7658
Epoch 42/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6847 - val_loss: 0.7651
Epoch 43/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6843 - val_loss: 0.7643
Epoch 44/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6839 - val_loss: 0.7635
Epoch 45/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6836 - val_loss: 0.7628
Epoch 46/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6832 - val_loss: 0.7620
Epoch 47/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6829 - val_loss: 0.7612
Epoch 48/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6826 - val_loss: 0.7604
Epoch 49/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6822 - val_loss: 0.7596
Epoch 50/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6819 - val_loss: 0.7588
Epoch 51/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6816 - val_loss: 0.7581
Epoch 52/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6812 - val_loss: 0.7573
Epoch 53/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6809 - val_loss: 0.7565
Epoch 54/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6805 - val_loss: 0.7557
Epoch 55/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6801 - val_loss: 0.7549
Epoch 56/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6798 - val_loss: 0.7541
Epoch 57/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6794 - val_loss: 0.7533
Epoch 58/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6791 - val_loss: 0.7525
Epoch 59/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6787 - val_loss: 0.7517
Epoch 60/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6784 - val_loss: 0.7509
Epoch 61/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6781 - val_loss: 0.7501
Epoch 62/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6777 - val_loss: 0.7493
Epoch 63/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6772 - val_loss: 0.7485
Epoch 64/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6769 - val_loss: 0.7476
Epoch 65/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6766 - val_loss: 0.7468
Epoch 66/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6763 - val_loss: 0.7460
Epoch 67/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6759 - val_loss: 0.7452
Epoch 68/68
79/79 [==============================] - 2s 28ms/step - loss: 0.6754 - val_loss: 0.7444
Execution time:  158.29453372955322
LSTM:
Mean Absolute Error: 0.6766
Root Mean Square Error: 0.9757
Mean Square Error: 0.9520

Train RMSE: 0.976
Train MSE: 0.952
Train MAE: 0.677
###########################

MODEL:  LSTM
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_41&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_41 (LSTM)               (None, 72, 55)            12540     
_________________________________________________________________
dropout_41 (Dropout)         (None, 72, 55)            0         
_________________________________________________________________
time_distributed_41 (TimeDis (None, 72, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6778 - val_loss: 0.6595
Epoch 2/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6759 - val_loss: 0.6569
Epoch 3/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6739 - val_loss: 0.6540
Epoch 4/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6717 - val_loss: 0.6510
Epoch 5/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6695 - val_loss: 0.6478
Epoch 6/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6672 - val_loss: 0.6446
Epoch 7/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6649 - val_loss: 0.6413
Epoch 8/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6625 - val_loss: 0.6379
Epoch 9/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6601 - val_loss: 0.6345
Epoch 10/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6577 - val_loss: 0.6310
Epoch 11/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6552 - val_loss: 0.6274
Epoch 12/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6526 - val_loss: 0.6237
Epoch 13/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6500 - val_loss: 0.6200
Epoch 14/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6473 - val_loss: 0.6161
Epoch 15/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6446 - val_loss: 0.6121
Epoch 16/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6418 - val_loss: 0.6079
Epoch 17/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6388 - val_loss: 0.6037
Epoch 18/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6359 - val_loss: 0.5993
Epoch 19/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6328 - val_loss: 0.5948
Epoch 20/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6297 - val_loss: 0.5902
Epoch 21/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6264 - val_loss: 0.5854
Epoch 22/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6230 - val_loss: 0.5804
Epoch 23/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6196 - val_loss: 0.5752
Epoch 24/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6160 - val_loss: 0.5699
Epoch 25/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6123 - val_loss: 0.5644
Epoch 26/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6086 - val_loss: 0.5588
Epoch 27/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6047 - val_loss: 0.5531
Epoch 28/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6008 - val_loss: 0.5472
Epoch 29/36
349/349 [==============================] - 9s 25ms/step - loss: 0.5969 - val_loss: 0.5412
Epoch 30/36
349/349 [==============================] - 9s 26ms/step - loss: 0.5929 - val_loss: 0.5352
Epoch 31/36
349/349 [==============================] - 9s 25ms/step - loss: 0.5889 - val_loss: 0.5292
Epoch 32/36
349/349 [==============================] - 9s 26ms/step - loss: 0.5849 - val_loss: 0.5231
Epoch 33/36
349/349 [==============================] - 9s 25ms/step - loss: 0.5809 - val_loss: 0.5170
Epoch 34/36
349/349 [==============================] - 9s 26ms/step - loss: 0.5771 - val_loss: 0.5109
Epoch 35/36
349/349 [==============================] - 9s 26ms/step - loss: 0.5730 - val_loss: 0.5048
Epoch 36/36
349/349 [==============================] - 9s 25ms/step - loss: 0.5695 - val_loss: 0.4991
Execution time:  324.0750644207001
LSTM:
Mean Absolute Error: 0.4830
Root Mean Square Error: 0.8228
Mean Square Error: 0.6770

Train RMSE: 0.823
Train MSE: 0.677
Train MAE: 0.483
###########################

MODEL:  LSTM
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_42&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_42 (LSTM)               (None, 72, 40)            6720      
_________________________________________________________________
dropout_42 (Dropout)         (None, 72, 40)            0         
_________________________________________________________________
time_distributed_42 (TimeDis (None, 72, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 36ms/step - loss: 0.8904 - val_loss: 1.2875
Epoch 2/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8904 - val_loss: 1.2874
Epoch 3/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8903 - val_loss: 1.2873
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8903 - val_loss: 1.2871
Epoch 5/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8902 - val_loss: 1.2870
Epoch 6/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8902 - val_loss: 1.2868
Epoch 7/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8901 - val_loss: 1.2867
Epoch 8/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8900 - val_loss: 1.2865
Epoch 9/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8900 - val_loss: 1.2864
Epoch 10/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8899 - val_loss: 1.2862
Epoch 11/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8898 - val_loss: 1.2861
Epoch 12/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8898 - val_loss: 1.2859
Epoch 13/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8897 - val_loss: 1.2858
Epoch 14/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8896 - val_loss: 1.2856
Epoch 15/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8895 - val_loss: 1.2854
Epoch 16/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8895 - val_loss: 1.2853
Epoch 17/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8894 - val_loss: 1.2851
Epoch 18/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8893 - val_loss: 1.2850
Epoch 19/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8893 - val_loss: 1.2848
Epoch 20/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8892 - val_loss: 1.2846
Epoch 21/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8891 - val_loss: 1.2845
Epoch 22/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8891 - val_loss: 1.2843
Epoch 23/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8890 - val_loss: 1.2841
Epoch 24/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8889 - val_loss: 1.2840
Epoch 25/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8889 - val_loss: 1.2838
Epoch 26/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8888 - val_loss: 1.2836
Epoch 27/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8887 - val_loss: 1.2835
Epoch 28/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8886 - val_loss: 1.2833
Epoch 29/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8886 - val_loss: 1.2831
Epoch 30/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8885 - val_loss: 1.2830
Epoch 31/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8884 - val_loss: 1.2828
Epoch 32/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8883 - val_loss: 1.2826
Epoch 33/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8883 - val_loss: 1.2824
Epoch 34/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8882 - val_loss: 1.2823
Epoch 35/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8881 - val_loss: 1.2821
Epoch 36/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8880 - val_loss: 1.2819
Epoch 37/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8880 - val_loss: 1.2817
Epoch 38/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8879 - val_loss: 1.2816
Epoch 39/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8878 - val_loss: 1.2814
Epoch 40/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8877 - val_loss: 1.2812
Epoch 41/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8876 - val_loss: 1.2810
Epoch 42/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8876 - val_loss: 1.2809
Epoch 43/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8875 - val_loss: 1.2807
Epoch 44/68
79/79 [==============================] - 2s 28ms/step - loss: 0.8874 - val_loss: 1.2805
Epoch 45/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8874 - val_loss: 1.2803
Epoch 46/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8872 - val_loss: 1.2802
Epoch 47/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8872 - val_loss: 1.2800
Epoch 48/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8871 - val_loss: 1.2798
Epoch 49/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8870 - val_loss: 1.2796
Epoch 50/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8869 - val_loss: 1.2794
Epoch 51/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8869 - val_loss: 1.2793
Epoch 52/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8868 - val_loss: 1.2791
Epoch 53/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8867 - val_loss: 1.2789
Epoch 54/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8867 - val_loss: 1.2787
Epoch 55/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8866 - val_loss: 1.2785
Epoch 56/68
79/79 [==============================] - 3s 32ms/step - loss: 0.8865 - val_loss: 1.2784
Epoch 57/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8864 - val_loss: 1.2782
Epoch 58/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8863 - val_loss: 1.2780
Epoch 59/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8862 - val_loss: 1.2778
Epoch 60/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8862 - val_loss: 1.2776
Epoch 61/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8861 - val_loss: 1.2774
Epoch 62/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8860 - val_loss: 1.2772
Epoch 63/68
79/79 [==============================] - 3s 32ms/step - loss: 0.8859 - val_loss: 1.2771
Epoch 64/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8858 - val_loss: 1.2769
Epoch 65/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8857 - val_loss: 1.2767
Epoch 66/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8857 - val_loss: 1.2765
Epoch 67/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8856 - val_loss: 1.2763
Epoch 68/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8855 - val_loss: 1.2761
Execution time:  163.17893505096436
LSTM:
Mean Absolute Error: 0.9169
Root Mean Square Error: 1.1115
Mean Square Error: 1.2354

Train RMSE: 1.111
Train MSE: 1.235
Train MAE: 0.917
###########################

MODEL:  LSTM
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_43&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_43 (LSTM)               (None, 72, 55)            12540     
_________________________________________________________________
dropout_43 (Dropout)         (None, 72, 55)            0         
_________________________________________________________________
time_distributed_43 (TimeDis (None, 72, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8813 - val_loss: 1.1289
Epoch 2/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8809 - val_loss: 1.1283
Epoch 3/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8806 - val_loss: 1.1277
Epoch 4/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8802 - val_loss: 1.1271
Epoch 5/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8798 - val_loss: 1.1264
Epoch 6/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8794 - val_loss: 1.1258
Epoch 7/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8790 - val_loss: 1.1251
Epoch 8/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8786 - val_loss: 1.1243
Epoch 9/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8781 - val_loss: 1.1236
Epoch 10/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8777 - val_loss: 1.1228
Epoch 11/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8772 - val_loss: 1.1221
Epoch 12/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8767 - val_loss: 1.1213
Epoch 13/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8763 - val_loss: 1.1205
Epoch 14/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8758 - val_loss: 1.1196
Epoch 15/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8753 - val_loss: 1.1188
Epoch 16/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8748 - val_loss: 1.1179
Epoch 17/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8743 - val_loss: 1.1170
Epoch 18/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8737 - val_loss: 1.1162
Epoch 19/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8732 - val_loss: 1.1152
Epoch 20/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8726 - val_loss: 1.1143
Epoch 21/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8721 - val_loss: 1.1133
Epoch 22/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8715 - val_loss: 1.1124
Epoch 23/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8709 - val_loss: 1.1114
Epoch 24/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8703 - val_loss: 1.1103
Epoch 25/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8697 - val_loss: 1.1093
Epoch 26/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8691 - val_loss: 1.1082
Epoch 27/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8684 - val_loss: 1.1071
Epoch 28/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8678 - val_loss: 1.1060
Epoch 29/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8671 - val_loss: 1.1049
Epoch 30/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8664 - val_loss: 1.1037
Epoch 31/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8657 - val_loss: 1.1025
Epoch 32/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8650 - val_loss: 1.1013
Epoch 33/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8643 - val_loss: 1.1000
Epoch 34/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8635 - val_loss: 1.0987
Epoch 35/36
349/349 [==============================] - 9s 25ms/step - loss: 0.8627 - val_loss: 1.0974
Epoch 36/36
349/349 [==============================] - 9s 26ms/step - loss: 0.8619 - val_loss: 1.0960
Execution time:  323.8963716030121
LSTM:
Mean Absolute Error: 0.8968
Root Mean Square Error: 1.0926
Mean Square Error: 1.1938

Train RMSE: 1.093
Train MSE: 1.194
Train MAE: 0.897
###########################

MODEL:  LSTM
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_44&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_44 (LSTM)               (None, 72, 40)            6720      
_________________________________________________________________
dropout_44 (Dropout)         (None, 72, 40)            0         
_________________________________________________________________
time_distributed_44 (TimeDis (None, 72, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 35ms/step - loss: 0.6281 - val_loss: 0.2935
Epoch 2/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5470 - val_loss: 0.2725
Epoch 3/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5319 - val_loss: 0.2593
Epoch 4/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5222 - val_loss: 0.2501
Epoch 5/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5171 - val_loss: 0.2421
Epoch 6/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5137 - val_loss: 0.2374
Epoch 7/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5113 - val_loss: 0.2335
Epoch 8/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5097 - val_loss: 0.2300
Epoch 9/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5081 - val_loss: 0.2269
Epoch 10/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5068 - val_loss: 0.2249
Epoch 11/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5059 - val_loss: 0.2233
Epoch 12/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5046 - val_loss: 0.2218
Epoch 13/68
79/79 [==============================] - 2s 28ms/step - loss: 0.5036 - val_loss: 0.2202
Epoch 14/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5027 - val_loss: 0.2192
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5021 - val_loss: 0.2184
Epoch 16/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5009 - val_loss: 0.2175
Epoch 17/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5002 - val_loss: 0.2169
Epoch 18/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4991 - val_loss: 0.2167
Epoch 19/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4984 - val_loss: 0.2163
Epoch 20/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4976 - val_loss: 0.2160
Epoch 21/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4966 - val_loss: 0.2163
Epoch 22/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4954 - val_loss: 0.2169
Epoch 23/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4945 - val_loss: 0.2169
Epoch 24/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4932 - val_loss: 0.2176
Epoch 25/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4919 - val_loss: 0.2187
Epoch 26/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4910 - val_loss: 0.2186
Epoch 27/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4899 - val_loss: 0.2188
Epoch 28/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4892 - val_loss: 0.2191
Epoch 29/68
79/79 [==============================] - 2s 28ms/step - loss: 0.4883 - val_loss: 0.2193
Epoch 30/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4877 - val_loss: 0.2193
Execution time:  72.93636870384216
LSTM:
Mean Absolute Error: 0.2586
Root Mean Square Error: 0.6588
Mean Square Error: 0.4341

Train RMSE: 0.659
Train MSE: 0.434
Train MAE: 0.259
###########################

MODEL:  LSTM
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_45&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_45 (LSTM)               (None, 72, 55)            12540     
_________________________________________________________________
dropout_45 (Dropout)         (None, 72, 55)            0         
_________________________________________________________________
time_distributed_45 (TimeDis (None, 72, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 9s 26ms/step - loss: 0.5411 - val_loss: 0.4008
Epoch 2/36
349/349 [==============================] - 9s 25ms/step - loss: 0.5039 - val_loss: 0.3932
Epoch 3/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4976 - val_loss: 0.3910
Epoch 4/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4927 - val_loss: 0.3904
Epoch 5/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4893 - val_loss: 0.3884
Epoch 6/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4867 - val_loss: 0.3856
Epoch 7/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4850 - val_loss: 0.3800
Epoch 8/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4838 - val_loss: 0.3774
Epoch 9/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4814 - val_loss: 0.3731
Epoch 10/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4789 - val_loss: 0.3697
Epoch 11/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4741 - val_loss: 0.3660
Epoch 12/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4717 - val_loss: 0.3641
Epoch 13/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4697 - val_loss: 0.3634
Epoch 14/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4694 - val_loss: 0.3626
Epoch 15/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4684 - val_loss: 0.3622
Epoch 16/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4676 - val_loss: 0.3619
Epoch 17/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4676 - val_loss: 0.3615
Epoch 18/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4666 - val_loss: 0.3612
Epoch 19/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4659 - val_loss: 0.3608
Epoch 20/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4657 - val_loss: 0.3607
Epoch 21/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4656 - val_loss: 0.3604
Epoch 22/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4654 - val_loss: 0.3601
Epoch 23/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4652 - val_loss: 0.3597
Epoch 24/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4645 - val_loss: 0.3592
Epoch 25/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4637 - val_loss: 0.3591
Epoch 26/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4629 - val_loss: 0.3580
Epoch 27/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4611 - val_loss: 0.3580
Epoch 28/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4601 - val_loss: 0.3582
Epoch 29/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4598 - val_loss: 0.3583
Epoch 30/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4588 - val_loss: 0.3586
Epoch 31/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4584 - val_loss: 0.3583
Epoch 32/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4575 - val_loss: 0.3588
Epoch 33/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4566 - val_loss: 0.3584
Epoch 34/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4560 - val_loss: 0.3590
Epoch 35/36
349/349 [==============================] - 9s 25ms/step - loss: 0.4546 - val_loss: 0.3590
Epoch 36/36
349/349 [==============================] - 9s 26ms/step - loss: 0.4549 - val_loss: 0.3587
Execution time:  323.77091455459595
LSTM:
Mean Absolute Error: 0.2971
Root Mean Square Error: 0.7749
Mean Square Error: 0.6005

Train RMSE: 0.775
Train MSE: 0.601
Train MAE: 0.297
###########################

MODEL:  LSTM
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_46&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_46 (LSTM)               (None, 72, 40)            6720      
_________________________________________________________________
dropout_46 (Dropout)         (None, 72, 40)            0         
_________________________________________________________________
time_distributed_46 (TimeDis (None, 72, 1)             41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 35ms/step - loss: 0.8535 - val_loss: 1.0639
Epoch 2/68
79/79 [==============================] - 2s 28ms/step - loss: 0.7285 - val_loss: 0.8927
Epoch 3/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6871 - val_loss: 0.8505
Epoch 4/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6710 - val_loss: 0.8344
Epoch 5/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6630 - val_loss: 0.8256
Epoch 6/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6582 - val_loss: 0.8204
Epoch 7/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6550 - val_loss: 0.8168
Epoch 8/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6527 - val_loss: 0.8140
Epoch 9/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6506 - val_loss: 0.8119
Epoch 10/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6488 - val_loss: 0.8101
Epoch 11/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6472 - val_loss: 0.8086
Epoch 12/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6457 - val_loss: 0.8073
Epoch 13/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6445 - val_loss: 0.8062
Epoch 14/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6434 - val_loss: 0.8053
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6425 - val_loss: 0.8044
Epoch 16/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6416 - val_loss: 0.8037
Epoch 17/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6410 - val_loss: 0.8031
Epoch 18/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6401 - val_loss: 0.8025
Epoch 19/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6395 - val_loss: 0.8020
Epoch 20/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6389 - val_loss: 0.8015
Epoch 21/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6383 - val_loss: 0.8010
Epoch 22/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6379 - val_loss: 0.8006
Epoch 23/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6375 - val_loss: 0.8003
Epoch 24/68
79/79 [==============================] - 3s 32ms/step - loss: 0.6371 - val_loss: 0.7999
Epoch 25/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6367 - val_loss: 0.7996
Epoch 26/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6365 - val_loss: 0.7993
Epoch 27/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6360 - val_loss: 0.7991
Epoch 28/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6356 - val_loss: 0.7988
Epoch 29/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6353 - val_loss: 0.7986
Epoch 30/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6349 - val_loss: 0.7984
Epoch 31/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6346 - val_loss: 0.7981
Epoch 32/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6343 - val_loss: 0.7979
Epoch 33/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6341 - val_loss: 0.7977
Epoch 34/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6337 - val_loss: 0.7976
Epoch 35/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6335 - val_loss: 0.7974
Epoch 36/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6333 - val_loss: 0.7972
Epoch 37/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6328 - val_loss: 0.7971
Epoch 38/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6327 - val_loss: 0.7969
Epoch 39/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6325 - val_loss: 0.7968
Epoch 40/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6324 - val_loss: 0.7966
Epoch 41/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6320 - val_loss: 0.7965
Epoch 42/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6319 - val_loss: 0.7964
Epoch 43/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6316 - val_loss: 0.7962
Epoch 44/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6312 - val_loss: 0.7961
Epoch 45/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6314 - val_loss: 0.7960
Epoch 46/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6307 - val_loss: 0.7959
Epoch 47/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6306 - val_loss: 0.7958
Epoch 48/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6306 - val_loss: 0.7957
Epoch 49/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6301 - val_loss: 0.7956
Epoch 50/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6301 - val_loss: 0.7955
Epoch 51/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6297 - val_loss: 0.7954
Epoch 52/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6296 - val_loss: 0.7953
Epoch 53/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6294 - val_loss: 0.7952
Epoch 54/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6306 - val_loss: 0.7951
Epoch 55/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6296 - val_loss: 0.7951
Epoch 56/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6290 - val_loss: 0.7950
Epoch 57/68
79/79 [==============================] - 2s 32ms/step - loss: 0.6288 - val_loss: 0.7949
Epoch 58/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6293 - val_loss: 0.7948
Epoch 59/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6286 - val_loss: 0.7948
Epoch 60/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6289 - val_loss: 0.7947
Epoch 61/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6286 - val_loss: 0.7946
Epoch 62/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6283 - val_loss: 0.7946
Epoch 63/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6280 - val_loss: 0.7945
Epoch 64/68
79/79 [==============================] - 2s 32ms/step - loss: 0.6292 - val_loss: 0.7944
Epoch 65/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6282 - val_loss: 0.7944
Epoch 66/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6277 - val_loss: 0.7943
Epoch 67/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6274 - val_loss: 0.7943
Epoch 68/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6285 - val_loss: 0.7942
Execution time:  166.01413583755493
LSTM:
Mean Absolute Error: 0.5579
Root Mean Square Error: 0.8281
Mean Square Error: 0.6858

Train RMSE: 0.828
Train MSE: 0.686
Train MAE: 0.558
###########################

MODEL:  LSTM
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_47&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_47 (LSTM)               (None, 72, 55)            12540     
_________________________________________________________________
dropout_47 (Dropout)         (None, 72, 55)            0         
_________________________________________________________________
time_distributed_47 (TimeDis (None, 72, 1)             56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 9s 26ms/step - loss: 0.7222 - val_loss: 0.7592
Epoch 2/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6680 - val_loss: 0.7469
Epoch 3/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6619 - val_loss: 0.7404
Epoch 4/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6591 - val_loss: 0.7391
Epoch 5/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6583 - val_loss: 0.7375
Epoch 6/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6573 - val_loss: 0.7361
Epoch 7/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6566 - val_loss: 0.7351
Epoch 8/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6561 - val_loss: 0.7331
Epoch 9/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6556 - val_loss: 0.7323
Epoch 10/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6546 - val_loss: 0.7314
Epoch 11/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6539 - val_loss: 0.7303
Epoch 12/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6533 - val_loss: 0.7297
Epoch 13/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6525 - val_loss: 0.7287
Epoch 14/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6525 - val_loss: 0.7282
Epoch 15/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6517 - val_loss: 0.7276
Epoch 16/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6516 - val_loss: 0.7271
Epoch 17/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6515 - val_loss: 0.7269
Epoch 18/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6511 - val_loss: 0.7262
Epoch 19/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6511 - val_loss: 0.7259
Epoch 20/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6505 - val_loss: 0.7253
Epoch 21/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6506 - val_loss: 0.7250
Epoch 22/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6503 - val_loss: 0.7247
Epoch 23/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6495 - val_loss: 0.7245
Epoch 24/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6494 - val_loss: 0.7242
Epoch 25/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6493 - val_loss: 0.7239
Epoch 26/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7236
Epoch 27/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7233
Epoch 28/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6489 - val_loss: 0.7229
Epoch 29/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6491 - val_loss: 0.7224
Epoch 30/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6488 - val_loss: 0.7222
Epoch 31/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6487 - val_loss: 0.7221
Epoch 32/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6485 - val_loss: 0.7217
Epoch 33/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6481 - val_loss: 0.7216
Epoch 34/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6481 - val_loss: 0.7213
Epoch 35/36
349/349 [==============================] - 9s 25ms/step - loss: 0.6482 - val_loss: 0.7209
Epoch 36/36
349/349 [==============================] - 9s 26ms/step - loss: 0.6480 - val_loss: 0.7208
Execution time:  329.16143250465393
LSTM:
Mean Absolute Error: 0.6018
Root Mean Square Error: 0.8835
Mean Square Error: 0.7806

Train RMSE: 0.884
Train MSE: 0.781
Train MAE: 0.602
###########################

MODEL:  LSTM
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_48&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_48 (LSTM)               (None, 144, 40)           6720      
_________________________________________________________________
dropout_48 (Dropout)         (None, 144, 40)           0         
_________________________________________________________________
time_distributed_48 (TimeDis (None, 144, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 70ms/step - loss: 0.6232 - val_loss: 0.2472
Epoch 2/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5862 - val_loss: 0.2524
Epoch 3/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5795 - val_loss: 0.2529
Epoch 4/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5746 - val_loss: 0.2520
Epoch 5/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5711 - val_loss: 0.2506
Epoch 6/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5673 - val_loss: 0.2513
Epoch 7/68
77/77 [==============================] - 4s 51ms/step - loss: 0.5655 - val_loss: 0.2521
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5607 - val_loss: 0.2501
Epoch 9/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5960 - val_loss: 0.2546
Epoch 10/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5774 - val_loss: 0.2470
Epoch 11/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5631 - val_loss: 0.2495
Epoch 12/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5620 - val_loss: 0.2398
Epoch 13/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5603 - val_loss: 0.2433
Epoch 14/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5428 - val_loss: 0.2463
Epoch 15/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5451 - val_loss: 0.2430
Epoch 16/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5362 - val_loss: 0.2405
Epoch 17/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5365 - val_loss: 0.2440
Epoch 18/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5344 - val_loss: 0.2427
Epoch 19/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5298 - val_loss: 0.2419
Epoch 20/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5292 - val_loss: 0.2429
Epoch 21/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5282 - val_loss: 0.2424
Epoch 22/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5279 - val_loss: 0.2424
Execution time:  94.87232208251953
LSTM:
Mean Absolute Error: 0.3893
Root Mean Square Error: 0.8575
Mean Square Error: 0.7353

Train RMSE: 0.858
Train MSE: 0.735
Train MAE: 0.389
###########################

MODEL:  LSTM
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_49&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_49 (LSTM)               (None, 144, 55)           12540     
_________________________________________________________________
dropout_49 (Dropout)         (None, 144, 55)           0         
_________________________________________________________________
time_distributed_49 (TimeDis (None, 144, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 16s 48ms/step - loss: 0.6048 - val_loss: 0.4179
Epoch 2/36
342/342 [==============================] - 16s 47ms/step - loss: 0.5798 - val_loss: 0.4155
Epoch 3/36
342/342 [==============================] - 16s 47ms/step - loss: 0.5738 - val_loss: 0.4137
Epoch 4/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5685 - val_loss: 0.4116
Epoch 5/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5651 - val_loss: 0.4098
Epoch 6/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5630 - val_loss: 0.4085
Epoch 7/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5605 - val_loss: 0.4061
Epoch 8/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5595 - val_loss: 0.4038
Epoch 9/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5582 - val_loss: 0.3985
Epoch 10/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5567 - val_loss: 0.3928
Epoch 11/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5548 - val_loss: 0.3927
Epoch 12/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5480 - val_loss: 0.3855
Epoch 13/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5602 - val_loss: 0.3961
Epoch 14/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5422 - val_loss: 0.3817
Epoch 15/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5411 - val_loss: 0.3816
Epoch 16/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5395 - val_loss: 0.3834
Epoch 17/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5357 - val_loss: 0.3781
Epoch 18/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5303 - val_loss: 0.3805
Epoch 19/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5287 - val_loss: 0.3823
Epoch 20/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5276 - val_loss: 0.3822
Epoch 21/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5229 - val_loss: 0.3817
Epoch 22/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5306 - val_loss: 0.3804
Epoch 23/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5210 - val_loss: 0.3811
Epoch 24/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5224 - val_loss: 0.3830
Epoch 25/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5228 - val_loss: 0.3795
Epoch 26/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5183 - val_loss: 0.3792
Epoch 27/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5198 - val_loss: 0.3764
Epoch 28/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5176 - val_loss: 0.3749
Epoch 29/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5211 - val_loss: 0.3749
Epoch 30/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5159 - val_loss: 0.3725
Epoch 31/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5156 - val_loss: 0.3749
Epoch 32/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5106 - val_loss: 0.3746
Epoch 33/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5101 - val_loss: 0.3756
Epoch 34/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5089 - val_loss: 0.3793
Epoch 35/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5097 - val_loss: 0.3785
Epoch 36/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5112 - val_loss: 0.3758
Execution time:  602.3874583244324
LSTM:
Mean Absolute Error: 0.3919
Root Mean Square Error: 0.8977
Mean Square Error: 0.8058

Train RMSE: 0.898
Train MSE: 0.806
Train MAE: 0.392
###########################

MODEL:  LSTM
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_50&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_50 (LSTM)               (None, 144, 40)           6720      
_________________________________________________________________
dropout_50 (Dropout)         (None, 144, 40)           0         
_________________________________________________________________
time_distributed_50 (TimeDis (None, 144, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 60ms/step - loss: 0.8295 - val_loss: 0.8891
Epoch 2/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6965 - val_loss: 0.8180
Epoch 3/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6926 - val_loss: 0.8076
Epoch 4/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6892 - val_loss: 0.8031
Epoch 5/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6877 - val_loss: 0.8006
Epoch 6/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6801 - val_loss: 0.7987
Epoch 7/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6784 - val_loss: 0.7974
Epoch 8/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6839 - val_loss: 0.7966
Epoch 9/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6775 - val_loss: 0.7958
Epoch 10/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6810 - val_loss: 0.7953
Epoch 11/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6814 - val_loss: 0.7952
Epoch 12/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6988 - val_loss: 0.7949
Epoch 13/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6976 - val_loss: 0.7946
Epoch 14/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6971 - val_loss: 0.7944
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6969 - val_loss: 0.7942
Epoch 16/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6967 - val_loss: 0.7940
Epoch 17/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6966 - val_loss: 0.7938
Epoch 18/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6965 - val_loss: 0.7936
Epoch 19/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6964 - val_loss: 0.7934
Epoch 20/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6963 - val_loss: 0.7933
Epoch 21/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6962 - val_loss: 0.7931
Epoch 22/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6961 - val_loss: 0.7930
Epoch 23/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6960 - val_loss: 0.7929
Epoch 24/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6959 - val_loss: 0.7927
Epoch 25/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6959 - val_loss: 0.7926
Epoch 26/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6958 - val_loss: 0.7925
Epoch 27/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6957 - val_loss: 0.7924
Epoch 28/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6945 - val_loss: 0.7922
Epoch 29/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6656 - val_loss: 0.7919
Epoch 30/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6752 - val_loss: 0.7919
Epoch 31/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6624 - val_loss: 0.7918
Epoch 32/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6674 - val_loss: 0.7916
Epoch 33/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6717 - val_loss: 0.7916
Epoch 34/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6680 - val_loss: 0.7916
Epoch 35/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6649 - val_loss: 0.7915
Epoch 36/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6603 - val_loss: 0.7915
Epoch 37/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6693 - val_loss: 0.7914
Epoch 38/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6815 - val_loss: 0.7914
Epoch 39/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6951 - val_loss: 0.7913
Epoch 40/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6951 - val_loss: 0.7913
Epoch 41/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6950 - val_loss: 0.7912
Epoch 42/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6950 - val_loss: 0.7912
Epoch 43/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6950 - val_loss: 0.7912
Epoch 44/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6949 - val_loss: 0.7911
Epoch 45/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6949 - val_loss: 0.7911
Epoch 46/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6948 - val_loss: 0.7910
Epoch 47/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6948 - val_loss: 0.7910
Epoch 48/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6855 - val_loss: 0.7910
Epoch 49/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6659 - val_loss: 0.7908
Epoch 50/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6741 - val_loss: 0.7908
Epoch 51/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6672 - val_loss: 0.7907
Epoch 52/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6815 - val_loss: 0.7909
Epoch 53/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6795 - val_loss: 0.7909
Epoch 54/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6761 - val_loss: 0.7910
Epoch 55/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6722 - val_loss: 0.7910
Epoch 56/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6710 - val_loss: 0.7910
Epoch 57/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6713 - val_loss: 0.7910
Epoch 58/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6705 - val_loss: 0.7910
Epoch 59/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6708 - val_loss: 0.7910
Epoch 60/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6699 - val_loss: 0.7910
Epoch 61/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6703 - val_loss: 0.7909
Execution time:  253.6297149658203
LSTM:
Mean Absolute Error: 0.6309
Root Mean Square Error: 0.9198
Mean Square Error: 0.8460

Train RMSE: 0.920
Train MSE: 0.846
Train MAE: 0.631
###########################

MODEL:  LSTM
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_51&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_51 (LSTM)               (None, 144, 55)           12540     
_________________________________________________________________
dropout_51 (Dropout)         (None, 144, 55)           0         
_________________________________________________________________
time_distributed_51 (TimeDis (None, 144, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 17s 49ms/step - loss: 0.7332 - val_loss: 0.7220
Epoch 2/36
342/342 [==============================] - 17s 50ms/step - loss: 0.7004 - val_loss: 0.7191
Epoch 3/36
342/342 [==============================] - 17s 48ms/step - loss: 0.6934 - val_loss: 0.7159
Epoch 4/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6928 - val_loss: 0.7138
Epoch 5/36
342/342 [==============================] - 17s 48ms/step - loss: 0.6942 - val_loss: 0.7150
Epoch 6/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6906 - val_loss: 0.7136
Epoch 7/36
342/342 [==============================] - 17s 48ms/step - loss: 0.6900 - val_loss: 0.7126
Epoch 8/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6901 - val_loss: 0.7118
Epoch 9/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6889 - val_loss: 0.7116
Epoch 10/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6888 - val_loss: 0.7112
Epoch 11/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6878 - val_loss: 0.7105
Epoch 12/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6946 - val_loss: 0.7107
Epoch 13/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6888 - val_loss: 0.7100
Epoch 14/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6877 - val_loss: 0.7090
Epoch 15/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6951 - val_loss: 0.7073
Epoch 16/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6912 - val_loss: 0.7080
Epoch 17/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6825 - val_loss: 0.7076
Epoch 18/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6858 - val_loss: 0.7070
Epoch 19/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6849 - val_loss: 0.7066
Epoch 20/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6845 - val_loss: 0.7066
Epoch 21/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6997 - val_loss: 0.7051
Epoch 22/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6894 - val_loss: 0.7048
Epoch 23/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6836 - val_loss: 0.7046
Epoch 24/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6829 - val_loss: 0.7046
Epoch 25/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6833 - val_loss: 0.7040
Epoch 26/36
342/342 [==============================] - 17s 50ms/step - loss: 0.6806 - val_loss: 0.7032
Epoch 27/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6817 - val_loss: 0.7041
Epoch 28/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6793 - val_loss: 0.7024
Epoch 29/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6890 - val_loss: 0.7029
Epoch 30/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6766 - val_loss: 0.7027
Epoch 31/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6762 - val_loss: 0.7025
Epoch 32/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6756 - val_loss: 0.7022
Epoch 33/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6876 - val_loss: 0.7021
Epoch 34/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6781 - val_loss: 0.7022
Epoch 35/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6771 - val_loss: 0.7020
Epoch 36/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6759 - val_loss: 0.7019
Execution time:  606.6077654361725
LSTM:
Mean Absolute Error: 0.6711
Root Mean Square Error: 0.9762
Mean Square Error: 0.9530

Train RMSE: 0.976
Train MSE: 0.953
Train MAE: 0.671
###########################

MODEL:  LSTM
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_52&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_52 (LSTM)               (None, 144, 40)           6720      
_________________________________________________________________
dropout_52 (Dropout)         (None, 144, 40)           0         
_________________________________________________________________
time_distributed_52 (TimeDis (None, 144, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 61ms/step - loss: 0.6840 - val_loss: 0.7594
Epoch 2/68
77/77 [==============================] - 4s 55ms/step - loss: 0.6838 - val_loss: 0.7590
Epoch 3/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6838 - val_loss: 0.7586
Epoch 4/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6835 - val_loss: 0.7582
Epoch 5/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6833 - val_loss: 0.7577
Epoch 6/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6832 - val_loss: 0.7572
Epoch 7/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6830 - val_loss: 0.7567
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6829 - val_loss: 0.7562
Epoch 9/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6827 - val_loss: 0.7557
Epoch 10/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6826 - val_loss: 0.7552
Epoch 11/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6824 - val_loss: 0.7547
Epoch 12/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6822 - val_loss: 0.7541
Epoch 13/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6820 - val_loss: 0.7536
Epoch 14/68
77/77 [==============================] - 4s 55ms/step - loss: 0.6819 - val_loss: 0.7531
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6816 - val_loss: 0.7525
Epoch 16/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6814 - val_loss: 0.7520
Epoch 17/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6813 - val_loss: 0.7514
Epoch 18/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6811 - val_loss: 0.7509
Epoch 19/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6809 - val_loss: 0.7503
Epoch 20/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6807 - val_loss: 0.7498
Epoch 21/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6806 - val_loss: 0.7492
Epoch 22/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6804 - val_loss: 0.7487
Epoch 23/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6801 - val_loss: 0.7481
Epoch 24/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6800 - val_loss: 0.7476
Epoch 25/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6798 - val_loss: 0.7470
Epoch 26/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6795 - val_loss: 0.7464
Epoch 27/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6794 - val_loss: 0.7459
Epoch 28/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6792 - val_loss: 0.7453
Epoch 29/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6790 - val_loss: 0.7447
Epoch 30/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6788 - val_loss: 0.7441
Epoch 31/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6786 - val_loss: 0.7436
Epoch 32/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6784 - val_loss: 0.7430
Epoch 33/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6782 - val_loss: 0.7424
Epoch 34/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6780 - val_loss: 0.7418
Epoch 35/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6778 - val_loss: 0.7413
Epoch 36/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6776 - val_loss: 0.7407
Epoch 37/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6774 - val_loss: 0.7401
Epoch 38/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6772 - val_loss: 0.7395
Epoch 39/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6770 - val_loss: 0.7389
Epoch 40/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6768 - val_loss: 0.7383
Epoch 41/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6766 - val_loss: 0.7377
Epoch 42/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6764 - val_loss: 0.7372
Epoch 43/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6763 - val_loss: 0.7366
Epoch 44/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6760 - val_loss: 0.7360
Epoch 45/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6758 - val_loss: 0.7354
Epoch 46/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6756 - val_loss: 0.7348
Epoch 47/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6754 - val_loss: 0.7342
Epoch 48/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6753 - val_loss: 0.7336
Epoch 49/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6751 - val_loss: 0.7330
Epoch 50/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6748 - val_loss: 0.7324
Epoch 51/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6746 - val_loss: 0.7318
Epoch 52/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6745 - val_loss: 0.7312
Epoch 53/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6742 - val_loss: 0.7306
Epoch 54/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6741 - val_loss: 0.7300
Epoch 55/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6738 - val_loss: 0.7294
Epoch 56/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6736 - val_loss: 0.7288
Epoch 57/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6735 - val_loss: 0.7282
Epoch 58/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6732 - val_loss: 0.7276
Epoch 59/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6731 - val_loss: 0.7270
Epoch 60/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6729 - val_loss: 0.7264
Epoch 61/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6726 - val_loss: 0.7258
Epoch 62/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6724 - val_loss: 0.7252
Epoch 63/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6723 - val_loss: 0.7245
Epoch 64/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6720 - val_loss: 0.7239
Epoch 65/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6719 - val_loss: 0.7233
Epoch 66/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6716 - val_loss: 0.7227
Epoch 67/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6715 - val_loss: 0.7221
Epoch 68/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6712 - val_loss: 0.7215
Execution time:  283.50412583351135
LSTM:
Mean Absolute Error: 0.6551
Root Mean Square Error: 0.9536
Mean Square Error: 0.9094

Train RMSE: 0.954
Train MSE: 0.909
Train MAE: 0.655
###########################

MODEL:  LSTM
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_53&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_53 (LSTM)               (None, 144, 55)           12540     
_________________________________________________________________
dropout_53 (Dropout)         (None, 144, 55)           0         
_________________________________________________________________
time_distributed_53 (TimeDis (None, 144, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 16s 48ms/step - loss: 0.6932 - val_loss: 0.6855
Epoch 2/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6915 - val_loss: 0.6824
Epoch 3/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6896 - val_loss: 0.6791
Epoch 4/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6875 - val_loss: 0.6756
Epoch 5/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6854 - val_loss: 0.6720
Epoch 6/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6832 - val_loss: 0.6683
Epoch 7/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6810 - val_loss: 0.6646
Epoch 8/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6789 - val_loss: 0.6609
Epoch 9/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6767 - val_loss: 0.6572
Epoch 10/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6746 - val_loss: 0.6536
Epoch 11/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6725 - val_loss: 0.6499
Epoch 12/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6704 - val_loss: 0.6461
Epoch 13/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6683 - val_loss: 0.6424
Epoch 14/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6662 - val_loss: 0.6386
Epoch 15/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6641 - val_loss: 0.6347
Epoch 16/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6619 - val_loss: 0.6308
Epoch 17/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6597 - val_loss: 0.6268
Epoch 18/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6575 - val_loss: 0.6227
Epoch 19/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6552 - val_loss: 0.6186
Epoch 20/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6529 - val_loss: 0.6143
Epoch 21/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6505 - val_loss: 0.6099
Epoch 22/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6482 - val_loss: 0.6055
Epoch 23/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6457 - val_loss: 0.6009
Epoch 24/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6432 - val_loss: 0.5963
Epoch 25/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6406 - val_loss: 0.5914
Epoch 26/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6381 - val_loss: 0.5865
Epoch 27/36
342/342 [==============================] - 17s 50ms/step - loss: 0.6355 - val_loss: 0.5815
Epoch 28/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6328 - val_loss: 0.5764
Epoch 29/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6302 - val_loss: 0.5713
Epoch 30/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6275 - val_loss: 0.5660
Epoch 31/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6248 - val_loss: 0.5607
Epoch 32/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6221 - val_loss: 0.5553
Epoch 33/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6194 - val_loss: 0.5499
Epoch 34/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6167 - val_loss: 0.5445
Epoch 35/36
342/342 [==============================] - 17s 50ms/step - loss: 0.6141 - val_loss: 0.5391
Epoch 36/36
342/342 [==============================] - 17s 49ms/step - loss: 0.6115 - val_loss: 0.5337
Execution time:  608.9561471939087
LSTM:
Mean Absolute Error: 0.5098
Root Mean Square Error: 0.8439
Mean Square Error: 0.7122

Train RMSE: 0.844
Train MSE: 0.712
Train MAE: 0.510
###########################

MODEL:  LSTM
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_54&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_54 (LSTM)               (None, 144, 40)           6720      
_________________________________________________________________
dropout_54 (Dropout)         (None, 144, 40)           0         
_________________________________________________________________
time_distributed_54 (TimeDis (None, 144, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 60ms/step - loss: 0.9011 - val_loss: 1.2813
Epoch 2/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9010 - val_loss: 1.2811
Epoch 3/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9010 - val_loss: 1.2810
Epoch 4/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9009 - val_loss: 1.2808
Epoch 5/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9009 - val_loss: 1.2806
Epoch 6/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9008 - val_loss: 1.2805
Epoch 7/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9007 - val_loss: 1.2803
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9007 - val_loss: 1.2801
Epoch 9/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9006 - val_loss: 1.2799
Epoch 10/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9005 - val_loss: 1.2797
Epoch 11/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9004 - val_loss: 1.2796
Epoch 12/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9004 - val_loss: 1.2794
Epoch 13/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9003 - val_loss: 1.2792
Epoch 14/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9002 - val_loss: 1.2790
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9001 - val_loss: 1.2788
Epoch 16/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9001 - val_loss: 1.2786
Epoch 17/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9000 - val_loss: 1.2784
Epoch 18/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8999 - val_loss: 1.2782
Epoch 19/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8998 - val_loss: 1.2780
Epoch 20/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8998 - val_loss: 1.2778
Epoch 21/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8997 - val_loss: 1.2776
Epoch 22/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8996 - val_loss: 1.2774
Epoch 23/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8995 - val_loss: 1.2771
Epoch 24/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8994 - val_loss: 1.2769
Epoch 25/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8994 - val_loss: 1.2767
Epoch 26/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8993 - val_loss: 1.2765
Epoch 27/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8992 - val_loss: 1.2763
Epoch 28/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8991 - val_loss: 1.2761
Epoch 29/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8990 - val_loss: 1.2759
Epoch 30/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2757
Epoch 31/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2754
Epoch 32/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8988 - val_loss: 1.2752
Epoch 33/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8987 - val_loss: 1.2750
Epoch 34/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8986 - val_loss: 1.2748
Epoch 35/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8985 - val_loss: 1.2746
Epoch 36/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8984 - val_loss: 1.2743
Epoch 37/68
77/77 [==============================] - 4s 54ms/step - loss: 0.8984 - val_loss: 1.2741
Epoch 38/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8983 - val_loss: 1.2739
Epoch 39/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8982 - val_loss: 1.2737
Epoch 40/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8981 - val_loss: 1.2735
Epoch 41/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8980 - val_loss: 1.2732
Epoch 42/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8979 - val_loss: 1.2730
Epoch 43/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8978 - val_loss: 1.2728
Epoch 44/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8977 - val_loss: 1.2726
Epoch 45/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8976 - val_loss: 1.2723
Epoch 46/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8976 - val_loss: 1.2721
Epoch 47/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8975 - val_loss: 1.2719
Epoch 48/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8974 - val_loss: 1.2716
Epoch 49/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8973 - val_loss: 1.2714
Epoch 50/68
77/77 [==============================] - 4s 51ms/step - loss: 0.8972 - val_loss: 1.2712
Epoch 51/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8971 - val_loss: 1.2709
Epoch 52/68
77/77 [==============================] - 4s 54ms/step - loss: 0.8970 - val_loss: 1.2707
Epoch 53/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8969 - val_loss: 1.2705
Epoch 54/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8968 - val_loss: 1.2702
Epoch 55/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8967 - val_loss: 1.2700
Epoch 56/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8966 - val_loss: 1.2698
Epoch 57/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8965 - val_loss: 1.2695
Epoch 58/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8964 - val_loss: 1.2693
Epoch 59/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8964 - val_loss: 1.2691
Epoch 60/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8963 - val_loss: 1.2688
Epoch 61/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8962 - val_loss: 1.2686
Epoch 62/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8961 - val_loss: 1.2683
Epoch 63/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8959 - val_loss: 1.2681
Epoch 64/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8959 - val_loss: 1.2679
Epoch 65/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8958 - val_loss: 1.2676
Epoch 66/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8957 - val_loss: 1.2674
Epoch 67/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8956 - val_loss: 1.2671
Epoch 68/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8955 - val_loss: 1.2669
Execution time:  281.46847128868103
LSTM:
Mean Absolute Error: 0.9132
Root Mean Square Error: 1.1095
Mean Square Error: 1.2309

Train RMSE: 1.109
Train MSE: 1.231
Train MAE: 0.913
###########################

MODEL:  LSTM
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_55&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_55 (LSTM)               (None, 144, 55)           12540     
_________________________________________________________________
dropout_55 (Dropout)         (None, 144, 55)           0         
_________________________________________________________________
time_distributed_55 (TimeDis (None, 144, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8935 - val_loss: 1.1304
Epoch 2/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8932 - val_loss: 1.1299
Epoch 3/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8929 - val_loss: 1.1293
Epoch 4/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8926 - val_loss: 1.1287
Epoch 5/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8922 - val_loss: 1.1280
Epoch 6/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8919 - val_loss: 1.1273
Epoch 7/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8915 - val_loss: 1.1267
Epoch 8/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8911 - val_loss: 1.1259
Epoch 9/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8907 - val_loss: 1.1252
Epoch 10/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8903 - val_loss: 1.1245
Epoch 11/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8899 - val_loss: 1.1237
Epoch 12/36
342/342 [==============================] - 16s 45ms/step - loss: 0.8895 - val_loss: 1.1229
Epoch 13/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8890 - val_loss: 1.1221
Epoch 14/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8886 - val_loss: 1.1213
Epoch 15/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8881 - val_loss: 1.1205
Epoch 16/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8877 - val_loss: 1.1196
Epoch 17/36
342/342 [==============================] - 16s 45ms/step - loss: 0.8872 - val_loss: 1.1188
Epoch 18/36
342/342 [==============================] - 16s 45ms/step - loss: 0.8867 - val_loss: 1.1179
Epoch 19/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8862 - val_loss: 1.1170
Epoch 20/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8857 - val_loss: 1.1160
Epoch 21/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8852 - val_loss: 1.1151
Epoch 22/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8847 - val_loss: 1.1141
Epoch 23/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8841 - val_loss: 1.1131
Epoch 24/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8836 - val_loss: 1.1121
Epoch 25/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8830 - val_loss: 1.1111
Epoch 26/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8824 - val_loss: 1.1100
Epoch 27/36
342/342 [==============================] - 16s 45ms/step - loss: 0.8819 - val_loss: 1.1089
Epoch 28/36
342/342 [==============================] - 16s 45ms/step - loss: 0.8812 - val_loss: 1.1078
Epoch 29/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8806 - val_loss: 1.1066
Epoch 30/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8800 - val_loss: 1.1055
Epoch 31/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8793 - val_loss: 1.1042
Epoch 32/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8786 - val_loss: 1.1030
Epoch 33/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8779 - val_loss: 1.1017
Epoch 34/36
342/342 [==============================] - 16s 46ms/step - loss: 0.8772 - val_loss: 1.1004
Epoch 35/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8765 - val_loss: 1.0990
Epoch 36/36
342/342 [==============================] - 15s 45ms/step - loss: 0.8757 - val_loss: 1.0976
Execution time:  562.3713281154633
LSTM:
Mean Absolute Error: 0.8973
Root Mean Square Error: 1.0944
Mean Square Error: 1.1978

Train RMSE: 1.094
Train MSE: 1.198
Train MAE: 0.897
###########################

MODEL:  LSTM
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_56&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_56 (LSTM)               (None, 144, 40)           6720      
_________________________________________________________________
dropout_56 (Dropout)         (None, 144, 40)           0         
_________________________________________________________________
time_distributed_56 (TimeDis (None, 144, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 64ms/step - loss: 0.6613 - val_loss: 0.4499
Epoch 2/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5872 - val_loss: 0.2570
Epoch 3/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5855 - val_loss: 0.2596
Epoch 4/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5816 - val_loss: 0.2567
Epoch 5/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5791 - val_loss: 0.2550
Epoch 6/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5767 - val_loss: 0.2532
Epoch 7/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5746 - val_loss: 0.2519
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5727 - val_loss: 0.2515
Epoch 9/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5705 - val_loss: 0.2518
Epoch 10/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5679 - val_loss: 0.2522
Epoch 11/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5641 - val_loss: 0.2536
Epoch 12/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5590 - val_loss: 0.2566
Epoch 13/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5518 - val_loss: 0.2621
Epoch 14/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5453 - val_loss: 0.2629
Epoch 15/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5414 - val_loss: 0.2630
Epoch 16/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5390 - val_loss: 0.2629
Epoch 17/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5373 - val_loss: 0.2622
Epoch 18/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5360 - val_loss: 0.2614
Execution time:  77.334885597229
LSTM:
Mean Absolute Error: 0.3890
Root Mean Square Error: 0.8297
Mean Square Error: 0.6883

Train RMSE: 0.830
Train MSE: 0.688
Train MAE: 0.389
###########################

MODEL:  LSTM
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_57&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_57 (LSTM)               (None, 144, 55)           12540     
_________________________________________________________________
dropout_57 (Dropout)         (None, 144, 55)           0         
_________________________________________________________________
time_distributed_57 (TimeDis (None, 144, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 16s 48ms/step - loss: 0.6009 - val_loss: 0.4123
Epoch 2/36
342/342 [==============================] - 16s 47ms/step - loss: 0.5773 - val_loss: 0.4180
Epoch 3/36
342/342 [==============================] - 16s 48ms/step - loss: 0.5724 - val_loss: 0.4196
Epoch 4/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5684 - val_loss: 0.4192
Epoch 5/36
342/342 [==============================] - 16s 48ms/step - loss: 0.5659 - val_loss: 0.4184
Epoch 6/36
342/342 [==============================] - 16s 48ms/step - loss: 0.5640 - val_loss: 0.4170
Epoch 7/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5621 - val_loss: 0.4151
Epoch 8/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5604 - val_loss: 0.4139
Epoch 9/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5590 - val_loss: 0.4109
Epoch 10/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5571 - val_loss: 0.4063
Epoch 11/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5552 - val_loss: 0.3969
Epoch 12/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5515 - val_loss: 0.3851
Epoch 13/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5448 - val_loss: 0.3754
Epoch 14/36
342/342 [==============================] - 16s 48ms/step - loss: 0.5350 - val_loss: 0.3748
Epoch 15/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5289 - val_loss: 0.3764
Epoch 16/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5188 - val_loss: 0.3771
Epoch 17/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5179 - val_loss: 0.3774
Epoch 18/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5188 - val_loss: 0.3771
Epoch 19/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5202 - val_loss: 0.3761
Epoch 20/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5175 - val_loss: 0.3768
Epoch 21/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5179 - val_loss: 0.3767
Epoch 22/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5162 - val_loss: 0.3767
Epoch 23/36
342/342 [==============================] - 17s 49ms/step - loss: 0.5158 - val_loss: 0.3768
Epoch 24/36
342/342 [==============================] - 17s 48ms/step - loss: 0.5136 - val_loss: 0.3785
Execution time:  400.83725786209106
LSTM:
Mean Absolute Error: 0.3808
Root Mean Square Error: 0.8723
Mean Square Error: 0.7609

Train RMSE: 0.872
Train MSE: 0.761
Train MAE: 0.381
###########################

MODEL:  LSTM
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_58&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_58 (LSTM)               (None, 144, 40)           6720      
_________________________________________________________________
dropout_58 (Dropout)         (None, 144, 40)           0         
_________________________________________________________________
time_distributed_58 (TimeDis (None, 144, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 59ms/step - loss: 0.8856 - val_loss: 1.1479
Epoch 2/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7761 - val_loss: 0.9216
Epoch 3/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7284 - val_loss: 0.8666
Epoch 4/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7135 - val_loss: 0.8446
Epoch 5/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7041 - val_loss: 0.8320
Epoch 6/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6973 - val_loss: 0.8239
Epoch 7/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6928 - val_loss: 0.8182
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6897 - val_loss: 0.8140
Epoch 9/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6873 - val_loss: 0.8109
Epoch 10/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6852 - val_loss: 0.8086
Epoch 11/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6832 - val_loss: 0.8069
Epoch 12/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6812 - val_loss: 0.8055
Epoch 13/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6792 - val_loss: 0.8044
Epoch 14/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6768 - val_loss: 0.8034
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6747 - val_loss: 0.8026
Epoch 16/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6724 - val_loss: 0.8019
Epoch 17/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6696 - val_loss: 0.8013
Epoch 18/68
77/77 [==============================] - 4s 50ms/step - loss: 0.6677 - val_loss: 0.8009
Epoch 19/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6622 - val_loss: 0.8004
Epoch 20/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6617 - val_loss: 0.8000
Epoch 21/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6606 - val_loss: 0.7996
Epoch 22/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6597 - val_loss: 0.7993
Epoch 23/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6593 - val_loss: 0.7990
Epoch 24/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6577 - val_loss: 0.7987
Epoch 25/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6575 - val_loss: 0.7985
Epoch 26/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6568 - val_loss: 0.7983
Epoch 27/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6561 - val_loss: 0.7981
Epoch 28/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6555 - val_loss: 0.7978
Epoch 29/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6551 - val_loss: 0.7976
Epoch 30/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6533 - val_loss: 0.7975
Epoch 31/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6523 - val_loss: 0.7972
Epoch 32/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6528 - val_loss: 0.7972
Epoch 33/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6502 - val_loss: 0.7971
Epoch 34/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6498 - val_loss: 0.7969
Epoch 35/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6496 - val_loss: 0.7968
Epoch 36/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6494 - val_loss: 0.7967
Epoch 37/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6491 - val_loss: 0.7966
Epoch 38/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6486 - val_loss: 0.7965
Epoch 39/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6482 - val_loss: 0.7964
Epoch 40/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6481 - val_loss: 0.7963
Epoch 41/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6478 - val_loss: 0.7963
Epoch 42/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6478 - val_loss: 0.7962
Epoch 43/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6482 - val_loss: 0.7961
Epoch 44/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6486 - val_loss: 0.7960
Epoch 45/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6469 - val_loss: 0.7960
Epoch 46/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6472 - val_loss: 0.7959
Epoch 47/68
77/77 [==============================] - 4s 50ms/step - loss: 0.6467 - val_loss: 0.7958
Epoch 48/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6468 - val_loss: 0.7958
Epoch 49/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6468 - val_loss: 0.7957
Epoch 50/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6465 - val_loss: 0.7957
Epoch 51/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6467 - val_loss: 0.7957
Epoch 52/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6452 - val_loss: 0.7956
Epoch 53/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6448 - val_loss: 0.7955
Epoch 54/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6474 - val_loss: 0.7955
Epoch 55/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6473 - val_loss: 0.7954
Epoch 56/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6458 - val_loss: 0.7954
Epoch 57/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6480 - val_loss: 0.7954
Epoch 58/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6454 - val_loss: 0.7953
Epoch 59/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6450 - val_loss: 0.7953
Epoch 60/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6441 - val_loss: 0.7952
Epoch 61/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6438 - val_loss: 0.7952
Epoch 62/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6437 - val_loss: 0.7952
Epoch 63/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6439 - val_loss: 0.7951
Epoch 64/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6437 - val_loss: 0.7951
Epoch 65/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6436 - val_loss: 0.7950
Epoch 66/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6430 - val_loss: 0.7950
Epoch 67/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6416 - val_loss: 0.7949
Epoch 68/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6429 - val_loss: 0.7949
Execution time:  280.67818784713745
LSTM:
Mean Absolute Error: 0.6319
Root Mean Square Error: 0.9309
Mean Square Error: 0.8666

Train RMSE: 0.931
Train MSE: 0.867
Train MAE: 0.632
###########################

MODEL:  LSTM
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_59&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_59 (LSTM)               (None, 144, 55)           12540     
_________________________________________________________________
dropout_59 (Dropout)         (None, 144, 55)           0         
_________________________________________________________________
time_distributed_59 (TimeDis (None, 144, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 16s 47ms/step - loss: 0.7371 - val_loss: 0.7355
Epoch 2/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6908 - val_loss: 0.7269
Epoch 3/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6881 - val_loss: 0.7233
Epoch 4/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6892 - val_loss: 0.7220
Epoch 5/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6854 - val_loss: 0.7205
Epoch 6/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6848 - val_loss: 0.7194
Epoch 7/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6834 - val_loss: 0.7189
Epoch 8/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6827 - val_loss: 0.7185
Epoch 9/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6833 - val_loss: 0.7179
Epoch 10/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6830 - val_loss: 0.7174
Epoch 11/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6827 - val_loss: 0.7171
Epoch 12/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6830 - val_loss: 0.7166
Epoch 13/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6826 - val_loss: 0.7160
Epoch 14/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6816 - val_loss: 0.7158
Epoch 15/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6812 - val_loss: 0.7155
Epoch 16/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6814 - val_loss: 0.7149
Epoch 17/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6827 - val_loss: 0.7148
Epoch 18/36
342/342 [==============================] - 16s 47ms/step - loss: 0.6802 - val_loss: 0.7145
Epoch 19/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6801 - val_loss: 0.7140
Epoch 20/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6801 - val_loss: 0.7138
Epoch 21/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6804 - val_loss: 0.7134
Epoch 22/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6801 - val_loss: 0.7133
Epoch 23/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7130
Epoch 24/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7128
Epoch 25/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7125
Epoch 26/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7123
Epoch 27/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6791 - val_loss: 0.7121
Epoch 28/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6788 - val_loss: 0.7118
Epoch 29/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7112
Epoch 30/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6789 - val_loss: 0.7109
Epoch 31/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6790 - val_loss: 0.7105
Epoch 32/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6666 - val_loss: 0.7103
Epoch 33/36
342/342 [==============================] - 16s 47ms/step - loss: 0.6666 - val_loss: 0.7101
Epoch 34/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6667 - val_loss: 0.7099
Epoch 35/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6668 - val_loss: 0.7097
Epoch 36/36
342/342 [==============================] - 16s 46ms/step - loss: 0.6667 - val_loss: 0.7095
Execution time:  571.1410245895386
LSTM:
Mean Absolute Error: 0.6455
Root Mean Square Error: 0.9380
Mean Square Error: 0.8798

Train RMSE: 0.938
Train MSE: 0.880
Train MAE: 0.646
###########################

MODEL:  LSTM
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_60&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_60 (LSTM)               (None, 432, 40)           6720      
_________________________________________________________________
dropout_60 (Dropout)         (None, 432, 40)           0         
_________________________________________________________________
time_distributed_60 (TimeDis (None, 432, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6438 - val_loss: 0.3788
Epoch 2/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6266 - val_loss: 0.3196
Epoch 3/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6227 - val_loss: 0.3657
Epoch 4/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6168 - val_loss: 0.3678
Epoch 5/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6146 - val_loss: 0.3690
Epoch 6/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6130 - val_loss: 0.3703
Epoch 7/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6117 - val_loss: 0.3709
Epoch 8/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6106 - val_loss: 0.3711
Epoch 9/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6098 - val_loss: 0.3714
Epoch 10/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6090 - val_loss: 0.3716
Epoch 11/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6084 - val_loss: 0.3716
Epoch 12/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6078 - val_loss: 0.3719
Execution time:  163.69914937019348
LSTM:
Mean Absolute Error: 0.5339
Root Mean Square Error: 0.9933
Mean Square Error: 0.9867

Train RMSE: 0.993
Train MSE: 0.987
Train MAE: 0.534
###########################

MODEL:  LSTM
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_61&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_61 (LSTM)               (None, 432, 55)           12540     
_________________________________________________________________
dropout_61 (Dropout)         (None, 432, 55)           0         
_________________________________________________________________
time_distributed_61 (TimeDis (None, 432, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 43s 136ms/step - loss: 0.6733 - val_loss: 0.3087
Epoch 2/36
317/317 [==============================] - 43s 135ms/step - loss: 0.6685 - val_loss: 0.2731
Epoch 3/36
317/317 [==============================] - 43s 135ms/step - loss: 0.6677 - val_loss: 0.2682
Epoch 4/36
317/317 [==============================] - 43s 136ms/step - loss: 0.6562 - val_loss: 0.2692
Epoch 5/36
317/317 [==============================] - 43s 136ms/step - loss: 0.6542 - val_loss: 0.2694
Epoch 6/36
317/317 [==============================] - 43s 137ms/step - loss: 0.6546 - val_loss: 0.2722
Epoch 7/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6545 - val_loss: 0.2748
Epoch 8/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6540 - val_loss: 0.2811
Epoch 9/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6527 - val_loss: 0.2806
Epoch 10/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6521 - val_loss: 0.2844
Epoch 11/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6513 - val_loss: 0.2840
Epoch 12/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6506 - val_loss: 0.2725
Epoch 13/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6507 - val_loss: 0.2721
Execution time:  558.6593346595764
LSTM:
Mean Absolute Error: 0.5364
Root Mean Square Error: 1.0084
Mean Square Error: 1.0168

Train RMSE: 1.008
Train MSE: 1.017
Train MAE: 0.536
###########################

MODEL:  LSTM
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_62&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_62 (LSTM)               (None, 432, 40)           6720      
_________________________________________________________________
dropout_62 (Dropout)         (None, 432, 40)           0         
_________________________________________________________________
time_distributed_62 (TimeDis (None, 432, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 13s 180ms/step - loss: 0.8786 - val_loss: 1.0165
Epoch 2/68
72/72 [==============================] - 13s 185ms/step - loss: 0.7079 - val_loss: 0.8192
Epoch 3/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6808 - val_loss: 0.7994
Epoch 4/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6767 - val_loss: 0.7932
Epoch 5/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6749 - val_loss: 0.7900
Epoch 6/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6738 - val_loss: 0.7881
Epoch 7/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6731 - val_loss: 0.7867
Epoch 8/68
72/72 [==============================] - 13s 187ms/step - loss: 0.6726 - val_loss: 0.7858
Epoch 9/68
72/72 [==============================] - 14s 191ms/step - loss: 0.6723 - val_loss: 0.7850
Epoch 10/68
72/72 [==============================] - 13s 187ms/step - loss: 0.6720 - val_loss: 0.7845
Epoch 11/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6717 - val_loss: 0.7840
Epoch 12/68
72/72 [==============================] - 14s 190ms/step - loss: 0.6715 - val_loss: 0.7836
Epoch 13/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6714 - val_loss: 0.7833
Epoch 14/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6712 - val_loss: 0.7830
Epoch 15/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6711 - val_loss: 0.7828
Epoch 16/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6710 - val_loss: 0.7826
Epoch 17/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6709 - val_loss: 0.7824
Epoch 18/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6708 - val_loss: 0.7822
Epoch 19/68
72/72 [==============================] - 13s 187ms/step - loss: 0.6708 - val_loss: 0.7821
Epoch 20/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6707 - val_loss: 0.7820
Epoch 21/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6706 - val_loss: 0.7818
Epoch 22/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6706 - val_loss: 0.7817
Epoch 23/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6705 - val_loss: 0.7816
Epoch 24/68
72/72 [==============================] - 13s 187ms/step - loss: 0.6705 - val_loss: 0.7815
Epoch 25/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6705 - val_loss: 0.7815
Epoch 26/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6704 - val_loss: 0.7814
Epoch 27/68
72/72 [==============================] - 14s 189ms/step - loss: 0.6704 - val_loss: 0.7813
Epoch 28/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6703 - val_loss: 0.7812
Epoch 29/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7812
Epoch 30/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7811
Epoch 31/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6703 - val_loss: 0.7811
Epoch 32/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7810
Epoch 33/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7809
Epoch 34/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6702 - val_loss: 0.7809
Epoch 35/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7809
Epoch 36/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6701 - val_loss: 0.7808
Epoch 37/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7808
Epoch 38/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 39/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 40/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 41/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7806
Epoch 42/68
72/72 [==============================] - 14s 197ms/step - loss: 0.6700 - val_loss: 0.7806
Epoch 43/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7806
Epoch 44/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 45/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 46/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 47/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6700 - val_loss: 0.7804
Epoch 48/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6699 - val_loss: 0.7804
Epoch 49/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6699 - val_loss: 0.7804
Epoch 50/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6699 - val_loss: 0.7804
Epoch 51/68
72/72 [==============================] - 14s 198ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 52/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 53/68
72/72 [==============================] - 14s 198ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 54/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 55/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 56/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 57/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7802
Epoch 58/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7802
Epoch 59/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7802
Epoch 60/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7802
Epoch 61/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 62/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 63/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 64/68
72/72 [==============================] - 14s 198ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 65/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 66/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 67/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 68/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7800
Execution time:  965.790694475174
LSTM:
Mean Absolute Error: 0.6863
Root Mean Square Error: 0.9969
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  LSTM
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_63&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_63 (LSTM)               (None, 432, 55)           12540     
_________________________________________________________________
dropout_63 (Dropout)         (None, 432, 55)           0         
_________________________________________________________________
time_distributed_63 (TimeDis (None, 432, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 42s 133ms/step - loss: 0.7745 - val_loss: 0.6149
Epoch 2/36
317/317 [==============================] - 43s 135ms/step - loss: 0.7011 - val_loss: 0.6123
Epoch 3/36
317/317 [==============================] - 42s 132ms/step - loss: 0.7005 - val_loss: 0.6114
Epoch 4/36
317/317 [==============================] - 42s 132ms/step - loss: 0.7002 - val_loss: 0.6109
Epoch 5/36
317/317 [==============================] - 42s 132ms/step - loss: 0.7000 - val_loss: 0.6105
Epoch 6/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6999 - val_loss: 0.6102
Epoch 7/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6998 - val_loss: 0.6100
Epoch 8/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6997 - val_loss: 0.6097
Epoch 9/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6996 - val_loss: 0.6095
Epoch 10/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6995 - val_loss: 0.6093
Epoch 11/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6995 - val_loss: 0.6092
Epoch 12/36
317/317 [==============================] - 42s 131ms/step - loss: 0.6994 - val_loss: 0.6090
Epoch 13/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6993 - val_loss: 0.6089
Epoch 14/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6993 - val_loss: 0.6087
Epoch 15/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6992 - val_loss: 0.6086
Epoch 16/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6992 - val_loss: 0.6085
Epoch 17/36
317/317 [==============================] - 42s 131ms/step - loss: 0.6991 - val_loss: 0.6084
Epoch 18/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6991 - val_loss: 0.6083
Epoch 19/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6991 - val_loss: 0.6082
Epoch 20/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6990 - val_loss: 0.6081
Epoch 21/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6990 - val_loss: 0.6080
Epoch 22/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6990 - val_loss: 0.6079
Epoch 23/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6990 - val_loss: 0.6079
Epoch 24/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6989 - val_loss: 0.6078
Epoch 25/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6078
Epoch 26/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 27/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 28/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 29/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 30/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 31/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 32/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 33/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 34/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 35/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 36/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6988 - val_loss: 0.6075
Execution time:  1521.076640844345
LSTM:
Mean Absolute Error: 0.6862
Root Mean Square Error: 0.9968
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  LSTM
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_64&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_64 (LSTM)               (None, 432, 40)           6720      
_________________________________________________________________
dropout_64 (Dropout)         (None, 432, 40)           0         
_________________________________________________________________
time_distributed_64 (TimeDis (None, 432, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6702 - val_loss: 0.7792
Epoch 2/68
72/72 [==============================] - 14s 192ms/step - loss: 0.6702 - val_loss: 0.7788
Epoch 3/68
72/72 [==============================] - 14s 193ms/step - loss: 0.6700 - val_loss: 0.7785
Epoch 4/68
72/72 [==============================] - 14s 192ms/step - loss: 0.6700 - val_loss: 0.7781
Epoch 5/68
72/72 [==============================] - 14s 198ms/step - loss: 0.6699 - val_loss: 0.7778
Epoch 6/68
72/72 [==============================] - 14s 197ms/step - loss: 0.6697 - val_loss: 0.7774
Epoch 7/68
72/72 [==============================] - 14s 197ms/step - loss: 0.6696 - val_loss: 0.7770
Epoch 8/68
72/72 [==============================] - 14s 197ms/step - loss: 0.6696 - val_loss: 0.7766
Epoch 9/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6694 - val_loss: 0.7763
Epoch 10/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6693 - val_loss: 0.7759
Epoch 11/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6692 - val_loss: 0.7755
Epoch 12/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6691 - val_loss: 0.7751
Epoch 13/68
72/72 [==============================] - 15s 204ms/step - loss: 0.6690 - val_loss: 0.7747
Epoch 14/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6689 - val_loss: 0.7743
Epoch 15/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6687 - val_loss: 0.7739
Epoch 16/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6687 - val_loss: 0.7735
Epoch 17/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6685 - val_loss: 0.7731
Epoch 18/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6684 - val_loss: 0.7727
Epoch 19/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6683 - val_loss: 0.7723
Epoch 20/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6682 - val_loss: 0.7719
Epoch 21/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6680 - val_loss: 0.7715
Epoch 22/68
72/72 [==============================] - 15s 205ms/step - loss: 0.6679 - val_loss: 0.7711
Epoch 23/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6678 - val_loss: 0.7706
Epoch 24/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6677 - val_loss: 0.7702
Epoch 25/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6676 - val_loss: 0.7698
Epoch 26/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6674 - val_loss: 0.7694
Epoch 27/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6673 - val_loss: 0.7690
Epoch 28/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6672 - val_loss: 0.7686
Epoch 29/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6671 - val_loss: 0.7681
Epoch 30/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6670 - val_loss: 0.7677
Epoch 31/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6668 - val_loss: 0.7673
Epoch 32/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6667 - val_loss: 0.7669
Epoch 33/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6666 - val_loss: 0.7664
Epoch 34/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6664 - val_loss: 0.7660
Epoch 35/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6663 - val_loss: 0.7656
Epoch 36/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6662 - val_loss: 0.7652
Epoch 37/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6661 - val_loss: 0.7647
Epoch 38/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6659 - val_loss: 0.7643
Epoch 39/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6658 - val_loss: 0.7639
Epoch 40/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6657 - val_loss: 0.7635
Epoch 41/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6656 - val_loss: 0.7630
Epoch 42/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6655 - val_loss: 0.7626
Epoch 43/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6653 - val_loss: 0.7622
Epoch 44/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6652 - val_loss: 0.7617
Epoch 45/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6651 - val_loss: 0.7613
Epoch 46/68
72/72 [==============================] - 15s 204ms/step - loss: 0.6650 - val_loss: 0.7609
Epoch 47/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6648 - val_loss: 0.7604
Epoch 48/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6647 - val_loss: 0.7600
Epoch 49/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6645 - val_loss: 0.7596
Epoch 50/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6644 - val_loss: 0.7591
Epoch 51/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6643 - val_loss: 0.7587
Epoch 52/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6642 - val_loss: 0.7583
Epoch 53/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6640 - val_loss: 0.7578
Epoch 54/68
72/72 [==============================] - 15s 204ms/step - loss: 0.6639 - val_loss: 0.7574
Epoch 55/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6638 - val_loss: 0.7570
Epoch 56/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6637 - val_loss: 0.7565
Epoch 57/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6635 - val_loss: 0.7561
Epoch 58/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6634 - val_loss: 0.7557
Epoch 59/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6633 - val_loss: 0.7552
Epoch 60/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6632 - val_loss: 0.7548
Epoch 61/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6630 - val_loss: 0.7543
Epoch 62/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6629 - val_loss: 0.7539
Epoch 63/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6627 - val_loss: 0.7535
Epoch 64/68
72/72 [==============================] - 15s 207ms/step - loss: 0.6626 - val_loss: 0.7530
Epoch 65/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6625 - val_loss: 0.7526
Epoch 66/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6624 - val_loss: 0.7522
Epoch 67/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6622 - val_loss: 0.7517
Epoch 68/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6621 - val_loss: 0.7513
Execution time:  1001.7487716674805
LSTM:
Mean Absolute Error: 0.6632
Root Mean Square Error: 0.9771
Mean Square Error: 0.9546

Train RMSE: 0.977
Train MSE: 0.955
Train MAE: 0.663
###########################

MODEL:  LSTM
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_65&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_65 (LSTM)               (None, 432, 55)           12540     
_________________________________________________________________
dropout_65 (Dropout)         (None, 432, 55)           0         
_________________________________________________________________
time_distributed_65 (TimeDis (None, 432, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6969 - val_loss: 0.5985
Epoch 2/36
317/317 [==============================] - 43s 134ms/step - loss: 0.6965 - val_loss: 0.5970
Epoch 3/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6961 - val_loss: 0.5955
Epoch 4/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6956 - val_loss: 0.5939
Epoch 5/36
317/317 [==============================] - 42s 131ms/step - loss: 0.6951 - val_loss: 0.5921
Epoch 6/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6945 - val_loss: 0.5904
Epoch 7/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6940 - val_loss: 0.5885
Epoch 8/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6934 - val_loss: 0.5866
Epoch 9/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6928 - val_loss: 0.5847
Epoch 10/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6922 - val_loss: 0.5827
Epoch 11/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6916 - val_loss: 0.5807
Epoch 12/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6910 - val_loss: 0.5786
Epoch 13/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6904 - val_loss: 0.5765
Epoch 14/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6898 - val_loss: 0.5744
Epoch 15/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6892 - val_loss: 0.5723
Epoch 16/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6886 - val_loss: 0.5702
Epoch 17/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6880 - val_loss: 0.5682
Epoch 18/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6874 - val_loss: 0.5663
Epoch 19/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6868 - val_loss: 0.5643
Epoch 20/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6863 - val_loss: 0.5624
Epoch 21/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6857 - val_loss: 0.5604
Epoch 22/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6851 - val_loss: 0.5584
Epoch 23/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6846 - val_loss: 0.5565
Epoch 24/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6840 - val_loss: 0.5545
Epoch 25/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6835 - val_loss: 0.5525
Epoch 26/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6829 - val_loss: 0.5505
Epoch 27/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6824 - val_loss: 0.5484
Epoch 28/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6818 - val_loss: 0.5463
Epoch 29/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6812 - val_loss: 0.5442
Epoch 30/36
317/317 [==============================] - 42s 132ms/step - loss: 0.6807 - val_loss: 0.5421
Epoch 31/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6801 - val_loss: 0.5399
Epoch 32/36
317/317 [==============================] - 42s 134ms/step - loss: 0.6795 - val_loss: 0.5377
Epoch 33/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6789 - val_loss: 0.5355
Epoch 34/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6783 - val_loss: 0.5332
Epoch 35/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6777 - val_loss: 0.5309
Epoch 36/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6770 - val_loss: 0.5285
Execution time:  1523.2913496494293
LSTM:
Mean Absolute Error: 0.6121
Root Mean Square Error: 0.9406
Mean Square Error: 0.8848

Train RMSE: 0.941
Train MSE: 0.885
Train MAE: 0.612
###########################

MODEL:  LSTM
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_66&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_66 (LSTM)               (None, 432, 40)           6720      
_________________________________________________________________
dropout_66 (Dropout)         (None, 432, 40)           0         
_________________________________________________________________
time_distributed_66 (TimeDis (None, 432, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 14s 193ms/step - loss: 0.9013 - val_loss: 1.2782
Epoch 2/68
72/72 [==============================] - 14s 197ms/step - loss: 0.9013 - val_loss: 1.2782
Epoch 3/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9012 - val_loss: 1.2781
Epoch 4/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9012 - val_loss: 1.2780
Epoch 5/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9012 - val_loss: 1.2780
Epoch 6/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9012 - val_loss: 1.2779
Epoch 7/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9011 - val_loss: 1.2778
Epoch 8/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9011 - val_loss: 1.2777
Epoch 9/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9011 - val_loss: 1.2776
Epoch 10/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9010 - val_loss: 1.2776
Epoch 11/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9010 - val_loss: 1.2775
Epoch 12/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9009 - val_loss: 1.2774
Epoch 13/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9009 - val_loss: 1.2773
Epoch 14/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9009 - val_loss: 1.2772
Epoch 15/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9008 - val_loss: 1.2771
Epoch 16/68
72/72 [==============================] - 15s 202ms/step - loss: 0.9008 - val_loss: 1.2771
Epoch 17/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9008 - val_loss: 1.2770
Epoch 18/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9007 - val_loss: 1.2769
Epoch 19/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9007 - val_loss: 1.2768
Epoch 20/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9006 - val_loss: 1.2767
Epoch 21/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9006 - val_loss: 1.2766
Epoch 22/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9006 - val_loss: 1.2765
Epoch 23/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9005 - val_loss: 1.2764
Epoch 24/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9005 - val_loss: 1.2763
Epoch 25/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9005 - val_loss: 1.2763
Epoch 26/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9004 - val_loss: 1.2762
Epoch 27/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9004 - val_loss: 1.2761
Epoch 28/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9003 - val_loss: 1.2760
Epoch 29/68
72/72 [==============================] - 15s 202ms/step - loss: 0.9003 - val_loss: 1.2759
Epoch 30/68
72/72 [==============================] - 14s 201ms/step - loss: 0.9002 - val_loss: 1.2758
Epoch 31/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9002 - val_loss: 1.2757
Epoch 32/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9002 - val_loss: 1.2756
Epoch 33/68
72/72 [==============================] - 15s 206ms/step - loss: 0.9001 - val_loss: 1.2755
Epoch 34/68
72/72 [==============================] - 15s 204ms/step - loss: 0.9001 - val_loss: 1.2754
Epoch 35/68
72/72 [==============================] - 15s 203ms/step - loss: 0.9000 - val_loss: 1.2753
Epoch 36/68
72/72 [==============================] - 15s 205ms/step - loss: 0.9000 - val_loss: 1.2752
Epoch 37/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8999 - val_loss: 1.2751
Epoch 38/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8999 - val_loss: 1.2750
Epoch 39/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8999 - val_loss: 1.2749
Epoch 40/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8998 - val_loss: 1.2748
Epoch 41/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8998 - val_loss: 1.2747
Epoch 42/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8997 - val_loss: 1.2746
Epoch 43/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8997 - val_loss: 1.2745
Epoch 44/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8996 - val_loss: 1.2744
Epoch 45/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8996 - val_loss: 1.2743
Epoch 46/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8995 - val_loss: 1.2742
Epoch 47/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8995 - val_loss: 1.2741
Epoch 48/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8994 - val_loss: 1.2740
Epoch 49/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8994 - val_loss: 1.2739
Epoch 50/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8994 - val_loss: 1.2738
Epoch 51/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8993 - val_loss: 1.2737
Epoch 52/68
72/72 [==============================] - 15s 205ms/step - loss: 0.8993 - val_loss: 1.2736
Epoch 53/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8992 - val_loss: 1.2735
Epoch 54/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8992 - val_loss: 1.2734
Epoch 55/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8991 - val_loss: 1.2733
Epoch 56/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8991 - val_loss: 1.2732
Epoch 57/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8990 - val_loss: 1.2731
Epoch 58/68
72/72 [==============================] - 15s 202ms/step - loss: 0.8990 - val_loss: 1.2730
Epoch 59/68
72/72 [==============================] - 15s 205ms/step - loss: 0.8989 - val_loss: 1.2729
Epoch 60/68
72/72 [==============================] - 15s 205ms/step - loss: 0.8989 - val_loss: 1.2728
Epoch 61/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8988 - val_loss: 1.2727
Epoch 62/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8988 - val_loss: 1.2726
Epoch 63/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8988 - val_loss: 1.2725
Epoch 64/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8987 - val_loss: 1.2724
Epoch 65/68
72/72 [==============================] - 15s 203ms/step - loss: 0.8987 - val_loss: 1.2723
Epoch 66/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8986 - val_loss: 1.2722
Epoch 67/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8986 - val_loss: 1.2720
Epoch 68/68
72/72 [==============================] - 15s 204ms/step - loss: 0.8985 - val_loss: 1.2719
Execution time:  1012.0301132202148
LSTM:
Mean Absolute Error: 0.9118
Root Mean Square Error: 1.1124
Mean Square Error: 1.2373

Train RMSE: 1.112
Train MSE: 1.237
Train MAE: 0.912
###########################

MODEL:  LSTM
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_67&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_67 (LSTM)               (None, 432, 55)           12540     
_________________________________________________________________
dropout_67 (Dropout)         (None, 432, 55)           0         
_________________________________________________________________
time_distributed_67 (TimeDis (None, 432, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 42s 131ms/step - loss: 0.9022 - val_loss: 1.0921
Epoch 2/36
317/317 [==============================] - 40s 127ms/step - loss: 0.9021 - val_loss: 1.0918
Epoch 3/36
317/317 [==============================] - 42s 132ms/step - loss: 0.9019 - val_loss: 1.0915
Epoch 4/36
317/317 [==============================] - 42s 133ms/step - loss: 0.9018 - val_loss: 1.0912
Epoch 5/36
317/317 [==============================] - 42s 131ms/step - loss: 0.9016 - val_loss: 1.0908
Epoch 6/36
317/317 [==============================] - 42s 134ms/step - loss: 0.9014 - val_loss: 1.0905
Epoch 7/36
317/317 [==============================] - 42s 132ms/step - loss: 0.9013 - val_loss: 1.0901
Epoch 8/36
317/317 [==============================] - 42s 133ms/step - loss: 0.9011 - val_loss: 1.0897
Epoch 9/36
317/317 [==============================] - 43s 134ms/step - loss: 0.9009 - val_loss: 1.0892
Epoch 10/36
317/317 [==============================] - 42s 133ms/step - loss: 0.9007 - val_loss: 1.0888
Epoch 11/36
317/317 [==============================] - 42s 132ms/step - loss: 0.9005 - val_loss: 1.0884
Epoch 12/36
317/317 [==============================] - 42s 134ms/step - loss: 0.9003 - val_loss: 1.0879
Epoch 13/36
317/317 [==============================] - 42s 132ms/step - loss: 0.9001 - val_loss: 1.0874
Epoch 14/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8998 - val_loss: 1.0870
Epoch 15/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8996 - val_loss: 1.0865
Epoch 16/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8994 - val_loss: 1.0860
Epoch 17/36
317/317 [==============================] - 43s 135ms/step - loss: 0.8992 - val_loss: 1.0855
Epoch 18/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8989 - val_loss: 1.0850
Epoch 19/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8987 - val_loss: 1.0845
Epoch 20/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8984 - val_loss: 1.0839
Epoch 21/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8982 - val_loss: 1.0834
Epoch 22/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8979 - val_loss: 1.0829
Epoch 23/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8977 - val_loss: 1.0823
Epoch 24/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8974 - val_loss: 1.0817
Epoch 25/36
317/317 [==============================] - 42s 132ms/step - loss: 0.8971 - val_loss: 1.0812
Epoch 26/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8969 - val_loss: 1.0806
Epoch 27/36
317/317 [==============================] - 42s 132ms/step - loss: 0.8966 - val_loss: 1.0800
Epoch 28/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8963 - val_loss: 1.0794
Epoch 29/36
317/317 [==============================] - 43s 134ms/step - loss: 0.8960 - val_loss: 1.0788
Epoch 30/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8957 - val_loss: 1.0782
Epoch 31/36
317/317 [==============================] - 43s 135ms/step - loss: 0.8954 - val_loss: 1.0776
Epoch 32/36
317/317 [==============================] - 42s 134ms/step - loss: 0.8951 - val_loss: 1.0770
Epoch 33/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8948 - val_loss: 1.0763
Epoch 34/36
317/317 [==============================] - 42s 133ms/step - loss: 0.8945 - val_loss: 1.0757
Epoch 35/36
317/317 [==============================] - 42s 131ms/step - loss: 0.8942 - val_loss: 1.0750
Epoch 36/36
317/317 [==============================] - 42s 131ms/step - loss: 0.8939 - val_loss: 1.0743
Execution time:  1524.1113595962524
LSTM:
Mean Absolute Error: 0.9095
Root Mean Square Error: 1.1113
Mean Square Error: 1.2350

Train RMSE: 1.111
Train MSE: 1.235
Train MAE: 0.909
###########################

MODEL:  LSTM
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_68&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_68 (LSTM)               (None, 432, 40)           6720      
_________________________________________________________________
dropout_68 (Dropout)         (None, 432, 40)           0         
_________________________________________________________________
time_distributed_68 (TimeDis (None, 432, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 14s 196ms/step - loss: 0.6511 - val_loss: 0.5898
Epoch 2/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6148 - val_loss: 0.3710
Epoch 3/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6157 - val_loss: 0.3619
Epoch 4/68
72/72 [==============================] - 13s 178ms/step - loss: 0.6125 - val_loss: 0.3605
Epoch 5/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6104 - val_loss: 0.3624
Epoch 6/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6088 - val_loss: 0.3623
Epoch 7/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6079 - val_loss: 0.3635
Epoch 8/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6072 - val_loss: 0.3644
Epoch 9/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6066 - val_loss: 0.3657
Epoch 10/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6061 - val_loss: 0.3664
Epoch 11/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6056 - val_loss: 0.3670
Epoch 12/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6051 - val_loss: 0.3678
Epoch 13/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6048 - val_loss: 0.3682
Epoch 14/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6044 - val_loss: 0.3687
Execution time:  189.3432776927948
LSTM:
Mean Absolute Error: 0.5275
Root Mean Square Error: 0.9867
Mean Square Error: 0.9735

Train RMSE: 0.987
Train MSE: 0.974
Train MAE: 0.528
###########################

MODEL:  LSTM
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_69&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_69 (LSTM)               (None, 432, 55)           12540     
_________________________________________________________________
dropout_69 (Dropout)         (None, 432, 55)           0         
_________________________________________________________________
time_distributed_69 (TimeDis (None, 432, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 42s 133ms/step - loss: 0.6639 - val_loss: 0.3176
Epoch 2/36
317/317 [==============================] - 43s 135ms/step - loss: 0.6491 - val_loss: 0.2959
Epoch 3/36
317/317 [==============================] - 43s 135ms/step - loss: 0.6439 - val_loss: 0.2798
Epoch 4/36
317/317 [==============================] - 43s 135ms/step - loss: 0.6456 - val_loss: 0.2750
Epoch 5/36
317/317 [==============================] - 47s 148ms/step - loss: 0.6475 - val_loss: 0.2747
Epoch 6/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6485 - val_loss: 0.2769
Epoch 7/36
317/317 [==============================] - 49s 153ms/step - loss: 0.6479 - val_loss: 0.2753
Epoch 8/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6476 - val_loss: 0.2815
Epoch 9/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6470 - val_loss: 0.2801
Epoch 10/36
317/317 [==============================] - 49s 154ms/step - loss: 0.6467 - val_loss: 0.2845
Epoch 11/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6458 - val_loss: 0.2784
Epoch 12/36
317/317 [==============================] - 49s 153ms/step - loss: 0.6457 - val_loss: 0.2860
Epoch 13/36
317/317 [==============================] - 49s 154ms/step - loss: 0.6449 - val_loss: 0.2836
Epoch 14/36
317/317 [==============================] - 49s 153ms/step - loss: 0.6446 - val_loss: 0.2829
Epoch 15/36
317/317 [==============================] - 49s 154ms/step - loss: 0.6441 - val_loss: 0.2812
Execution time:  707.2453815937042
LSTM:
Mean Absolute Error: 0.5238
Root Mean Square Error: 1.0161
Mean Square Error: 1.0325

Train RMSE: 1.016
Train MSE: 1.032
Train MAE: 0.524
###########################

MODEL:  LSTM
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_70&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_70 (LSTM)               (None, 432, 40)           6720      
_________________________________________________________________
dropout_70 (Dropout)         (None, 432, 40)           0         
_________________________________________________________________
time_distributed_70 (TimeDis (None, 432, 1)            41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 14s 191ms/step - loss: 0.8934 - val_loss: 1.2299
Epoch 2/68
72/72 [==============================] - 13s 186ms/step - loss: 0.8167 - val_loss: 0.9677
Epoch 3/68
72/72 [==============================] - 13s 186ms/step - loss: 0.7171 - val_loss: 0.8530
Epoch 4/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6911 - val_loss: 0.8188
Epoch 5/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6831 - val_loss: 0.8061
Epoch 6/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6796 - val_loss: 0.7998
Epoch 7/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6776 - val_loss: 0.7961
Epoch 8/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6764 - val_loss: 0.7936
Epoch 9/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6755 - val_loss: 0.7918
Epoch 10/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6748 - val_loss: 0.7904
Epoch 11/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6742 - val_loss: 0.7893
Epoch 12/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6738 - val_loss: 0.7884
Epoch 13/68
72/72 [==============================] - 13s 178ms/step - loss: 0.6734 - val_loss: 0.7876
Epoch 14/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6731 - val_loss: 0.7870
Epoch 15/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6729 - val_loss: 0.7865
Epoch 16/68
72/72 [==============================] - 13s 178ms/step - loss: 0.6726 - val_loss: 0.7860
Epoch 17/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6724 - val_loss: 0.7856
Epoch 18/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6722 - val_loss: 0.7852
Epoch 19/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6721 - val_loss: 0.7848
Epoch 20/68
72/72 [==============================] - 13s 178ms/step - loss: 0.6719 - val_loss: 0.7845
Epoch 21/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6718 - val_loss: 0.7843
Epoch 22/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6717 - val_loss: 0.7840
Epoch 23/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6716 - val_loss: 0.7838
Epoch 24/68
72/72 [==============================] - 14s 196ms/step - loss: 0.6715 - val_loss: 0.7836
Epoch 25/68
72/72 [==============================] - 14s 197ms/step - loss: 0.6713 - val_loss: 0.7834
Epoch 26/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6713 - val_loss: 0.7832
Epoch 27/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6712 - val_loss: 0.7830
Epoch 28/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6711 - val_loss: 0.7828
Epoch 29/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6710 - val_loss: 0.7827
Epoch 30/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6710 - val_loss: 0.7825
Epoch 31/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6709 - val_loss: 0.7824
Epoch 32/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6708 - val_loss: 0.7823
Epoch 33/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6708 - val_loss: 0.7822
Epoch 34/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6707 - val_loss: 0.7820
Epoch 35/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6707 - val_loss: 0.7819
Epoch 36/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6706 - val_loss: 0.7818
Epoch 37/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6706 - val_loss: 0.7817
Epoch 38/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6705 - val_loss: 0.7816
Epoch 39/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6705 - val_loss: 0.7815
Epoch 40/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6704 - val_loss: 0.7815
Epoch 41/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6704 - val_loss: 0.7814
Epoch 42/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6704 - val_loss: 0.7813
Epoch 43/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6703 - val_loss: 0.7812
Epoch 44/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6703 - val_loss: 0.7811
Epoch 45/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6702 - val_loss: 0.7811
Epoch 46/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6702 - val_loss: 0.7810
Epoch 47/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6702 - val_loss: 0.7809
Epoch 48/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6702 - val_loss: 0.7809
Epoch 49/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7808
Epoch 50/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6701 - val_loss: 0.7808
Epoch 51/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 52/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 53/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6700 - val_loss: 0.7806
Epoch 54/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6700 - val_loss: 0.7806
Epoch 55/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 56/68
72/72 [==============================] - 15s 202ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 57/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7804
Epoch 58/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7804
Epoch 59/68
72/72 [==============================] - 14s 199ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 60/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 61/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 62/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 63/68
72/72 [==============================] - 14s 200ms/step - loss: 0.6698 - val_loss: 0.7802
Epoch 64/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7802
Epoch 65/68
72/72 [==============================] - 15s 203ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 66/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 67/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 68/68
72/72 [==============================] - 14s 201ms/step - loss: 0.6698 - val_loss: 0.7800
Execution time:  963.6103754043579
LSTM:
Mean Absolute Error: 0.6863
Root Mean Square Error: 0.9969
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  LSTM
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_71&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_71 (LSTM)               (None, 432, 55)           12540     
_________________________________________________________________
dropout_71 (Dropout)         (None, 432, 55)           0         
_________________________________________________________________
time_distributed_71 (TimeDis (None, 432, 1)            56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 45s 141ms/step - loss: 0.8145 - val_loss: 0.6359
Epoch 2/36
317/317 [==============================] - 43s 137ms/step - loss: 0.7157 - val_loss: 0.6212
Epoch 3/36
317/317 [==============================] - 43s 135ms/step - loss: 0.7051 - val_loss: 0.6175
Epoch 4/36
317/317 [==============================] - 43s 136ms/step - loss: 0.7035 - val_loss: 0.6155
Epoch 5/36
317/317 [==============================] - 49s 153ms/step - loss: 0.7026 - val_loss: 0.6142
Epoch 6/36
317/317 [==============================] - 48s 153ms/step - loss: 0.7019 - val_loss: 0.6133
Epoch 7/36
317/317 [==============================] - 49s 153ms/step - loss: 0.7015 - val_loss: 0.6126
Epoch 8/36
317/317 [==============================] - 48s 153ms/step - loss: 0.7011 - val_loss: 0.6121
Epoch 9/36
317/317 [==============================] - 49s 153ms/step - loss: 0.7008 - val_loss: 0.6116
Epoch 10/36
317/317 [==============================] - 49s 153ms/step - loss: 0.7005 - val_loss: 0.6112
Epoch 11/36
317/317 [==============================] - 48s 153ms/step - loss: 0.7003 - val_loss: 0.6108
Epoch 12/36
317/317 [==============================] - 49s 153ms/step - loss: 0.7001 - val_loss: 0.6104
Epoch 13/36
317/317 [==============================] - 49s 153ms/step - loss: 0.6999 - val_loss: 0.6101
Epoch 14/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6997 - val_loss: 0.6097
Epoch 15/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6996 - val_loss: 0.6094
Epoch 16/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6995 - val_loss: 0.6091
Epoch 17/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6993 - val_loss: 0.6088
Epoch 18/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6992 - val_loss: 0.6086
Epoch 19/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6992 - val_loss: 0.6084
Epoch 20/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6991 - val_loss: 0.6082
Epoch 21/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6990 - val_loss: 0.6081
Epoch 22/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6990 - val_loss: 0.6080
Epoch 23/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6990 - val_loss: 0.6079
Epoch 24/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6078
Epoch 25/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 26/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 27/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 28/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 29/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 30/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 31/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 32/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 33/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 34/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 35/36
317/317 [==============================] - 48s 153ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 36/36
317/317 [==============================] - 48s 152ms/step - loss: 0.6988 - val_loss: 0.6075
Execution time:  1727.5521295070648
LSTM:
Mean Absolute Error: 0.6862
Root Mean Square Error: 0.9968
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  LSTM
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_72&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_72 (LSTM)               (None, 1008, 40)          6720      
_________________________________________________________________
dropout_72 (Dropout)         (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_72 (TimeDis (None, 1008, 1)           41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6410 - val_loss: 0.4830
Epoch 2/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6079 - val_loss: 0.6219
Epoch 3/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5764 - val_loss: 0.4767
Epoch 4/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5729 - val_loss: 0.4666
Epoch 5/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5678 - val_loss: 0.4491
Epoch 6/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5650 - val_loss: 0.4424
Epoch 7/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5633 - val_loss: 0.4348
Epoch 8/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5619 - val_loss: 0.4298
Epoch 9/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5608 - val_loss: 0.4249
Epoch 10/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5600 - val_loss: 0.4211
Epoch 11/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5592 - val_loss: 0.4175
Epoch 12/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5585 - val_loss: 0.4145
Epoch 13/68
60/60 [==============================] - 24s 394ms/step - loss: 0.5580 - val_loss: 0.4115
Epoch 14/68
60/60 [==============================] - 24s 396ms/step - loss: 0.5575 - val_loss: 0.4092
Epoch 15/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5570 - val_loss: 0.4067
Epoch 16/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5566 - val_loss: 0.4050
Epoch 17/68
60/60 [==============================] - 25s 411ms/step - loss: 0.5562 - val_loss: 0.4026
Epoch 18/68
60/60 [==============================] - 24s 408ms/step - loss: 0.5559 - val_loss: 0.4020
Epoch 19/68
60/60 [==============================] - 25s 411ms/step - loss: 0.5556 - val_loss: 0.3998
Epoch 20/68
60/60 [==============================] - 24s 408ms/step - loss: 0.5553 - val_loss: 0.3987
Epoch 21/68
60/60 [==============================] - 24s 407ms/step - loss: 0.5550 - val_loss: 0.3971
Epoch 22/68
60/60 [==============================] - 24s 408ms/step - loss: 0.5548 - val_loss: 0.3958
Epoch 23/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5545 - val_loss: 0.3946
Epoch 24/68
60/60 [==============================] - 25s 411ms/step - loss: 0.5544 - val_loss: 0.3933
Epoch 25/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5542 - val_loss: 0.3932
Epoch 26/68
60/60 [==============================] - 25s 410ms/step - loss: 0.5539 - val_loss: 0.3921
Epoch 27/68
60/60 [==============================] - 25s 409ms/step - loss: 0.5536 - val_loss: 0.3914
Epoch 28/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5535 - val_loss: 0.3903
Epoch 29/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5533 - val_loss: 0.3896
Epoch 30/68
60/60 [==============================] - 25s 409ms/step - loss: 0.5531 - val_loss: 0.3886
Epoch 31/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5530 - val_loss: 0.3882
Epoch 32/68
60/60 [==============================] - 25s 410ms/step - loss: 0.5528 - val_loss: 0.3871
Epoch 33/68
60/60 [==============================] - 25s 410ms/step - loss: 0.5527 - val_loss: 0.3866
Epoch 34/68
60/60 [==============================] - 24s 408ms/step - loss: 0.5526 - val_loss: 0.3856
Epoch 35/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5524 - val_loss: 0.3854
Epoch 36/68
60/60 [==============================] - 24s 407ms/step - loss: 0.5523 - val_loss: 0.3844
Epoch 37/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5522 - val_loss: 0.3845
Epoch 38/68
60/60 [==============================] - 25s 409ms/step - loss: 0.5521 - val_loss: 0.3836
Epoch 39/68
60/60 [==============================] - 25s 411ms/step - loss: 0.5519 - val_loss: 0.3835
Epoch 40/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5518 - val_loss: 0.3827
Epoch 41/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5517 - val_loss: 0.3827
Epoch 42/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5516 - val_loss: 0.3819
Epoch 43/68
60/60 [==============================] - 24s 407ms/step - loss: 0.5515 - val_loss: 0.3817
Epoch 44/68
60/60 [==============================] - 24s 408ms/step - loss: 0.5514 - val_loss: 0.3811
Epoch 45/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5512 - val_loss: 0.3808
Epoch 46/68
60/60 [==============================] - 24s 407ms/step - loss: 0.5511 - val_loss: 0.3802
Epoch 47/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5509 - val_loss: 0.3799
Epoch 48/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5510 - val_loss: 0.3792
Epoch 49/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5509 - val_loss: 0.3790
Epoch 50/68
60/60 [==============================] - 24s 407ms/step - loss: 0.5508 - val_loss: 0.3785
Epoch 51/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5507 - val_loss: 0.3784
Epoch 52/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5507 - val_loss: 0.3778
Epoch 53/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5506 - val_loss: 0.3779
Epoch 54/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5505 - val_loss: 0.3771
Epoch 55/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5504 - val_loss: 0.3771
Epoch 56/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5504 - val_loss: 0.3765
Epoch 57/68
60/60 [==============================] - 25s 412ms/step - loss: 0.5503 - val_loss: 0.3765
Epoch 58/68
60/60 [==============================] - 26s 431ms/step - loss: 0.5502 - val_loss: 0.3757
Epoch 59/68
60/60 [==============================] - 25s 421ms/step - loss: 0.5501 - val_loss: 0.3758
Epoch 60/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5501 - val_loss: 0.3750
Epoch 61/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5500 - val_loss: 0.3755
Epoch 62/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5500 - val_loss: 0.3749
Epoch 63/68
60/60 [==============================] - 24s 408ms/step - loss: 0.5498 - val_loss: 0.3751
Epoch 64/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5498 - val_loss: 0.3744
Epoch 65/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5496 - val_loss: 0.3743
Epoch 66/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5497 - val_loss: 0.3737
Epoch 67/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5496 - val_loss: 0.3736
Epoch 68/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5496 - val_loss: 0.3731
Execution time:  1682.1987023353577
LSTM:
Mean Absolute Error: 0.6438
Root Mean Square Error: 1.1469
Mean Square Error: 1.3153

Train RMSE: 1.147
Train MSE: 1.315
Train MAE: 0.644
###########################

MODEL:  LSTM
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_73&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_73 (LSTM)               (None, 1008, 55)          12540     
_________________________________________________________________
dropout_73 (Dropout)         (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_73 (TimeDis (None, 1008, 1)           56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 88s 329ms/step - loss: 0.6362 - val_loss: 0.4384
Epoch 2/36
266/266 [==============================] - 89s 335ms/step - loss: 0.6149 - val_loss: 0.3621
Epoch 3/36
266/266 [==============================] - 91s 340ms/step - loss: 0.6031 - val_loss: 0.2902
Epoch 4/36
266/266 [==============================] - 89s 335ms/step - loss: 0.5936 - val_loss: 0.2951
Epoch 5/36
266/266 [==============================] - 89s 336ms/step - loss: 0.5927 - val_loss: 0.2913
Epoch 6/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5905 - val_loss: 0.2906
Epoch 7/36
266/266 [==============================] - 87s 329ms/step - loss: 0.5895 - val_loss: 0.2906
Epoch 8/36
266/266 [==============================] - 89s 333ms/step - loss: 0.5890 - val_loss: 0.2908
Epoch 9/36
266/266 [==============================] - 89s 335ms/step - loss: 0.5888 - val_loss: 0.2883
Epoch 10/36
266/266 [==============================] - 93s 349ms/step - loss: 0.5891 - val_loss: 0.2878
Epoch 11/36
266/266 [==============================] - 93s 348ms/step - loss: 0.5891 - val_loss: 0.2883
Epoch 12/36
266/266 [==============================] - 92s 346ms/step - loss: 0.5886 - val_loss: 0.2865
Epoch 13/36
266/266 [==============================] - 94s 352ms/step - loss: 0.5886 - val_loss: 0.2870
Epoch 14/36
266/266 [==============================] - 93s 349ms/step - loss: 0.5893 - val_loss: 0.2984
Epoch 15/36
266/266 [==============================] - 94s 353ms/step - loss: 0.5892 - val_loss: 0.2970
Epoch 16/36
266/266 [==============================] - 92s 346ms/step - loss: 0.5890 - val_loss: 0.2989
Epoch 17/36
266/266 [==============================] - 92s 346ms/step - loss: 0.5886 - val_loss: 0.2970
Epoch 18/36
266/266 [==============================] - 92s 345ms/step - loss: 0.5887 - val_loss: 0.2995
Epoch 19/36
266/266 [==============================] - 93s 348ms/step - loss: 0.5885 - val_loss: 0.2973
Epoch 20/36
266/266 [==============================] - 93s 350ms/step - loss: 0.5884 - val_loss: 0.3020
Epoch 21/36
266/266 [==============================] - 93s 351ms/step - loss: 0.5884 - val_loss: 0.2998
Epoch 22/36
266/266 [==============================] - 91s 342ms/step - loss: 0.5883 - val_loss: 0.3021
Execution time:  2014.745056629181
LSTM:
Mean Absolute Error: 0.6489
Root Mean Square Error: 1.1585
Mean Square Error: 1.3421

Train RMSE: 1.158
Train MSE: 1.342
Train MAE: 0.649
###########################

MODEL:  LSTM
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_74&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_74 (LSTM)               (None, 1008, 40)          6720      
_________________________________________________________________
dropout_74 (Dropout)         (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_74 (TimeDis (None, 1008, 1)           41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9254 - val_loss: 0.9972
Epoch 2/68
60/60 [==============================] - 25s 409ms/step - loss: 0.7074 - val_loss: 0.8439
Epoch 3/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6824 - val_loss: 0.8327
Epoch 4/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6787 - val_loss: 0.8288
Epoch 5/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6771 - val_loss: 0.8269
Epoch 6/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6761 - val_loss: 0.8256
Epoch 7/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6755 - val_loss: 0.8248
Epoch 8/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6751 - val_loss: 0.8242
Epoch 9/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6747 - val_loss: 0.8238
Epoch 10/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6745 - val_loss: 0.8235
Epoch 11/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6743 - val_loss: 0.8232
Epoch 12/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6741 - val_loss: 0.8229
Epoch 13/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6740 - val_loss: 0.8227
Epoch 14/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6739 - val_loss: 0.8226
Epoch 15/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6737 - val_loss: 0.8224
Epoch 16/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6737 - val_loss: 0.8223
Epoch 17/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6736 - val_loss: 0.8222
Epoch 18/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6735 - val_loss: 0.8221
Epoch 19/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6735 - val_loss: 0.8220
Epoch 20/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6734 - val_loss: 0.8219
Epoch 21/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6734 - val_loss: 0.8219
Epoch 22/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6733 - val_loss: 0.8218
Epoch 23/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6733 - val_loss: 0.8217
Epoch 24/68
60/60 [==============================] - 25s 414ms/step - loss: 0.6732 - val_loss: 0.8217
Epoch 25/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6732 - val_loss: 0.8216
Epoch 26/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6732 - val_loss: 0.8216
Epoch 27/68
60/60 [==============================] - 25s 415ms/step - loss: 0.6731 - val_loss: 0.8215
Epoch 28/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6731 - val_loss: 0.8215
Epoch 29/68
60/60 [==============================] - 25s 414ms/step - loss: 0.6731 - val_loss: 0.8214
Epoch 30/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6731 - val_loss: 0.8214
Epoch 31/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6730 - val_loss: 0.8214
Epoch 32/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6730 - val_loss: 0.8214
Epoch 33/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6730 - val_loss: 0.8213
Epoch 34/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6730 - val_loss: 0.8213
Epoch 35/68
60/60 [==============================] - 25s 414ms/step - loss: 0.6730 - val_loss: 0.8213
Epoch 36/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8212
Epoch 37/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6729 - val_loss: 0.8212
Epoch 38/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8212
Epoch 39/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6729 - val_loss: 0.8212
Epoch 40/68
60/60 [==============================] - 25s 414ms/step - loss: 0.6729 - val_loss: 0.8211
Epoch 41/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8211
Epoch 42/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6729 - val_loss: 0.8211
Epoch 43/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6729 - val_loss: 0.8211
Epoch 44/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6728 - val_loss: 0.8211
Epoch 45/68
60/60 [==============================] - 25s 414ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 46/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 47/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 48/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 49/68
60/60 [==============================] - 25s 415ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 50/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 51/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 52/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 53/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 54/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 55/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6727 - val_loss: 0.8209
Epoch 56/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6727 - val_loss: 0.8209
Epoch 57/68
60/60 [==============================] - 25s 415ms/step - loss: 0.6727 - val_loss: 0.8209
Epoch 58/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6727 - val_loss: 0.8209
Epoch 59/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6727 - val_loss: 0.8209
Epoch 60/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6727 - val_loss: 0.8209
Epoch 61/68
60/60 [==============================] - 25s 415ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 62/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 63/68
60/60 [==============================] - 25s 413ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 64/68
60/60 [==============================] - 25s 414ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 65/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 66/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 67/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 68/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6727 - val_loss: 0.8208
Execution time:  1707.1715667247772
LSTM:
Mean Absolute Error: 0.6894
Root Mean Square Error: 1.0230
Mean Square Error: 1.0466

Train RMSE: 1.023
Train MSE: 1.047
Train MAE: 0.689
###########################

MODEL:  LSTM
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_75&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_75 (LSTM)               (None, 1008, 55)          12540     
_________________________________________________________________
dropout_75 (Dropout)         (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_75 (TimeDis (None, 1008, 1)           56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 91s 344ms/step - loss: 0.7783 - val_loss: 0.6613
Epoch 2/36
266/266 [==============================] - 93s 350ms/step - loss: 0.6974 - val_loss: 0.6586
Epoch 3/36
266/266 [==============================] - 93s 349ms/step - loss: 0.6965 - val_loss: 0.6577
Epoch 4/36
266/266 [==============================] - 93s 351ms/step - loss: 0.6962 - val_loss: 0.6573
Epoch 5/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6960 - val_loss: 0.6570
Epoch 6/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6959 - val_loss: 0.6568
Epoch 7/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6958 - val_loss: 0.6566
Epoch 8/36
266/266 [==============================] - 98s 368ms/step - loss: 0.6957 - val_loss: 0.6565
Epoch 9/36
266/266 [==============================] - 98s 367ms/step - loss: 0.6957 - val_loss: 0.6564
Epoch 10/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6956 - val_loss: 0.6563
Epoch 11/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6956 - val_loss: 0.6562
Epoch 12/36
266/266 [==============================] - 98s 368ms/step - loss: 0.6955 - val_loss: 0.6562
Epoch 13/36
266/266 [==============================] - 99s 372ms/step - loss: 0.6955 - val_loss: 0.6561
Epoch 14/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6955 - val_loss: 0.6561
Epoch 15/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6955 - val_loss: 0.6560
Epoch 16/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6954 - val_loss: 0.6560
Epoch 17/36
266/266 [==============================] - 98s 369ms/step - loss: 0.6954 - val_loss: 0.6560
Epoch 18/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 19/36
266/266 [==============================] - 97s 364ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 20/36
266/266 [==============================] - 100s 376ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 21/36
266/266 [==============================] - 99s 374ms/step - loss: 0.6954 - val_loss: 0.6558
Epoch 22/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 23/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 24/36
266/266 [==============================] - 98s 367ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 25/36
266/266 [==============================] - 98s 367ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 26/36
266/266 [==============================] - 98s 369ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 27/36
266/266 [==============================] - 98s 367ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 28/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 29/36
266/266 [==============================] - 98s 370ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 30/36
266/266 [==============================] - 98s 370ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 31/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 32/36
266/266 [==============================] - 96s 359ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 33/36
266/266 [==============================] - 98s 369ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 34/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 35/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 36/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6556
Execution time:  3504.303931236267
LSTM:
Mean Absolute Error: 0.6893
Root Mean Square Error: 1.0230
Mean Square Error: 1.0465

Train RMSE: 1.023
Train MSE: 1.046
Train MAE: 0.689
###########################

MODEL:  LSTM
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_76&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_76 (LSTM)               (None, 1008, 40)          6720      
_________________________________________________________________
dropout_76 (Dropout)         (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_76 (TimeDis (None, 1008, 1)           41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6708 - val_loss: 0.8231
Epoch 2/68
60/60 [==============================] - 23s 391ms/step - loss: 0.6707 - val_loss: 0.8229
Epoch 3/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6706 - val_loss: 0.8227
Epoch 4/68
60/60 [==============================] - 24s 393ms/step - loss: 0.6705 - val_loss: 0.8225
Epoch 5/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6705 - val_loss: 0.8223
Epoch 6/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6703 - val_loss: 0.8221
Epoch 7/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6703 - val_loss: 0.8219
Epoch 8/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6701 - val_loss: 0.8217
Epoch 9/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6700 - val_loss: 0.8215
Epoch 10/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6699 - val_loss: 0.8213
Epoch 11/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6698 - val_loss: 0.8211
Epoch 12/68
60/60 [==============================] - 24s 392ms/step - loss: 0.6697 - val_loss: 0.8209
Epoch 13/68
60/60 [==============================] - 24s 392ms/step - loss: 0.6696 - val_loss: 0.8207
Epoch 14/68
60/60 [==============================] - 23s 389ms/step - loss: 0.6695 - val_loss: 0.8205
Epoch 15/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6694 - val_loss: 0.8202
Epoch 16/68
60/60 [==============================] - 23s 391ms/step - loss: 0.6692 - val_loss: 0.8200
Epoch 17/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6691 - val_loss: 0.8198
Epoch 18/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6690 - val_loss: 0.8196
Epoch 19/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6689 - val_loss: 0.8193
Epoch 20/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6687 - val_loss: 0.8191
Epoch 21/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6686 - val_loss: 0.8188
Epoch 22/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6685 - val_loss: 0.8186
Epoch 23/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6684 - val_loss: 0.8184
Epoch 24/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6682 - val_loss: 0.8181
Epoch 25/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6681 - val_loss: 0.8179
Epoch 26/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6680 - val_loss: 0.8176
Epoch 27/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6678 - val_loss: 0.8174
Epoch 28/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6677 - val_loss: 0.8171
Epoch 29/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6676 - val_loss: 0.8169
Epoch 30/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6674 - val_loss: 0.8166
Epoch 31/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6673 - val_loss: 0.8164
Epoch 32/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6672 - val_loss: 0.8161
Epoch 33/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6670 - val_loss: 0.8159
Epoch 34/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6669 - val_loss: 0.8156
Epoch 35/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6668 - val_loss: 0.8154
Epoch 36/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6666 - val_loss: 0.8151
Epoch 37/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6665 - val_loss: 0.8148
Epoch 38/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6664 - val_loss: 0.8146
Epoch 39/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6662 - val_loss: 0.8143
Epoch 40/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6661 - val_loss: 0.8141
Epoch 41/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6660 - val_loss: 0.8138
Epoch 42/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6658 - val_loss: 0.8135
Epoch 43/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6657 - val_loss: 0.8133
Epoch 44/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6655 - val_loss: 0.8130
Epoch 45/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6654 - val_loss: 0.8127
Epoch 46/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6653 - val_loss: 0.8125
Epoch 47/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6651 - val_loss: 0.8122
Epoch 48/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6650 - val_loss: 0.8119
Epoch 49/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6648 - val_loss: 0.8117
Epoch 50/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6647 - val_loss: 0.8114
Epoch 51/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6646 - val_loss: 0.8111
Epoch 52/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6644 - val_loss: 0.8109
Epoch 53/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6643 - val_loss: 0.8106
Epoch 54/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6641 - val_loss: 0.8103
Epoch 55/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6640 - val_loss: 0.8100
Epoch 56/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6639 - val_loss: 0.8098
Epoch 57/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6637 - val_loss: 0.8095
Epoch 58/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6636 - val_loss: 0.8092
Epoch 59/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6634 - val_loss: 0.8090
Epoch 60/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6633 - val_loss: 0.8087
Epoch 61/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6631 - val_loss: 0.8084
Epoch 62/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6630 - val_loss: 0.8081
Epoch 63/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6629 - val_loss: 0.8078
Epoch 64/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6627 - val_loss: 0.8076
Epoch 65/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6626 - val_loss: 0.8073
Epoch 66/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6624 - val_loss: 0.8070
Epoch 67/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6623 - val_loss: 0.8067
Epoch 68/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6622 - val_loss: 0.8065
Execution time:  1653.1658599376678
LSTM:
Mean Absolute Error: 0.6908
Root Mean Square Error: 1.0334
Mean Square Error: 1.0678

Train RMSE: 1.033
Train MSE: 1.068
Train MAE: 0.691
###########################

MODEL:  LSTM
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_77&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_77 (LSTM)               (None, 1008, 55)          12540     
_________________________________________________________________
dropout_77 (Dropout)         (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_77 (TimeDis (None, 1008, 1)           56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 91s 341ms/step - loss: 0.6932 - val_loss: 0.6754
Epoch 2/36
266/266 [==============================] - 93s 351ms/step - loss: 0.6927 - val_loss: 0.6748
Epoch 3/36
266/266 [==============================] - 94s 354ms/step - loss: 0.6922 - val_loss: 0.6742
Epoch 4/36
266/266 [==============================] - 96s 361ms/step - loss: 0.6916 - val_loss: 0.6736
Epoch 5/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6910 - val_loss: 0.6728
Epoch 6/36
266/266 [==============================] - 98s 370ms/step - loss: 0.6904 - val_loss: 0.6720
Epoch 7/36
266/266 [==============================] - 100s 375ms/step - loss: 0.6897 - val_loss: 0.6712
Epoch 8/36
266/266 [==============================] - 97s 363ms/step - loss: 0.6891 - val_loss: 0.6702
Epoch 9/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6884 - val_loss: 0.6693
Epoch 10/36
266/266 [==============================] - 97s 363ms/step - loss: 0.6877 - val_loss: 0.6683
Epoch 11/36
266/266 [==============================] - 98s 368ms/step - loss: 0.6869 - val_loss: 0.6673
Epoch 12/36
266/266 [==============================] - 95s 356ms/step - loss: 0.6862 - val_loss: 0.6662
Epoch 13/36
266/266 [==============================] - 96s 361ms/step - loss: 0.6854 - val_loss: 0.6652
Epoch 14/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6846 - val_loss: 0.6640
Epoch 15/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6839 - val_loss: 0.6629
Epoch 16/36
266/266 [==============================] - 97s 363ms/step - loss: 0.6831 - val_loss: 0.6617
Epoch 17/36
266/266 [==============================] - 98s 370ms/step - loss: 0.6822 - val_loss: 0.6605
Epoch 18/36
266/266 [==============================] - 98s 367ms/step - loss: 0.6814 - val_loss: 0.6593
Epoch 19/36
266/266 [==============================] - 96s 361ms/step - loss: 0.6805 - val_loss: 0.6581
Epoch 20/36
266/266 [==============================] - 97s 364ms/step - loss: 0.6797 - val_loss: 0.6568
Epoch 21/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6788 - val_loss: 0.6555
Epoch 22/36
266/266 [==============================] - 95s 356ms/step - loss: 0.6779 - val_loss: 0.6542
Epoch 23/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6770 - val_loss: 0.6528
Epoch 24/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6760 - val_loss: 0.6514
Epoch 25/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6751 - val_loss: 0.6500
Epoch 26/36
266/266 [==============================] - 97s 363ms/step - loss: 0.6741 - val_loss: 0.6486
Epoch 27/36
266/266 [==============================] - 97s 364ms/step - loss: 0.6732 - val_loss: 0.6471
Epoch 28/36
266/266 [==============================] - 98s 369ms/step - loss: 0.6722 - val_loss: 0.6456
Epoch 29/36
266/266 [==============================] - 97s 364ms/step - loss: 0.6712 - val_loss: 0.6441
Epoch 30/36
266/266 [==============================] - 96s 359ms/step - loss: 0.6702 - val_loss: 0.6426
Epoch 31/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6691 - val_loss: 0.6410
Epoch 32/36
266/266 [==============================] - 98s 368ms/step - loss: 0.6681 - val_loss: 0.6394
Epoch 33/36
266/266 [==============================] - 97s 363ms/step - loss: 0.6670 - val_loss: 0.6377
Epoch 34/36
266/266 [==============================] - 97s 364ms/step - loss: 0.6659 - val_loss: 0.6360
Epoch 35/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6648 - val_loss: 0.6343
Epoch 36/36
266/266 [==============================] - 96s 362ms/step - loss: 0.6637 - val_loss: 0.6325
Execution time:  3482.5707383155823
LSTM:
Mean Absolute Error: 0.7240
Root Mean Square Error: 1.0838
Mean Square Error: 1.1747

Train RMSE: 1.084
Train MSE: 1.175
Train MAE: 0.724
###########################

MODEL:  LSTM
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_78&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_78 (LSTM)               (None, 1008, 40)          6720      
_________________________________________________________________
dropout_78 (Dropout)         (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_78 (TimeDis (None, 1008, 1)           41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 24s 400ms/step - loss: 0.9761 - val_loss: 1.3214
Epoch 2/68
60/60 [==============================] - 23s 384ms/step - loss: 0.9760 - val_loss: 1.3213
Epoch 3/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9760 - val_loss: 1.3213
Epoch 4/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9760 - val_loss: 1.3212
Epoch 5/68
60/60 [==============================] - 23s 389ms/step - loss: 0.9759 - val_loss: 1.3212
Epoch 6/68
60/60 [==============================] - 23s 390ms/step - loss: 0.9759 - val_loss: 1.3211
Epoch 7/68
60/60 [==============================] - 23s 388ms/step - loss: 0.9759 - val_loss: 1.3211
Epoch 8/68
60/60 [==============================] - 24s 393ms/step - loss: 0.9758 - val_loss: 1.3210
Epoch 9/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9758 - val_loss: 1.3209
Epoch 10/68
60/60 [==============================] - 23s 385ms/step - loss: 0.9757 - val_loss: 1.3209
Epoch 11/68
60/60 [==============================] - 23s 389ms/step - loss: 0.9757 - val_loss: 1.3208
Epoch 12/68
60/60 [==============================] - 23s 391ms/step - loss: 0.9757 - val_loss: 1.3208
Epoch 13/68
60/60 [==============================] - 23s 391ms/step - loss: 0.9756 - val_loss: 1.3207
Epoch 14/68
60/60 [==============================] - 23s 386ms/step - loss: 0.9756 - val_loss: 1.3207
Epoch 15/68
60/60 [==============================] - 23s 386ms/step - loss: 0.9755 - val_loss: 1.3206
Epoch 16/68
60/60 [==============================] - 23s 389ms/step - loss: 0.9755 - val_loss: 1.3205
Epoch 17/68
60/60 [==============================] - 23s 390ms/step - loss: 0.9754 - val_loss: 1.3205
Epoch 18/68
60/60 [==============================] - 23s 386ms/step - loss: 0.9754 - val_loss: 1.3204
Epoch 19/68
60/60 [==============================] - 24s 392ms/step - loss: 0.9753 - val_loss: 1.3203
Epoch 20/68
60/60 [==============================] - 24s 400ms/step - loss: 0.9753 - val_loss: 1.3203
Epoch 21/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9753 - val_loss: 1.3202
Epoch 22/68
60/60 [==============================] - 24s 395ms/step - loss: 0.9752 - val_loss: 1.3201
Epoch 23/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9752 - val_loss: 1.3201
Epoch 24/68
60/60 [==============================] - 24s 393ms/step - loss: 0.9751 - val_loss: 1.3200
Epoch 25/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9751 - val_loss: 1.3199
Epoch 26/68
60/60 [==============================] - 24s 395ms/step - loss: 0.9750 - val_loss: 1.3199
Epoch 27/68
60/60 [==============================] - 24s 400ms/step - loss: 0.9750 - val_loss: 1.3198
Epoch 28/68
60/60 [==============================] - 24s 402ms/step - loss: 0.9749 - val_loss: 1.3197
Epoch 29/68
60/60 [==============================] - 24s 399ms/step - loss: 0.9749 - val_loss: 1.3196
Epoch 30/68
60/60 [==============================] - 24s 399ms/step - loss: 0.9748 - val_loss: 1.3196
Epoch 31/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9748 - val_loss: 1.3195
Epoch 32/68
60/60 [==============================] - 24s 397ms/step - loss: 0.9747 - val_loss: 1.3194
Epoch 33/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9747 - val_loss: 1.3194
Epoch 34/68
60/60 [==============================] - 24s 393ms/step - loss: 0.9746 - val_loss: 1.3193
Epoch 35/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9746 - val_loss: 1.3192
Epoch 36/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9745 - val_loss: 1.3191
Epoch 37/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9745 - val_loss: 1.3191
Epoch 38/68
60/60 [==============================] - 24s 399ms/step - loss: 0.9744 - val_loss: 1.3190
Epoch 39/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9743 - val_loss: 1.3189
Epoch 40/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9743 - val_loss: 1.3188
Epoch 41/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9742 - val_loss: 1.3188
Epoch 42/68
60/60 [==============================] - 24s 393ms/step - loss: 0.9742 - val_loss: 1.3187
Epoch 43/68
60/60 [==============================] - 24s 400ms/step - loss: 0.9741 - val_loss: 1.3186
Epoch 44/68
60/60 [==============================] - 24s 399ms/step - loss: 0.9741 - val_loss: 1.3185
Epoch 45/68
60/60 [==============================] - 24s 392ms/step - loss: 0.9740 - val_loss: 1.3185
Epoch 46/68
60/60 [==============================] - 24s 392ms/step - loss: 0.9740 - val_loss: 1.3184
Epoch 47/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9739 - val_loss: 1.3183
Epoch 48/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9738 - val_loss: 1.3182
Epoch 49/68
60/60 [==============================] - 24s 399ms/step - loss: 0.9738 - val_loss: 1.3181
Epoch 50/68
60/60 [==============================] - 23s 391ms/step - loss: 0.9737 - val_loss: 1.3181
Epoch 51/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9737 - val_loss: 1.3180
Epoch 52/68
60/60 [==============================] - 24s 400ms/step - loss: 0.9736 - val_loss: 1.3179
Epoch 53/68
60/60 [==============================] - 24s 395ms/step - loss: 0.9736 - val_loss: 1.3178
Epoch 54/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9735 - val_loss: 1.3178
Epoch 55/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9735 - val_loss: 1.3177
Epoch 56/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9734 - val_loss: 1.3176
Epoch 57/68
60/60 [==============================] - 24s 399ms/step - loss: 0.9733 - val_loss: 1.3175
Epoch 58/68
60/60 [==============================] - 24s 399ms/step - loss: 0.9733 - val_loss: 1.3174
Epoch 59/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9732 - val_loss: 1.3174
Epoch 60/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9732 - val_loss: 1.3173
Epoch 61/68
60/60 [==============================] - 24s 395ms/step - loss: 0.9731 - val_loss: 1.3172
Epoch 62/68
60/60 [==============================] - 24s 394ms/step - loss: 0.9731 - val_loss: 1.3171
Epoch 63/68
60/60 [==============================] - 24s 396ms/step - loss: 0.9730 - val_loss: 1.3170
Epoch 64/68
60/60 [==============================] - 24s 393ms/step - loss: 0.9729 - val_loss: 1.3169
Epoch 65/68
60/60 [==============================] - 24s 398ms/step - loss: 0.9729 - val_loss: 1.3169
Epoch 66/68
60/60 [==============================] - 24s 397ms/step - loss: 0.9728 - val_loss: 1.3168
Epoch 67/68
60/60 [==============================] - 24s 395ms/step - loss: 0.9728 - val_loss: 1.3167
Epoch 68/68
60/60 [==============================] - 24s 397ms/step - loss: 0.9727 - val_loss: 1.3166
Execution time:  1637.7548382282257
LSTM:
Mean Absolute Error: 0.9131
Root Mean Square Error: 1.1321
Mean Square Error: 1.2817

Train RMSE: 1.132
Train MSE: 1.282
Train MAE: 0.913
###########################

MODEL:  LSTM
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_79&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_79 (LSTM)               (None, 1008, 55)          12540     
_________________________________________________________________
dropout_79 (Dropout)         (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_79 (TimeDis (None, 1008, 1)           56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 95s 356ms/step - loss: 0.9769 - val_loss: 1.1420
Epoch 2/36
266/266 [==============================] - 97s 363ms/step - loss: 0.9767 - val_loss: 1.1417
Epoch 3/36
266/266 [==============================] - 94s 353ms/step - loss: 0.9764 - val_loss: 1.1414
Epoch 4/36
266/266 [==============================] - 94s 352ms/step - loss: 0.9762 - val_loss: 1.1411
Epoch 5/36
266/266 [==============================] - 97s 366ms/step - loss: 0.9759 - val_loss: 1.1408
Epoch 6/36
266/266 [==============================] - 96s 362ms/step - loss: 0.9757 - val_loss: 1.1404
Epoch 7/36
266/266 [==============================] - 95s 358ms/step - loss: 0.9754 - val_loss: 1.1400
Epoch 8/36
266/266 [==============================] - 97s 366ms/step - loss: 0.9751 - val_loss: 1.1396
Epoch 9/36
266/266 [==============================] - 97s 364ms/step - loss: 0.9748 - val_loss: 1.1392
Epoch 10/36
266/266 [==============================] - 95s 356ms/step - loss: 0.9744 - val_loss: 1.1388
Epoch 11/36
266/266 [==============================] - 95s 356ms/step - loss: 0.9741 - val_loss: 1.1383
Epoch 12/36
266/266 [==============================] - 96s 363ms/step - loss: 0.9737 - val_loss: 1.1378
Epoch 13/36
266/266 [==============================] - 98s 368ms/step - loss: 0.9734 - val_loss: 1.1373
Epoch 14/36
266/266 [==============================] - 95s 357ms/step - loss: 0.9730 - val_loss: 1.1368
Epoch 15/36
266/266 [==============================] - 97s 366ms/step - loss: 0.9726 - val_loss: 1.1363
Epoch 16/36
266/266 [==============================] - 97s 366ms/step - loss: 0.9722 - val_loss: 1.1357
Epoch 17/36
266/266 [==============================] - 97s 364ms/step - loss: 0.9718 - val_loss: 1.1351
Epoch 18/36
266/266 [==============================] - 98s 367ms/step - loss: 0.9714 - val_loss: 1.1345
Epoch 19/36
266/266 [==============================] - 96s 362ms/step - loss: 0.9710 - val_loss: 1.1339
Epoch 20/36
266/266 [==============================] - 97s 363ms/step - loss: 0.9706 - val_loss: 1.1333
Epoch 21/36
266/266 [==============================] - 97s 364ms/step - loss: 0.9701 - val_loss: 1.1326
Epoch 22/36
266/266 [==============================] - 98s 369ms/step - loss: 0.9696 - val_loss: 1.1319
Epoch 23/36
266/266 [==============================] - 96s 361ms/step - loss: 0.9692 - val_loss: 1.1312
Epoch 24/36
266/266 [==============================] - 97s 364ms/step - loss: 0.9687 - val_loss: 1.1305
Epoch 25/36
266/266 [==============================] - 95s 359ms/step - loss: 0.9682 - val_loss: 1.1297
Epoch 26/36
266/266 [==============================] - 96s 359ms/step - loss: 0.9677 - val_loss: 1.1289
Epoch 27/36
266/266 [==============================] - 98s 367ms/step - loss: 0.9671 - val_loss: 1.1281
Epoch 28/36
266/266 [==============================] - 96s 363ms/step - loss: 0.9666 - val_loss: 1.1273
Epoch 29/36
266/266 [==============================] - 97s 363ms/step - loss: 0.9660 - val_loss: 1.1264
Epoch 30/36
266/266 [==============================] - 96s 363ms/step - loss: 0.9654 - val_loss: 1.1255
Epoch 31/36
266/266 [==============================] - 97s 366ms/step - loss: 0.9648 - val_loss: 1.1246
Epoch 32/36
266/266 [==============================] - 96s 362ms/step - loss: 0.9642 - val_loss: 1.1236
Epoch 33/36
266/266 [==============================] - 95s 357ms/step - loss: 0.9636 - val_loss: 1.1226
Epoch 34/36
266/266 [==============================] - 97s 363ms/step - loss: 0.9629 - val_loss: 1.1215
Epoch 35/36
266/266 [==============================] - 97s 365ms/step - loss: 0.9622 - val_loss: 1.1204
Epoch 36/36
266/266 [==============================] - 97s 364ms/step - loss: 0.9615 - val_loss: 1.1193
Execution time:  3482.1177275180817
LSTM:
Mean Absolute Error: 0.8975
Root Mean Square Error: 1.1198
Mean Square Error: 1.2539

Train RMSE: 1.120
Train MSE: 1.254
Train MAE: 0.897
###########################

MODEL:  LSTM
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_80&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_80 (LSTM)               (None, 1008, 40)          6720      
_________________________________________________________________
dropout_80 (Dropout)         (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_80 (TimeDis (None, 1008, 1)           41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6533 - val_loss: 0.7524
Epoch 2/68
60/60 [==============================] - 24s 393ms/step - loss: 0.5911 - val_loss: 0.5156
Epoch 3/68
60/60 [==============================] - 24s 393ms/step - loss: 0.5720 - val_loss: 0.4768
Epoch 4/68
60/60 [==============================] - 24s 396ms/step - loss: 0.5663 - val_loss: 0.4574
Epoch 5/68
60/60 [==============================] - 23s 390ms/step - loss: 0.5645 - val_loss: 0.4485
Epoch 6/68
60/60 [==============================] - 24s 392ms/step - loss: 0.5629 - val_loss: 0.4418
Epoch 7/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5618 - val_loss: 0.4370
Epoch 8/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5609 - val_loss: 0.4330
Epoch 9/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5601 - val_loss: 0.4296
Epoch 10/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5594 - val_loss: 0.4268
Epoch 11/68
60/60 [==============================] - 24s 395ms/step - loss: 0.5589 - val_loss: 0.4242
Epoch 12/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5582 - val_loss: 0.4216
Epoch 13/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5578 - val_loss: 0.4191
Epoch 14/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5574 - val_loss: 0.4174
Epoch 15/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5569 - val_loss: 0.4155
Epoch 16/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5565 - val_loss: 0.4138
Epoch 17/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5562 - val_loss: 0.4121
Epoch 18/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5558 - val_loss: 0.4106
Epoch 19/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5556 - val_loss: 0.4092
Epoch 20/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5552 - val_loss: 0.4078
Epoch 21/68
60/60 [==============================] - 24s 396ms/step - loss: 0.5549 - val_loss: 0.4065
Epoch 22/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5546 - val_loss: 0.4052
Epoch 23/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5543 - val_loss: 0.4039
Epoch 24/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5541 - val_loss: 0.4028
Epoch 25/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5539 - val_loss: 0.4019
Epoch 26/68
60/60 [==============================] - 24s 398ms/step - loss: 0.5536 - val_loss: 0.4009
Epoch 27/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5534 - val_loss: 0.3998
Epoch 28/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5531 - val_loss: 0.3990
Epoch 29/68
60/60 [==============================] - 24s 395ms/step - loss: 0.5529 - val_loss: 0.3981
Epoch 30/68
60/60 [==============================] - 24s 394ms/step - loss: 0.5527 - val_loss: 0.3973
Epoch 31/68
60/60 [==============================] - 24s 398ms/step - loss: 0.5525 - val_loss: 0.3965
Epoch 32/68
60/60 [==============================] - 24s 398ms/step - loss: 0.5522 - val_loss: 0.3958
Epoch 33/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5520 - val_loss: 0.3952
Epoch 34/68
60/60 [==============================] - 24s 398ms/step - loss: 0.5517 - val_loss: 0.3949
Epoch 35/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5514 - val_loss: 0.3950
Epoch 36/68
60/60 [==============================] - 24s 393ms/step - loss: 0.5510 - val_loss: 0.3958
Epoch 37/68
60/60 [==============================] - 24s 395ms/step - loss: 0.5507 - val_loss: 0.3972
Epoch 38/68
60/60 [==============================] - 24s 398ms/step - loss: 0.5502 - val_loss: 0.3995
Epoch 39/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5499 - val_loss: 0.4035
Epoch 40/68
60/60 [==============================] - 24s 395ms/step - loss: 0.5494 - val_loss: 0.4056
Epoch 41/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5481 - val_loss: 0.3929
Epoch 42/68
60/60 [==============================] - 23s 388ms/step - loss: 0.5479 - val_loss: 0.3913
Epoch 43/68
60/60 [==============================] - 24s 398ms/step - loss: 0.5408 - val_loss: 0.4078
Epoch 44/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5442 - val_loss: 0.4222
Epoch 45/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5334 - val_loss: 0.4460
Epoch 46/68
60/60 [==============================] - 24s 392ms/step - loss: 0.5312 - val_loss: 0.4634
Epoch 47/68
60/60 [==============================] - 24s 396ms/step - loss: 0.5302 - val_loss: 0.4718
Epoch 48/68
60/60 [==============================] - 24s 394ms/step - loss: 0.5299 - val_loss: 0.4763
Epoch 49/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5297 - val_loss: 0.4807
Epoch 50/68
60/60 [==============================] - 24s 392ms/step - loss: 0.5296 - val_loss: 0.4813
Epoch 51/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5295 - val_loss: 0.4829
Epoch 52/68
60/60 [==============================] - 23s 390ms/step - loss: 0.5294 - val_loss: 0.4832
Execution time:  1262.1854693889618
LSTM:
Mean Absolute Error: 0.6645
Root Mean Square Error: 1.1549
Mean Square Error: 1.3337

Train RMSE: 1.155
Train MSE: 1.334
Train MAE: 0.665
###########################

MODEL:  LSTM
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_81&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_81 (LSTM)               (None, 1008, 55)          12540     
_________________________________________________________________
dropout_81 (Dropout)         (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_81 (TimeDis (None, 1008, 1)           56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 92s 345ms/step - loss: 0.6391 - val_loss: 0.4254
Epoch 2/36
266/266 [==============================] - 99s 373ms/step - loss: 0.6013 - val_loss: 0.3253
Epoch 3/36
266/266 [==============================] - 99s 371ms/step - loss: 0.5972 - val_loss: 0.3108
Epoch 4/36
266/266 [==============================] - 99s 373ms/step - loss: 0.5951 - val_loss: 0.3018
Epoch 5/36
266/266 [==============================] - 98s 368ms/step - loss: 0.5940 - val_loss: 0.2989
Epoch 6/36
266/266 [==============================] - 93s 349ms/step - loss: 0.5934 - val_loss: 0.3008
Epoch 7/36
266/266 [==============================] - 91s 343ms/step - loss: 0.5929 - val_loss: 0.3040
Epoch 8/36
266/266 [==============================] - 93s 351ms/step - loss: 0.5921 - val_loss: 0.3025
Epoch 9/36
266/266 [==============================] - 95s 356ms/step - loss: 0.5910 - val_loss: 0.3030
Epoch 10/36
266/266 [==============================] - 93s 351ms/step - loss: 0.5902 - val_loss: 0.3036
Epoch 11/36
266/266 [==============================] - 92s 347ms/step - loss: 0.5896 - val_loss: 0.3022
Epoch 12/36
266/266 [==============================] - 94s 352ms/step - loss: 0.5891 - val_loss: 0.3032
Epoch 13/36
266/266 [==============================] - 93s 348ms/step - loss: 0.5887 - val_loss: 0.3041
Epoch 14/36
266/266 [==============================] - 93s 349ms/step - loss: 0.5884 - val_loss: 0.3026
Epoch 15/36
266/266 [==============================] - 93s 349ms/step - loss: 0.5881 - val_loss: 0.3050
Execution time:  1424.8833267688751
LSTM:
Mean Absolute Error: 0.6535
Root Mean Square Error: 1.1601
Mean Square Error: 1.3459

Train RMSE: 1.160
Train MSE: 1.346
Train MAE: 0.653
###########################

MODEL:  LSTM
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_82&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_82 (LSTM)               (None, 1008, 40)          6720      
_________________________________________________________________
dropout_82 (Dropout)         (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_82 (TimeDis (None, 1008, 1)           41        
=================================================================
Total params: 6,761
Trainable params: 6,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 24s 406ms/step - loss: 0.9654 - val_loss: 1.2824
Epoch 2/68
60/60 [==============================] - 24s 392ms/step - loss: 0.8706 - val_loss: 1.0379
Epoch 3/68
60/60 [==============================] - 24s 396ms/step - loss: 0.7462 - val_loss: 0.9000
Epoch 4/68
60/60 [==============================] - 24s 400ms/step - loss: 0.7017 - val_loss: 0.8584
Epoch 5/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6893 - val_loss: 0.8451
Epoch 6/68
60/60 [==============================] - 23s 391ms/step - loss: 0.6844 - val_loss: 0.8387
Epoch 7/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6817 - val_loss: 0.8350
Epoch 8/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6801 - val_loss: 0.8325
Epoch 9/68
60/60 [==============================] - 24s 393ms/step - loss: 0.6789 - val_loss: 0.8308
Epoch 10/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6781 - val_loss: 0.8295
Epoch 11/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6774 - val_loss: 0.8285
Epoch 12/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6769 - val_loss: 0.8276
Epoch 13/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6765 - val_loss: 0.8270
Epoch 14/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6761 - val_loss: 0.8264
Epoch 15/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6758 - val_loss: 0.8259
Epoch 16/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6756 - val_loss: 0.8255
Epoch 17/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6753 - val_loss: 0.8251
Epoch 18/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6751 - val_loss: 0.8248
Epoch 19/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6750 - val_loss: 0.8245
Epoch 20/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6748 - val_loss: 0.8243
Epoch 21/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6747 - val_loss: 0.8241
Epoch 22/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6745 - val_loss: 0.8239
Epoch 23/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6744 - val_loss: 0.8237
Epoch 24/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6743 - val_loss: 0.8235
Epoch 25/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6742 - val_loss: 0.8233
Epoch 26/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6741 - val_loss: 0.8232
Epoch 27/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6741 - val_loss: 0.8231
Epoch 28/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6740 - val_loss: 0.8229
Epoch 29/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6739 - val_loss: 0.8228
Epoch 30/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6738 - val_loss: 0.8227
Epoch 31/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6738 - val_loss: 0.8226
Epoch 32/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6737 - val_loss: 0.8225
Epoch 33/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6737 - val_loss: 0.8224
Epoch 34/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6736 - val_loss: 0.8223
Epoch 35/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6736 - val_loss: 0.8223
Epoch 36/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6735 - val_loss: 0.8222
Epoch 37/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6735 - val_loss: 0.8221
Epoch 38/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6734 - val_loss: 0.8221
Epoch 39/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6734 - val_loss: 0.8220
Epoch 40/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6734 - val_loss: 0.8219
Epoch 41/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6733 - val_loss: 0.8219
Epoch 42/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6733 - val_loss: 0.8218
Epoch 43/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6733 - val_loss: 0.8218
Epoch 44/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6732 - val_loss: 0.8217
Epoch 45/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6732 - val_loss: 0.8217
Epoch 46/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6732 - val_loss: 0.8216
Epoch 47/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6731 - val_loss: 0.8216
Epoch 48/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6731 - val_loss: 0.8215
Epoch 49/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6731 - val_loss: 0.8215
Epoch 50/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6731 - val_loss: 0.8214
Epoch 51/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6730 - val_loss: 0.8214
Epoch 52/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6730 - val_loss: 0.8214
Epoch 53/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6730 - val_loss: 0.8214
Epoch 54/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6730 - val_loss: 0.8213
Epoch 55/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6730 - val_loss: 0.8213
Epoch 56/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6729 - val_loss: 0.8213
Epoch 57/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6729 - val_loss: 0.8212
Epoch 58/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6729 - val_loss: 0.8212
Epoch 59/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6729 - val_loss: 0.8212
Epoch 60/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6729 - val_loss: 0.8211
Epoch 61/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6729 - val_loss: 0.8211
Epoch 62/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6728 - val_loss: 0.8211
Epoch 63/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6728 - val_loss: 0.8211
Epoch 64/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6728 - val_loss: 0.8211
Epoch 65/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 66/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 67/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6728 - val_loss: 0.8210
Epoch 68/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8210
Execution time:  1665.0534462928772
LSTM:
Mean Absolute Error: 0.6895
Root Mean Square Error: 1.0230
Mean Square Error: 1.0466

Train RMSE: 1.023
Train MSE: 1.047
Train MAE: 0.689
###########################

MODEL:  LSTM
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_83&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_83 (LSTM)               (None, 1008, 55)          12540     
_________________________________________________________________
dropout_83 (Dropout)         (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_83 (TimeDis (None, 1008, 1)           56        
=================================================================
Total params: 12,596
Trainable params: 12,596
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 96s 359ms/step - loss: 0.8270 - val_loss: 0.6715
Epoch 2/36
266/266 [==============================] - 97s 365ms/step - loss: 0.7005 - val_loss: 0.6626
Epoch 3/36
266/266 [==============================] - 94s 353ms/step - loss: 0.6982 - val_loss: 0.6603
Epoch 4/36
266/266 [==============================] - 92s 344ms/step - loss: 0.6973 - val_loss: 0.6592
Epoch 5/36
266/266 [==============================] - 93s 348ms/step - loss: 0.6969 - val_loss: 0.6585
Epoch 6/36
266/266 [==============================] - 93s 348ms/step - loss: 0.6966 - val_loss: 0.6580
Epoch 7/36
266/266 [==============================] - 92s 346ms/step - loss: 0.6963 - val_loss: 0.6576
Epoch 8/36
266/266 [==============================] - 95s 355ms/step - loss: 0.6962 - val_loss: 0.6573
Epoch 9/36
266/266 [==============================] - 93s 349ms/step - loss: 0.6960 - val_loss: 0.6571
Epoch 10/36
266/266 [==============================] - 93s 351ms/step - loss: 0.6959 - val_loss: 0.6569
Epoch 11/36
266/266 [==============================] - 97s 364ms/step - loss: 0.6958 - val_loss: 0.6567
Epoch 12/36
266/266 [==============================] - 97s 363ms/step - loss: 0.6957 - val_loss: 0.6566
Epoch 13/36
266/266 [==============================] - 95s 358ms/step - loss: 0.6957 - val_loss: 0.6564
Epoch 14/36
266/266 [==============================] - 94s 354ms/step - loss: 0.6956 - val_loss: 0.6563
Epoch 15/36
266/266 [==============================] - 96s 359ms/step - loss: 0.6956 - val_loss: 0.6562
Epoch 16/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6955 - val_loss: 0.6561
Epoch 17/36
266/266 [==============================] - 95s 359ms/step - loss: 0.6955 - val_loss: 0.6561
Epoch 18/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6954 - val_loss: 0.6560
Epoch 19/36
266/266 [==============================] - 94s 355ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 20/36
266/266 [==============================] - 95s 356ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 21/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6954 - val_loss: 0.6558
Epoch 22/36
266/266 [==============================] - 93s 351ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 23/36
266/266 [==============================] - 94s 354ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 24/36
266/266 [==============================] - 97s 363ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 25/36
266/266 [==============================] - 94s 355ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 26/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 27/36
266/266 [==============================] - 95s 357ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 28/36
266/266 [==============================] - 95s 356ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 29/36
266/266 [==============================] - 97s 364ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 30/36
266/266 [==============================] - 93s 351ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 31/36
266/266 [==============================] - 97s 366ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 32/36
266/266 [==============================] - 95s 357ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 33/36
266/266 [==============================] - 95s 356ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 34/36
266/266 [==============================] - 97s 365ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 35/36
266/266 [==============================] - 96s 363ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 36/36
266/266 [==============================] - 96s 360ms/step - loss: 0.6953 - val_loss: 0.6556
Execution time:  3432.49423289299
LSTM:
Mean Absolute Error: 0.6893
Root Mean Square Error: 1.0230
Mean Square Error: 1.0465

Train RMSE: 1.023
Train MSE: 1.046
Train MAE: 0.689
###########################

MODEL:  GRU
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_84&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru (GRU)                    (None, 6, 40)             5160      
_________________________________________________________________
dropout_84 (Dropout)         (None, 6, 40)             0         
_________________________________________________________________
time_distributed_84 (TimeDis (None, 6, 1)              41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5337 - val_loss: 0.2678
Epoch 2/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3511 - val_loss: 0.2222
Epoch 3/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3131 - val_loss: 0.1869
Epoch 4/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2993 - val_loss: 0.1687
Epoch 5/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2911 - val_loss: 0.1550
Epoch 6/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2846 - val_loss: 0.1484
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2815 - val_loss: 0.1408
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2780 - val_loss: 0.1386
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2773 - val_loss: 0.1396
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2760 - val_loss: 0.1379
Epoch 11/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.1380
Epoch 12/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1397
Epoch 13/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2749 - val_loss: 0.1388
Epoch 14/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2742 - val_loss: 0.1371
Epoch 15/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.1374
Epoch 16/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.1371
Epoch 17/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2729 - val_loss: 0.1391
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2724 - val_loss: 0.1357
Epoch 19/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.1375
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2723 - val_loss: 0.1374
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.1371
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2711 - val_loss: 0.1351
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2718 - val_loss: 0.1343
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2709 - val_loss: 0.1356
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2706 - val_loss: 0.1348
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2701 - val_loss: 0.1348
Epoch 27/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2704 - val_loss: 0.1342
Epoch 28/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2703 - val_loss: 0.1321
Epoch 29/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2694 - val_loss: 0.1341
Epoch 30/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2701 - val_loss: 0.1318
Epoch 31/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2698 - val_loss: 0.1322
Epoch 32/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2693 - val_loss: 0.1325
Epoch 33/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2693 - val_loss: 0.1326
Epoch 34/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2691 - val_loss: 0.1316
Epoch 35/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2689 - val_loss: 0.1313
Epoch 36/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2690 - val_loss: 0.1280
Epoch 37/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2688 - val_loss: 0.1297
Epoch 38/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2682 - val_loss: 0.1285
Epoch 39/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2681 - val_loss: 0.1257
Epoch 40/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2681 - val_loss: 0.1265
Epoch 41/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2675 - val_loss: 0.1259
Epoch 42/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2675 - val_loss: 0.1287
Epoch 43/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2672 - val_loss: 0.1268
Epoch 44/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2668 - val_loss: 0.1276
Epoch 45/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2671 - val_loss: 0.1245
Epoch 46/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2665 - val_loss: 0.1238
Epoch 47/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2664 - val_loss: 0.1226
Epoch 48/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2668 - val_loss: 0.1249
Epoch 49/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2662 - val_loss: 0.1213
Epoch 50/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2662 - val_loss: 0.1220
Epoch 51/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2654 - val_loss: 0.1216
Epoch 52/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2655 - val_loss: 0.1191
Epoch 53/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2651 - val_loss: 0.1192
Epoch 54/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2653 - val_loss: 0.1165
Epoch 55/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2647 - val_loss: 0.1172
Epoch 56/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2646 - val_loss: 0.1155
Epoch 57/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2641 - val_loss: 0.1190
Epoch 58/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2645 - val_loss: 0.1187
Epoch 59/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2643 - val_loss: 0.1175
Epoch 60/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2641 - val_loss: 0.1144
Epoch 61/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2636 - val_loss: 0.1151
Epoch 62/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2638 - val_loss: 0.1162
Epoch 63/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2638 - val_loss: 0.1161
Epoch 64/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2643 - val_loss: 0.1110
Epoch 65/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2632 - val_loss: 0.1122
Epoch 66/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2630 - val_loss: 0.1127
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2633 - val_loss: 0.1137
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2629 - val_loss: 0.1113
Execution time:  30.32288146018982
GRU:
Mean Absolute Error: 0.1603
Root Mean Square Error: 0.5823
Mean Square Error: 0.3391

Train RMSE: 0.582
Train MSE: 0.339
Train MAE: 0.160
###########################

MODEL:  GRU
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_85&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_1 (GRU)                  (None, 6, 55)             9570      
_________________________________________________________________
dropout_85 (Dropout)         (None, 6, 55)             0         
_________________________________________________________________
time_distributed_85 (TimeDis (None, 6, 1)              56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 6ms/step - loss: 0.3530 - val_loss: 0.2207
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2762 - val_loss: 0.2055
Epoch 3/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2702 - val_loss: 0.2060
Epoch 4/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2690 - val_loss: 0.2056
Epoch 5/36
355/355 [==============================] - 2s 5ms/step - loss: 0.2684 - val_loss: 0.2063
Epoch 6/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2677 - val_loss: 0.2053
Epoch 7/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2671 - val_loss: 0.2049
Epoch 8/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2669 - val_loss: 0.2040
Epoch 9/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2660 - val_loss: 0.2033
Epoch 10/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2659 - val_loss: 0.2035
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2653 - val_loss: 0.2020
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2649 - val_loss: 0.2005
Epoch 13/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2645 - val_loss: 0.1994
Epoch 14/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2639 - val_loss: 0.1982
Epoch 15/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2633 - val_loss: 0.1975
Epoch 16/36
355/355 [==============================] - 2s 5ms/step - loss: 0.2628 - val_loss: 0.1964
Epoch 17/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2625 - val_loss: 0.1946
Epoch 18/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2623 - val_loss: 0.1931
Epoch 19/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2617 - val_loss: 0.1935
Epoch 20/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2611 - val_loss: 0.1907
Epoch 21/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2612 - val_loss: 0.1890
Epoch 22/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2606 - val_loss: 0.1890
Epoch 23/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2600 - val_loss: 0.1872
Epoch 24/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2600 - val_loss: 0.1869
Epoch 25/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2596 - val_loss: 0.1865
Epoch 26/36
355/355 [==============================] - 2s 5ms/step - loss: 0.2594 - val_loss: 0.1847
Epoch 27/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2589 - val_loss: 0.1837
Epoch 28/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2589 - val_loss: 0.1825
Epoch 29/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2583 - val_loss: 0.1832
Epoch 30/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2585 - val_loss: 0.1818
Epoch 31/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2581 - val_loss: 0.1815
Epoch 32/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2581 - val_loss: 0.1810
Epoch 33/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2577 - val_loss: 0.1798
Epoch 34/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2580 - val_loss: 0.1812
Epoch 35/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2578 - val_loss: 0.1800
Epoch 36/36
355/355 [==============================] - 2s 5ms/step - loss: 0.2579 - val_loss: 0.1794
Execution time:  58.817262411117554
GRU:
Mean Absolute Error: 0.1541
Root Mean Square Error: 0.5817
Mean Square Error: 0.3383

Train RMSE: 0.582
Train MSE: 0.338
Train MAE: 0.154
###########################

MODEL:  GRU
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_86&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_2 (GRU)                  (None, 6, 40)             5160      
_________________________________________________________________
dropout_86 (Dropout)         (None, 6, 40)             0         
_________________________________________________________________
time_distributed_86 (TimeDis (None, 6, 1)              41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 11ms/step - loss: 0.8019 - val_loss: 0.9501
Epoch 2/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5860 - val_loss: 0.8503
Epoch 3/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5540 - val_loss: 0.8299
Epoch 4/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5399 - val_loss: 0.8183
Epoch 5/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5285 - val_loss: 0.8105
Epoch 6/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5181 - val_loss: 0.8049
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5125 - val_loss: 0.8013
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5076 - val_loss: 0.7987
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5043 - val_loss: 0.7970
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5020 - val_loss: 0.7958
Epoch 11/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5002 - val_loss: 0.7950
Epoch 12/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4991 - val_loss: 0.7944
Epoch 13/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4981 - val_loss: 0.7941
Epoch 14/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4970 - val_loss: 0.7938
Epoch 15/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4966 - val_loss: 0.7936
Epoch 16/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4958 - val_loss: 0.7934
Epoch 17/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4957 - val_loss: 0.7933
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.7932
Epoch 19/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4949 - val_loss: 0.7931
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 0.7930
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4947 - val_loss: 0.7930
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4941 - val_loss: 0.7929
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4944 - val_loss: 0.7929
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4942 - val_loss: 0.7929
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7928
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928
Epoch 27/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7928
Epoch 28/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7928
Epoch 29/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7928
Epoch 30/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7928
Epoch 31/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927
Epoch 32/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927
Epoch 33/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 34/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 35/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 36/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 37/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 38/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 39/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 40/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 41/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 42/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 43/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 44/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 45/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927
Epoch 46/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 47/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927
Epoch 48/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 49/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 50/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 51/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 52/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927
Epoch 53/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927
Epoch 54/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927
Epoch 55/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927
Epoch 56/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 57/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927
Epoch 58/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 59/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927
Epoch 60/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927
Epoch 61/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927
Epoch 62/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.7927
Epoch 63/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927
Epoch 64/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927
Epoch 65/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927
Epoch 66/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.7927
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4930 - val_loss: 0.7927
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.7927
Execution time:  29.47386884689331
GRU:
Mean Absolute Error: 0.4832
Root Mean Square Error: 0.7518
Mean Square Error: 0.5651

Train RMSE: 0.752
Train MSE: 0.565
Train MAE: 0.483
###########################

MODEL:  GRU
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_87&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_3 (GRU)                  (None, 6, 55)             9570      
_________________________________________________________________
dropout_87 (Dropout)         (None, 6, 55)             0         
_________________________________________________________________
time_distributed_87 (TimeDis (None, 6, 1)              56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 6ms/step - loss: 0.6176 - val_loss: 0.6844
Epoch 2/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5212 - val_loss: 0.6629
Epoch 3/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5022 - val_loss: 0.6550
Epoch 4/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4961 - val_loss: 0.6523
Epoch 5/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4938 - val_loss: 0.6514
Epoch 6/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4925 - val_loss: 0.6506
Epoch 7/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4922 - val_loss: 0.6503
Epoch 8/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4920 - val_loss: 0.6501
Epoch 9/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4920 - val_loss: 0.6498
Epoch 10/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4918 - val_loss: 0.6498
Epoch 11/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6496
Epoch 12/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6498
Epoch 13/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6495
Epoch 14/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6496
Epoch 15/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6494
Epoch 16/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6498
Epoch 17/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6497
Epoch 18/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6495
Epoch 19/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4915 - val_loss: 0.6496
Epoch 20/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4911 - val_loss: 0.6496
Epoch 21/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6495
Epoch 22/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4908 - val_loss: 0.6498
Epoch 23/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4907 - val_loss: 0.6496
Epoch 24/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4908 - val_loss: 0.6494
Epoch 25/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4908 - val_loss: 0.6496
Epoch 26/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4907 - val_loss: 0.6494
Epoch 27/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4905 - val_loss: 0.6499
Epoch 28/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4903 - val_loss: 0.6496
Epoch 29/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4904 - val_loss: 0.6494
Epoch 30/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4905 - val_loss: 0.6499
Epoch 31/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4904 - val_loss: 0.6496
Epoch 32/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4901 - val_loss: 0.6495
Epoch 33/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4899 - val_loss: 0.6497
Epoch 34/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4901 - val_loss: 0.6495
Epoch 35/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4900 - val_loss: 0.6495
Epoch 36/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4902 - val_loss: 0.6495
Execution time:  60.4005651473999
GRU:
Mean Absolute Error: 0.4832
Root Mean Square Error: 0.7517
Mean Square Error: 0.5650

Train RMSE: 0.752
Train MSE: 0.565
Train MAE: 0.483
###########################

MODEL:  GRU
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_88&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_4 (GRU)                  (None, 6, 40)             5160      
_________________________________________________________________
dropout_88 (Dropout)         (None, 6, 40)             0         
_________________________________________________________________
time_distributed_88 (TimeDis (None, 6, 1)              41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 11ms/step - loss: 0.7307 - val_loss: 0.8310
Epoch 2/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7300 - val_loss: 0.8305
Epoch 3/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7302 - val_loss: 0.8300
Epoch 4/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7292 - val_loss: 0.8295
Epoch 5/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7292 - val_loss: 0.8290
Epoch 6/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7287 - val_loss: 0.8285
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7284 - val_loss: 0.8279
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7281 - val_loss: 0.8274
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7277 - val_loss: 0.8269
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7275 - val_loss: 0.8264
Epoch 11/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7270 - val_loss: 0.8258
Epoch 12/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7267 - val_loss: 0.8253
Epoch 13/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7264 - val_loss: 0.8247
Epoch 14/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7259 - val_loss: 0.8242
Epoch 15/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7257 - val_loss: 0.8237
Epoch 16/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7253 - val_loss: 0.8231
Epoch 17/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7249 - val_loss: 0.8226
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7244 - val_loss: 0.8220
Epoch 19/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7242 - val_loss: 0.8215
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7238 - val_loss: 0.8209
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7238 - val_loss: 0.8204
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7231 - val_loss: 0.8198
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7229 - val_loss: 0.8193
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7225 - val_loss: 0.8187
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7222 - val_loss: 0.8182
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7216 - val_loss: 0.8176
Epoch 27/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7216 - val_loss: 0.8170
Epoch 28/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7211 - val_loss: 0.8165
Epoch 29/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7208 - val_loss: 0.8159
Epoch 30/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7204 - val_loss: 0.8154
Epoch 31/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7202 - val_loss: 0.8148
Epoch 32/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7197 - val_loss: 0.8143
Epoch 33/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7194 - val_loss: 0.8137
Epoch 34/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7189 - val_loss: 0.8131
Epoch 35/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7189 - val_loss: 0.8126
Epoch 36/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7182 - val_loss: 0.8120
Epoch 37/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7179 - val_loss: 0.8115
Epoch 38/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7173 - val_loss: 0.8109
Epoch 39/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7173 - val_loss: 0.8103
Epoch 40/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7169 - val_loss: 0.8098
Epoch 41/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7165 - val_loss: 0.8092
Epoch 42/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7161 - val_loss: 0.8086
Epoch 43/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7158 - val_loss: 0.8081
Epoch 44/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7154 - val_loss: 0.8075
Epoch 45/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7153 - val_loss: 0.8070
Epoch 46/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7148 - val_loss: 0.8064
Epoch 47/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7143 - val_loss: 0.8058
Epoch 48/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7140 - val_loss: 0.8053
Epoch 49/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7135 - val_loss: 0.8047
Epoch 50/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7134 - val_loss: 0.8041
Epoch 51/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7129 - val_loss: 0.8036
Epoch 52/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7129 - val_loss: 0.8030
Epoch 53/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7126 - val_loss: 0.8025
Epoch 54/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7119 - val_loss: 0.8019
Epoch 55/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7116 - val_loss: 0.8013
Epoch 56/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7115 - val_loss: 0.8008
Epoch 57/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7109 - val_loss: 0.8002
Epoch 58/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7105 - val_loss: 0.7996
Epoch 59/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7102 - val_loss: 0.7991
Epoch 60/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7097 - val_loss: 0.7985
Epoch 61/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7095 - val_loss: 0.7979
Epoch 62/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7090 - val_loss: 0.7974
Epoch 63/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7088 - val_loss: 0.7968
Epoch 64/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7086 - val_loss: 0.7962
Epoch 65/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7079 - val_loss: 0.7957
Epoch 66/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7078 - val_loss: 0.7951
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7074 - val_loss: 0.7945
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7069 - val_loss: 0.7940
Execution time:  28.98744297027588
GRU:
Mean Absolute Error: 0.7161
Root Mean Square Error: 1.0171
Mean Square Error: 1.0346

Train RMSE: 1.017
Train MSE: 1.035
Train MAE: 0.716
###########################

MODEL:  GRU
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_89&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_5 (GRU)                  (None, 6, 55)             9570      
_________________________________________________________________
dropout_89 (Dropout)         (None, 6, 55)             0         
_________________________________________________________________
time_distributed_89 (TimeDis (None, 6, 1)              56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 6ms/step - loss: 0.7437 - val_loss: 0.7267
Epoch 2/36
355/355 [==============================] - 2s 4ms/step - loss: 0.7412 - val_loss: 0.7236
Epoch 3/36
355/355 [==============================] - 2s 5ms/step - loss: 0.7387 - val_loss: 0.7205
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7363 - val_loss: 0.7173
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7339 - val_loss: 0.7141
Epoch 6/36
355/355 [==============================] - 2s 4ms/step - loss: 0.7313 - val_loss: 0.7108
Epoch 7/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7288 - val_loss: 0.7075
Epoch 8/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7261 - val_loss: 0.7042
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7238 - val_loss: 0.7008
Epoch 10/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7214 - val_loss: 0.6974
Epoch 11/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7189 - val_loss: 0.6939
Epoch 12/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7165 - val_loss: 0.6904
Epoch 13/36
355/355 [==============================] - 2s 5ms/step - loss: 0.7138 - val_loss: 0.6869
Epoch 14/36
355/355 [==============================] - 2s 4ms/step - loss: 0.7112 - val_loss: 0.6834
Epoch 15/36
355/355 [==============================] - 2s 4ms/step - loss: 0.7087 - val_loss: 0.6799
Epoch 16/36
355/355 [==============================] - 2s 4ms/step - loss: 0.7062 - val_loss: 0.6763
Epoch 17/36
355/355 [==============================] - 1s 4ms/step - loss: 0.7036 - val_loss: 0.6727
Epoch 18/36
355/355 [==============================] - 2s 4ms/step - loss: 0.7010 - val_loss: 0.6691
Epoch 19/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6983 - val_loss: 0.6655
Epoch 20/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6959 - val_loss: 0.6618
Epoch 21/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6932 - val_loss: 0.6581
Epoch 22/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6906 - val_loss: 0.6544
Epoch 23/36
355/355 [==============================] - 2s 5ms/step - loss: 0.6881 - val_loss: 0.6507
Epoch 24/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6854 - val_loss: 0.6470
Epoch 25/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6829 - val_loss: 0.6433
Epoch 26/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6800 - val_loss: 0.6396
Epoch 27/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6773 - val_loss: 0.6359
Epoch 28/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6748 - val_loss: 0.6323
Epoch 29/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6720 - val_loss: 0.6286
Epoch 30/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6696 - val_loss: 0.6250
Epoch 31/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6668 - val_loss: 0.6214
Epoch 32/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6644 - val_loss: 0.6178
Epoch 33/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6617 - val_loss: 0.6143
Epoch 34/36
355/355 [==============================] - 2s 5ms/step - loss: 0.6590 - val_loss: 0.6107
Epoch 35/36
355/355 [==============================] - 2s 5ms/step - loss: 0.6564 - val_loss: 0.6072
Epoch 36/36
355/355 [==============================] - 2s 4ms/step - loss: 0.6538 - val_loss: 0.6036
Execution time:  58.58254289627075
GRU:
Mean Absolute Error: 0.6401
Root Mean Square Error: 0.9427
Mean Square Error: 0.8886

Train RMSE: 0.943
Train MSE: 0.889
Train MAE: 0.640
###########################

MODEL:  GRU
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_90&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_6 (GRU)                  (None, 6, 40)             5160      
_________________________________________________________________
dropout_90 (Dropout)         (None, 6, 40)             0         
_________________________________________________________________
time_distributed_90 (TimeDis (None, 6, 1)              41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8835 - val_loss: 1.2923
Epoch 2/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8835 - val_loss: 1.2921
Epoch 3/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8834 - val_loss: 1.2920
Epoch 4/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8834 - val_loss: 1.2918
Epoch 5/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8831 - val_loss: 1.2917
Epoch 6/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8831 - val_loss: 1.2915
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8830 - val_loss: 1.2914
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8830 - val_loss: 1.2912
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8828 - val_loss: 1.2910
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8828 - val_loss: 1.2909
Epoch 11/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8826 - val_loss: 1.2907
Epoch 12/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8825 - val_loss: 1.2905
Epoch 13/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8825 - val_loss: 1.2904
Epoch 14/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8823 - val_loss: 1.2902
Epoch 15/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8822 - val_loss: 1.2900
Epoch 16/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8821 - val_loss: 1.2899
Epoch 17/68
80/80 [==============================] - 0s 4ms/step - loss: 0.8819 - val_loss: 1.2897
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8819 - val_loss: 1.2895
Epoch 19/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8818 - val_loss: 1.2894
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8817 - val_loss: 1.2892
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8816 - val_loss: 1.2890
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8815 - val_loss: 1.2888
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8814 - val_loss: 1.2887
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8812 - val_loss: 1.2885
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8811 - val_loss: 1.2883
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8811 - val_loss: 1.2881
Epoch 27/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8809 - val_loss: 1.2880
Epoch 28/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8810 - val_loss: 1.2878
Epoch 29/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8807 - val_loss: 1.2876
Epoch 30/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8806 - val_loss: 1.2874
Epoch 31/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8806 - val_loss: 1.2872
Epoch 32/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8804 - val_loss: 1.2871
Epoch 33/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8803 - val_loss: 1.2869
Epoch 34/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8802 - val_loss: 1.2867
Epoch 35/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8801 - val_loss: 1.2865
Epoch 36/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8800 - val_loss: 1.2863
Epoch 37/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8799 - val_loss: 1.2862
Epoch 38/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8798 - val_loss: 1.2860
Epoch 39/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8796 - val_loss: 1.2858
Epoch 40/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8796 - val_loss: 1.2856
Epoch 41/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8794 - val_loss: 1.2854
Epoch 42/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8794 - val_loss: 1.2852
Epoch 43/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8792 - val_loss: 1.2851
Epoch 44/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8791 - val_loss: 1.2849
Epoch 45/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8789 - val_loss: 1.2847
Epoch 46/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8789 - val_loss: 1.2845
Epoch 47/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8787 - val_loss: 1.2843
Epoch 48/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8787 - val_loss: 1.2841
Epoch 49/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8786 - val_loss: 1.2839
Epoch 50/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8784 - val_loss: 1.2838
Epoch 51/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8782 - val_loss: 1.2836
Epoch 52/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8783 - val_loss: 1.2834
Epoch 53/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8782 - val_loss: 1.2832
Epoch 54/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8779 - val_loss: 1.2830
Epoch 55/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8779 - val_loss: 1.2828
Epoch 56/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8778 - val_loss: 1.2826
Epoch 57/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8776 - val_loss: 1.2824
Epoch 58/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8775 - val_loss: 1.2823
Epoch 59/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8774 - val_loss: 1.2821
Epoch 60/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8772 - val_loss: 1.2819
Epoch 61/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8771 - val_loss: 1.2817
Epoch 62/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8770 - val_loss: 1.2815
Epoch 63/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8769 - val_loss: 1.2813
Epoch 64/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8768 - val_loss: 1.2811
Epoch 65/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8767 - val_loss: 1.2809
Epoch 66/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8766 - val_loss: 1.2807
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8765 - val_loss: 1.2805
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.8763 - val_loss: 1.2803
Execution time:  29.0505051612854
GRU:
Mean Absolute Error: 0.9153
Root Mean Square Error: 1.1092
Mean Square Error: 1.2303

Train RMSE: 1.109
Train MSE: 1.230
Train MAE: 0.915
###########################

MODEL:  GRU
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_91&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_7 (GRU)                  (None, 6, 55)             9570      
_________________________________________________________________
dropout_91 (Dropout)         (None, 6, 55)             0         
_________________________________________________________________
time_distributed_91 (TimeDis (None, 6, 1)              56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 6ms/step - loss: 0.8706 - val_loss: 1.1245
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8700 - val_loss: 1.1237
Epoch 3/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8694 - val_loss: 1.1229
Epoch 4/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8688 - val_loss: 1.1220
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8683 - val_loss: 1.1212
Epoch 6/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8677 - val_loss: 1.1203
Epoch 7/36
355/355 [==============================] - 2s 5ms/step - loss: 0.8671 - val_loss: 1.1194
Epoch 8/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8664 - val_loss: 1.1184
Epoch 9/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8658 - val_loss: 1.1175
Epoch 10/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8652 - val_loss: 1.1165
Epoch 11/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8645 - val_loss: 1.1156
Epoch 12/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8639 - val_loss: 1.1146
Epoch 13/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8632 - val_loss: 1.1136
Epoch 14/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8626 - val_loss: 1.1126
Epoch 15/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8619 - val_loss: 1.1115
Epoch 16/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8612 - val_loss: 1.1105
Epoch 17/36
355/355 [==============================] - 2s 5ms/step - loss: 0.8605 - val_loss: 1.1095
Epoch 18/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8598 - val_loss: 1.1084
Epoch 19/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8592 - val_loss: 1.1073
Epoch 20/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8584 - val_loss: 1.1062
Epoch 21/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8577 - val_loss: 1.1051
Epoch 22/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8570 - val_loss: 1.1040
Epoch 23/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8562 - val_loss: 1.1029
Epoch 24/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8556 - val_loss: 1.1018
Epoch 25/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8549 - val_loss: 1.1007
Epoch 26/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8541 - val_loss: 1.0995
Epoch 27/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8533 - val_loss: 1.0984
Epoch 28/36
355/355 [==============================] - 2s 5ms/step - loss: 0.8526 - val_loss: 1.0972
Epoch 29/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8518 - val_loss: 1.0960
Epoch 30/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8511 - val_loss: 1.0948
Epoch 31/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8503 - val_loss: 1.0936
Epoch 32/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8495 - val_loss: 1.0924
Epoch 33/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8488 - val_loss: 1.0912
Epoch 34/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8479 - val_loss: 1.0900
Epoch 35/36
355/355 [==============================] - 2s 4ms/step - loss: 0.8471 - val_loss: 1.0887
Epoch 36/36
355/355 [==============================] - 1s 4ms/step - loss: 0.8463 - val_loss: 1.0875
Execution time:  59.125470876693726
GRU:
Mean Absolute Error: 0.8920
Root Mean Square Error: 1.0852
Mean Square Error: 1.1776

Train RMSE: 1.085
Train MSE: 1.178
Train MAE: 0.892
###########################

MODEL:  GRU
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_92&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_8 (GRU)                  (None, 6, 40)             5160      
_________________________________________________________________
dropout_92 (Dropout)         (None, 6, 40)             0         
_________________________________________________________________
time_distributed_92 (TimeDis (None, 6, 1)              41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 11ms/step - loss: 0.5682 - val_loss: 0.3907
Epoch 2/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3996 - val_loss: 0.2718
Epoch 3/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3555 - val_loss: 0.2483
Epoch 4/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3362 - val_loss: 0.2260
Epoch 5/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3208 - val_loss: 0.2041
Epoch 6/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3099 - val_loss: 0.1891
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.3034 - val_loss: 0.1781
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2978 - val_loss: 0.1705
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2946 - val_loss: 0.1641
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2912 - val_loss: 0.1586
Epoch 11/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2883 - val_loss: 0.1542
Epoch 12/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2856 - val_loss: 0.1506
Epoch 13/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2837 - val_loss: 0.1480
Epoch 14/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2816 - val_loss: 0.1459
Epoch 15/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2803 - val_loss: 0.1439
Epoch 16/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2788 - val_loss: 0.1427
Epoch 17/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2785 - val_loss: 0.1436
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2772 - val_loss: 0.1408
Epoch 19/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2771 - val_loss: 0.1405
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.1409
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2762 - val_loss: 0.1401
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.1387
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.1390
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2747 - val_loss: 0.1389
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2742 - val_loss: 0.1391
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2740 - val_loss: 0.1404
Epoch 27/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2741 - val_loss: 0.1415
Epoch 28/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2741 - val_loss: 0.1386
Epoch 29/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2731 - val_loss: 0.1398
Epoch 30/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2740 - val_loss: 0.1377
Epoch 31/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2734 - val_loss: 0.1392
Epoch 32/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2731 - val_loss: 0.1390
Epoch 33/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2731 - val_loss: 0.1386
Epoch 34/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2724 - val_loss: 0.1382
Epoch 35/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2727 - val_loss: 0.1395
Epoch 36/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2724 - val_loss: 0.1380
Epoch 37/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2727 - val_loss: 0.1385
Epoch 38/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2720 - val_loss: 0.1384
Epoch 39/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.1381
Epoch 40/68
80/80 [==============================] - 0s 5ms/step - loss: 0.2723 - val_loss: 0.1383
Execution time:  18.75528335571289
GRU:
Mean Absolute Error: 0.1621
Root Mean Square Error: 0.5837
Mean Square Error: 0.3407

Train RMSE: 0.584
Train MSE: 0.341
Train MAE: 0.162
###########################

MODEL:  GRU
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_93&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_9 (GRU)                  (None, 6, 55)             9570      
_________________________________________________________________
dropout_93 (Dropout)         (None, 6, 55)             0         
_________________________________________________________________
time_distributed_93 (TimeDis (None, 6, 1)              56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 6ms/step - loss: 0.4491 - val_loss: 0.2632
Epoch 2/36
355/355 [==============================] - 1s 4ms/step - loss: 0.3034 - val_loss: 0.2277
Epoch 3/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2823 - val_loss: 0.2129
Epoch 4/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2730 - val_loss: 0.2062
Epoch 5/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2696 - val_loss: 0.2057
Epoch 6/36
355/355 [==============================] - 1s 4ms/step - loss: 0.2682 - val_loss: 0.2050
Epoch 7/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2676 - val_loss: 0.2053
Epoch 8/36
355/355 [==============================] - 2s 5ms/step - loss: 0.2672 - val_loss: 0.2046
Epoch 9/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2668 - val_loss: 0.2049
Epoch 10/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2667 - val_loss: 0.2056
Epoch 11/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2666 - val_loss: 0.2052
Epoch 12/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2662 - val_loss: 0.2046
Epoch 13/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2660 - val_loss: 0.2036
Epoch 14/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2657 - val_loss: 0.2044
Epoch 15/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2652 - val_loss: 0.2040
Epoch 16/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2655 - val_loss: 0.2039
Epoch 17/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2651 - val_loss: 0.2029
Epoch 18/36
355/355 [==============================] - 2s 5ms/step - loss: 0.2647 - val_loss: 0.2026
Epoch 19/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2647 - val_loss: 0.2025
Epoch 20/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2644 - val_loss: 0.2015
Epoch 21/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2643 - val_loss: 0.2018
Epoch 22/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2639 - val_loss: 0.2008
Epoch 23/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2637 - val_loss: 0.2002
Epoch 24/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2636 - val_loss: 0.2003
Epoch 25/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2634 - val_loss: 0.2004
Epoch 26/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2632 - val_loss: 0.1989
Epoch 27/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2628 - val_loss: 0.1985
Epoch 28/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2627 - val_loss: 0.1973
Epoch 29/36
355/355 [==============================] - 2s 5ms/step - loss: 0.2625 - val_loss: 0.1970
Epoch 30/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2623 - val_loss: 0.1968
Epoch 31/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2621 - val_loss: 0.1965
Epoch 32/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2621 - val_loss: 0.1961
Epoch 33/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2620 - val_loss: 0.1951
Epoch 34/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2619 - val_loss: 0.1945
Epoch 35/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2615 - val_loss: 0.1940
Epoch 36/36
355/355 [==============================] - 2s 4ms/step - loss: 0.2614 - val_loss: 0.1937
Execution time:  58.51086139678955
GRU:
Mean Absolute Error: 0.1542
Root Mean Square Error: 0.5819
Mean Square Error: 0.3386

Train RMSE: 0.582
Train MSE: 0.339
Train MAE: 0.154
###########################

MODEL:  GRU
sequence:  1h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_94&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_10 (GRU)                 (None, 6, 40)             5160      
_________________________________________________________________
dropout_94 (Dropout)         (None, 6, 40)             0         
_________________________________________________________________
time_distributed_94 (TimeDis (None, 6, 1)              41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 25ms/step - loss: 0.8606 - val_loss: 1.2053
Epoch 2/68
80/80 [==============================] - 0s 5ms/step - loss: 0.7485 - val_loss: 0.9521
Epoch 3/68
80/80 [==============================] - 0s 5ms/step - loss: 0.6109 - val_loss: 0.8779
Epoch 4/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5771 - val_loss: 0.8552
Epoch 5/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5623 - val_loss: 0.8424
Epoch 6/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5535 - val_loss: 0.8336
Epoch 7/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5467 - val_loss: 0.8269
Epoch 8/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5414 - val_loss: 0.8214
Epoch 9/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5370 - val_loss: 0.8171
Epoch 10/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5330 - val_loss: 0.8133
Epoch 11/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5291 - val_loss: 0.8101
Epoch 12/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5259 - val_loss: 0.8074
Epoch 13/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5226 - val_loss: 0.8050
Epoch 14/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5197 - val_loss: 0.8031
Epoch 15/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5169 - val_loss: 0.8015
Epoch 16/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5143 - val_loss: 0.8002
Epoch 17/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5116 - val_loss: 0.7990
Epoch 18/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5087 - val_loss: 0.7980
Epoch 19/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5061 - val_loss: 0.7971
Epoch 20/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5039 - val_loss: 0.7963
Epoch 21/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5024 - val_loss: 0.7956
Epoch 22/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5007 - val_loss: 0.7951
Epoch 23/68
80/80 [==============================] - 0s 5ms/step - loss: 0.5000 - val_loss: 0.7946
Epoch 24/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4990 - val_loss: 0.7943
Epoch 25/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4979 - val_loss: 0.7940
Epoch 26/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4976 - val_loss: 0.7938
Epoch 27/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4969 - val_loss: 0.7936
Epoch 28/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4966 - val_loss: 0.7935
Epoch 29/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4960 - val_loss: 0.7934
Epoch 30/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4957 - val_loss: 0.7933
Epoch 31/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4956 - val_loss: 0.7932
Epoch 32/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4954 - val_loss: 0.7931
Epoch 33/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4949 - val_loss: 0.7931
Epoch 34/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4950 - val_loss: 0.7930
Epoch 35/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 0.7930
Epoch 36/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4947 - val_loss: 0.7930
Epoch 37/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4946 - val_loss: 0.7929
Epoch 38/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4942 - val_loss: 0.7929
Epoch 39/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4943 - val_loss: 0.7929
Epoch 40/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4944 - val_loss: 0.7929
Epoch 41/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928
Epoch 42/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4943 - val_loss: 0.7928
Epoch 43/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928
Epoch 44/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4941 - val_loss: 0.7928
Epoch 45/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.7928
Epoch 46/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928
Epoch 47/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.7928
Epoch 48/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928
Epoch 49/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928
Epoch 50/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7928
Epoch 51/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.7928
Epoch 52/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927
Epoch 53/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927
Epoch 54/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.7927
Epoch 55/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927
Epoch 56/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 57/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 58/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.7927
Epoch 59/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 60/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927
Epoch 61/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.7927
Epoch 62/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4937 - val_loss: 0.7927
Epoch 63/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 64/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 65/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Epoch 66/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 67/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.7927
Epoch 68/68
80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.7927
Execution time:  29.92873191833496
GRU:
Mean Absolute Error: 0.4842
Root Mean Square Error: 0.7517
Mean Square Error: 0.5650

Train RMSE: 0.752
Train MSE: 0.565
Train MAE: 0.484
###########################

MODEL:  GRU
sequence:  1h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_95&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_11 (GRU)                 (None, 6, 55)             9570      
_________________________________________________________________
dropout_95 (Dropout)         (None, 6, 55)             0         
_________________________________________________________________
time_distributed_95 (TimeDis (None, 6, 1)              56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
355/355 [==============================] - 2s 6ms/step - loss: 0.6840 - val_loss: 0.7218
Epoch 2/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5524 - val_loss: 0.6933
Epoch 3/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5363 - val_loss: 0.6797
Epoch 4/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5235 - val_loss: 0.6695
Epoch 5/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5112 - val_loss: 0.6626
Epoch 6/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5052 - val_loss: 0.6584
Epoch 7/36
355/355 [==============================] - 2s 4ms/step - loss: 0.5013 - val_loss: 0.6561
Epoch 8/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4984 - val_loss: 0.6545
Epoch 9/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4962 - val_loss: 0.6533
Epoch 10/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4945 - val_loss: 0.6526
Epoch 11/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4936 - val_loss: 0.6520
Epoch 12/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4931 - val_loss: 0.6516
Epoch 13/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4927 - val_loss: 0.6511
Epoch 14/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4924 - val_loss: 0.6511
Epoch 15/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4922 - val_loss: 0.6509
Epoch 16/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4921 - val_loss: 0.6507
Epoch 17/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4919 - val_loss: 0.6506
Epoch 18/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4918 - val_loss: 0.6505
Epoch 19/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4917 - val_loss: 0.6504
Epoch 20/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6503
Epoch 21/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4916 - val_loss: 0.6501
Epoch 22/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4915 - val_loss: 0.6502
Epoch 23/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4914 - val_loss: 0.6501
Epoch 24/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4914 - val_loss: 0.6501
Epoch 25/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4914 - val_loss: 0.6501
Epoch 26/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6500
Epoch 27/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6500
Epoch 28/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6499
Epoch 29/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6500
Epoch 30/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6500
Epoch 31/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6500
Epoch 32/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4913 - val_loss: 0.6499
Epoch 33/36
355/355 [==============================] - 2s 5ms/step - loss: 0.4911 - val_loss: 0.6499
Epoch 34/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6499
Epoch 35/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4912 - val_loss: 0.6499
Epoch 36/36
355/355 [==============================] - 2s 4ms/step - loss: 0.4913 - val_loss: 0.6499
Execution time:  59.047602891922
GRU:
Mean Absolute Error: 0.4829
Root Mean Square Error: 0.7516
Mean Square Error: 0.5649

Train RMSE: 0.752
Train MSE: 0.565
Train MAE: 0.483
###########################

MODEL:  GRU
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_96&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_12 (GRU)                 (None, 18, 40)            5160      
_________________________________________________________________
dropout_96 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_96 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5396 - val_loss: 0.2678
Epoch 2/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3853 - val_loss: 0.1975
Epoch 3/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3651 - val_loss: 0.1826
Epoch 4/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3572 - val_loss: 0.1735
Epoch 5/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3532 - val_loss: 0.1693
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3500 - val_loss: 0.1671
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3480 - val_loss: 0.1620
Epoch 8/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3466 - val_loss: 0.1598
Epoch 9/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3457 - val_loss: 0.1577
Epoch 10/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3447 - val_loss: 0.1577
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3437 - val_loss: 0.1546
Epoch 12/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3424 - val_loss: 0.1529
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3419 - val_loss: 0.1524
Epoch 14/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3409 - val_loss: 0.1516
Epoch 15/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3405 - val_loss: 0.1501
Epoch 16/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3402 - val_loss: 0.1510
Epoch 17/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3397 - val_loss: 0.1502
Epoch 18/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3392 - val_loss: 0.1503
Epoch 19/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3388 - val_loss: 0.1492
Epoch 20/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3385 - val_loss: 0.1491
Epoch 21/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3381 - val_loss: 0.1483
Epoch 22/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3378 - val_loss: 0.1495
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3373 - val_loss: 0.1494
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3375 - val_loss: 0.1492
Epoch 25/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3369 - val_loss: 0.1494
Epoch 26/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3368 - val_loss: 0.1487
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3362 - val_loss: 0.1474
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3361 - val_loss: 0.1477
Epoch 29/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3360 - val_loss: 0.1468
Epoch 30/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3355 - val_loss: 0.1476
Epoch 31/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3350 - val_loss: 0.1466
Epoch 32/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3349 - val_loss: 0.1455
Epoch 33/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3350 - val_loss: 0.1466
Epoch 34/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3346 - val_loss: 0.1451
Epoch 35/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3342 - val_loss: 0.1438
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3338 - val_loss: 0.1455
Epoch 37/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3338 - val_loss: 0.1445
Epoch 38/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3337 - val_loss: 0.1440
Epoch 39/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3333 - val_loss: 0.1428
Epoch 40/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3332 - val_loss: 0.1431
Epoch 41/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3331 - val_loss: 0.1428
Epoch 42/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3327 - val_loss: 0.1425
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3329 - val_loss: 0.1415
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3323 - val_loss: 0.1424
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3319 - val_loss: 0.1425
Epoch 46/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3319 - val_loss: 0.1417
Epoch 47/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3317 - val_loss: 0.1399
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3314 - val_loss: 0.1412
Epoch 49/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3316 - val_loss: 0.1401
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3315 - val_loss: 0.1409
Epoch 51/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3313 - val_loss: 0.1410
Epoch 52/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3309 - val_loss: 0.1405
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3310 - val_loss: 0.1400
Epoch 54/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3307 - val_loss: 0.1405
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3306 - val_loss: 0.1399
Epoch 56/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3304 - val_loss: 0.1395
Epoch 57/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3301 - val_loss: 0.1398
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3305 - val_loss: 0.1386
Epoch 59/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3303 - val_loss: 0.1390
Epoch 60/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3297 - val_loss: 0.1389
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3298 - val_loss: 0.1397
Epoch 62/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3298 - val_loss: 0.1393
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3294 - val_loss: 0.1381
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3294 - val_loss: 0.1398
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3292 - val_loss: 0.1380
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3294 - val_loss: 0.1368
Epoch 67/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3290 - val_loss: 0.1380
Epoch 68/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3290 - val_loss: 0.1376
Execution time:  53.84108328819275
GRU:
Mean Absolute Error: 0.1646
Root Mean Square Error: 0.5868
Mean Square Error: 0.3443

Train RMSE: 0.587
Train MSE: 0.344
Train MAE: 0.165
###########################

MODEL:  GRU
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_97&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_13 (GRU)                 (None, 18, 55)            9570      
_________________________________________________________________
dropout_97 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_97 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 4s 10ms/step - loss: 0.4036 - val_loss: 0.2626
Epoch 2/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3464 - val_loss: 0.2529
Epoch 3/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3403 - val_loss: 0.2502
Epoch 4/36
354/354 [==============================] - ETA: 0s - loss: 0.337 - 3s 8ms/step - loss: 0.3373 - val_loss: 0.2502
Epoch 5/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3353 - val_loss: 0.2499
Epoch 6/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3346 - val_loss: 0.2505
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3339 - val_loss: 0.2513
Epoch 8/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3331 - val_loss: 0.2505
Epoch 9/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3327 - val_loss: 0.2513
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3319 - val_loss: 0.2509
Epoch 11/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3317 - val_loss: 0.2504
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3312 - val_loss: 0.2506
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3309 - val_loss: 0.2508
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3305 - val_loss: 0.2507
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2508
Execution time:  49.02683734893799
GRU:
Mean Absolute Error: 0.1724
Root Mean Square Error: 0.5859
Mean Square Error: 0.3433

Train RMSE: 0.586
Train MSE: 0.343
Train MAE: 0.172
###########################

MODEL:  GRU
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_98&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_14 (GRU)                 (None, 18, 40)            5160      
_________________________________________________________________
dropout_98 (Dropout)         (None, 18, 40)            0         
_________________________________________________________________
time_distributed_98 (TimeDis (None, 18, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7738 - val_loss: 0.8696
Epoch 2/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5983 - val_loss: 0.8288
Epoch 3/68
80/80 [==============================] - 1s 8ms/step - loss: 0.5775 - val_loss: 0.8174
Epoch 4/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5643 - val_loss: 0.8096
Epoch 5/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5567 - val_loss: 0.8055
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5513 - val_loss: 0.8024
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5475 - val_loss: 0.8002
Epoch 8/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5447 - val_loss: 0.7986
Epoch 9/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5426 - val_loss: 0.7973
Epoch 10/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5413 - val_loss: 0.7964
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5400 - val_loss: 0.7956
Epoch 12/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5392 - val_loss: 0.7951
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5384 - val_loss: 0.7946
Epoch 14/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5376 - val_loss: 0.7942
Epoch 15/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5371 - val_loss: 0.7939
Epoch 16/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5365 - val_loss: 0.7937
Epoch 17/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5361 - val_loss: 0.7935
Epoch 18/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5357 - val_loss: 0.7933
Epoch 19/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7932
Epoch 20/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5350 - val_loss: 0.7930
Epoch 21/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7929
Epoch 22/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5345 - val_loss: 0.7929
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5342 - val_loss: 0.7928
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5342 - val_loss: 0.7927
Epoch 25/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5338 - val_loss: 0.7927
Epoch 26/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5337 - val_loss: 0.7926
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7926
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5333 - val_loss: 0.7926
Epoch 29/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5331 - val_loss: 0.7925
Epoch 30/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5329 - val_loss: 0.7925
Epoch 31/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5330 - val_loss: 0.7925
Epoch 32/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5328 - val_loss: 0.7925
Epoch 33/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5326 - val_loss: 0.7924
Epoch 34/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5325 - val_loss: 0.7924
Epoch 35/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5325 - val_loss: 0.7924
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5324 - val_loss: 0.7924
Epoch 37/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5324 - val_loss: 0.7924
Epoch 38/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5323 - val_loss: 0.7924
Epoch 39/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5321 - val_loss: 0.7924
Epoch 40/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5322 - val_loss: 0.7924
Epoch 41/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923
Epoch 42/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5321 - val_loss: 0.7923
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.7923
Epoch 46/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5318 - val_loss: 0.7923
Epoch 47/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5318 - val_loss: 0.7923
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.7923
Epoch 49/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7923
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5315 - val_loss: 0.7923
Epoch 51/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7923
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5316 - val_loss: 0.7923
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5314 - val_loss: 0.7923
Epoch 54/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5314 - val_loss: 0.7923
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5315 - val_loss: 0.7923
Epoch 56/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5312 - val_loss: 0.7923
Epoch 57/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5311 - val_loss: 0.7923
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5310 - val_loss: 0.7923
Epoch 59/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5311 - val_loss: 0.7923
Epoch 60/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5310 - val_loss: 0.7923
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5309 - val_loss: 0.7923
Epoch 62/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5308 - val_loss: 0.7923
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5308 - val_loss: 0.7923
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5310 - val_loss: 0.7923
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5307 - val_loss: 0.7923
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5307 - val_loss: 0.7923
Epoch 67/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5306 - val_loss: 0.7923
Epoch 68/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5306 - val_loss: 0.7923
Execution time:  55.7631356716156
GRU:
Mean Absolute Error: 0.4952
Root Mean Square Error: 0.7642
Mean Square Error: 0.5841

Train RMSE: 0.764
Train MSE: 0.584
Train MAE: 0.495
###########################

MODEL:  GRU
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_99&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_15 (GRU)                 (None, 18, 55)            9570      
_________________________________________________________________
dropout_99 (Dropout)         (None, 18, 55)            0         
_________________________________________________________________
time_distributed_99 (TimeDis (None, 18, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 4s 10ms/step - loss: 0.6307 - val_loss: 0.6957
Epoch 2/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5556 - val_loss: 0.6808
Epoch 3/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5446 - val_loss: 0.6747
Epoch 4/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5390 - val_loss: 0.6715
Epoch 5/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5366 - val_loss: 0.6698
Epoch 6/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5349 - val_loss: 0.6686
Epoch 7/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5341 - val_loss: 0.6680
Epoch 8/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5335 - val_loss: 0.6674
Epoch 9/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5330 - val_loss: 0.6672
Epoch 10/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5328 - val_loss: 0.6669
Epoch 11/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5325 - val_loss: 0.6666
Epoch 12/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5323 - val_loss: 0.6665
Epoch 13/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5321 - val_loss: 0.6663
Epoch 14/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5320 - val_loss: 0.6662
Epoch 15/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5317 - val_loss: 0.6659
Epoch 16/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5318 - val_loss: 0.6661
Epoch 17/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5317 - val_loss: 0.6658
Epoch 18/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5315 - val_loss: 0.6658
Epoch 19/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5315 - val_loss: 0.6659
Epoch 20/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6659
Epoch 21/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6658
Epoch 22/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6659
Epoch 23/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5311 - val_loss: 0.6658
Epoch 24/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5311 - val_loss: 0.6658
Epoch 25/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5309 - val_loss: 0.6657
Epoch 26/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5307 - val_loss: 0.6657
Epoch 27/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5304 - val_loss: 0.6658
Epoch 28/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5304 - val_loss: 0.6657
Epoch 29/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5304 - val_loss: 0.6655
Epoch 30/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5301 - val_loss: 0.6656
Epoch 31/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5302 - val_loss: 0.6655
Epoch 32/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5299 - val_loss: 0.6654
Epoch 33/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5300 - val_loss: 0.6654
Epoch 34/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5299 - val_loss: 0.6655
Epoch 35/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5296 - val_loss: 0.6653
Epoch 36/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5296 - val_loss: 0.6653
Execution time:  114.3955659866333
GRU:
Mean Absolute Error: 0.4969
Root Mean Square Error: 0.7676
Mean Square Error: 0.5891

Train RMSE: 0.768
Train MSE: 0.589
Train MAE: 0.497
###########################

MODEL:  GRU
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_100&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_16 (GRU)                 (None, 18, 40)            5160      
_________________________________________________________________
dropout_100 (Dropout)        (None, 18, 40)            0         
_________________________________________________________________
time_distributed_100 (TimeDi (None, 18, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.7073 - val_loss: 0.8008
Epoch 2/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7068 - val_loss: 0.8002
Epoch 3/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7064 - val_loss: 0.7995
Epoch 4/68
80/80 [==============================] - 1s 10ms/step - loss: 0.7061 - val_loss: 0.7988
Epoch 5/68
80/80 [==============================] - 1s 10ms/step - loss: 0.7055 - val_loss: 0.7981
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7051 - val_loss: 0.7974
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7045 - val_loss: 0.7966
Epoch 8/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7042 - val_loss: 0.7959
Epoch 9/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7039 - val_loss: 0.7952
Epoch 10/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7032 - val_loss: 0.7944
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7029 - val_loss: 0.7937
Epoch 12/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7023 - val_loss: 0.7929
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7019 - val_loss: 0.7922
Epoch 14/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7015 - val_loss: 0.7914
Epoch 15/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7009 - val_loss: 0.7907
Epoch 16/68
80/80 [==============================] - 1s 9ms/step - loss: 0.7006 - val_loss: 0.7899
Epoch 17/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6999 - val_loss: 0.7891
Epoch 18/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6995 - val_loss: 0.7884
Epoch 19/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6989 - val_loss: 0.7876
Epoch 20/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6985 - val_loss: 0.7868
Epoch 21/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6981 - val_loss: 0.7860
Epoch 22/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6978 - val_loss: 0.7853
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6972 - val_loss: 0.7845
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6966 - val_loss: 0.7837
Epoch 25/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6963 - val_loss: 0.7829
Epoch 26/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6957 - val_loss: 0.7821
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6952 - val_loss: 0.7813
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6947 - val_loss: 0.7805
Epoch 29/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6943 - val_loss: 0.7798
Epoch 30/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6938 - val_loss: 0.7790
Epoch 31/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6933 - val_loss: 0.7782
Epoch 32/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6932 - val_loss: 0.7774
Epoch 33/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6925 - val_loss: 0.7766
Epoch 34/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6918 - val_loss: 0.7758
Epoch 35/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6915 - val_loss: 0.7750
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6910 - val_loss: 0.7742
Epoch 37/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6906 - val_loss: 0.7734
Epoch 38/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6898 - val_loss: 0.7726
Epoch 39/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6895 - val_loss: 0.7718
Epoch 40/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6891 - val_loss: 0.7710
Epoch 41/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6887 - val_loss: 0.7702
Epoch 42/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6881 - val_loss: 0.7694
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6875 - val_loss: 0.7686
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6870 - val_loss: 0.7678
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6866 - val_loss: 0.7670
Epoch 46/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6861 - val_loss: 0.7661
Epoch 47/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6857 - val_loss: 0.7653
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6850 - val_loss: 0.7645
Epoch 49/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6845 - val_loss: 0.7637
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6843 - val_loss: 0.7629
Epoch 51/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6839 - val_loss: 0.7621
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6832 - val_loss: 0.7613
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6827 - val_loss: 0.7605
Epoch 54/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6823 - val_loss: 0.7596
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6820 - val_loss: 0.7588
Epoch 56/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6812 - val_loss: 0.7580
Epoch 57/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6808 - val_loss: 0.7572
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6805 - val_loss: 0.7564
Epoch 59/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6797 - val_loss: 0.7555
Epoch 60/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6792 - val_loss: 0.7547
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6791 - val_loss: 0.7539
Epoch 62/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6782 - val_loss: 0.7531
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6779 - val_loss: 0.7523
Epoch 64/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6776 - val_loss: 0.7514
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6770 - val_loss: 0.7506
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6762 - val_loss: 0.7498
Epoch 67/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6759 - val_loss: 0.7489
Epoch 68/68
80/80 [==============================] - 1s 10ms/step - loss: 0.6755 - val_loss: 0.7481
Execution time:  54.31034302711487
GRU:
Mean Absolute Error: 0.6786
Root Mean Square Error: 0.9734
Mean Square Error: 0.9476

Train RMSE: 0.973
Train MSE: 0.948
Train MAE: 0.679
###########################

MODEL:  GRU
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_101&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_17 (GRU)                 (None, 18, 55)            9570      
_________________________________________________________________
dropout_101 (Dropout)        (None, 18, 55)            0         
_________________________________________________________________
time_distributed_101 (TimeDi (None, 18, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 3s 10ms/step - loss: 0.7134 - val_loss: 0.6944
Epoch 2/36
354/354 [==============================] - 3s 9ms/step - loss: 0.7111 - val_loss: 0.6913
Epoch 3/36
354/354 [==============================] - 3s 8ms/step - loss: 0.7086 - val_loss: 0.6881
Epoch 4/36
354/354 [==============================] - 3s 9ms/step - loss: 0.7063 - val_loss: 0.6849
Epoch 5/36
354/354 [==============================] - 3s 9ms/step - loss: 0.7039 - val_loss: 0.6817
Epoch 6/36
354/354 [==============================] - 3s 9ms/step - loss: 0.7015 - val_loss: 0.6784
Epoch 7/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6992 - val_loss: 0.6751
Epoch 8/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6969 - val_loss: 0.6717
Epoch 9/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6944 - val_loss: 0.6682
Epoch 10/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6920 - val_loss: 0.6648
Epoch 11/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6895 - val_loss: 0.6613
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6871 - val_loss: 0.6577
Epoch 13/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6848 - val_loss: 0.6541
Epoch 14/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6821 - val_loss: 0.6505
Epoch 15/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6796 - val_loss: 0.6468
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6771 - val_loss: 0.6431
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6746 - val_loss: 0.6394
Epoch 18/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6721 - val_loss: 0.6357
Epoch 19/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6697 - val_loss: 0.6319
Epoch 20/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6670 - val_loss: 0.6282
Epoch 21/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6645 - val_loss: 0.6244
Epoch 22/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6618 - val_loss: 0.6207
Epoch 23/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6593 - val_loss: 0.6169
Epoch 24/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6566 - val_loss: 0.6131
Epoch 25/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6542 - val_loss: 0.6094
Epoch 26/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6514 - val_loss: 0.6056
Epoch 27/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6489 - val_loss: 0.6018
Epoch 28/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6463 - val_loss: 0.5981
Epoch 29/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6436 - val_loss: 0.5943
Epoch 30/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6409 - val_loss: 0.5904
Epoch 31/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6383 - val_loss: 0.5866
Epoch 32/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6355 - val_loss: 0.5827
Epoch 33/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6328 - val_loss: 0.5788
Epoch 34/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6299 - val_loss: 0.5748
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.6273 - val_loss: 0.5708
Epoch 36/36
354/354 [==============================] - 3s 9ms/step - loss: 0.6244 - val_loss: 0.5667
Execution time:  112.655020236969
GRU:
Mean Absolute Error: 0.5996
Root Mean Square Error: 0.9010
Mean Square Error: 0.8118

Train RMSE: 0.901
Train MSE: 0.812
Train MAE: 0.600
###########################

MODEL:  GRU
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_102&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_18 (GRU)                 (None, 18, 40)            5160      
_________________________________________________________________
dropout_102 (Dropout)        (None, 18, 40)            0         
_________________________________________________________________
time_distributed_102 (TimeDi (None, 18, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8886 - val_loss: 1.2951
Epoch 2/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8885 - val_loss: 1.2949
Epoch 3/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8884 - val_loss: 1.2947
Epoch 4/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8883 - val_loss: 1.2945
Epoch 5/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8882 - val_loss: 1.2944
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8881 - val_loss: 1.2942
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8879 - val_loss: 1.2940
Epoch 8/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8878 - val_loss: 1.2938
Epoch 9/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8877 - val_loss: 1.2936
Epoch 10/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8876 - val_loss: 1.2934
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8875 - val_loss: 1.2932
Epoch 12/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8874 - val_loss: 1.2930
Epoch 13/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8873 - val_loss: 1.2929
Epoch 14/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8872 - val_loss: 1.2927
Epoch 15/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8871 - val_loss: 1.2925
Epoch 16/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8870 - val_loss: 1.2923
Epoch 17/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8869 - val_loss: 1.2921
Epoch 18/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8868 - val_loss: 1.2919
Epoch 19/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8867 - val_loss: 1.2917
Epoch 20/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8865 - val_loss: 1.2915
Epoch 21/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8865 - val_loss: 1.2913
Epoch 22/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8863 - val_loss: 1.2911
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8862 - val_loss: 1.2909
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8861 - val_loss: 1.2907
Epoch 25/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8860 - val_loss: 1.2905
Epoch 26/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8858 - val_loss: 1.2903
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8858 - val_loss: 1.2901
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8857 - val_loss: 1.2899
Epoch 29/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8855 - val_loss: 1.2896
Epoch 30/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8853 - val_loss: 1.2894
Epoch 31/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8853 - val_loss: 1.2892
Epoch 32/68
80/80 [==============================] - 1s 11ms/step - loss: 0.8852 - val_loss: 1.2890
Epoch 33/68
80/80 [==============================] - 1s 11ms/step - loss: 0.8850 - val_loss: 1.2888
Epoch 34/68
80/80 [==============================] - 1s 11ms/step - loss: 0.8849 - val_loss: 1.2886
Epoch 35/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8848 - val_loss: 1.2884
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8847 - val_loss: 1.2882
Epoch 37/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8846 - val_loss: 1.2880
Epoch 38/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8844 - val_loss: 1.2878
Epoch 39/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8843 - val_loss: 1.2876
Epoch 40/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8842 - val_loss: 1.2874
Epoch 41/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8841 - val_loss: 1.2871
Epoch 42/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8839 - val_loss: 1.2869
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8838 - val_loss: 1.2867
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8837 - val_loss: 1.2865
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8836 - val_loss: 1.2863
Epoch 46/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8834 - val_loss: 1.2861
Epoch 47/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8834 - val_loss: 1.2859
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8832 - val_loss: 1.2856
Epoch 49/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8831 - val_loss: 1.2854
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8830 - val_loss: 1.2852
Epoch 51/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8828 - val_loss: 1.2850
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8827 - val_loss: 1.2848
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8826 - val_loss: 1.2846
Epoch 54/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8825 - val_loss: 1.2844
Epoch 55/68
80/80 [==============================] - 1s 10ms/step - loss: 0.8824 - val_loss: 1.2841
Epoch 56/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8822 - val_loss: 1.2839
Epoch 57/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8821 - val_loss: 1.2837
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8820 - val_loss: 1.2835
Epoch 59/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8818 - val_loss: 1.2833
Epoch 60/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8818 - val_loss: 1.2830
Epoch 61/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8817 - val_loss: 1.2828
Epoch 62/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8815 - val_loss: 1.2826
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8814 - val_loss: 1.2824
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8813 - val_loss: 1.2822
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8812 - val_loss: 1.2819
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8810 - val_loss: 1.2817
Epoch 67/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8808 - val_loss: 1.2815
Epoch 68/68
80/80 [==============================] - 1s 9ms/step - loss: 0.8808 - val_loss: 1.2813
Execution time:  55.4357807636261
GRU:
Mean Absolute Error: 0.9175
Root Mean Square Error: 1.1116
Mean Square Error: 1.2357

Train RMSE: 1.112
Train MSE: 1.236
Train MAE: 0.918
###########################

MODEL:  GRU
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_103&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_19 (GRU)                 (None, 18, 55)            9570      
_________________________________________________________________
dropout_103 (Dropout)        (None, 18, 55)            0         
_________________________________________________________________
time_distributed_103 (TimeDi (None, 18, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 4s 10ms/step - loss: 0.8838 - val_loss: 1.1372
Epoch 2/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8833 - val_loss: 1.1364
Epoch 3/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8828 - val_loss: 1.1356
Epoch 4/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8822 - val_loss: 1.1347
Epoch 5/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8817 - val_loss: 1.1338
Epoch 6/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8811 - val_loss: 1.1329
Epoch 7/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8805 - val_loss: 1.1320
Epoch 8/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8799 - val_loss: 1.1311
Epoch 9/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8793 - val_loss: 1.1301
Epoch 10/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8787 - val_loss: 1.1291
Epoch 11/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8781 - val_loss: 1.1282
Epoch 12/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8774 - val_loss: 1.1272
Epoch 13/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8768 - val_loss: 1.1261
Epoch 14/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8761 - val_loss: 1.1251
Epoch 15/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8755 - val_loss: 1.1241
Epoch 16/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8748 - val_loss: 1.1230
Epoch 17/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8741 - val_loss: 1.1220
Epoch 18/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8735 - val_loss: 1.1209
Epoch 19/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8728 - val_loss: 1.1198
Epoch 20/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8722 - val_loss: 1.1187
Epoch 21/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8715 - val_loss: 1.1176
Epoch 22/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8708 - val_loss: 1.1165
Epoch 23/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8701 - val_loss: 1.1153
Epoch 24/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8694 - val_loss: 1.1142
Epoch 25/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8687 - val_loss: 1.1130
Epoch 26/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8679 - val_loss: 1.1118
Epoch 27/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8672 - val_loss: 1.1106
Epoch 28/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8665 - val_loss: 1.1094
Epoch 29/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8657 - val_loss: 1.1082
Epoch 30/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8650 - val_loss: 1.1070
Epoch 31/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8642 - val_loss: 1.1057
Epoch 32/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8635 - val_loss: 1.1045
Epoch 33/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8627 - val_loss: 1.1032
Epoch 34/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8619 - val_loss: 1.1019
Epoch 35/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8611 - val_loss: 1.1006
Epoch 36/36
354/354 [==============================] - 3s 9ms/step - loss: 0.8603 - val_loss: 1.0993
Execution time:  114.53077507019043
GRU:
Mean Absolute Error: 0.9040
Root Mean Square Error: 1.0992
Mean Square Error: 1.2081

Train RMSE: 1.099
Train MSE: 1.208
Train MAE: 0.904
###########################

MODEL:  GRU
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_104&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_20 (GRU)                 (None, 18, 40)            5160      
_________________________________________________________________
dropout_104 (Dropout)        (None, 18, 40)            0         
_________________________________________________________________
time_distributed_104 (TimeDi (None, 18, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.6381 - val_loss: 0.4800
Epoch 2/68
80/80 [==============================] - 1s 9ms/step - loss: 0.4388 - val_loss: 0.2479
Epoch 3/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3912 - val_loss: 0.2251
Epoch 4/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3778 - val_loss: 0.2079
Epoch 5/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3696 - val_loss: 0.1983
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3639 - val_loss: 0.1900
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3600 - val_loss: 0.1835
Epoch 8/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3571 - val_loss: 0.1798
Epoch 9/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3546 - val_loss: 0.1762
Epoch 10/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3533 - val_loss: 0.1737
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3512 - val_loss: 0.1700
Epoch 12/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3495 - val_loss: 0.1678
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3485 - val_loss: 0.1664
Epoch 14/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3473 - val_loss: 0.1645
Epoch 15/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3466 - val_loss: 0.1626
Epoch 16/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3458 - val_loss: 0.1618
Epoch 17/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3450 - val_loss: 0.1610
Epoch 18/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3443 - val_loss: 0.1598
Epoch 19/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3439 - val_loss: 0.1593
Epoch 20/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3435 - val_loss: 0.1584
Epoch 21/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3430 - val_loss: 0.1575
Epoch 22/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3425 - val_loss: 0.1568
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3421 - val_loss: 0.1574
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3422 - val_loss: 0.1568
Epoch 25/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3413 - val_loss: 0.1552
Epoch 26/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3411 - val_loss: 0.1551
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3407 - val_loss: 0.1554
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3403 - val_loss: 0.1546
Epoch 29/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3400 - val_loss: 0.1541
Epoch 30/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3398 - val_loss: 0.1539
Epoch 31/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3391 - val_loss: 0.1535
Epoch 32/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3392 - val_loss: 0.1531
Epoch 33/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3389 - val_loss: 0.1542
Epoch 34/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3391 - val_loss: 0.1532
Epoch 35/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3384 - val_loss: 0.1520
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3379 - val_loss: 0.1525
Epoch 37/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3382 - val_loss: 0.1522
Epoch 38/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3382 - val_loss: 0.1521
Epoch 39/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3375 - val_loss: 0.1529
Epoch 40/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3377 - val_loss: 0.1525
Epoch 41/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3375 - val_loss: 0.1523
Epoch 42/68
80/80 [==============================] - 1s 10ms/step - loss: 0.3374 - val_loss: 0.1524
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3372 - val_loss: 0.1522
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3371 - val_loss: 0.1523
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.3367 - val_loss: 0.1526
Execution time:  36.559377908706665
GRU:
Mean Absolute Error: 0.1722
Root Mean Square Error: 0.5872
Mean Square Error: 0.3448

Train RMSE: 0.587
Train MSE: 0.345
Train MAE: 0.172
###########################

MODEL:  GRU
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_105&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_21 (GRU)                 (None, 18, 55)            9570      
_________________________________________________________________
dropout_105 (Dropout)        (None, 18, 55)            0         
_________________________________________________________________
time_distributed_105 (TimeDi (None, 18, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 4s 10ms/step - loss: 0.4113 - val_loss: 0.2753
Epoch 2/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3545 - val_loss: 0.2615
Epoch 3/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3448 - val_loss: 0.2550
Epoch 4/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3401 - val_loss: 0.2522
Epoch 5/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3371 - val_loss: 0.2502
Epoch 6/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3352 - val_loss: 0.2493
Epoch 7/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3341 - val_loss: 0.2489
Epoch 8/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3332 - val_loss: 0.2488
Epoch 9/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3326 - val_loss: 0.2491
Epoch 10/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3318 - val_loss: 0.2489
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3316 - val_loss: 0.2490
Epoch 12/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3312 - val_loss: 0.2498
Epoch 13/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3309 - val_loss: 0.2497
Epoch 14/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3306 - val_loss: 0.2495
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3306 - val_loss: 0.2493
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.2493
Epoch 17/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3297 - val_loss: 0.2492
Epoch 18/36
354/354 [==============================] - 3s 9ms/step - loss: 0.3298 - val_loss: 0.2492
Execution time:  58.06269812583923
GRU:
Mean Absolute Error: 0.1653
Root Mean Square Error: 0.5843
Mean Square Error: 0.3414

Train RMSE: 0.584
Train MSE: 0.341
Train MAE: 0.165
###########################

MODEL:  GRU
sequence:  3h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_106&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_22 (GRU)                 (None, 18, 40)            5160      
_________________________________________________________________
dropout_106 (Dropout)        (None, 18, 40)            0         
_________________________________________________________________
time_distributed_106 (TimeDi (None, 18, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 1s 15ms/step - loss: 0.8612 - val_loss: 1.1599
Epoch 2/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6962 - val_loss: 0.8829
Epoch 3/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6138 - val_loss: 0.8483
Epoch 4/68
80/80 [==============================] - 1s 9ms/step - loss: 0.6000 - val_loss: 0.8358
Epoch 5/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5919 - val_loss: 0.8285
Epoch 6/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5862 - val_loss: 0.8234
Epoch 7/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5808 - val_loss: 0.8195
Epoch 8/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5762 - val_loss: 0.8166
Epoch 9/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5711 - val_loss: 0.8136
Epoch 10/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5668 - val_loss: 0.8110
Epoch 11/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5621 - val_loss: 0.8086
Epoch 12/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5584 - val_loss: 0.8063
Epoch 13/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5554 - val_loss: 0.8044
Epoch 14/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5528 - val_loss: 0.8027
Epoch 15/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5507 - val_loss: 0.8013
Epoch 16/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5487 - val_loss: 0.8001
Epoch 17/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5472 - val_loss: 0.7991
Epoch 18/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5458 - val_loss: 0.7982
Epoch 19/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5444 - val_loss: 0.7975
Epoch 20/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5435 - val_loss: 0.7968
Epoch 21/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5425 - val_loss: 0.7963
Epoch 22/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5417 - val_loss: 0.7958
Epoch 23/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5409 - val_loss: 0.7954
Epoch 24/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5405 - val_loss: 0.7951
Epoch 25/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5396 - val_loss: 0.7947
Epoch 26/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5393 - val_loss: 0.7945
Epoch 27/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5386 - val_loss: 0.7942
Epoch 28/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5382 - val_loss: 0.7940
Epoch 29/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5377 - val_loss: 0.7938
Epoch 30/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5374 - val_loss: 0.7937
Epoch 31/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5369 - val_loss: 0.7935
Epoch 32/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5367 - val_loss: 0.7934
Epoch 33/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5364 - val_loss: 0.7933
Epoch 34/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5361 - val_loss: 0.7932
Epoch 35/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5359 - val_loss: 0.7931
Epoch 36/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5357 - val_loss: 0.7930
Epoch 37/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5355 - val_loss: 0.7929
Epoch 38/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5353 - val_loss: 0.7929
Epoch 39/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5350 - val_loss: 0.7928
Epoch 40/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5350 - val_loss: 0.7928
Epoch 41/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5347 - val_loss: 0.7927
Epoch 42/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5346 - val_loss: 0.7927
Epoch 43/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5345 - val_loss: 0.7927
Epoch 44/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5344 - val_loss: 0.7926
Epoch 45/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5343 - val_loss: 0.7926
Epoch 46/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5341 - val_loss: 0.7926
Epoch 47/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5341 - val_loss: 0.7926
Epoch 48/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5340 - val_loss: 0.7926
Epoch 49/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5340 - val_loss: 0.7925
Epoch 50/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5337 - val_loss: 0.7925
Epoch 51/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5338 - val_loss: 0.7925
Epoch 52/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5336 - val_loss: 0.7925
Epoch 53/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7925
Epoch 54/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5335 - val_loss: 0.7925
Epoch 55/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5334 - val_loss: 0.7925
Epoch 56/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5333 - val_loss: 0.7924
Epoch 57/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5332 - val_loss: 0.7924
Epoch 58/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5330 - val_loss: 0.7924
Epoch 59/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5332 - val_loss: 0.7924
Epoch 60/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5330 - val_loss: 0.7924
Epoch 61/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5330 - val_loss: 0.7924
Epoch 62/68
80/80 [==============================] - 1s 10ms/step - loss: 0.5329 - val_loss: 0.7924
Epoch 63/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5329 - val_loss: 0.7924
Epoch 64/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5329 - val_loss: 0.7924
Epoch 65/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5328 - val_loss: 0.7924
Epoch 66/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5328 - val_loss: 0.7924
Epoch 67/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5326 - val_loss: 0.7924
Epoch 68/68
80/80 [==============================] - 1s 9ms/step - loss: 0.5326 - val_loss: 0.7924
Execution time:  54.99783754348755
GRU:
Mean Absolute Error: 0.4916
Root Mean Square Error: 0.7556
Mean Square Error: 0.5710

Train RMSE: 0.756
Train MSE: 0.571
Train MAE: 0.492
###########################

MODEL:  GRU
sequence:  3h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_107&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_23 (GRU)                 (None, 18, 55)            9570      
_________________________________________________________________
dropout_107 (Dropout)        (None, 18, 55)            0         
_________________________________________________________________
time_distributed_107 (TimeDi (None, 18, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
354/354 [==============================] - 3s 10ms/step - loss: 0.6854 - val_loss: 0.7243
Epoch 2/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5915 - val_loss: 0.7093
Epoch 3/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5769 - val_loss: 0.6974
Epoch 4/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5637 - val_loss: 0.6880
Epoch 5/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5536 - val_loss: 0.6816
Epoch 6/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5480 - val_loss: 0.6785
Epoch 7/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5433 - val_loss: 0.6751
Epoch 8/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5407 - val_loss: 0.6728
Epoch 9/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5388 - val_loss: 0.6712
Epoch 10/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5375 - val_loss: 0.6700
Epoch 11/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5364 - val_loss: 0.6692
Epoch 12/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5357 - val_loss: 0.6685
Epoch 13/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5351 - val_loss: 0.6680
Epoch 14/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5347 - val_loss: 0.6676
Epoch 15/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5343 - val_loss: 0.6674
Epoch 16/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5339 - val_loss: 0.6671
Epoch 17/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5336 - val_loss: 0.6670
Epoch 18/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5333 - val_loss: 0.6668
Epoch 19/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5331 - val_loss: 0.6667
Epoch 20/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5329 - val_loss: 0.6666
Epoch 21/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5326 - val_loss: 0.6665
Epoch 22/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5325 - val_loss: 0.6665
Epoch 23/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5324 - val_loss: 0.6664
Epoch 24/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5322 - val_loss: 0.6664
Epoch 25/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5321 - val_loss: 0.6663
Epoch 26/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5319 - val_loss: 0.6663
Epoch 27/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5318 - val_loss: 0.6663
Epoch 28/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5317 - val_loss: 0.6663
Epoch 29/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5316 - val_loss: 0.6662
Epoch 30/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5316 - val_loss: 0.6662
Epoch 31/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5315 - val_loss: 0.6661
Epoch 32/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5314 - val_loss: 0.6660
Epoch 33/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5313 - val_loss: 0.6660
Epoch 34/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5313 - val_loss: 0.6660
Epoch 35/36
354/354 [==============================] - 3s 8ms/step - loss: 0.5313 - val_loss: 0.6659
Epoch 36/36
354/354 [==============================] - 3s 9ms/step - loss: 0.5312 - val_loss: 0.6659
Execution time:  113.31456589698792
GRU:
Mean Absolute Error: 0.4932
Root Mean Square Error: 0.7567
Mean Square Error: 0.5726

Train RMSE: 0.757
Train MSE: 0.573
Train MAE: 0.493
###########################

MODEL:  GRU
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_108&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_24 (GRU)                 (None, 36, 40)            5160      
_________________________________________________________________
dropout_108 (Dropout)        (None, 36, 40)            0         
_________________________________________________________________
time_distributed_108 (TimeDi (None, 36, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.5819 - val_loss: 0.2680
Epoch 2/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4452 - val_loss: 0.1949
Epoch 3/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4257 - val_loss: 0.1879
Epoch 4/68
80/80 [==============================] - 1s 18ms/step - loss: 0.4207 - val_loss: 0.1808
Epoch 5/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4174 - val_loss: 0.1766
Epoch 6/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4155 - val_loss: 0.1723
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4138 - val_loss: 0.1703
Epoch 8/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4126 - val_loss: 0.1702
Epoch 9/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4114 - val_loss: 0.1689
Epoch 10/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4109 - val_loss: 0.1685
Epoch 11/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4100 - val_loss: 0.1670
Epoch 12/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4097 - val_loss: 0.1656
Epoch 13/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4093 - val_loss: 0.1656
Epoch 14/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4090 - val_loss: 0.1648
Epoch 15/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4085 - val_loss: 0.1652
Epoch 16/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4083 - val_loss: 0.1641
Epoch 17/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4080 - val_loss: 0.1639
Epoch 18/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4077 - val_loss: 0.1629
Epoch 19/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4071 - val_loss: 0.1634
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4072 - val_loss: 0.1630
Epoch 21/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4067 - val_loss: 0.1639
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4066 - val_loss: 0.1625
Epoch 23/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4062 - val_loss: 0.1635
Epoch 24/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4059 - val_loss: 0.1631
Epoch 25/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4058 - val_loss: 0.1633
Epoch 26/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4056 - val_loss: 0.1628
Epoch 27/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4053 - val_loss: 0.1631
Epoch 28/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4052 - val_loss: 0.1624
Epoch 29/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4050 - val_loss: 0.1631
Epoch 30/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4048 - val_loss: 0.1625
Epoch 31/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4048 - val_loss: 0.1632
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4048 - val_loss: 0.1624
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4047 - val_loss: 0.1646
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4044 - val_loss: 0.1625
Epoch 35/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4044 - val_loss: 0.1639
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4040 - val_loss: 0.1634
Epoch 37/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4039 - val_loss: 0.1635
Epoch 38/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4039 - val_loss: 0.1630
Epoch 39/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4037 - val_loss: 0.1641
Epoch 40/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4033 - val_loss: 0.1637
Epoch 41/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4037 - val_loss: 0.1620
Epoch 42/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4037 - val_loss: 0.1645
Epoch 43/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4031 - val_loss: 0.1635
Epoch 44/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4031 - val_loss: 0.1633
Epoch 45/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4030 - val_loss: 0.1636
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4031 - val_loss: 0.1635
Epoch 47/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4029 - val_loss: 0.1643
Epoch 48/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4027 - val_loss: 0.1634
Epoch 49/68
80/80 [==============================] - 1s 15ms/step - loss: 0.4024 - val_loss: 0.1639
Epoch 50/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4022 - val_loss: 0.1640
Epoch 51/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4023 - val_loss: 0.1645
Execution time:  69.33568620681763
GRU:
Mean Absolute Error: 0.1818
Root Mean Square Error: 0.5931
Mean Square Error: 0.3518

Train RMSE: 0.593
Train MSE: 0.352
Train MAE: 0.182
###########################

MODEL:  GRU
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_109&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_25 (GRU)                 (None, 36, 55)            9570      
_________________________________________________________________
dropout_109 (Dropout)        (None, 36, 55)            0         
_________________________________________________________________
time_distributed_109 (TimeDi (None, 36, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 6s 16ms/step - loss: 0.4612 - val_loss: 0.3154
Epoch 2/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4119 - val_loss: 0.3107
Epoch 3/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4074 - val_loss: 0.3069
Epoch 4/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4048 - val_loss: 0.3047
Epoch 5/36
352/352 [==============================] - 5s 14ms/step - loss: 0.4028 - val_loss: 0.3035
Epoch 6/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4015 - val_loss: 0.3039
Epoch 7/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4009 - val_loss: 0.3042
Epoch 8/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4002 - val_loss: 0.3042
Epoch 9/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3998 - val_loss: 0.3044
Epoch 10/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3987 - val_loss: 0.3044
Epoch 11/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4005 - val_loss: 0.3063
Epoch 12/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3978 - val_loss: 0.3053
Epoch 13/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3986 - val_loss: 0.3079
Epoch 14/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3963 - val_loss: 0.3060
Epoch 15/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3995 - val_loss: 0.3063
Execution time:  81.67205429077148
GRU:
Mean Absolute Error: 0.1802
Root Mean Square Error: 0.5921
Mean Square Error: 0.3506

Train RMSE: 0.592
Train MSE: 0.351
Train MAE: 0.180
###########################

MODEL:  GRU
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_110&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_26 (GRU)                 (None, 36, 40)            5160      
_________________________________________________________________
dropout_110 (Dropout)        (None, 36, 40)            0         
_________________________________________________________________
time_distributed_110 (TimeDi (None, 36, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.8081 - val_loss: 0.8694
Epoch 2/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6387 - val_loss: 0.8244
Epoch 3/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6175 - val_loss: 0.8135
Epoch 4/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6055 - val_loss: 0.8078
Epoch 5/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5986 - val_loss: 0.8044
Epoch 6/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5936 - val_loss: 0.8015
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5913 - val_loss: 0.7998
Epoch 8/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5890 - val_loss: 0.7985
Epoch 9/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5870 - val_loss: 0.7975
Epoch 10/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5853 - val_loss: 0.7966
Epoch 11/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5838 - val_loss: 0.7959
Epoch 12/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5824 - val_loss: 0.7953
Epoch 13/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5815 - val_loss: 0.7948
Epoch 14/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5806 - val_loss: 0.7944
Epoch 15/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5801 - val_loss: 0.7940
Epoch 16/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5794 - val_loss: 0.7938
Epoch 17/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5789 - val_loss: 0.7935
Epoch 18/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5785 - val_loss: 0.7933
Epoch 19/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5783 - val_loss: 0.7931
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5778 - val_loss: 0.7930
Epoch 21/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5775 - val_loss: 0.7928
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5772 - val_loss: 0.7927
Epoch 23/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5771 - val_loss: 0.7926
Epoch 24/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5767 - val_loss: 0.7925
Epoch 25/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5766 - val_loss: 0.7924
Epoch 26/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5763 - val_loss: 0.7924
Epoch 27/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5762 - val_loss: 0.7923
Epoch 28/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5758 - val_loss: 0.7922
Epoch 29/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5755 - val_loss: 0.7922
Epoch 30/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5754 - val_loss: 0.7921
Epoch 31/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5753 - val_loss: 0.7921
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5752 - val_loss: 0.7921
Epoch 33/68
80/80 [==============================] - 1s 18ms/step - loss: 0.5748 - val_loss: 0.7920
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5747 - val_loss: 0.7920
Epoch 35/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5745 - val_loss: 0.7920
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5743 - val_loss: 0.7919
Epoch 37/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5742 - val_loss: 0.7919
Epoch 38/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5739 - val_loss: 0.7919
Epoch 39/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5739 - val_loss: 0.7919
Epoch 40/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5736 - val_loss: 0.7919
Epoch 41/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5735 - val_loss: 0.7918
Epoch 42/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5734 - val_loss: 0.7918
Epoch 43/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5731 - val_loss: 0.7918
Epoch 44/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5731 - val_loss: 0.7918
Epoch 45/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5729 - val_loss: 0.7918
Epoch 46/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5729 - val_loss: 0.7918
Epoch 47/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5728 - val_loss: 0.7918
Epoch 48/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5725 - val_loss: 0.7918
Epoch 49/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5726 - val_loss: 0.7917
Epoch 50/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.7917
Epoch 51/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5722 - val_loss: 0.7917
Epoch 52/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5721 - val_loss: 0.7917
Epoch 53/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5721 - val_loss: 0.7917
Epoch 54/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5720 - val_loss: 0.7917
Epoch 55/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5719 - val_loss: 0.7917
Epoch 56/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5719 - val_loss: 0.7917
Epoch 57/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5717 - val_loss: 0.7917
Epoch 58/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5717 - val_loss: 0.7917
Epoch 59/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5715 - val_loss: 0.7917
Epoch 60/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5713 - val_loss: 0.7917
Epoch 61/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5713 - val_loss: 0.7917
Epoch 62/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5712 - val_loss: 0.7917
Epoch 63/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5711 - val_loss: 0.7917
Epoch 64/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5711 - val_loss: 0.7917
Epoch 65/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5710 - val_loss: 0.7917
Epoch 66/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5708 - val_loss: 0.7917
Epoch 67/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5707 - val_loss: 0.7917
Epoch 68/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5706 - val_loss: 0.7917
Execution time:  92.3845694065094
GRU:
Mean Absolute Error: 0.5047
Root Mean Square Error: 0.7613
Mean Square Error: 0.5796

Train RMSE: 0.761
Train MSE: 0.580
Train MAE: 0.505
###########################

MODEL:  GRU
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_111&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_27 (GRU)                 (None, 36, 55)            9570      
_________________________________________________________________
dropout_111 (Dropout)        (None, 36, 55)            0         
_________________________________________________________________
time_distributed_111 (TimeDi (None, 36, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 6s 16ms/step - loss: 0.6552 - val_loss: 0.7173
Epoch 2/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6056 - val_loss: 0.7086
Epoch 3/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5998 - val_loss: 0.7008
Epoch 4/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5830 - val_loss: 0.6958
Epoch 5/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5787 - val_loss: 0.6936
Epoch 6/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5767 - val_loss: 0.6926
Epoch 7/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5754 - val_loss: 0.6917
Epoch 8/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5746 - val_loss: 0.6911
Epoch 9/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5740 - val_loss: 0.6905
Epoch 10/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5737 - val_loss: 0.6903
Epoch 11/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5735 - val_loss: 0.6900
Epoch 12/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5733 - val_loss: 0.6898
Epoch 13/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5730 - val_loss: 0.6895
Epoch 14/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5727 - val_loss: 0.6893
Epoch 15/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5724 - val_loss: 0.6890
Epoch 16/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5721 - val_loss: 0.6889
Epoch 17/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5718 - val_loss: 0.6886
Epoch 18/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5716 - val_loss: 0.6883
Epoch 19/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5713 - val_loss: 0.6878
Epoch 20/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5712 - val_loss: 0.6874
Epoch 21/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5709 - val_loss: 0.6874
Epoch 22/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5708 - val_loss: 0.6874
Epoch 23/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5705 - val_loss: 0.6871
Epoch 24/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5703 - val_loss: 0.6870
Epoch 25/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5700 - val_loss: 0.6867
Epoch 26/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5699 - val_loss: 0.6860
Epoch 27/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5701 - val_loss: 0.6865
Epoch 28/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5691 - val_loss: 0.6864
Epoch 29/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5692 - val_loss: 0.6859
Epoch 30/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5694 - val_loss: 0.6863
Epoch 31/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5686 - val_loss: 0.6861
Epoch 32/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5683 - val_loss: 0.6859
Epoch 33/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5680 - val_loss: 0.6857
Epoch 34/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5684 - val_loss: 0.6858
Epoch 35/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5680 - val_loss: 0.6857
Epoch 36/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5678 - val_loss: 0.6854
Execution time:  192.367205619812
GRU:
Mean Absolute Error: 0.5089
Root Mean Square Error: 0.7733
Mean Square Error: 0.5980

Train RMSE: 0.773
Train MSE: 0.598
Train MAE: 0.509
###########################

MODEL:  GRU
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_112&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_28 (GRU)                 (None, 36, 40)            5160      
_________________________________________________________________
dropout_112 (Dropout)        (None, 36, 40)            0         
_________________________________________________________________
time_distributed_112 (TimeDi (None, 36, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.6755 - val_loss: 0.7478
Epoch 2/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6751 - val_loss: 0.7471
Epoch 3/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6750 - val_loss: 0.7465
Epoch 4/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6747 - val_loss: 0.7458
Epoch 5/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6741 - val_loss: 0.7451
Epoch 6/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6737 - val_loss: 0.7444
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6733 - val_loss: 0.7436
Epoch 8/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6730 - val_loss: 0.7429
Epoch 9/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6727 - val_loss: 0.7421
Epoch 10/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6721 - val_loss: 0.7414
Epoch 11/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6716 - val_loss: 0.7406
Epoch 12/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6714 - val_loss: 0.7399
Epoch 13/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6710 - val_loss: 0.7391
Epoch 14/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6707 - val_loss: 0.7383
Epoch 15/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6702 - val_loss: 0.7375
Epoch 16/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6697 - val_loss: 0.7367
Epoch 17/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6694 - val_loss: 0.7359
Epoch 18/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6690 - val_loss: 0.7351
Epoch 19/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6686 - val_loss: 0.7343
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6681 - val_loss: 0.7335
Epoch 21/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6676 - val_loss: 0.7327
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6672 - val_loss: 0.7319
Epoch 23/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6670 - val_loss: 0.7311
Epoch 24/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6663 - val_loss: 0.7303
Epoch 25/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6660 - val_loss: 0.7295
Epoch 26/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6655 - val_loss: 0.7287
Epoch 27/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6652 - val_loss: 0.7278
Epoch 28/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6648 - val_loss: 0.7270
Epoch 29/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6642 - val_loss: 0.7262
Epoch 30/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6638 - val_loss: 0.7253
Epoch 31/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6633 - val_loss: 0.7245
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6629 - val_loss: 0.7237
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6625 - val_loss: 0.7228
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6621 - val_loss: 0.7220
Epoch 35/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6616 - val_loss: 0.7211
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6614 - val_loss: 0.7203
Epoch 37/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6608 - val_loss: 0.7195
Epoch 38/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6604 - val_loss: 0.7186
Epoch 39/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6601 - val_loss: 0.7178
Epoch 40/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6596 - val_loss: 0.7169
Epoch 41/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6591 - val_loss: 0.7160
Epoch 42/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6586 - val_loss: 0.7152
Epoch 43/68
80/80 [==============================] - ETA: 0s - loss: 0.658 - 1s 16ms/step - loss: 0.6581 - val_loss: 0.7143
Epoch 44/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6579 - val_loss: 0.7135
Epoch 45/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6575 - val_loss: 0.7126
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6571 - val_loss: 0.7117
Epoch 47/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6567 - val_loss: 0.7109
Epoch 48/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6561 - val_loss: 0.7100
Epoch 49/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6556 - val_loss: 0.7091
Epoch 50/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6554 - val_loss: 0.7083
Epoch 51/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6548 - val_loss: 0.7074
Epoch 52/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6545 - val_loss: 0.7065
Epoch 53/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6541 - val_loss: 0.7057
Epoch 54/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6533 - val_loss: 0.7048
Epoch 55/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6530 - val_loss: 0.7039
Epoch 56/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6525 - val_loss: 0.7030
Epoch 57/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6522 - val_loss: 0.7021
Epoch 58/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6517 - val_loss: 0.7013
Epoch 59/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6513 - val_loss: 0.7004
Epoch 60/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6511 - val_loss: 0.6995
Epoch 61/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6505 - val_loss: 0.6986
Epoch 62/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6500 - val_loss: 0.6977
Epoch 63/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6496 - val_loss: 0.6968
Epoch 64/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6491 - val_loss: 0.6959
Epoch 65/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6487 - val_loss: 0.6951
Epoch 66/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6483 - val_loss: 0.6942
Epoch 67/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6477 - val_loss: 0.6933
Epoch 68/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6474 - val_loss: 0.6924
Execution time:  91.26407361030579
GRU:
Mean Absolute Error: 0.6350
Root Mean Square Error: 0.9270
Mean Square Error: 0.8594

Train RMSE: 0.927
Train MSE: 0.859
Train MAE: 0.635
###########################

MODEL:  GRU
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_113&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_29 (GRU)                 (None, 36, 55)            9570      
_________________________________________________________________
dropout_113 (Dropout)        (None, 36, 55)            0         
_________________________________________________________________
time_distributed_113 (TimeDi (None, 36, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 6s 16ms/step - loss: 0.6907 - val_loss: 0.6671
Epoch 2/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6880 - val_loss: 0.6633
Epoch 3/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6850 - val_loss: 0.6594
Epoch 4/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6821 - val_loss: 0.6553
Epoch 5/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6792 - val_loss: 0.6512
Epoch 6/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6762 - val_loss: 0.6470
Epoch 7/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6732 - val_loss: 0.6428
Epoch 8/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6701 - val_loss: 0.6385
Epoch 9/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6671 - val_loss: 0.6342
Epoch 10/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6640 - val_loss: 0.6298
Epoch 11/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6610 - val_loss: 0.6254
Epoch 12/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6579 - val_loss: 0.6210
Epoch 13/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6548 - val_loss: 0.6165
Epoch 14/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6517 - val_loss: 0.6121
Epoch 15/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6486 - val_loss: 0.6076
Epoch 16/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6454 - val_loss: 0.6030
Epoch 17/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6423 - val_loss: 0.5985
Epoch 18/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6392 - val_loss: 0.5939
Epoch 19/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6359 - val_loss: 0.5892
Epoch 20/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6328 - val_loss: 0.5846
Epoch 21/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6295 - val_loss: 0.5799
Epoch 22/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6261 - val_loss: 0.5751
Epoch 23/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6228 - val_loss: 0.5703
Epoch 24/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6195 - val_loss: 0.5655
Epoch 25/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6161 - val_loss: 0.5606
Epoch 26/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6126 - val_loss: 0.5556
Epoch 27/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6092 - val_loss: 0.5505
Epoch 28/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6058 - val_loss: 0.5455
Epoch 29/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6021 - val_loss: 0.5403
Epoch 30/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5987 - val_loss: 0.5351
Epoch 31/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5949 - val_loss: 0.5299
Epoch 32/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5914 - val_loss: 0.5246
Epoch 33/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5878 - val_loss: 0.5192
Epoch 34/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5841 - val_loss: 0.5139
Epoch 35/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5804 - val_loss: 0.5085
Epoch 36/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5767 - val_loss: 0.5031
Execution time:  190.9497594833374
GRU:
Mean Absolute Error: 0.5200
Root Mean Square Error: 0.8287
Mean Square Error: 0.6867

Train RMSE: 0.829
Train MSE: 0.687
Train MAE: 0.520
###########################

MODEL:  GRU
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_114&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_30 (GRU)                 (None, 36, 40)            5160      
_________________________________________________________________
dropout_114 (Dropout)        (None, 36, 40)            0         
_________________________________________________________________
time_distributed_114 (TimeDi (None, 36, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 22ms/step - loss: 0.8825 - val_loss: 1.2782
Epoch 2/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8825 - val_loss: 1.2781
Epoch 3/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8824 - val_loss: 1.2779
Epoch 4/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8823 - val_loss: 1.2777
Epoch 5/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8821 - val_loss: 1.2776
Epoch 6/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8821 - val_loss: 1.2774
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8820 - val_loss: 1.2772
Epoch 8/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8819 - val_loss: 1.2770
Epoch 9/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8818 - val_loss: 1.2769
Epoch 10/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8818 - val_loss: 1.2767
Epoch 11/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8816 - val_loss: 1.2765
Epoch 12/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8815 - val_loss: 1.2763
Epoch 13/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8814 - val_loss: 1.2761
Epoch 14/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8814 - val_loss: 1.2759
Epoch 15/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8812 - val_loss: 1.2757
Epoch 16/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8811 - val_loss: 1.2755
Epoch 17/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8810 - val_loss: 1.2753
Epoch 18/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8809 - val_loss: 1.2751
Epoch 19/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8809 - val_loss: 1.2750
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8807 - val_loss: 1.2748
Epoch 21/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8807 - val_loss: 1.2746
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8805 - val_loss: 1.2744
Epoch 23/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8804 - val_loss: 1.2742
Epoch 24/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8803 - val_loss: 1.2740
Epoch 25/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8803 - val_loss: 1.2737
Epoch 26/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8801 - val_loss: 1.2735
Epoch 27/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8800 - val_loss: 1.2733
Epoch 28/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8800 - val_loss: 1.2731
Epoch 29/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8798 - val_loss: 1.2729
Epoch 30/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8797 - val_loss: 1.2727
Epoch 31/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8796 - val_loss: 1.2725
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8795 - val_loss: 1.2723
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8794 - val_loss: 1.2721
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8793 - val_loss: 1.2719
Epoch 35/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8792 - val_loss: 1.2717
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8791 - val_loss: 1.2715
Epoch 37/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8790 - val_loss: 1.2712
Epoch 38/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8789 - val_loss: 1.2710ETA: 0s - lo
Epoch 39/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8787 - val_loss: 1.2708
Epoch 40/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8786 - val_loss: 1.2706
Epoch 41/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8785 - val_loss: 1.2704
Epoch 42/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8785 - val_loss: 1.2702
Epoch 43/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8783 - val_loss: 1.2700
Epoch 44/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8782 - val_loss: 1.2697
Epoch 45/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8781 - val_loss: 1.2695
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8780 - val_loss: 1.2693
Epoch 47/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8779 - val_loss: 1.2691
Epoch 48/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8778 - val_loss: 1.2689
Epoch 49/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8777 - val_loss: 1.2686
Epoch 50/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8775 - val_loss: 1.2684
Epoch 51/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8774 - val_loss: 1.2682
Epoch 52/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8773 - val_loss: 1.2680
Epoch 53/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8772 - val_loss: 1.2678
Epoch 54/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8771 - val_loss: 1.2675
Epoch 55/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8770 - val_loss: 1.2673
Epoch 56/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8769 - val_loss: 1.2671
Epoch 57/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8768 - val_loss: 1.2669
Epoch 58/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8766 - val_loss: 1.2666
Epoch 59/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8765 - val_loss: 1.2664
Epoch 60/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8764 - val_loss: 1.2662
Epoch 61/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8763 - val_loss: 1.2660
Epoch 62/68
80/80 [==============================] - 1s 17ms/step - loss: 0.8761 - val_loss: 1.2657
Epoch 63/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8760 - val_loss: 1.2655
Epoch 64/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8760 - val_loss: 1.2653
Epoch 65/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8758 - val_loss: 1.2651
Epoch 66/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8757 - val_loss: 1.2648
Epoch 67/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8756 - val_loss: 1.2646
Epoch 68/68
80/80 [==============================] - 1s 16ms/step - loss: 0.8755 - val_loss: 1.2644
Execution time:  92.91871929168701
GRU:
Mean Absolute Error: 0.9057
Root Mean Square Error: 1.0989
Mean Square Error: 1.2076

Train RMSE: 1.099
Train MSE: 1.208
Train MAE: 0.906
###########################

MODEL:  GRU
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_115&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_31 (GRU)                 (None, 36, 55)            9570      
_________________________________________________________________
dropout_115 (Dropout)        (None, 36, 55)            0         
_________________________________________________________________
time_distributed_115 (TimeDi (None, 36, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 6s 16ms/step - loss: 0.8822 - val_loss: 1.1311
Epoch 2/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8817 - val_loss: 1.1303
Epoch 3/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8812 - val_loss: 1.1294
Epoch 4/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8806 - val_loss: 1.1285
Epoch 5/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8800 - val_loss: 1.1276
Epoch 6/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8794 - val_loss: 1.1267
Epoch 7/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8788 - val_loss: 1.1257
Epoch 8/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8782 - val_loss: 1.1247
Epoch 9/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8775 - val_loss: 1.1237
Epoch 10/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8769 - val_loss: 1.1227
Epoch 11/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8763 - val_loss: 1.1217
Epoch 12/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8756 - val_loss: 1.1207
Epoch 13/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8750 - val_loss: 1.1196
Epoch 14/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8743 - val_loss: 1.1186
Epoch 15/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8736 - val_loss: 1.1175
Epoch 16/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8730 - val_loss: 1.1164
Epoch 17/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8723 - val_loss: 1.1153
Epoch 18/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8716 - val_loss: 1.1142
Epoch 19/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8709 - val_loss: 1.1131
Epoch 20/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8702 - val_loss: 1.1120
Epoch 21/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8696 - val_loss: 1.1109
Epoch 22/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8689 - val_loss: 1.1097
Epoch 23/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8682 - val_loss: 1.1086
Epoch 24/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8674 - val_loss: 1.1074
Epoch 25/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8667 - val_loss: 1.1062
Epoch 26/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8660 - val_loss: 1.1051
Epoch 27/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8653 - val_loss: 1.1039
Epoch 28/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8646 - val_loss: 1.1027
Epoch 29/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8638 - val_loss: 1.1014
Epoch 30/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8631 - val_loss: 1.1002
Epoch 31/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8623 - val_loss: 1.0990
Epoch 32/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8615 - val_loss: 1.0977
Epoch 33/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8608 - val_loss: 1.0965
Epoch 34/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8600 - val_loss: 1.0952
Epoch 35/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8592 - val_loss: 1.0939
Epoch 36/36
352/352 [==============================] - 5s 15ms/step - loss: 0.8584 - val_loss: 1.0926
Execution time:  190.12778568267822
GRU:
Mean Absolute Error: 0.8958
Root Mean Square Error: 1.0894
Mean Square Error: 1.1868

Train RMSE: 1.089
Train MSE: 1.187
Train MAE: 0.896
###########################

MODEL:  GRU
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_116&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_32 (GRU)                 (None, 36, 40)            5160      
_________________________________________________________________
dropout_116 (Dropout)        (None, 36, 40)            0         
_________________________________________________________________
time_distributed_116 (TimeDi (None, 36, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 21ms/step - loss: 0.6181 - val_loss: 0.3863
Epoch 2/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4670 - val_loss: 0.2303
Epoch 3/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4419 - val_loss: 0.2191
Epoch 4/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4339 - val_loss: 0.2114
Epoch 5/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4286 - val_loss: 0.2051
Epoch 6/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4252 - val_loss: 0.1990
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4225 - val_loss: 0.1948
Epoch 8/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4200 - val_loss: 0.1928
Epoch 9/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4183 - val_loss: 0.1895
Epoch 10/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4169 - val_loss: 0.1875
Epoch 11/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4157 - val_loss: 0.1848
Epoch 12/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4145 - val_loss: 0.1831
Epoch 13/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4137 - val_loss: 0.1823
Epoch 14/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4128 - val_loss: 0.1801
Epoch 15/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4122 - val_loss: 0.1799
Epoch 16/68
80/80 [==============================] - 1s 18ms/step - loss: 0.4117 - val_loss: 0.1783
Epoch 17/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4112 - val_loss: 0.1774
Epoch 18/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4109 - val_loss: 0.1765
Epoch 19/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4100 - val_loss: 0.1768
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4099 - val_loss: 0.1763
Epoch 21/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4093 - val_loss: 0.1762
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4091 - val_loss: 0.1755
Epoch 23/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4086 - val_loss: 0.1753
Epoch 24/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4082 - val_loss: 0.1751
Epoch 25/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4078 - val_loss: 0.1747
Epoch 26/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4078 - val_loss: 0.1740
Epoch 27/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4073 - val_loss: 0.1742
Epoch 28/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4070 - val_loss: 0.1741
Epoch 29/68
80/80 [==============================] - 1s 17ms/step - loss: 0.4068 - val_loss: 0.1731
Epoch 30/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4067 - val_loss: 0.1731
Epoch 31/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4066 - val_loss: 0.1731
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4065 - val_loss: 0.1726
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4061 - val_loss: 0.1733
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4059 - val_loss: 0.1723
Epoch 35/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4056 - val_loss: 0.1724
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4056 - val_loss: 0.1722
Epoch 37/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4053 - val_loss: 0.1721
Epoch 38/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4051 - val_loss: 0.1716
Epoch 39/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4047 - val_loss: 0.1714
Epoch 40/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4048 - val_loss: 0.1718
Epoch 41/68
80/80 [==============================] - 1s 18ms/step - loss: 0.4046 - val_loss: 0.1705
Epoch 42/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4046 - val_loss: 0.1722
Epoch 43/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4045 - val_loss: 0.1715
Epoch 44/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4041 - val_loss: 0.1712
Epoch 45/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4041 - val_loss: 0.1706
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4040 - val_loss: 0.1715
Epoch 47/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4038 - val_loss: 0.1709
Epoch 48/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4039 - val_loss: 0.1713
Epoch 49/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4036 - val_loss: 0.1714
Epoch 50/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4031 - val_loss: 0.1715
Epoch 51/68
80/80 [==============================] - 1s 16ms/step - loss: 0.4034 - val_loss: 0.1713
Execution time:  69.06371903419495
GRU:
Mean Absolute Error: 0.1800
Root Mean Square Error: 0.5908
Mean Square Error: 0.3490

Train RMSE: 0.591
Train MSE: 0.349
Train MAE: 0.180
###########################

MODEL:  GRU
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_117&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_33 (GRU)                 (None, 36, 55)            9570      
_________________________________________________________________
dropout_117 (Dropout)        (None, 36, 55)            0         
_________________________________________________________________
time_distributed_117 (TimeDi (None, 36, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - ETA: 0s - loss: 0.4735WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.
352/352 [==============================] - 6s 16ms/step - loss: 0.4735 - val_loss: 0.3242
Epoch 2/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4169 - val_loss: 0.3156
Epoch 3/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4097 - val_loss: 0.3106
Epoch 4/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4061 - val_loss: 0.3085
Epoch 5/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4036 - val_loss: 0.3067
Epoch 6/36
352/352 [==============================] - 5s 15ms/step - loss: 0.4020 - val_loss: 0.3055
Epoch 7/36
352/352 [==============================] - 5s 14ms/step - loss: 0.4008 - val_loss: 0.3049
Epoch 8/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3995 - val_loss: 0.3045
Epoch 9/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3985 - val_loss: 0.3041
Epoch 10/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3979 - val_loss: 0.3043
Epoch 11/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3974 - val_loss: 0.3043
Epoch 12/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3969 - val_loss: 0.3046
Epoch 13/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3966 - val_loss: 0.3045
Epoch 14/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3963 - val_loss: 0.3049
Epoch 15/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3961 - val_loss: 0.3050
Epoch 16/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3959 - val_loss: 0.3051
Epoch 17/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3957 - val_loss: 0.3053
Epoch 18/36
352/352 [==============================] - 5s 14ms/step - loss: 0.3954 - val_loss: 0.3054
Epoch 19/36
352/352 [==============================] - 5s 15ms/step - loss: 0.3952 - val_loss: 0.3055
Execution time:  101.70372176170349
GRU:
Mean Absolute Error: 0.1737
Root Mean Square Error: 0.5875
Mean Square Error: 0.3452

Train RMSE: 0.588
Train MSE: 0.345
Train MAE: 0.174
###########################

MODEL:  GRU
sequence:  6h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_118&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_34 (GRU)                 (None, 36, 40)            5160      
_________________________________________________________________
dropout_118 (Dropout)        (None, 36, 40)            0         
_________________________________________________________________
time_distributed_118 (TimeDi (None, 36, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
80/80 [==============================] - 2s 22ms/step - loss: 0.8559 - val_loss: 1.1152
Epoch 2/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6865 - val_loss: 0.8572
Epoch 3/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6333 - val_loss: 0.8358
Epoch 4/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6230 - val_loss: 0.8266
Epoch 5/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6165 - val_loss: 0.8210
Epoch 6/68
80/80 [==============================] - 1s 17ms/step - loss: 0.6116 - val_loss: 0.8170
Epoch 7/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6077 - val_loss: 0.8139
Epoch 8/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6045 - val_loss: 0.8112
Epoch 9/68
80/80 [==============================] - 1s 16ms/step - loss: 0.6018 - val_loss: 0.8090
Epoch 10/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5996 - val_loss: 0.8071
Epoch 11/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5975 - val_loss: 0.8055
Epoch 12/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5958 - val_loss: 0.8041
Epoch 13/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5943 - val_loss: 0.8030
Epoch 14/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5928 - val_loss: 0.8019
Epoch 15/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5915 - val_loss: 0.8010
Epoch 16/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5904 - val_loss: 0.8001
Epoch 17/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5892 - val_loss: 0.7994
Epoch 18/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5883 - val_loss: 0.7987
Epoch 19/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5873 - val_loss: 0.7981
Epoch 20/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5865 - val_loss: 0.7975
Epoch 21/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5856 - val_loss: 0.7970
Epoch 22/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5849 - val_loss: 0.7966
Epoch 23/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5842 - val_loss: 0.7962
Epoch 24/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5835 - val_loss: 0.7958
Epoch 25/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5829 - val_loss: 0.7955
Epoch 26/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5824 - val_loss: 0.7952
Epoch 27/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5819 - val_loss: 0.7949
Epoch 28/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5814 - val_loss: 0.7947
Epoch 29/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5810 - val_loss: 0.7945
Epoch 30/68
80/80 [==============================] - 1s 18ms/step - loss: 0.5806 - val_loss: 0.7943
Epoch 31/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5804 - val_loss: 0.7941
Epoch 32/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5800 - val_loss: 0.7939
Epoch 33/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5797 - val_loss: 0.7938
Epoch 34/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5793 - val_loss: 0.7936
Epoch 35/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5792 - val_loss: 0.7935
Epoch 36/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5788 - val_loss: 0.7934
Epoch 37/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5786 - val_loss: 0.7933
Epoch 38/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5783 - val_loss: 0.7932
Epoch 39/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5782 - val_loss: 0.7931
Epoch 40/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5780 - val_loss: 0.7930
Epoch 41/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5779 - val_loss: 0.7929
Epoch 42/68
80/80 [==============================] - 1s 18ms/step - loss: 0.5777 - val_loss: 0.7928
Epoch 43/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5775 - val_loss: 0.7928
Epoch 44/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5773 - val_loss: 0.7927
Epoch 45/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5772 - val_loss: 0.7927
Epoch 46/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5770 - val_loss: 0.7926
Epoch 47/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5770 - val_loss: 0.7926
Epoch 48/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5768 - val_loss: 0.7925
Epoch 49/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5766 - val_loss: 0.7925
Epoch 50/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5766 - val_loss: 0.7924
Epoch 51/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5765 - val_loss: 0.7924
Epoch 52/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5763 - val_loss: 0.7923
Epoch 53/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5763 - val_loss: 0.7923
Epoch 54/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5762 - val_loss: 0.7923
Epoch 55/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5761 - val_loss: 0.7923
Epoch 56/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5760 - val_loss: 0.7922
Epoch 57/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5759 - val_loss: 0.7922
Epoch 58/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5758 - val_loss: 0.7922
Epoch 59/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5758 - val_loss: 0.7922
Epoch 60/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5756 - val_loss: 0.7921
Epoch 61/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5757 - val_loss: 0.7921
Epoch 62/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5756 - val_loss: 0.7921
Epoch 63/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5756 - val_loss: 0.7921
Epoch 64/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5755 - val_loss: 0.7921
Epoch 65/68
80/80 [==============================] - 1s 15ms/step - loss: 0.5753 - val_loss: 0.7920
Epoch 66/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5754 - val_loss: 0.7920
Epoch 67/68
80/80 [==============================] - 1s 17ms/step - loss: 0.5752 - val_loss: 0.7920
Epoch 68/68
80/80 [==============================] - 1s 16ms/step - loss: 0.5752 - val_loss: 0.7920
Execution time:  93.11560082435608
GRU:
Mean Absolute Error: 0.5004
Root Mean Square Error: 0.7609
Mean Square Error: 0.5790

Train RMSE: 0.761
Train MSE: 0.579
Train MAE: 0.500
###########################

MODEL:  GRU
sequence:  6h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_119&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_35 (GRU)                 (None, 36, 55)            9570      
_________________________________________________________________
dropout_119 (Dropout)        (None, 36, 55)            0         
_________________________________________________________________
time_distributed_119 (TimeDi (None, 36, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
352/352 [==============================] - 6s 16ms/step - loss: 0.6812 - val_loss: 0.7407
Epoch 2/36
352/352 [==============================] - 5s 15ms/step - loss: 0.6193 - val_loss: 0.7291
Epoch 3/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6115 - val_loss: 0.7231
Epoch 4/36
352/352 [==============================] - 5s 14ms/step - loss: 0.6044 - val_loss: 0.7175
Epoch 5/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5982 - val_loss: 0.7140
Epoch 6/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5953 - val_loss: 0.7110
Epoch 7/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5926 - val_loss: 0.7079
Epoch 8/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5890 - val_loss: 0.7050
Epoch 9/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5862 - val_loss: 0.7025
Epoch 10/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5844 - val_loss: 0.7003
Epoch 11/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5828 - val_loss: 0.6983
Epoch 12/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5813 - val_loss: 0.6969
Epoch 13/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5800 - val_loss: 0.6956
Epoch 14/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5784 - val_loss: 0.6948
Epoch 15/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5774 - val_loss: 0.6941
Epoch 16/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5766 - val_loss: 0.6935
Epoch 17/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5761 - val_loss: 0.6931
Epoch 18/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5756 - val_loss: 0.6926
Epoch 19/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5753 - val_loss: 0.6922
Epoch 20/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5750 - val_loss: 0.6919
Epoch 21/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5747 - val_loss: 0.6916
Epoch 22/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5745 - val_loss: 0.6913
Epoch 23/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5743 - val_loss: 0.6912
Epoch 24/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5741 - val_loss: 0.6910
Epoch 25/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5739 - val_loss: 0.6909
Epoch 26/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5738 - val_loss: 0.6907
Epoch 27/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5737 - val_loss: 0.6906
Epoch 28/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5735 - val_loss: 0.6905
Epoch 29/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5735 - val_loss: 0.6904
Epoch 30/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5734 - val_loss: 0.6903
Epoch 31/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5733 - val_loss: 0.6903
Epoch 32/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5731 - val_loss: 0.6902
Epoch 33/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5731 - val_loss: 0.6902
Epoch 34/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5730 - val_loss: 0.6901
Epoch 35/36
352/352 [==============================] - 5s 14ms/step - loss: 0.5729 - val_loss: 0.6901
Epoch 36/36
352/352 [==============================] - 5s 15ms/step - loss: 0.5728 - val_loss: 0.6900
Execution time:  189.12827682495117
GRU:
Mean Absolute Error: 0.4993
Root Mean Square Error: 0.7598
Mean Square Error: 0.5772

Train RMSE: 0.760
Train MSE: 0.577
Train MAE: 0.499
###########################

MODEL:  GRU
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_120&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_36 (GRU)                 (None, 72, 40)            5160      
_________________________________________________________________
dropout_120 (Dropout)        (None, 72, 40)            0         
_________________________________________________________________
time_distributed_120 (TimeDi (None, 72, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 34ms/step - loss: 0.5957 - val_loss: 0.2600
Epoch 2/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5161 - val_loss: 0.1999
Epoch 3/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5065 - val_loss: 0.1969
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5030 - val_loss: 0.1951
Epoch 5/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5012 - val_loss: 0.1942
Epoch 6/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4999 - val_loss: 0.1946
Epoch 7/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4985 - val_loss: 0.1942
Epoch 8/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4977 - val_loss: 0.1942
Epoch 9/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4968 - val_loss: 0.1941
Epoch 10/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4959 - val_loss: 0.1943
Epoch 11/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4955 - val_loss: 0.1942
Epoch 12/68
79/79 [==============================] - 2s 31ms/step - loss: 0.4947 - val_loss: 0.1944
Epoch 13/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4944 - val_loss: 0.1942
Epoch 14/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4937 - val_loss: 0.1943
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4937 - val_loss: 0.1943
Epoch 16/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4930 - val_loss: 0.1941
Epoch 17/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4929 - val_loss: 0.1941
Epoch 18/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4923 - val_loss: 0.1943
Epoch 19/68
79/79 [==============================] - 2s 31ms/step - loss: 0.4922 - val_loss: 0.1941
Epoch 20/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4918 - val_loss: 0.1940
Epoch 21/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4914 - val_loss: 0.1946
Epoch 22/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4910 - val_loss: 0.1944
Epoch 23/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4912 - val_loss: 0.1946
Epoch 24/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4908 - val_loss: 0.1943
Epoch 25/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4902 - val_loss: 0.1959
Epoch 26/68
79/79 [==============================] - 2s 31ms/step - loss: 0.4899 - val_loss: 0.1956
Epoch 27/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4894 - val_loss: 0.1961
Epoch 28/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4892 - val_loss: 0.1964
Epoch 29/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4885 - val_loss: 0.1964
Epoch 30/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4880 - val_loss: 0.1968
Execution time:  73.99497961997986
GRU:
Mean Absolute Error: 0.2082
Root Mean Square Error: 0.6079
Mean Square Error: 0.3695

Train RMSE: 0.608
Train MSE: 0.370
Train MAE: 0.208
###########################

MODEL:  GRU
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_121&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_37 (GRU)                 (None, 72, 55)            9570      
_________________________________________________________________
dropout_121 (Dropout)        (None, 72, 55)            0         
_________________________________________________________________
time_distributed_121 (TimeDi (None, 72, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 10s 29ms/step - loss: 0.5349 - val_loss: 0.3759
Epoch 2/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4941 - val_loss: 0.3747
Epoch 3/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4910 - val_loss: 0.3741
Epoch 4/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4885 - val_loss: 0.3738
Epoch 5/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4871 - val_loss: 0.3744
Epoch 6/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4857 - val_loss: 0.3745
Epoch 7/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4868 - val_loss: 0.3768
Epoch 8/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4836 - val_loss: 0.3759
Epoch 9/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4835 - val_loss: 0.3768
Epoch 10/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4817 - val_loss: 0.3765
Epoch 11/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4806 - val_loss: 0.3767
Epoch 12/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4807 - val_loss: 0.3785
Epoch 13/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4777 - val_loss: 0.3798
Epoch 14/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4771 - val_loss: 0.3803
Execution time:  139.44272685050964
GRU:
Mean Absolute Error: 0.2152
Root Mean Square Error: 0.6169
Mean Square Error: 0.3805

Train RMSE: 0.617
Train MSE: 0.381
Train MAE: 0.215
###########################

MODEL:  GRU
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_122&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_38 (GRU)                 (None, 72, 40)            5160      
_________________________________________________________________
dropout_122 (Dropout)        (None, 72, 40)            0         
_________________________________________________________________
time_distributed_122 (TimeDi (None, 72, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 35ms/step - loss: 0.8141 - val_loss: 0.8641
Epoch 2/68
79/79 [==============================] - 2s 32ms/step - loss: 0.6860 - val_loss: 0.8193
Epoch 3/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6728 - val_loss: 0.8093
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6682 - val_loss: 0.8057
Epoch 5/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6655 - val_loss: 0.8033
Epoch 6/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6628 - val_loss: 0.8021
Epoch 7/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6582 - val_loss: 0.8004
Epoch 8/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6618 - val_loss: 0.7995
Epoch 9/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6616 - val_loss: 0.7987
Epoch 10/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6628 - val_loss: 0.7982
Epoch 11/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6622 - val_loss: 0.7977
Epoch 12/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6615 - val_loss: 0.7974
Epoch 13/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6608 - val_loss: 0.7972
Epoch 14/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6582 - val_loss: 0.7968
Epoch 15/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6589 - val_loss: 0.7965
Epoch 16/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6587 - val_loss: 0.7962
Epoch 17/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6582 - val_loss: 0.7960
Epoch 18/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6576 - val_loss: 0.7958
Epoch 19/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6569 - val_loss: 0.7957
Epoch 20/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6547 - val_loss: 0.7955
Epoch 21/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6540 - val_loss: 0.7954
Epoch 22/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6535 - val_loss: 0.7950
Epoch 23/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6551 - val_loss: 0.7950
Epoch 24/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6527 - val_loss: 0.7949
Epoch 25/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6518 - val_loss: 0.7948
Epoch 26/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6514 - val_loss: 0.7948
Epoch 27/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6506 - val_loss: 0.7949
Epoch 28/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6444 - val_loss: 0.7949
Epoch 29/68
79/79 [==============================] - 3s 32ms/step - loss: 0.6429 - val_loss: 0.7947
Epoch 30/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6436 - val_loss: 0.7947
Epoch 31/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6411 - val_loss: 0.7947
Epoch 32/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6398 - val_loss: 0.7947
Epoch 33/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6390 - val_loss: 0.7946
Epoch 34/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6384 - val_loss: 0.7946
Epoch 35/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6372 - val_loss: 0.7945
Epoch 36/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6368 - val_loss: 0.7944
Epoch 37/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6415 - val_loss: 0.7942
Epoch 38/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6521 - val_loss: 0.7942
Epoch 39/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6501 - val_loss: 0.7941
Epoch 40/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6498 - val_loss: 0.7941
Epoch 41/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6495 - val_loss: 0.7941
Epoch 42/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6445 - val_loss: 0.7941
Epoch 43/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6359 - val_loss: 0.7940
Epoch 44/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6356 - val_loss: 0.7940
Epoch 45/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6352 - val_loss: 0.7940
Epoch 46/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6350 - val_loss: 0.7939
Epoch 47/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6340 - val_loss: 0.7939
Epoch 48/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6338 - val_loss: 0.7938
Epoch 49/68
79/79 [==============================] - 2s 32ms/step - loss: 0.6335 - val_loss: 0.7937
Epoch 50/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6333 - val_loss: 0.7937
Epoch 51/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6332 - val_loss: 0.7936
Epoch 52/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6335 - val_loss: 0.7936
Epoch 53/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6331 - val_loss: 0.7935
Epoch 54/68
79/79 [==============================] - ETA: 0s - loss: 0.632 - 2s 29ms/step - loss: 0.6325 - val_loss: 0.7934
Epoch 55/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6337 - val_loss: 0.7934
Epoch 56/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6325 - val_loss: 0.7934
Epoch 57/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6321 - val_loss: 0.7933
Epoch 58/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6326 - val_loss: 0.7932
Epoch 59/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6326 - val_loss: 0.7932
Epoch 60/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6322 - val_loss: 0.7932
Epoch 61/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6323 - val_loss: 0.7931
Epoch 62/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6319 - val_loss: 0.7931
Epoch 63/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6312 - val_loss: 0.7930
Epoch 64/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6318 - val_loss: 0.7930
Epoch 65/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6316 - val_loss: 0.7929
Epoch 66/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6317 - val_loss: 0.7929
Epoch 67/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6308 - val_loss: 0.7928
Epoch 68/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6307 - val_loss: 0.7928
Execution time:  165.44869828224182
GRU:
Mean Absolute Error: 0.6108
Root Mean Square Error: 0.9075
Mean Square Error: 0.8236

Train RMSE: 0.908
Train MSE: 0.824
Train MAE: 0.611
###########################

MODEL:  GRU
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_123&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_39 (GRU)                 (None, 72, 55)            9570      
_________________________________________________________________
dropout_123 (Dropout)        (None, 72, 55)            0         
_________________________________________________________________
time_distributed_123 (TimeDi (None, 72, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 10s 29ms/step - loss: 0.7047 - val_loss: 0.7305
Epoch 2/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6636 - val_loss: 0.7330
Epoch 3/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6517 - val_loss: 0.7269
Epoch 4/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6351 - val_loss: 0.7265
Epoch 5/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6317 - val_loss: 0.7234
Epoch 6/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6308 - val_loss: 0.7226
Epoch 7/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6301 - val_loss: 0.7221
Epoch 8/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6294 - val_loss: 0.7218
Epoch 9/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6292 - val_loss: 0.7213
Epoch 10/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6278 - val_loss: 0.7197
Epoch 11/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6269 - val_loss: 0.7197
Epoch 12/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6274 - val_loss: 0.7176
Epoch 13/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6263 - val_loss: 0.7148
Epoch 14/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6241 - val_loss: 0.7098
Epoch 15/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6241 - val_loss: 0.7087
Epoch 16/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6177 - val_loss: 0.7038
Epoch 17/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6143 - val_loss: 0.7027
Epoch 18/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6097 - val_loss: 0.7013
Epoch 19/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6257 - val_loss: 0.7006
Epoch 20/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6105 - val_loss: 0.7012
Epoch 21/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6045 - val_loss: 0.7011
Epoch 22/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6078 - val_loss: 0.7009
Epoch 23/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6065 - val_loss: 0.7014
Epoch 24/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6023 - val_loss: 0.7011
Epoch 25/36
349/349 [==============================] - 10s 28ms/step - loss: 0.5998 - val_loss: 0.7000
Epoch 26/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6004 - val_loss: 0.6994
Epoch 27/36
349/349 [==============================] - 10s 28ms/step - loss: 0.5991 - val_loss: 0.7003
Epoch 28/36
349/349 [==============================] - 10s 28ms/step - loss: 0.5977 - val_loss: 0.7015
Epoch 29/36
349/349 [==============================] - 10s 27ms/step - loss: 0.5970 - val_loss: 0.7014
Epoch 30/36
349/349 [==============================] - 10s 27ms/step - loss: 0.5966 - val_loss: 0.7019
Epoch 31/36
349/349 [==============================] - 10s 27ms/step - loss: 0.5963 - val_loss: 0.7014
Epoch 32/36
349/349 [==============================] - 9s 27ms/step - loss: 0.5961 - val_loss: 0.7017
Epoch 33/36
349/349 [==============================] - 10s 27ms/step - loss: 0.5955 - val_loss: 0.7019
Epoch 34/36
349/349 [==============================] - 9s 27ms/step - loss: 0.5940 - val_loss: 0.6998
Epoch 35/36
349/349 [==============================] - 10s 27ms/step - loss: 0.5960 - val_loss: 0.6976
Epoch 36/36
349/349 [==============================] - 10s 27ms/step - loss: 0.5956 - val_loss: 0.7002
Execution time:  352.48098826408386
GRU:
Mean Absolute Error: 0.5834
Root Mean Square Error: 0.8860
Mean Square Error: 0.7850

Train RMSE: 0.886
Train MSE: 0.785
Train MAE: 0.583
###########################

MODEL:  GRU
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_124&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_40 (GRU)                 (None, 72, 40)            5160      
_________________________________________________________________
dropout_124 (Dropout)        (None, 72, 40)            0         
_________________________________________________________________
time_distributed_124 (TimeDi (None, 72, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 34ms/step - loss: 0.7000 - val_loss: 0.7982
Epoch 2/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6998 - val_loss: 0.7976
Epoch 3/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6995 - val_loss: 0.7970
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6993 - val_loss: 0.7963
Epoch 5/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6990 - val_loss: 0.7957
Epoch 6/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6987 - val_loss: 0.7950
Epoch 7/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6984 - val_loss: 0.7943
Epoch 8/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6980 - val_loss: 0.7936
Epoch 9/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6978 - val_loss: 0.7929
Epoch 10/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6975 - val_loss: 0.7922
Epoch 11/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6973 - val_loss: 0.7914
Epoch 12/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6968 - val_loss: 0.7907
Epoch 13/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6965 - val_loss: 0.7899
Epoch 14/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6962 - val_loss: 0.7892
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6959 - val_loss: 0.7884
Epoch 16/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6956 - val_loss: 0.7876
Epoch 17/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6952 - val_loss: 0.7869
Epoch 18/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6949 - val_loss: 0.7861
Epoch 19/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6946 - val_loss: 0.7853
Epoch 20/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6942 - val_loss: 0.7845
Epoch 21/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6940 - val_loss: 0.7837
Epoch 22/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6936 - val_loss: 0.7829
Epoch 23/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6932 - val_loss: 0.7822
Epoch 24/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6929 - val_loss: 0.7814
Epoch 25/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6924 - val_loss: 0.7806
Epoch 26/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6921 - val_loss: 0.7798
Epoch 27/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6919 - val_loss: 0.7790
Epoch 28/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6916 - val_loss: 0.7782
Epoch 29/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6912 - val_loss: 0.7773
Epoch 30/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6909 - val_loss: 0.7765
Epoch 31/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6905 - val_loss: 0.7757
Epoch 32/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6902 - val_loss: 0.7749
Epoch 33/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6899 - val_loss: 0.7741
Epoch 34/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6895 - val_loss: 0.7733
Epoch 35/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6891 - val_loss: 0.7725
Epoch 36/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6888 - val_loss: 0.7716
Epoch 37/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6884 - val_loss: 0.7708
Epoch 38/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6881 - val_loss: 0.7700
Epoch 39/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6878 - val_loss: 0.7692
Epoch 40/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6875 - val_loss: 0.7683
Epoch 41/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6871 - val_loss: 0.7675
Epoch 42/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6869 - val_loss: 0.7667
Epoch 43/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6863 - val_loss: 0.7658
Epoch 44/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6860 - val_loss: 0.7650
Epoch 45/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6857 - val_loss: 0.7642
Epoch 46/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6853 - val_loss: 0.7633
Epoch 47/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6850 - val_loss: 0.7625
Epoch 48/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6847 - val_loss: 0.7616
Epoch 49/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6842 - val_loss: 0.7608
Epoch 50/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6839 - val_loss: 0.7600
Epoch 51/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6835 - val_loss: 0.7591
Epoch 52/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6832 - val_loss: 0.7583
Epoch 53/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6828 - val_loss: 0.7574
Epoch 54/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6826 - val_loss: 0.7566
Epoch 55/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6823 - val_loss: 0.7557
Epoch 56/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6818 - val_loss: 0.7549
Epoch 57/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6815 - val_loss: 0.7540
Epoch 58/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6811 - val_loss: 0.7532
Epoch 59/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6807 - val_loss: 0.7523
Epoch 60/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6805 - val_loss: 0.7515
Epoch 61/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6801 - val_loss: 0.7506
Epoch 62/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6798 - val_loss: 0.7498
Epoch 63/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6794 - val_loss: 0.7489
Epoch 64/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6791 - val_loss: 0.7481
Epoch 65/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6786 - val_loss: 0.7472
Epoch 66/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6783 - val_loss: 0.7464
Epoch 67/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6780 - val_loss: 0.7455
Epoch 68/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6776 - val_loss: 0.7446
Execution time:  164.49325609207153
GRU:
Mean Absolute Error: 0.6788
Root Mean Square Error: 0.9745
Mean Square Error: 0.9497

Train RMSE: 0.975
Train MSE: 0.950
Train MAE: 0.679
###########################

MODEL:  GRU
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_125&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_41 (GRU)                 (None, 72, 55)            9570      
_________________________________________________________________
dropout_125 (Dropout)        (None, 72, 55)            0         
_________________________________________________________________
time_distributed_125 (TimeDi (None, 72, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 10s 29ms/step - loss: 0.7241 - val_loss: 0.7141
Epoch 2/36
349/349 [==============================] - 10s 27ms/step - loss: 0.7226 - val_loss: 0.7119
Epoch 3/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7210 - val_loss: 0.7095
Epoch 4/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7194 - val_loss: 0.7071
Epoch 5/36
349/349 [==============================] - 9s 27ms/step - loss: 0.7177 - val_loss: 0.7046
Epoch 6/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7159 - val_loss: 0.7021
Epoch 7/36
349/349 [==============================] - 10s 27ms/step - loss: 0.7141 - val_loss: 0.6995
Epoch 8/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7124 - val_loss: 0.6968
Epoch 9/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7106 - val_loss: 0.6942
Epoch 10/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7089 - val_loss: 0.6915
Epoch 11/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7071 - val_loss: 0.6888
Epoch 12/36
349/349 [==============================] - 10s 27ms/step - loss: 0.7053 - val_loss: 0.6860
Epoch 13/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7035 - val_loss: 0.6832
Epoch 14/36
349/349 [==============================] - 10s 28ms/step - loss: 0.7016 - val_loss: 0.6804
Epoch 15/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6999 - val_loss: 0.6775
Epoch 16/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6980 - val_loss: 0.6746
Epoch 17/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6962 - val_loss: 0.6717
Epoch 18/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6943 - val_loss: 0.6688
Epoch 19/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6925 - val_loss: 0.6658
Epoch 20/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6906 - val_loss: 0.6628
Epoch 21/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6887 - val_loss: 0.6597
Epoch 22/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6868 - val_loss: 0.6567
Epoch 23/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6849 - val_loss: 0.6537
Epoch 24/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6830 - val_loss: 0.6507
Epoch 25/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6812 - val_loss: 0.6477
Epoch 26/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6794 - val_loss: 0.6448
Epoch 27/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6776 - val_loss: 0.6419
Epoch 28/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6758 - val_loss: 0.6390
Epoch 29/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6740 - val_loss: 0.6362
Epoch 30/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6722 - val_loss: 0.6333
Epoch 31/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6704 - val_loss: 0.6304
Epoch 32/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6686 - val_loss: 0.6275
Epoch 33/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6668 - val_loss: 0.6246
Epoch 34/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6649 - val_loss: 0.6217
Epoch 35/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6631 - val_loss: 0.6188
Epoch 36/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6612 - val_loss: 0.6158
Execution time:  350.4732291698456
GRU:
Mean Absolute Error: 0.6327
Root Mean Square Error: 0.9383
Mean Square Error: 0.8804

Train RMSE: 0.938
Train MSE: 0.880
Train MAE: 0.633
###########################

MODEL:  GRU
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_126&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_42 (GRU)                 (None, 72, 40)            5160      
_________________________________________________________________
dropout_126 (Dropout)        (None, 72, 40)            0         
_________________________________________________________________
time_distributed_126 (TimeDi (None, 72, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 34ms/step - loss: 0.8948 - val_loss: 1.2981
Epoch 2/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8947 - val_loss: 1.2980
Epoch 3/68
79/79 [==============================] - 2s 32ms/step - loss: 0.8947 - val_loss: 1.2978
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8946 - val_loss: 1.2976
Epoch 5/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8945 - val_loss: 1.2974
Epoch 6/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8944 - val_loss: 1.2972
Epoch 7/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8943 - val_loss: 1.2970
Epoch 8/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8942 - val_loss: 1.2969
Epoch 9/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8942 - val_loss: 1.2967
Epoch 10/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8941 - val_loss: 1.2965
Epoch 11/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8940 - val_loss: 1.2963
Epoch 12/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8939 - val_loss: 1.2960
Epoch 13/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8939 - val_loss: 1.2958
Epoch 14/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8937 - val_loss: 1.2956
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8937 - val_loss: 1.2954
Epoch 16/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8936 - val_loss: 1.2952
Epoch 17/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8934 - val_loss: 1.2950
Epoch 18/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8934 - val_loss: 1.2948
Epoch 19/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8933 - val_loss: 1.2946
Epoch 20/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8932 - val_loss: 1.2944
Epoch 21/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8931 - val_loss: 1.2941
Epoch 22/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8930 - val_loss: 1.2939
Epoch 23/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8929 - val_loss: 1.2937
Epoch 24/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8928 - val_loss: 1.2935
Epoch 25/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8927 - val_loss: 1.2933
Epoch 26/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8926 - val_loss: 1.2931
Epoch 27/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8925 - val_loss: 1.2928
Epoch 28/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8924 - val_loss: 1.2926
Epoch 29/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8923 - val_loss: 1.2924
Epoch 30/68
79/79 [==============================] - 3s 32ms/step - loss: 0.8922 - val_loss: 1.2922
Epoch 31/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8921 - val_loss: 1.2919
Epoch 32/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8920 - val_loss: 1.2917
Epoch 33/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8919 - val_loss: 1.2915
Epoch 34/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8918 - val_loss: 1.2913
Epoch 35/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8917 - val_loss: 1.2910
Epoch 36/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8916 - val_loss: 1.2908
Epoch 37/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8915 - val_loss: 1.2906
Epoch 38/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8914 - val_loss: 1.2904
Epoch 39/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8913 - val_loss: 1.2901
Epoch 40/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8912 - val_loss: 1.2899
Epoch 41/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8911 - val_loss: 1.2897
Epoch 42/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8910 - val_loss: 1.2895
Epoch 43/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8909 - val_loss: 1.2892
Epoch 44/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8908 - val_loss: 1.2890
Epoch 45/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8907 - val_loss: 1.2888
Epoch 46/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8906 - val_loss: 1.2885
Epoch 47/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8905 - val_loss: 1.2883
Epoch 48/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8904 - val_loss: 1.2881
Epoch 49/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8903 - val_loss: 1.2878
Epoch 50/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8902 - val_loss: 1.2876
Epoch 51/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8901 - val_loss: 1.2874
Epoch 52/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8900 - val_loss: 1.2872
Epoch 53/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8899 - val_loss: 1.2869
Epoch 54/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8898 - val_loss: 1.2867
Epoch 55/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8897 - val_loss: 1.2865
Epoch 56/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8896 - val_loss: 1.2862
Epoch 57/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8895 - val_loss: 1.2860
Epoch 58/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8894 - val_loss: 1.2857
Epoch 59/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8892 - val_loss: 1.2855
Epoch 60/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8892 - val_loss: 1.2853
Epoch 61/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8891 - val_loss: 1.2850
Epoch 62/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8890 - val_loss: 1.2848
Epoch 63/68
79/79 [==============================] - 2s 30ms/step - loss: 0.8889 - val_loss: 1.2846
Epoch 64/68
79/79 [==============================] - 2s 31ms/step - loss: 0.8888 - val_loss: 1.2843
Epoch 65/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8886 - val_loss: 1.2841
Epoch 66/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8885 - val_loss: 1.2839
Epoch 67/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8884 - val_loss: 1.2836
Epoch 68/68
79/79 [==============================] - 2s 29ms/step - loss: 0.8883 - val_loss: 1.2834
Execution time:  164.11080074310303
GRU:
Mean Absolute Error: 0.9233
Root Mean Square Error: 1.1187
Mean Square Error: 1.2514

Train RMSE: 1.119
Train MSE: 1.251
Train MAE: 0.923
###########################

MODEL:  GRU
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_127&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_43 (GRU)                 (None, 72, 55)            9570      
_________________________________________________________________
dropout_127 (Dropout)        (None, 72, 55)            0         
_________________________________________________________________
time_distributed_127 (TimeDi (None, 72, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 10s 29ms/step - loss: 0.8820 - val_loss: 1.1299
Epoch 2/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8816 - val_loss: 1.1292
Epoch 3/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8812 - val_loss: 1.1285
Epoch 4/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8807 - val_loss: 1.1276
Epoch 5/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8802 - val_loss: 1.1268
Epoch 6/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8797 - val_loss: 1.1259
Epoch 7/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8792 - val_loss: 1.1250
Epoch 8/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8786 - val_loss: 1.1240
Epoch 9/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8780 - val_loss: 1.1231
Epoch 10/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8774 - val_loss: 1.1221
Epoch 11/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8769 - val_loss: 1.1211
Epoch 12/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8763 - val_loss: 1.1201
Epoch 13/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8757 - val_loss: 1.1190
Epoch 14/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8750 - val_loss: 1.1179
Epoch 15/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8744 - val_loss: 1.1169
Epoch 16/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8738 - val_loss: 1.1158
Epoch 17/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8732 - val_loss: 1.1146
Epoch 18/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8725 - val_loss: 1.1135
Epoch 19/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8718 - val_loss: 1.1124
Epoch 20/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8711 - val_loss: 1.1112
Epoch 21/36
349/349 [==============================] - 9s 27ms/step - loss: 0.8705 - val_loss: 1.1100
Epoch 22/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8698 - val_loss: 1.1088
Epoch 23/36
349/349 [==============================] - 9s 27ms/step - loss: 0.8691 - val_loss: 1.1076
Epoch 24/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8684 - val_loss: 1.1063
Epoch 25/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8676 - val_loss: 1.1051
Epoch 26/36
349/349 [==============================] - 9s 27ms/step - loss: 0.8669 - val_loss: 1.1038
Epoch 27/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8661 - val_loss: 1.1025
Epoch 28/36
349/349 [==============================] - 9s 27ms/step - loss: 0.8654 - val_loss: 1.1012
Epoch 29/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8646 - val_loss: 1.0998
Epoch 30/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8638 - val_loss: 1.0985
Epoch 31/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8631 - val_loss: 1.0971
Epoch 32/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8623 - val_loss: 1.0957
Epoch 33/36
349/349 [==============================] - 9s 27ms/step - loss: 0.8615 - val_loss: 1.0943
Epoch 34/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8606 - val_loss: 1.0928
Epoch 35/36
349/349 [==============================] - 10s 28ms/step - loss: 0.8598 - val_loss: 1.0914
Epoch 36/36
349/349 [==============================] - 10s 27ms/step - loss: 0.8590 - val_loss: 1.0899
Execution time:  351.1460087299347
GRU:
Mean Absolute Error: 0.8933
Root Mean Square Error: 1.0897
Mean Square Error: 1.1876

Train RMSE: 1.090
Train MSE: 1.188
Train MAE: 0.893
###########################

MODEL:  GRU
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_128&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_44 (GRU)                 (None, 72, 40)            5160      
_________________________________________________________________
dropout_128 (Dropout)        (None, 72, 40)            0         
_________________________________________________________________
time_distributed_128 (TimeDi (None, 72, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 35ms/step - loss: 0.6398 - val_loss: 0.4376
Epoch 2/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5306 - val_loss: 0.2128
Epoch 3/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5148 - val_loss: 0.2122
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5080 - val_loss: 0.2102
Epoch 5/68
79/79 [==============================] - 2s 30ms/step - loss: 0.5044 - val_loss: 0.2080
Epoch 6/68
79/79 [==============================] - 2s 29ms/step - loss: 0.5024 - val_loss: 0.2065
Epoch 7/68
79/79 [==============================] - 2s 31ms/step - loss: 0.5009 - val_loss: 0.2056
Epoch 8/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4999 - val_loss: 0.2046
Epoch 9/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4988 - val_loss: 0.2037
Epoch 10/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4981 - val_loss: 0.2028
Epoch 11/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4976 - val_loss: 0.2022
Epoch 12/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4968 - val_loss: 0.2018
Epoch 13/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4964 - val_loss: 0.2013
Epoch 14/68
79/79 [==============================] - 2s 31ms/step - loss: 0.4959 - val_loss: 0.2007
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4956 - val_loss: 0.2010
Epoch 16/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4948 - val_loss: 0.2006
Epoch 17/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4947 - val_loss: 0.2004
Epoch 18/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4942 - val_loss: 0.2004
Epoch 19/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4939 - val_loss: 0.2003
Epoch 20/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4936 - val_loss: 0.1997
Epoch 21/68
79/79 [==============================] - 2s 31ms/step - loss: 0.4932 - val_loss: 0.2001
Epoch 22/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4928 - val_loss: 0.1999
Epoch 23/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4929 - val_loss: 0.1995
Epoch 24/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4925 - val_loss: 0.1997
Epoch 25/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4922 - val_loss: 0.2000
Epoch 26/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4920 - val_loss: 0.1998
Epoch 27/68
79/79 [==============================] - 2s 31ms/step - loss: 0.4916 - val_loss: 0.2000
Epoch 28/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4915 - val_loss: 0.2000
Epoch 29/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4913 - val_loss: 0.1994
Epoch 30/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4911 - val_loss: 0.1997
Epoch 31/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4910 - val_loss: 0.1998
Epoch 32/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4908 - val_loss: 0.1998
Epoch 33/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4906 - val_loss: 0.1999
Epoch 34/68
79/79 [==============================] - 3s 32ms/step - loss: 0.4903 - val_loss: 0.2000
Epoch 35/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4901 - val_loss: 0.1997
Epoch 36/68
79/79 [==============================] - 2s 30ms/step - loss: 0.4902 - val_loss: 0.2003
Epoch 37/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4899 - val_loss: 0.1997
Epoch 38/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4897 - val_loss: 0.2000
Epoch 39/68
79/79 [==============================] - 2s 29ms/step - loss: 0.4895 - val_loss: 0.2001
Execution time:  96.05110597610474
GRU:
Mean Absolute Error: 0.1998
Root Mean Square Error: 0.6027
Mean Square Error: 0.3632

Train RMSE: 0.603
Train MSE: 0.363
Train MAE: 0.200
###########################

MODEL:  GRU
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_129&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_45 (GRU)                 (None, 72, 55)            9570      
_________________________________________________________________
dropout_129 (Dropout)        (None, 72, 55)            0         
_________________________________________________________________
time_distributed_129 (TimeDi (None, 72, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 10s 29ms/step - loss: 0.5401 - val_loss: 0.3813
Epoch 2/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4946 - val_loss: 0.3798
Epoch 3/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4903 - val_loss: 0.3786
Epoch 4/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4879 - val_loss: 0.3786
Epoch 5/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4865 - val_loss: 0.3787
Epoch 6/36
349/349 [==============================] - 9s 27ms/step - loss: 0.4852 - val_loss: 0.3791
Epoch 7/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4843 - val_loss: 0.3785
Epoch 8/36
349/349 [==============================] - 9s 27ms/step - loss: 0.4835 - val_loss: 0.3783
Epoch 9/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4826 - val_loss: 0.3778
Epoch 10/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4819 - val_loss: 0.3775
Epoch 11/36
349/349 [==============================] - 9s 27ms/step - loss: 0.4813 - val_loss: 0.3773
Epoch 12/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4808 - val_loss: 0.3773
Epoch 13/36
349/349 [==============================] - 9s 27ms/step - loss: 0.4805 - val_loss: 0.3773
Epoch 14/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4802 - val_loss: 0.3772
Epoch 15/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4799 - val_loss: 0.3770
Epoch 16/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4796 - val_loss: 0.3773
Epoch 17/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4794 - val_loss: 0.3770
Epoch 18/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4793 - val_loss: 0.3769
Epoch 19/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4790 - val_loss: 0.3771
Epoch 20/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4784 - val_loss: 0.3771
Epoch 21/36
349/349 [==============================] - 9s 27ms/step - loss: 0.4780 - val_loss: 0.3772
Epoch 22/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4775 - val_loss: 0.3772
Epoch 23/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4768 - val_loss: 0.3771
Epoch 24/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4764 - val_loss: 0.3765
Epoch 25/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4761 - val_loss: 0.3756
Epoch 26/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4754 - val_loss: 0.3751
Epoch 27/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4746 - val_loss: 0.3748
Epoch 28/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4741 - val_loss: 0.3746
Epoch 29/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4734 - val_loss: 0.3745
Epoch 30/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4729 - val_loss: 0.3738
Epoch 31/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4722 - val_loss: 0.3736
Epoch 32/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4715 - val_loss: 0.3731
Epoch 33/36
349/349 [==============================] - 9s 27ms/step - loss: 0.4704 - val_loss: 0.3727
Epoch 34/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4694 - val_loss: 0.3719
Epoch 35/36
349/349 [==============================] - 10s 28ms/step - loss: 0.4685 - val_loss: 0.3718
Epoch 36/36
349/349 [==============================] - 10s 27ms/step - loss: 0.4676 - val_loss: 0.3715
Execution time:  349.9762351512909
GRU:
Mean Absolute Error: 0.2497
Root Mean Square Error: 0.6709
Mean Square Error: 0.4501

Train RMSE: 0.671
Train MSE: 0.450
Train MAE: 0.250
###########################

MODEL:  GRU
sequence:  12h
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_130&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_46 (GRU)                 (None, 72, 40)            5160      
_________________________________________________________________
dropout_130 (Dropout)        (None, 72, 40)            0         
_________________________________________________________________
time_distributed_130 (TimeDi (None, 72, 1)             41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
79/79 [==============================] - 3s 34ms/step - loss: 0.8776 - val_loss: 1.1863
Epoch 2/68
79/79 [==============================] - 2s 29ms/step - loss: 0.7551 - val_loss: 0.8824
Epoch 3/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6846 - val_loss: 0.8439
Epoch 4/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6704 - val_loss: 0.8307
Epoch 5/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6607 - val_loss: 0.8223
Epoch 6/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6550 - val_loss: 0.8163
Epoch 7/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6514 - val_loss: 0.8123
Epoch 8/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6488 - val_loss: 0.8093
Epoch 9/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6468 - val_loss: 0.8070
Epoch 10/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6451 - val_loss: 0.8051
Epoch 11/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6438 - val_loss: 0.8036
Epoch 12/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6426 - val_loss: 0.8023
Epoch 13/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6417 - val_loss: 0.8013
Epoch 14/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6409 - val_loss: 0.8005
Epoch 15/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6400 - val_loss: 0.7997
Epoch 16/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6394 - val_loss: 0.7991
Epoch 17/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6388 - val_loss: 0.7985
Epoch 18/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6381 - val_loss: 0.7980
Epoch 19/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6377 - val_loss: 0.7976
Epoch 20/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6372 - val_loss: 0.7972
Epoch 21/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6368 - val_loss: 0.7968
Epoch 22/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6364 - val_loss: 0.7965
Epoch 23/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6361 - val_loss: 0.7963
Epoch 24/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6356 - val_loss: 0.7960
Epoch 25/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6353 - val_loss: 0.7958
Epoch 26/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6350 - val_loss: 0.7955
Epoch 27/68
79/79 [==============================] - 2s 32ms/step - loss: 0.6348 - val_loss: 0.7953
Epoch 28/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6345 - val_loss: 0.7952
Epoch 29/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6341 - val_loss: 0.7950
Epoch 30/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6339 - val_loss: 0.7948
Epoch 31/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6337 - val_loss: 0.7947
Epoch 32/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6335 - val_loss: 0.7946
Epoch 33/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6334 - val_loss: 0.7944
Epoch 34/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6331 - val_loss: 0.7943
Epoch 35/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6329 - val_loss: 0.7942s - l
Epoch 36/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6328 - val_loss: 0.7941
Epoch 37/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6325 - val_loss: 0.7940
Epoch 38/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6324 - val_loss: 0.7939
Epoch 39/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6323 - val_loss: 0.7938
Epoch 40/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6321 - val_loss: 0.7937
Epoch 41/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6319 - val_loss: 0.7937
Epoch 42/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6318 - val_loss: 0.7936
Epoch 43/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6316 - val_loss: 0.7935
Epoch 44/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6316 - val_loss: 0.7934
Epoch 45/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6314 - val_loss: 0.7934
Epoch 46/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6313 - val_loss: 0.7933
Epoch 47/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6312 - val_loss: 0.7933
Epoch 48/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6310 - val_loss: 0.7932
Epoch 49/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6308 - val_loss: 0.7932
Epoch 50/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6308 - val_loss: 0.7931
Epoch 51/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6307 - val_loss: 0.7931
Epoch 52/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6306 - val_loss: 0.7930
Epoch 53/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6305 - val_loss: 0.7930
Epoch 54/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6304 - val_loss: 0.7930
Epoch 55/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6302 - val_loss: 0.7929
Epoch 56/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6302 - val_loss: 0.7929
Epoch 57/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6300 - val_loss: 0.7928
Epoch 58/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6300 - val_loss: 0.7928
Epoch 59/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6299 - val_loss: 0.7928
Epoch 60/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6298 - val_loss: 0.7927
Epoch 61/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6298 - val_loss: 0.7927
Epoch 62/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6296 - val_loss: 0.7927
Epoch 63/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6295 - val_loss: 0.7927
Epoch 64/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6294 - val_loss: 0.7926
Epoch 65/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6293 - val_loss: 0.7926
Epoch 66/68
79/79 [==============================] - 2s 29ms/step - loss: 0.6293 - val_loss: 0.7926
Epoch 67/68
79/79 [==============================] - 2s 30ms/step - loss: 0.6292 - val_loss: 0.7926
Epoch 68/68
79/79 [==============================] - 2s 31ms/step - loss: 0.6291 - val_loss: 0.7925
Execution time:  164.49348998069763
GRU:
Mean Absolute Error: 0.5264
Root Mean Square Error: 0.7791
Mean Square Error: 0.6070

Train RMSE: 0.779
Train MSE: 0.607
Train MAE: 0.526
###########################

MODEL:  GRU
sequence:  12h
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_131&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_47 (GRU)                 (None, 72, 55)            9570      
_________________________________________________________________
dropout_131 (Dropout)        (None, 72, 55)            0         
_________________________________________________________________
time_distributed_131 (TimeDi (None, 72, 1)             56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
349/349 [==============================] - 10s 29ms/step - loss: 0.7229 - val_loss: 0.7491
Epoch 2/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6741 - val_loss: 0.7435
Epoch 3/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6714 - val_loss: 0.7417
Epoch 4/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6672 - val_loss: 0.7402
Epoch 5/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6493 - val_loss: 0.7393
Epoch 6/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6385 - val_loss: 0.7365
Epoch 7/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6368 - val_loss: 0.7342
Epoch 8/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6356 - val_loss: 0.7323
Epoch 9/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6345 - val_loss: 0.7306
Epoch 10/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6336 - val_loss: 0.7289
Epoch 11/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6326 - val_loss: 0.7273
Epoch 12/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6320 - val_loss: 0.7259
Epoch 13/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6314 - val_loss: 0.7247
Epoch 14/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6307 - val_loss: 0.7239
Epoch 15/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6298 - val_loss: 0.7233
Epoch 16/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6290 - val_loss: 0.7230
Epoch 17/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6284 - val_loss: 0.7228
Epoch 18/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6278 - val_loss: 0.7226
Epoch 19/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6272 - val_loss: 0.7223
Epoch 20/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6267 - val_loss: 0.7222
Epoch 21/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6262 - val_loss: 0.7218
Epoch 22/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6257 - val_loss: 0.7212
Epoch 23/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6253 - val_loss: 0.7205
Epoch 24/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6249 - val_loss: 0.7195
Epoch 25/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6245 - val_loss: 0.7186
Epoch 26/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6241 - val_loss: 0.7175
Epoch 27/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6237 - val_loss: 0.7164
Epoch 28/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6233 - val_loss: 0.7153
Epoch 29/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6228 - val_loss: 0.7140
Epoch 30/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6223 - val_loss: 0.7131
Epoch 31/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6219 - val_loss: 0.7119
Epoch 32/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6212 - val_loss: 0.7111
Epoch 33/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6204 - val_loss: 0.7103
Epoch 34/36
349/349 [==============================] - 9s 27ms/step - loss: 0.6194 - val_loss: 0.7096
Epoch 35/36
349/349 [==============================] - 10s 28ms/step - loss: 0.6184 - val_loss: 0.7088
Epoch 36/36
349/349 [==============================] - 10s 27ms/step - loss: 0.6172 - val_loss: 0.7082
Execution time:  349.2170240879059
GRU:
Mean Absolute Error: 0.5517
Root Mean Square Error: 0.8189
Mean Square Error: 0.6707

Train RMSE: 0.819
Train MSE: 0.671
Train MAE: 0.552
###########################

MODEL:  GRU
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_132&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_48 (GRU)                 (None, 144, 40)           5160      
_________________________________________________________________
dropout_132 (Dropout)        (None, 144, 40)           0         
_________________________________________________________________
time_distributed_132 (TimeDi (None, 144, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 4s 58ms/step - loss: 0.6494 - val_loss: 0.2595
Epoch 2/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5901 - val_loss: 0.2032
Epoch 3/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5851 - val_loss: 0.2118
Epoch 4/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5818 - val_loss: 0.2142
Epoch 5/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5794 - val_loss: 0.2150
Epoch 6/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5779 - val_loss: 0.2157
Epoch 7/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5763 - val_loss: 0.2155
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5749 - val_loss: 0.2155
Epoch 9/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5733 - val_loss: 0.2156
Epoch 10/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5718 - val_loss: 0.2158
Epoch 11/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5699 - val_loss: 0.2164
Epoch 12/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5676 - val_loss: 0.2166
Execution time:  53.778528690338135
GRU:
Mean Absolute Error: 0.2877
Root Mean Square Error: 0.6909
Mean Square Error: 0.4773

Train RMSE: 0.691
Train MSE: 0.477
Train MAE: 0.288
###########################

MODEL:  GRU
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_133&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_49 (GRU)                 (None, 144, 55)           9570      
_________________________________________________________________
dropout_133 (Dropout)        (None, 144, 55)           0         
_________________________________________________________________
time_distributed_133 (TimeDi (None, 144, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 19s 55ms/step - loss: 0.5987 - val_loss: 0.4014
Epoch 2/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5785 - val_loss: 0.4043
Epoch 3/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5744 - val_loss: 0.4074
Epoch 4/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5741 - val_loss: 0.4099
Epoch 5/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5678 - val_loss: 0.4141
Epoch 6/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5685 - val_loss: 0.4086
Epoch 7/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5628 - val_loss: 0.4089
Epoch 8/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5615 - val_loss: 0.4114
Epoch 9/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5616 - val_loss: 0.4114
Epoch 10/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5607 - val_loss: 0.4110
Epoch 11/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5599 - val_loss: 0.4085
Execution time:  202.80041694641113
GRU:
Mean Absolute Error: 0.2784
Root Mean Square Error: 0.6900
Mean Square Error: 0.4762

Train RMSE: 0.690
Train MSE: 0.476
Train MAE: 0.278
###########################

MODEL:  GRU
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_134&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_50 (GRU)                 (None, 144, 40)           5160      
_________________________________________________________________
dropout_134 (Dropout)        (None, 144, 40)           0         
_________________________________________________________________
time_distributed_134 (TimeDi (None, 144, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 60ms/step - loss: 0.8522 - val_loss: 0.8824
Epoch 2/68
77/77 [==============================] - 4s 54ms/step - loss: 0.7174 - val_loss: 0.8257
Epoch 3/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6989 - val_loss: 0.8105
Epoch 4/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6923 - val_loss: 0.8044
Epoch 5/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6890 - val_loss: 0.8011
Epoch 6/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6871 - val_loss: 0.7994
Epoch 7/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6853 - val_loss: 0.7985
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6833 - val_loss: 0.7977
Epoch 9/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6813 - val_loss: 0.7971
Epoch 10/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6793 - val_loss: 0.7965
Epoch 11/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6775 - val_loss: 0.7959
Epoch 12/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6760 - val_loss: 0.7955
Epoch 13/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6744 - val_loss: 0.7948
Epoch 14/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6736 - val_loss: 0.7947
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6714 - val_loss: 0.7940
Epoch 16/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6709 - val_loss: 0.7939
Epoch 17/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6693 - val_loss: 0.7935
Epoch 18/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6687 - val_loss: 0.7934
Epoch 19/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6671 - val_loss: 0.7929
Epoch 20/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6658 - val_loss: 0.7925
Epoch 21/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6672 - val_loss: 0.7926
Epoch 22/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6636 - val_loss: 0.7923
Epoch 23/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6656 - val_loss: 0.7923
Epoch 24/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6639 - val_loss: 0.7921
Epoch 25/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6625 - val_loss: 0.7921
Epoch 26/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6717 - val_loss: 0.7931
Epoch 27/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6628 - val_loss: 0.7918
Epoch 28/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6596 - val_loss: 0.7918
Epoch 29/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6568 - val_loss: 0.7918
Epoch 30/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6553 - val_loss: 0.7917
Epoch 31/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6546 - val_loss: 0.7916
Epoch 32/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6536 - val_loss: 0.7916
Epoch 33/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6530 - val_loss: 0.7916
Epoch 34/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6523 - val_loss: 0.7915
Epoch 35/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6534 - val_loss: 0.7914
Epoch 36/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6528 - val_loss: 0.7914
Epoch 37/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6536 - val_loss: 0.7915
Epoch 38/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6485 - val_loss: 0.7915
Epoch 39/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6471 - val_loss: 0.7914
Epoch 40/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6446 - val_loss: 0.7914
Epoch 41/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6434 - val_loss: 0.7914
Epoch 42/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6491 - val_loss: 0.7915
Epoch 43/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6440 - val_loss: 0.7914
Epoch 44/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6424 - val_loss: 0.7914
Epoch 45/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6438 - val_loss: 0.7914
Epoch 46/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6408 - val_loss: 0.7913
Epoch 47/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6400 - val_loss: 0.7913
Epoch 48/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6392 - val_loss: 0.7913
Epoch 49/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6382 - val_loss: 0.7912
Epoch 50/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6374 - val_loss: 0.7912
Epoch 51/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6365 - val_loss: 0.7912
Epoch 52/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6356 - val_loss: 0.7911
Epoch 53/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6347 - val_loss: 0.7911
Epoch 54/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6351 - val_loss: 0.7911
Epoch 55/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6332 - val_loss: 0.7911
Epoch 56/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6329 - val_loss: 0.7910
Epoch 57/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6309 - val_loss: 0.7910
Epoch 58/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6296 - val_loss: 0.7910
Epoch 59/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6309 - val_loss: 0.7911
Epoch 60/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6340 - val_loss: 0.7910
Epoch 61/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6302 - val_loss: 0.7909
Epoch 62/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6290 - val_loss: 0.7908
Epoch 63/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6290 - val_loss: 0.7908
Epoch 64/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6287 - val_loss: 0.7908
Epoch 65/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6270 - val_loss: 0.7908
Epoch 66/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6278 - val_loss: 0.7908
Epoch 67/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6268 - val_loss: 0.7908
Epoch 68/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6279 - val_loss: 0.7907
Execution time:  283.308287858963
GRU:
Mean Absolute Error: 0.6338
Root Mean Square Error: 0.9317
Mean Square Error: 0.8681

Train RMSE: 0.932
Train MSE: 0.868
Train MAE: 0.634
###########################

MODEL:  GRU
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_135&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_51 (GRU)                 (None, 144, 55)           9570      
_________________________________________________________________
dropout_135 (Dropout)        (None, 144, 55)           0         
_________________________________________________________________
time_distributed_135 (TimeDi (None, 144, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 19s 55ms/step - loss: 0.7280 - val_loss: 0.7161
Epoch 2/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6865 - val_loss: 0.7164
Epoch 3/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6942 - val_loss: 0.7147
Epoch 4/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6929 - val_loss: 0.7129
Epoch 5/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6912 - val_loss: 0.7111
Epoch 6/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6899 - val_loss: 0.7095
Epoch 7/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6924 - val_loss: 0.7086
Epoch 8/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6898 - val_loss: 0.7081
Epoch 9/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6830 - val_loss: 0.7073
Epoch 10/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6830 - val_loss: 0.7061
Epoch 11/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6840 - val_loss: 0.7056
Epoch 12/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6814 - val_loss: 0.7048
Epoch 13/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6729 - val_loss: 0.7050
Epoch 14/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6759 - val_loss: 0.7093
Epoch 15/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6744 - val_loss: 0.7064
Epoch 16/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6717 - val_loss: 0.7054
Epoch 17/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6693 - val_loss: 0.7050
Epoch 18/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6723 - val_loss: 0.7053
Epoch 19/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6769 - val_loss: 0.7043
Epoch 20/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6654 - val_loss: 0.7039
Epoch 21/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6659 - val_loss: 0.7036
Epoch 22/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6653 - val_loss: 0.7034
Epoch 23/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6646 - val_loss: 0.7040
Epoch 24/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6670 - val_loss: 0.7035
Epoch 25/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6669 - val_loss: 0.7036
Epoch 26/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6647 - val_loss: 0.7038
Epoch 27/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6613 - val_loss: 0.7028
Epoch 28/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6617 - val_loss: 0.7026
Epoch 29/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6603 - val_loss: 0.7025
Epoch 30/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6603 - val_loss: 0.7024
Epoch 31/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6604 - val_loss: 0.7023
Epoch 32/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6613 - val_loss: 0.7023
Epoch 33/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6622 - val_loss: 0.7019
Epoch 34/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6634 - val_loss: 0.7020
Epoch 35/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6619 - val_loss: 0.7019
Epoch 36/36
342/342 [==============================] - 18s 54ms/step - loss: 0.6615 - val_loss: 0.7019
Execution time:  662.8875730037689
GRU:
Mean Absolute Error: 0.6897
Root Mean Square Error: 0.9872
Mean Square Error: 0.9745

Train RMSE: 0.987
Train MSE: 0.975
Train MAE: 0.690
###########################

MODEL:  GRU
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_136&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_52 (GRU)                 (None, 144, 40)           5160      
_________________________________________________________________
dropout_136 (Dropout)        (None, 144, 40)           0         
_________________________________________________________________
time_distributed_136 (TimeDi (None, 144, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 60ms/step - loss: 0.7071 - val_loss: 0.8241
Epoch 2/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7069 - val_loss: 0.8236
Epoch 3/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7067 - val_loss: 0.8230
Epoch 4/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7065 - val_loss: 0.8225
Epoch 5/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7063 - val_loss: 0.8219
Epoch 6/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7061 - val_loss: 0.8213
Epoch 7/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7058 - val_loss: 0.8206
Epoch 8/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7056 - val_loss: 0.8200
Epoch 9/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7054 - val_loss: 0.8194
Epoch 10/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7052 - val_loss: 0.8187
Epoch 11/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7050 - val_loss: 0.8181
Epoch 12/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7047 - val_loss: 0.8174
Epoch 13/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7045 - val_loss: 0.8168
Epoch 14/68
77/77 [==============================] - 4s 51ms/step - loss: 0.7042 - val_loss: 0.8161
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7041 - val_loss: 0.8154
Epoch 16/68
77/77 [==============================] - 4s 54ms/step - loss: 0.7037 - val_loss: 0.8147
Epoch 17/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7036 - val_loss: 0.8141
Epoch 18/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7034 - val_loss: 0.8134
Epoch 19/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7030 - val_loss: 0.8127
Epoch 20/68
77/77 [==============================] - 4s 54ms/step - loss: 0.7028 - val_loss: 0.8120
Epoch 21/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7027 - val_loss: 0.8113
Epoch 22/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7023 - val_loss: 0.8106
Epoch 23/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7021 - val_loss: 0.8100
Epoch 24/68
77/77 [==============================] - 4s 54ms/step - loss: 0.7018 - val_loss: 0.8093
Epoch 25/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7015 - val_loss: 0.8086
Epoch 26/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7013 - val_loss: 0.8079
Epoch 27/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7011 - val_loss: 0.8072
Epoch 28/68
77/77 [==============================] - 4s 54ms/step - loss: 0.7008 - val_loss: 0.8065
Epoch 29/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7005 - val_loss: 0.8058
Epoch 30/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7003 - val_loss: 0.8051
Epoch 31/68
77/77 [==============================] - 4s 52ms/step - loss: 0.7001 - val_loss: 0.8044
Epoch 32/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6999 - val_loss: 0.8037
Epoch 33/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6996 - val_loss: 0.8030
Epoch 34/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6994 - val_loss: 0.8023
Epoch 35/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6991 - val_loss: 0.8016
Epoch 36/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6989 - val_loss: 0.8009
Epoch 37/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6986 - val_loss: 0.8002
Epoch 38/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6984 - val_loss: 0.7995
Epoch 39/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6980 - val_loss: 0.7988
Epoch 40/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6981 - val_loss: 0.7981
Epoch 41/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6977 - val_loss: 0.7974
Epoch 42/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6974 - val_loss: 0.7967
Epoch 43/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6972 - val_loss: 0.7960
Epoch 44/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6969 - val_loss: 0.7953
Epoch 45/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6967 - val_loss: 0.7946
Epoch 46/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6964 - val_loss: 0.7939
Epoch 47/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6962 - val_loss: 0.7932
Epoch 48/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6959 - val_loss: 0.7925
Epoch 49/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6956 - val_loss: 0.7918
Epoch 50/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6954 - val_loss: 0.7911
Epoch 51/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6952 - val_loss: 0.7904
Epoch 52/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6950 - val_loss: 0.7897
Epoch 53/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6947 - val_loss: 0.7890
Epoch 54/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6944 - val_loss: 0.7883
Epoch 55/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6942 - val_loss: 0.7875
Epoch 56/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6940 - val_loss: 0.7868
Epoch 57/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6937 - val_loss: 0.7861
Epoch 58/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6935 - val_loss: 0.7854
Epoch 59/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6932 - val_loss: 0.7847
Epoch 60/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6930 - val_loss: 0.7840
Epoch 61/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6929 - val_loss: 0.7833
Epoch 62/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6925 - val_loss: 0.7826
Epoch 63/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6923 - val_loss: 0.7819
Epoch 64/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6920 - val_loss: 0.7812
Epoch 65/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6917 - val_loss: 0.7805
Epoch 66/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6915 - val_loss: 0.7798
Epoch 67/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6913 - val_loss: 0.7791
Epoch 68/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6910 - val_loss: 0.7784
Execution time:  283.5206711292267
GRU:
Mean Absolute Error: 0.7071
Root Mean Square Error: 1.0158
Mean Square Error: 1.0318

Train RMSE: 1.016
Train MSE: 1.032
Train MAE: 0.707
###########################

MODEL:  GRU
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_137&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_53 (GRU)                 (None, 144, 55)           9570      
_________________________________________________________________
dropout_137 (Dropout)        (None, 144, 55)           0         
_________________________________________________________________
time_distributed_137 (TimeDi (None, 144, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 18s 53ms/step - loss: 0.7182 - val_loss: 0.7184
Epoch 2/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7170 - val_loss: 0.7162
Epoch 3/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7157 - val_loss: 0.7137
Epoch 4/36
342/342 [==============================] - 18s 51ms/step - loss: 0.7142 - val_loss: 0.7111
Epoch 5/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7128 - val_loss: 0.7084
Epoch 6/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7112 - val_loss: 0.7056
Epoch 7/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7097 - val_loss: 0.7028
Epoch 8/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7081 - val_loss: 0.6999
Epoch 9/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7066 - val_loss: 0.6970
Epoch 10/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7050 - val_loss: 0.6940
Epoch 11/36
342/342 [==============================] - 18s 53ms/step - loss: 0.7034 - val_loss: 0.6910
Epoch 12/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7018 - val_loss: 0.6880
Epoch 13/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7002 - val_loss: 0.6849
Epoch 14/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6985 - val_loss: 0.6819
Epoch 15/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6969 - val_loss: 0.6787
Epoch 16/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6952 - val_loss: 0.6756
Epoch 17/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6935 - val_loss: 0.6724
Epoch 18/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6919 - val_loss: 0.6691
Epoch 19/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6902 - val_loss: 0.6659
Epoch 20/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6885 - val_loss: 0.6627
Epoch 21/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6869 - val_loss: 0.6595
Epoch 22/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6854 - val_loss: 0.6564
Epoch 23/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6838 - val_loss: 0.6535
Epoch 24/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6824 - val_loss: 0.6506
Epoch 25/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6809 - val_loss: 0.6477
Epoch 26/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6795 - val_loss: 0.6449
Epoch 27/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6780 - val_loss: 0.6421
Epoch 28/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6766 - val_loss: 0.6393
Epoch 29/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6753 - val_loss: 0.6365
Epoch 30/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6738 - val_loss: 0.6337
Epoch 31/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6724 - val_loss: 0.6308
Epoch 32/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6710 - val_loss: 0.6279
Epoch 33/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6695 - val_loss: 0.6250
Epoch 34/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6681 - val_loss: 0.6220
Epoch 35/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6666 - val_loss: 0.6191
Epoch 36/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6652 - val_loss: 0.6161
Execution time:  648.0484073162079
GRU:
Mean Absolute Error: 0.6327
Root Mean Square Error: 0.9534
Mean Square Error: 0.9090

Train RMSE: 0.953
Train MSE: 0.909
Train MAE: 0.633
###########################

MODEL:  GRU
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_138&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_54 (GRU)                 (None, 144, 40)           5160      
_________________________________________________________________
dropout_138 (Dropout)        (None, 144, 40)           0         
_________________________________________________________________
time_distributed_138 (TimeDi (None, 144, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 60ms/step - loss: 0.9025 - val_loss: 1.2862
Epoch 2/68
77/77 [==============================] - 4s 54ms/step - loss: 0.9024 - val_loss: 1.2861
Epoch 3/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9024 - val_loss: 1.2860
Epoch 4/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9023 - val_loss: 1.2859
Epoch 5/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9023 - val_loss: 1.2858
Epoch 6/68
77/77 [==============================] - 4s 54ms/step - loss: 0.9022 - val_loss: 1.2856
Epoch 7/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9022 - val_loss: 1.2855
Epoch 8/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9021 - val_loss: 1.2854
Epoch 9/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9021 - val_loss: 1.2852
Epoch 10/68
77/77 [==============================] - 4s 55ms/step - loss: 0.9020 - val_loss: 1.2851
Epoch 11/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9020 - val_loss: 1.2849
Epoch 12/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9019 - val_loss: 1.2848
Epoch 13/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9018 - val_loss: 1.2846
Epoch 14/68
77/77 [==============================] - 4s 54ms/step - loss: 0.9018 - val_loss: 1.2845
Epoch 15/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9017 - val_loss: 1.2843
Epoch 16/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9016 - val_loss: 1.2842
Epoch 17/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9016 - val_loss: 1.2840
Epoch 18/68
77/77 [==============================] - 4s 55ms/step - loss: 0.9015 - val_loss: 1.2839
Epoch 19/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9015 - val_loss: 1.2837
Epoch 20/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9014 - val_loss: 1.2836
Epoch 21/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9013 - val_loss: 1.2834
Epoch 22/68
77/77 [==============================] - 4s 54ms/step - loss: 0.9013 - val_loss: 1.2832
Epoch 23/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9012 - val_loss: 1.2831
Epoch 24/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9012 - val_loss: 1.2829
Epoch 25/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9011 - val_loss: 1.2828
Epoch 26/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9010 - val_loss: 1.2826
Epoch 27/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9009 - val_loss: 1.2824
Epoch 28/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9009 - val_loss: 1.2823
Epoch 29/68
77/77 [==============================] - 4s 54ms/step - loss: 0.9008 - val_loss: 1.2821
Epoch 30/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9007 - val_loss: 1.2819
Epoch 31/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9007 - val_loss: 1.2818
Epoch 32/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9006 - val_loss: 1.2816
Epoch 33/68
77/77 [==============================] - 4s 54ms/step - loss: 0.9006 - val_loss: 1.2814
Epoch 34/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9005 - val_loss: 1.2813
Epoch 35/68
77/77 [==============================] - 4s 51ms/step - loss: 0.9004 - val_loss: 1.2811
Epoch 36/68
77/77 [==============================] - 4s 51ms/step - loss: 0.9003 - val_loss: 1.2809
Epoch 37/68
77/77 [==============================] - 4s 53ms/step - loss: 0.9003 - val_loss: 1.2808
Epoch 38/68
77/77 [==============================] - 4s 51ms/step - loss: 0.9002 - val_loss: 1.2806
Epoch 39/68
77/77 [==============================] - 4s 52ms/step - loss: 0.9001 - val_loss: 1.2804
Epoch 40/68
77/77 [==============================] - 4s 51ms/step - loss: 0.9000 - val_loss: 1.2802
Epoch 41/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8999 - val_loss: 1.2801
Epoch 42/68
77/77 [==============================] - 4s 51ms/step - loss: 0.8999 - val_loss: 1.2799
Epoch 43/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8998 - val_loss: 1.2797
Epoch 44/68
77/77 [==============================] - 4s 51ms/step - loss: 0.8997 - val_loss: 1.2795
Epoch 45/68
77/77 [==============================] - 4s 54ms/step - loss: 0.8997 - val_loss: 1.2794
Epoch 46/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8996 - val_loss: 1.2792
Epoch 47/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8995 - val_loss: 1.2790
Epoch 48/68
77/77 [==============================] - 4s 51ms/step - loss: 0.8994 - val_loss: 1.2788
Epoch 49/68
77/77 [==============================] - 4s 54ms/step - loss: 0.8994 - val_loss: 1.2787
Epoch 50/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8993 - val_loss: 1.2785
Epoch 51/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8992 - val_loss: 1.2783
Epoch 52/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8992 - val_loss: 1.2781
Epoch 53/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8991 - val_loss: 1.2780
Epoch 54/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8990 - val_loss: 1.2778
Epoch 55/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2776
Epoch 56/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8989 - val_loss: 1.2774
Epoch 57/68
77/77 [==============================] - 4s 54ms/step - loss: 0.8988 - val_loss: 1.2772
Epoch 58/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8987 - val_loss: 1.2771
Epoch 59/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8986 - val_loss: 1.2769
Epoch 60/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8985 - val_loss: 1.2767
Epoch 61/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8985 - val_loss: 1.2765
Epoch 62/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8984 - val_loss: 1.2763
Epoch 63/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8983 - val_loss: 1.2762
Epoch 64/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8983 - val_loss: 1.2760
Epoch 65/68
77/77 [==============================] - 4s 53ms/step - loss: 0.8982 - val_loss: 1.2758
Epoch 66/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8981 - val_loss: 1.2756
Epoch 67/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8981 - val_loss: 1.2754
Epoch 68/68
77/77 [==============================] - 4s 54ms/step - loss: 0.8980 - val_loss: 1.2753
Execution time:  283.69112038612366
GRU:
Mean Absolute Error: 0.9187
Root Mean Square Error: 1.1143
Mean Square Error: 1.2417

Train RMSE: 1.114
Train MSE: 1.242
Train MAE: 0.919
###########################

MODEL:  GRU
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_139&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_55 (GRU)                 (None, 144, 55)           9570      
_________________________________________________________________
dropout_139 (Dropout)        (None, 144, 55)           0         
_________________________________________________________________
time_distributed_139 (TimeDi (None, 144, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 19s 54ms/step - loss: 0.8976 - val_loss: 1.1373
Epoch 2/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8973 - val_loss: 1.1367
Epoch 3/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8970 - val_loss: 1.1361
Epoch 4/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8967 - val_loss: 1.1355
Epoch 5/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8963 - val_loss: 1.1348
Epoch 6/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8959 - val_loss: 1.1341
Epoch 7/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8955 - val_loss: 1.1333
Epoch 8/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8951 - val_loss: 1.1326
Epoch 9/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8947 - val_loss: 1.1318
Epoch 10/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8942 - val_loss: 1.1310
Epoch 11/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8938 - val_loss: 1.1302
Epoch 12/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8933 - val_loss: 1.1293
Epoch 13/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8928 - val_loss: 1.1285
Epoch 14/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8924 - val_loss: 1.1276
Epoch 15/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8919 - val_loss: 1.1268
Epoch 16/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8914 - val_loss: 1.1259
Epoch 17/36
342/342 [==============================] - 19s 54ms/step - loss: 0.8909 - val_loss: 1.1250
Epoch 18/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8904 - val_loss: 1.1241
Epoch 19/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8899 - val_loss: 1.1231
Epoch 20/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8894 - val_loss: 1.1222
Epoch 21/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8889 - val_loss: 1.1212
Epoch 22/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8884 - val_loss: 1.1203
Epoch 23/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8878 - val_loss: 1.1193
Epoch 24/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8873 - val_loss: 1.1183
Epoch 25/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8867 - val_loss: 1.1173
Epoch 26/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8862 - val_loss: 1.1163
Epoch 27/36
342/342 [==============================] - 18s 54ms/step - loss: 0.8856 - val_loss: 1.1153
Epoch 28/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8851 - val_loss: 1.1142
Epoch 29/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8845 - val_loss: 1.1132
Epoch 30/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8839 - val_loss: 1.1121
Epoch 31/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8833 - val_loss: 1.1110
Epoch 32/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8827 - val_loss: 1.1099
Epoch 33/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8822 - val_loss: 1.1088
Epoch 34/36
342/342 [==============================] - 18s 54ms/step - loss: 0.8815 - val_loss: 1.1077
Epoch 35/36
342/342 [==============================] - 18s 53ms/step - loss: 0.8809 - val_loss: 1.1066
Epoch 36/36
342/342 [==============================] - 18s 52ms/step - loss: 0.8803 - val_loss: 1.1054
Execution time:  659.218207359314
GRU:
Mean Absolute Error: 0.9098
Root Mean Square Error: 1.1077
Mean Square Error: 1.2270

Train RMSE: 1.108
Train MSE: 1.227
Train MAE: 0.910
###########################

MODEL:  GRU
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_140&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_56 (GRU)                 (None, 144, 40)           5160      
_________________________________________________________________
dropout_140 (Dropout)        (None, 144, 40)           0         
_________________________________________________________________
time_distributed_140 (TimeDi (None, 144, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 60ms/step - loss: 0.6632 - val_loss: 0.5478
Epoch 2/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5915 - val_loss: 0.2371
Epoch 3/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5816 - val_loss: 0.2284
Epoch 4/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5793 - val_loss: 0.2266
Epoch 5/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5777 - val_loss: 0.2249
Epoch 6/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5764 - val_loss: 0.2237
Epoch 7/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5752 - val_loss: 0.2227
Epoch 8/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5745 - val_loss: 0.2225
Epoch 9/68
77/77 [==============================] - 4s 54ms/step - loss: 0.5738 - val_loss: 0.2221
Epoch 10/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5734 - val_loss: 0.2221
Epoch 11/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5725 - val_loss: 0.2221
Epoch 12/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5719 - val_loss: 0.2221
Epoch 13/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5711 - val_loss: 0.2225
Epoch 14/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5705 - val_loss: 0.2222
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5702 - val_loss: 0.2230
Epoch 16/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5696 - val_loss: 0.2237
Epoch 17/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5690 - val_loss: 0.2233
Epoch 18/68
77/77 [==============================] - 4s 53ms/step - loss: 0.5683 - val_loss: 0.2242
Epoch 19/68
77/77 [==============================] - 4s 52ms/step - loss: 0.5678 - val_loss: 0.2246
Execution time:  81.1926691532135
GRU:
Mean Absolute Error: 0.2799
Root Mean Square Error: 0.6787
Mean Square Error: 0.4607

Train RMSE: 0.679
Train MSE: 0.461
Train MAE: 0.280
###########################

MODEL:  GRU
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_141&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_57 (GRU)                 (None, 144, 55)           9570      
_________________________________________________________________
dropout_141 (Dropout)        (None, 144, 55)           0         
_________________________________________________________________
time_distributed_141 (TimeDi (None, 144, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 19s 55ms/step - loss: 0.6136 - val_loss: 0.4010
Epoch 2/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5783 - val_loss: 0.4061
Epoch 3/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5738 - val_loss: 0.4088
Epoch 4/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5711 - val_loss: 0.4100
Epoch 5/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5692 - val_loss: 0.4102
Epoch 6/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5675 - val_loss: 0.4102
Epoch 7/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5655 - val_loss: 0.4108
Epoch 8/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5646 - val_loss: 0.4105
Epoch 9/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5628 - val_loss: 0.4109
Epoch 10/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5618 - val_loss: 0.4097
Epoch 11/36
342/342 [==============================] - 18s 53ms/step - loss: 0.5600 - val_loss: 0.4089
Execution time:  204.0517075061798
GRU:
Mean Absolute Error: 0.2568
Root Mean Square Error: 0.6574
Mean Square Error: 0.4322

Train RMSE: 0.657
Train MSE: 0.432
Train MAE: 0.257
###########################

MODEL:  GRU
sequence:  1d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_142&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_58 (GRU)                 (None, 144, 40)           5160      
_________________________________________________________________
dropout_142 (Dropout)        (None, 144, 40)           0         
_________________________________________________________________
time_distributed_142 (TimeDi (None, 144, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
77/77 [==============================] - 5s 63ms/step - loss: 0.8919 - val_loss: 1.2114
Epoch 2/68
77/77 [==============================] - 4s 52ms/step - loss: 0.8036 - val_loss: 0.8898
Epoch 3/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7302 - val_loss: 0.8500
Epoch 4/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7150 - val_loss: 0.8341
Epoch 5/68
77/77 [==============================] - 4s 54ms/step - loss: 0.7066 - val_loss: 0.8243
Epoch 6/68
77/77 [==============================] - 4s 53ms/step - loss: 0.7010 - val_loss: 0.8171
Epoch 7/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6968 - val_loss: 0.8120
Epoch 8/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6939 - val_loss: 0.8085
Epoch 9/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6916 - val_loss: 0.8059
Epoch 10/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6899 - val_loss: 0.8039
Epoch 11/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6884 - val_loss: 0.8025
Epoch 12/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6871 - val_loss: 0.8013
Epoch 13/68
77/77 [==============================] - 4s 55ms/step - loss: 0.6860 - val_loss: 0.8003
Epoch 14/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6849 - val_loss: 0.7996
Epoch 15/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6840 - val_loss: 0.7989
Epoch 16/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6831 - val_loss: 0.7984
Epoch 17/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6822 - val_loss: 0.7979
Epoch 18/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6813 - val_loss: 0.7974
Epoch 19/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6805 - val_loss: 0.7970
Epoch 20/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6798 - val_loss: 0.7967
Epoch 21/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6789 - val_loss: 0.7963
Epoch 22/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6782 - val_loss: 0.7960
Epoch 23/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6775 - val_loss: 0.7957
Epoch 24/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6768 - val_loss: 0.7954
Epoch 25/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6761 - val_loss: 0.7952
Epoch 26/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6755 - val_loss: 0.7950
Epoch 27/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6748 - val_loss: 0.7948
Epoch 28/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6742 - val_loss: 0.7946
Epoch 29/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6736 - val_loss: 0.7944
Epoch 30/68
77/77 [==============================] - 4s 51ms/step - loss: 0.6730 - val_loss: 0.7942
Epoch 31/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6724 - val_loss: 0.7941
Epoch 32/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6719 - val_loss: 0.7939
Epoch 33/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6712 - val_loss: 0.7938
Epoch 34/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6706 - val_loss: 0.7936
Epoch 35/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6701 - val_loss: 0.7935
Epoch 36/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6695 - val_loss: 0.7934
Epoch 37/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6691 - val_loss: 0.7933
Epoch 38/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6685 - val_loss: 0.7932
Epoch 39/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6680 - val_loss: 0.7931
Epoch 40/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6675 - val_loss: 0.7930
Epoch 41/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6669 - val_loss: 0.7929
Epoch 42/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6664 - val_loss: 0.7928
Epoch 43/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6660 - val_loss: 0.7928
Epoch 44/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6655 - val_loss: 0.7927
Epoch 45/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6649 - val_loss: 0.7926
Epoch 46/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6646 - val_loss: 0.7925
Epoch 47/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6641 - val_loss: 0.7924
Epoch 48/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6635 - val_loss: 0.7923
Epoch 49/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6630 - val_loss: 0.7923
Epoch 50/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6625 - val_loss: 0.7922
Epoch 51/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6620 - val_loss: 0.7921
Epoch 52/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6614 - val_loss: 0.7920
Epoch 53/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6609 - val_loss: 0.7920
Epoch 54/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6603 - val_loss: 0.7919
Epoch 55/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6595 - val_loss: 0.7919
Epoch 56/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6589 - val_loss: 0.7918
Epoch 57/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6582 - val_loss: 0.7917
Epoch 58/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6574 - val_loss: 0.7917
Epoch 59/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6567 - val_loss: 0.7917
Epoch 60/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6560 - val_loss: 0.7916
Epoch 61/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6552 - val_loss: 0.7916
Epoch 62/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6540 - val_loss: 0.7916
Epoch 63/68
77/77 [==============================] - 4s 52ms/step - loss: 0.6505 - val_loss: 0.7916
Epoch 64/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6501 - val_loss: 0.7916
Epoch 65/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6499 - val_loss: 0.7916
Epoch 66/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6489 - val_loss: 0.7916
Epoch 67/68
77/77 [==============================] - 4s 53ms/step - loss: 0.6476 - val_loss: 0.7916
Epoch 68/68
77/77 [==============================] - 4s 54ms/step - loss: 0.6474 - val_loss: 0.7916
Execution time:  283.4575936794281
GRU:
Mean Absolute Error: 0.6195
Root Mean Square Error: 0.9082
Mean Square Error: 0.8248

Train RMSE: 0.908
Train MSE: 0.825
Train MAE: 0.619
###########################

MODEL:  GRU
sequence:  1d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_143&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_59 (GRU)                 (None, 144, 55)           9570      
_________________________________________________________________
dropout_143 (Dropout)        (None, 144, 55)           0         
_________________________________________________________________
time_distributed_143 (TimeDi (None, 144, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
342/342 [==============================] - 19s 54ms/step - loss: 0.7670 - val_loss: 0.7488
Epoch 2/36
342/342 [==============================] - 18s 53ms/step - loss: 0.7253 - val_loss: 0.7443
Epoch 3/36
342/342 [==============================] - 18s 52ms/step - loss: 0.7198 - val_loss: 0.7423
Epoch 4/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6994 - val_loss: 0.7455
Epoch 5/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6853 - val_loss: 0.7429
Epoch 6/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6837 - val_loss: 0.7403
Epoch 7/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6823 - val_loss: 0.7377
Epoch 8/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6803 - val_loss: 0.7351
Epoch 9/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6780 - val_loss: 0.7323
Epoch 10/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6767 - val_loss: 0.7301
Epoch 11/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6755 - val_loss: 0.7283
Epoch 12/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6740 - val_loss: 0.7266
Epoch 13/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6725 - val_loss: 0.7250
Epoch 14/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6710 - val_loss: 0.7243
Epoch 15/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6701 - val_loss: 0.7224
Epoch 16/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6684 - val_loss: 0.7218
Epoch 17/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6678 - val_loss: 0.7199
Epoch 18/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6668 - val_loss: 0.7195
Epoch 19/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6662 - val_loss: 0.7178
Epoch 20/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6649 - val_loss: 0.7177
Epoch 21/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6634 - val_loss: 0.7168
Epoch 22/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6630 - val_loss: 0.7157
Epoch 23/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6619 - val_loss: 0.7154
Epoch 24/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6608 - val_loss: 0.7144
Epoch 25/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6600 - val_loss: 0.7137
Epoch 26/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6590 - val_loss: 0.7132
Epoch 27/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6599 - val_loss: 0.7126
Epoch 28/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6576 - val_loss: 0.7121
Epoch 29/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6550 - val_loss: 0.7117
Epoch 30/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6552 - val_loss: 0.7112
Epoch 31/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6542 - val_loss: 0.7104
Epoch 32/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6542 - val_loss: 0.7103
Epoch 33/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6518 - val_loss: 0.7096
Epoch 34/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6529 - val_loss: 0.7089
Epoch 35/36
342/342 [==============================] - 18s 53ms/step - loss: 0.6538 - val_loss: 0.7093
Epoch 36/36
342/342 [==============================] - 18s 52ms/step - loss: 0.6499 - val_loss: 0.7085
Execution time:  651.64852643013
GRU:
Mean Absolute Error: 0.6304
Root Mean Square Error: 0.9240
Mean Square Error: 0.8537

Train RMSE: 0.924
Train MSE: 0.854
Train MAE: 0.630
###########################

MODEL:  GRU
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_144&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_60 (GRU)                 (None, 432, 40)           5160      
_________________________________________________________________
dropout_144 (Dropout)        (None, 432, 40)           0         
_________________________________________________________________
time_distributed_144 (TimeDi (None, 432, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 12s 168ms/step - loss: 0.6512 - val_loss: 0.5451
Epoch 2/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6206 - val_loss: 0.3505
Epoch 3/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6209 - val_loss: 0.3727
Epoch 4/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6154 - val_loss: 0.3703
Epoch 5/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6133 - val_loss: 0.3711
Epoch 6/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6117 - val_loss: 0.3726
Epoch 7/68
72/72 [==============================] - 12s 174ms/step - loss: 0.6101 - val_loss: 0.3737
Epoch 8/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6087 - val_loss: 0.3747
Epoch 9/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6074 - val_loss: 0.3775
Epoch 10/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6059 - val_loss: 0.3813
Epoch 11/68
72/72 [==============================] - 12s 170ms/step - loss: 0.6043 - val_loss: 0.3887
Epoch 12/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6014 - val_loss: 0.4051
Execution time:  153.90134501457214
GRU:
Mean Absolute Error: 0.5390
Root Mean Square Error: 0.9866
Mean Square Error: 0.9734

Train RMSE: 0.987
Train MSE: 0.973
Train MAE: 0.539
###########################

MODEL:  GRU
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_145&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_61 (GRU)                 (None, 432, 55)           9570      
_________________________________________________________________
dropout_145 (Dropout)        (None, 432, 55)           0         
_________________________________________________________________
time_distributed_145 (TimeDi (None, 432, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 46s 145ms/step - loss: 0.6645 - val_loss: 0.3214
Epoch 2/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6557 - val_loss: 0.3085
Epoch 3/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6437 - val_loss: 0.3834
Epoch 4/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6358 - val_loss: 0.3631
Epoch 5/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6322 - val_loss: 0.3620
Epoch 6/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6296 - val_loss: 0.3304
Epoch 7/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6270 - val_loss: 0.3210
Epoch 8/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6406 - val_loss: 0.3264
Epoch 9/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6259 - val_loss: 0.3212
Epoch 10/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6299 - val_loss: 0.3629
Epoch 11/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6248 - val_loss: 0.3792
Epoch 12/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6221 - val_loss: 0.3719
Execution time:  547.9326717853546
GRU:
Mean Absolute Error: 0.5462
Root Mean Square Error: 0.9696
Mean Square Error: 0.9401

Train RMSE: 0.970
Train MSE: 0.940
Train MAE: 0.546
###########################

MODEL:  GRU
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_146&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_62 (GRU)                 (None, 432, 40)           5160      
_________________________________________________________________
dropout_146 (Dropout)        (None, 432, 40)           0         
_________________________________________________________________
time_distributed_146 (TimeDi (None, 432, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 13s 177ms/step - loss: 0.8856 - val_loss: 1.1049
Epoch 2/68
72/72 [==============================] - 13s 185ms/step - loss: 0.7238 - val_loss: 0.8069
Epoch 3/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6774 - val_loss: 0.7934
Epoch 4/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6745 - val_loss: 0.7889
Epoch 5/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6732 - val_loss: 0.7866
Epoch 6/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6724 - val_loss: 0.7851
Epoch 7/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6719 - val_loss: 0.7841
Epoch 8/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6716 - val_loss: 0.7835
Epoch 9/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6713 - val_loss: 0.7829
Epoch 10/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6711 - val_loss: 0.7825
Epoch 11/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6709 - val_loss: 0.7822
Epoch 12/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6708 - val_loss: 0.7820
Epoch 13/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6707 - val_loss: 0.7818
Epoch 14/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6706 - val_loss: 0.7816
Epoch 15/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6705 - val_loss: 0.7814
Epoch 16/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6704 - val_loss: 0.7813
Epoch 17/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6704 - val_loss: 0.7812
Epoch 18/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7811
Epoch 19/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6703 - val_loss: 0.7810
Epoch 20/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6702 - val_loss: 0.7809
Epoch 21/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6702 - val_loss: 0.7808
Epoch 22/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6702 - val_loss: 0.7808
Epoch 23/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 24/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6701 - val_loss: 0.7806
Epoch 25/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6701 - val_loss: 0.7806
Epoch 26/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 27/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 28/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6700 - val_loss: 0.7804
Epoch 29/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6700 - val_loss: 0.7804
Epoch 30/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6699 - val_loss: 0.7804
Epoch 31/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 32/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 33/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 34/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 35/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 36/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 37/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7802
Epoch 38/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 39/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 40/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 41/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 42/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 43/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 44/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 45/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 46/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 47/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 48/68
72/72 [==============================] - 13s 178ms/step - loss: 0.6697 - val_loss: 0.7800
Epoch 49/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 50/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 51/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 52/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 53/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 54/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 55/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 56/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 57/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 58/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 59/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 60/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 61/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 62/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 63/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 64/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 65/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 66/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 67/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6696 - val_loss: 0.7797
Epoch 68/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6696 - val_loss: 0.7797
Execution time:  908.5840327739716
GRU:
Mean Absolute Error: 0.6862
Root Mean Square Error: 0.9968
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  GRU
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_147&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_63 (GRU)                 (None, 432, 55)           9570      
_________________________________________________________________
dropout_147 (Dropout)        (None, 432, 55)           0         
_________________________________________________________________
time_distributed_147 (TimeDi (None, 432, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 46s 144ms/step - loss: 0.7748 - val_loss: 0.6152
Epoch 2/36
317/317 [==============================] - 45s 143ms/step - loss: 0.7075 - val_loss: 0.6102
Epoch 3/36
317/317 [==============================] - 46s 144ms/step - loss: 0.6997 - val_loss: 0.6092
Epoch 4/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6995 - val_loss: 0.6089
Epoch 5/36
317/317 [==============================] - 51s 160ms/step - loss: 0.6994 - val_loss: 0.6086
Epoch 6/36
317/317 [==============================] - 49s 156ms/step - loss: 0.6993 - val_loss: 0.6085
Epoch 7/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6992 - val_loss: 0.6084
Epoch 8/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6992 - val_loss: 0.6083
Epoch 9/36
317/317 [==============================] - 45s 141ms/step - loss: 0.6991 - val_loss: 0.6082
Epoch 10/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6991 - val_loss: 0.6081
Epoch 11/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6990 - val_loss: 0.6080
Epoch 12/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6990 - val_loss: 0.6080
Epoch 13/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6990 - val_loss: 0.6079
Epoch 14/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6990 - val_loss: 0.6079
Epoch 15/36
317/317 [==============================] - 45s 141ms/step - loss: 0.6990 - val_loss: 0.6078
Epoch 16/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6078
Epoch 17/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 18/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 19/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 20/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 21/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 22/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 23/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 24/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 25/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 26/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 27/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6075
Epoch 28/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 29/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 30/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 31/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 32/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 33/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 34/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 35/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 36/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Execution time:  1643.392021894455
GRU:
Mean Absolute Error: 0.6862
Root Mean Square Error: 0.9968
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  GRU
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_148&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_64 (GRU)                 (None, 432, 40)           5160      
_________________________________________________________________
dropout_148 (Dropout)        (None, 432, 40)           0         
_________________________________________________________________
time_distributed_148 (TimeDi (None, 432, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6778 - val_loss: 0.8169
Epoch 2/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6778 - val_loss: 0.8166
Epoch 3/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6776 - val_loss: 0.8163
Epoch 4/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6775 - val_loss: 0.8160
Epoch 5/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6775 - val_loss: 0.8157
Epoch 6/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6773 - val_loss: 0.8153
Epoch 7/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6773 - val_loss: 0.8150
Epoch 8/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6771 - val_loss: 0.8146
Epoch 9/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6770 - val_loss: 0.8143
Epoch 10/68
72/72 [==============================] - 13s 187ms/step - loss: 0.6769 - val_loss: 0.8139
Epoch 11/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6768 - val_loss: 0.8135
Epoch 12/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6767 - val_loss: 0.8131
Epoch 13/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6766 - val_loss: 0.8128
Epoch 14/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6765 - val_loss: 0.8124
Epoch 15/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6764 - val_loss: 0.8120
Epoch 16/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6762 - val_loss: 0.8116
Epoch 17/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6761 - val_loss: 0.8112
Epoch 18/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6759 - val_loss: 0.8108
Epoch 19/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6758 - val_loss: 0.8104
Epoch 20/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6757 - val_loss: 0.8100
Epoch 21/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6756 - val_loss: 0.8096
Epoch 22/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6755 - val_loss: 0.8092
Epoch 23/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6753 - val_loss: 0.8088
Epoch 24/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6753 - val_loss: 0.8084
Epoch 25/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6751 - val_loss: 0.8080
Epoch 26/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6750 - val_loss: 0.8076
Epoch 27/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6748 - val_loss: 0.8072
Epoch 28/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6747 - val_loss: 0.8068
Epoch 29/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6746 - val_loss: 0.8063
Epoch 30/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6744 - val_loss: 0.8059
Epoch 31/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6744 - val_loss: 0.8055
Epoch 32/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6742 - val_loss: 0.8051
Epoch 33/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6740 - val_loss: 0.8047
Epoch 34/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6740 - val_loss: 0.8042
Epoch 35/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6738 - val_loss: 0.8038
Epoch 36/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6737 - val_loss: 0.8034
Epoch 37/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6735 - val_loss: 0.8030
Epoch 38/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6735 - val_loss: 0.8025
Epoch 39/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6733 - val_loss: 0.8021
Epoch 40/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6731 - val_loss: 0.8017
Epoch 41/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6730 - val_loss: 0.8012
Epoch 42/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6729 - val_loss: 0.8008
Epoch 43/68
72/72 [==============================] - 13s 180ms/step - loss: 0.6727 - val_loss: 0.8004
Epoch 44/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6726 - val_loss: 0.7999
Epoch 45/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6724 - val_loss: 0.7995
Epoch 46/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6723 - val_loss: 0.7991
Epoch 47/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6722 - val_loss: 0.7986
Epoch 48/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6720 - val_loss: 0.7982
Epoch 49/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6719 - val_loss: 0.7978
Epoch 50/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6718 - val_loss: 0.7973
Epoch 51/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6717 - val_loss: 0.7969
Epoch 52/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6715 - val_loss: 0.7964
Epoch 53/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6714 - val_loss: 0.7960
Epoch 54/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6712 - val_loss: 0.7956
Epoch 55/68
72/72 [==============================] - 14s 188ms/step - loss: 0.6711 - val_loss: 0.7951
Epoch 56/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6710 - val_loss: 0.7947
Epoch 57/68
72/72 [==============================] - 13s 182ms/step - loss: 0.6708 - val_loss: 0.7942
Epoch 58/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6707 - val_loss: 0.7938
Epoch 59/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6706 - val_loss: 0.7933
Epoch 60/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6704 - val_loss: 0.7929
Epoch 61/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6703 - val_loss: 0.7925
Epoch 62/68
72/72 [==============================] - 13s 187ms/step - loss: 0.6701 - val_loss: 0.7920
Epoch 63/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6701 - val_loss: 0.7916
Epoch 64/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6699 - val_loss: 0.7911
Epoch 65/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6698 - val_loss: 0.7907
Epoch 66/68
72/72 [==============================] - 13s 186ms/step - loss: 0.6696 - val_loss: 0.7902
Epoch 67/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6694 - val_loss: 0.7898
Epoch 68/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6693 - val_loss: 0.7893
Execution time:  911.485524892807
GRU:
Mean Absolute Error: 0.7125
Root Mean Square Error: 1.0362
Mean Square Error: 1.0736

Train RMSE: 1.036
Train MSE: 1.074
Train MAE: 0.713
###########################

MODEL:  GRU
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_149&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_65 (GRU)                 (None, 432, 55)           9570      
_________________________________________________________________
dropout_149 (Dropout)        (None, 432, 55)           0         
_________________________________________________________________
time_distributed_149 (TimeDi (None, 432, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6995 - val_loss: 0.6094
Epoch 2/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6991 - val_loss: 0.6081
Epoch 3/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6987 - val_loss: 0.6066
Epoch 4/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6982 - val_loss: 0.6049
Epoch 5/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6977 - val_loss: 0.6033
Epoch 6/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6971 - val_loss: 0.6015
Epoch 7/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6966 - val_loss: 0.5997
Epoch 8/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6960 - val_loss: 0.5978
Epoch 9/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6954 - val_loss: 0.5959
Epoch 10/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6948 - val_loss: 0.5939
Epoch 11/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6942 - val_loss: 0.5919
Epoch 12/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6935 - val_loss: 0.5898
Epoch 13/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6929 - val_loss: 0.5877
Epoch 14/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6923 - val_loss: 0.5856
Epoch 15/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6916 - val_loss: 0.5835
Epoch 16/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6909 - val_loss: 0.5813
Epoch 17/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6903 - val_loss: 0.5791
Epoch 18/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6895 - val_loss: 0.5769
Epoch 19/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6889 - val_loss: 0.5747
Epoch 20/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6882 - val_loss: 0.5725
Epoch 21/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6875 - val_loss: 0.5703
Epoch 22/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6869 - val_loss: 0.5681
Epoch 23/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6863 - val_loss: 0.5659
Epoch 24/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6856 - val_loss: 0.5639
Epoch 25/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6850 - val_loss: 0.5620
Epoch 26/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6844 - val_loss: 0.5601
Epoch 27/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6839 - val_loss: 0.5583
Epoch 28/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6834 - val_loss: 0.5565
Epoch 29/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6829 - val_loss: 0.5547
Epoch 30/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6824 - val_loss: 0.5530
Epoch 31/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6819 - val_loss: 0.5513
Epoch 32/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6814 - val_loss: 0.5495
Epoch 33/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6809 - val_loss: 0.5478
Epoch 34/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6805 - val_loss: 0.5461
Epoch 35/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6800 - val_loss: 0.5444
Epoch 36/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6795 - val_loss: 0.5427
Execution time:  1631.1546838283539
GRU:
Mean Absolute Error: 0.6410
Root Mean Square Error: 0.9688
Mean Square Error: 0.9385

Train RMSE: 0.969
Train MSE: 0.938
Train MAE: 0.641
###########################

MODEL:  GRU
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_150&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_66 (GRU)                 (None, 432, 40)           5160      
_________________________________________________________________
dropout_150 (Dropout)        (None, 432, 40)           0         
_________________________________________________________________
time_distributed_150 (TimeDi (None, 432, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 12s 173ms/step - loss: 0.9007 - val_loss: 1.2721
Epoch 2/68
72/72 [==============================] - 13s 184ms/step - loss: 0.9006 - val_loss: 1.2720
Epoch 3/68
72/72 [==============================] - 13s 182ms/step - loss: 0.9006 - val_loss: 1.2719
Epoch 4/68
72/72 [==============================] - 13s 180ms/step - loss: 0.9006 - val_loss: 1.2718
Epoch 5/68
72/72 [==============================] - 13s 182ms/step - loss: 0.9005 - val_loss: 1.2718
Epoch 6/68
72/72 [==============================] - 13s 181ms/step - loss: 0.9005 - val_loss: 1.2717
Epoch 7/68
72/72 [==============================] - 13s 182ms/step - loss: 0.9004 - val_loss: 1.2716
Epoch 8/68
72/72 [==============================] - 13s 182ms/step - loss: 0.9004 - val_loss: 1.2715
Epoch 9/68
72/72 [==============================] - 13s 181ms/step - loss: 0.9004 - val_loss: 1.2714
Epoch 10/68
72/72 [==============================] - 13s 178ms/step - loss: 0.9003 - val_loss: 1.2712
Epoch 11/68
72/72 [==============================] - 13s 183ms/step - loss: 0.9003 - val_loss: 1.2711
Epoch 12/68
72/72 [==============================] - 13s 183ms/step - loss: 0.9002 - val_loss: 1.2710
Epoch 13/68
72/72 [==============================] - 13s 182ms/step - loss: 0.9002 - val_loss: 1.2709
Epoch 14/68
72/72 [==============================] - 13s 181ms/step - loss: 0.9001 - val_loss: 1.2708
Epoch 15/68
72/72 [==============================] - 13s 182ms/step - loss: 0.9000 - val_loss: 1.2707
Epoch 16/68
72/72 [==============================] - 13s 179ms/step - loss: 0.9000 - val_loss: 1.2706
Epoch 17/68
72/72 [==============================] - 13s 181ms/step - loss: 0.9000 - val_loss: 1.2705
Epoch 18/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8999 - val_loss: 1.2703
Epoch 19/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8998 - val_loss: 1.2702
Epoch 20/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8998 - val_loss: 1.2701
Epoch 21/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8997 - val_loss: 1.2700
Epoch 22/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8997 - val_loss: 1.2699
Epoch 23/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8996 - val_loss: 1.2697
Epoch 24/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8996 - val_loss: 1.2696
Epoch 25/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8995 - val_loss: 1.2695
Epoch 26/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8994 - val_loss: 1.2693
Epoch 27/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8994 - val_loss: 1.2692
Epoch 28/68
72/72 [==============================] - 13s 178ms/step - loss: 0.8993 - val_loss: 1.2691
Epoch 29/68
72/72 [==============================] - 13s 180ms/step - loss: 0.8993 - val_loss: 1.2690
Epoch 30/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8992 - val_loss: 1.2688
Epoch 31/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8992 - val_loss: 1.2687
Epoch 32/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8991 - val_loss: 1.2686
Epoch 33/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8990 - val_loss: 1.2684
Epoch 34/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8990 - val_loss: 1.2683
Epoch 35/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8989 - val_loss: 1.2682
Epoch 36/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8988 - val_loss: 1.2680
Epoch 37/68
72/72 [==============================] - 13s 180ms/step - loss: 0.8988 - val_loss: 1.2679
Epoch 38/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8987 - val_loss: 1.2678
Epoch 39/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8987 - val_loss: 1.2676
Epoch 40/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8986 - val_loss: 1.2675
Epoch 41/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8985 - val_loss: 1.2673
Epoch 42/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8985 - val_loss: 1.2672
Epoch 43/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8984 - val_loss: 1.2671
Epoch 44/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8983 - val_loss: 1.2669
Epoch 45/68
72/72 [==============================] - 13s 180ms/step - loss: 0.8983 - val_loss: 1.2668
Epoch 46/68
72/72 [==============================] - 13s 180ms/step - loss: 0.8982 - val_loss: 1.2666
Epoch 47/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8981 - val_loss: 1.2665
Epoch 48/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8980 - val_loss: 1.2664
Epoch 49/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8980 - val_loss: 1.2662
Epoch 50/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8979 - val_loss: 1.2661
Epoch 51/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8979 - val_loss: 1.2659
Epoch 52/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8978 - val_loss: 1.2658
Epoch 53/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8977 - val_loss: 1.2656
Epoch 54/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8977 - val_loss: 1.2655
Epoch 55/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8976 - val_loss: 1.2653
Epoch 56/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8975 - val_loss: 1.2652
Epoch 57/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8974 - val_loss: 1.2651
Epoch 58/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8974 - val_loss: 1.2649
Epoch 59/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8973 - val_loss: 1.2648
Epoch 60/68
72/72 [==============================] - 13s 181ms/step - loss: 0.8972 - val_loss: 1.2646
Epoch 61/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8972 - val_loss: 1.2645
Epoch 62/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8971 - val_loss: 1.2643
Epoch 63/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8970 - val_loss: 1.2642
Epoch 64/68
72/72 [==============================] - 13s 180ms/step - loss: 0.8970 - val_loss: 1.2640
Epoch 65/68
72/72 [==============================] - 13s 183ms/step - loss: 0.8969 - val_loss: 1.2639
Epoch 66/68
72/72 [==============================] - 13s 185ms/step - loss: 0.8968 - val_loss: 1.2637
Epoch 67/68
72/72 [==============================] - 13s 184ms/step - loss: 0.8968 - val_loss: 1.2636
Epoch 68/68
72/72 [==============================] - 13s 182ms/step - loss: 0.8967 - val_loss: 1.2634
Execution time:  905.6367120742798
GRU:
Mean Absolute Error: 0.9031
Root Mean Square Error: 1.1018
Mean Square Error: 1.2140

Train RMSE: 1.102
Train MSE: 1.214
Train MAE: 0.903
###########################

MODEL:  GRU
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_151&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_67 (GRU)                 (None, 432, 55)           9570      
_________________________________________________________________
dropout_151 (Dropout)        (None, 432, 55)           0         
_________________________________________________________________
time_distributed_151 (TimeDi (None, 432, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 48s 152ms/step - loss: 0.9021 - val_loss: 1.0917
Epoch 2/36
317/317 [==============================] - 45s 143ms/step - loss: 0.9020 - val_loss: 1.0913
Epoch 3/36
317/317 [==============================] - 45s 142ms/step - loss: 0.9018 - val_loss: 1.0908
Epoch 4/36
317/317 [==============================] - 45s 144ms/step - loss: 0.9015 - val_loss: 1.0903
Epoch 5/36
317/317 [==============================] - 45s 143ms/step - loss: 0.9013 - val_loss: 1.0898
Epoch 6/36
317/317 [==============================] - 45s 143ms/step - loss: 0.9010 - val_loss: 1.0892
Epoch 7/36
317/317 [==============================] - 46s 144ms/step - loss: 0.9008 - val_loss: 1.0886
Epoch 8/36
317/317 [==============================] - 45s 143ms/step - loss: 0.9005 - val_loss: 1.0880
Epoch 9/36
317/317 [==============================] - 45s 143ms/step - loss: 0.9002 - val_loss: 1.0873
Epoch 10/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8999 - val_loss: 1.0867
Epoch 11/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8996 - val_loss: 1.0860
Epoch 12/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8992 - val_loss: 1.0853
Epoch 13/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8989 - val_loss: 1.0845
Epoch 14/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8985 - val_loss: 1.0838
Epoch 15/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8982 - val_loss: 1.0831
Epoch 16/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8978 - val_loss: 1.0823
Epoch 17/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8975 - val_loss: 1.0815
Epoch 18/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8971 - val_loss: 1.0807
Epoch 19/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8967 - val_loss: 1.0799
Epoch 20/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8963 - val_loss: 1.0790
Epoch 21/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8959 - val_loss: 1.0782
Epoch 22/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8955 - val_loss: 1.0773
Epoch 23/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8951 - val_loss: 1.0765
Epoch 24/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8946 - val_loss: 1.0756
Epoch 25/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8942 - val_loss: 1.0747
Epoch 26/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8938 - val_loss: 1.0737
Epoch 27/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8933 - val_loss: 1.0728
Epoch 28/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8929 - val_loss: 1.0718
Epoch 29/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8924 - val_loss: 1.0709
Epoch 30/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8919 - val_loss: 1.0699
Epoch 31/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8915 - val_loss: 1.0689
Epoch 32/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8910 - val_loss: 1.0678
Epoch 33/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8905 - val_loss: 1.0668
Epoch 34/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8900 - val_loss: 1.0657
Epoch 35/36
317/317 [==============================] - 45s 143ms/step - loss: 0.8895 - val_loss: 1.0647
Epoch 36/36
317/317 [==============================] - 46s 144ms/step - loss: 0.8889 - val_loss: 1.0636
Execution time:  1647.4934480190277
GRU:
Mean Absolute Error: 0.9025
Root Mean Square Error: 1.1050
Mean Square Error: 1.2209

Train RMSE: 1.105
Train MSE: 1.221
Train MAE: 0.903
###########################

MODEL:  GRU
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_152&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_68 (GRU)                 (None, 432, 40)           5160      
_________________________________________________________________
dropout_152 (Dropout)        (None, 432, 40)           0         
_________________________________________________________________
time_distributed_152 (TimeDi (None, 432, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6662 - val_loss: 0.7085
Epoch 2/68
72/72 [==============================] - 13s 183ms/step - loss: 0.6291 - val_loss: 0.5535
Epoch 3/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6074 - val_loss: 0.4236
Epoch 4/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6085 - val_loss: 0.4031
Epoch 5/68
72/72 [==============================] - 13s 184ms/step - loss: 0.6089 - val_loss: 0.4006
Epoch 6/68
72/72 [==============================] - 13s 185ms/step - loss: 0.6085 - val_loss: 0.4003
Epoch 7/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6080 - val_loss: 0.4003
Epoch 8/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6076 - val_loss: 0.4002
Epoch 9/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6073 - val_loss: 0.3999
Epoch 10/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6069 - val_loss: 0.3996
Epoch 11/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6066 - val_loss: 0.3993
Epoch 12/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6062 - val_loss: 0.3988
Epoch 13/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6059 - val_loss: 0.3982
Epoch 14/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6056 - val_loss: 0.3975
Epoch 15/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6052 - val_loss: 0.3966
Epoch 16/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6049 - val_loss: 0.3959
Epoch 17/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6047 - val_loss: 0.3952
Epoch 18/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6044 - val_loss: 0.3946
Epoch 19/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6041 - val_loss: 0.3938
Epoch 20/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6039 - val_loss: 0.3930
Epoch 21/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6034 - val_loss: 0.3920
Epoch 22/68
72/72 [==============================] - 13s 181ms/step - loss: 0.6032 - val_loss: 0.3913
Epoch 23/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6027 - val_loss: 0.3904
Epoch 24/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6025 - val_loss: 0.3897
Epoch 25/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6022 - val_loss: 0.3888
Epoch 26/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6019 - val_loss: 0.3880
Epoch 27/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6016 - val_loss: 0.3874
Epoch 28/68
72/72 [==============================] - 12s 171ms/step - loss: 0.6013 - val_loss: 0.3870
Epoch 29/68
72/72 [==============================] - 12s 170ms/step - loss: 0.6011 - val_loss: 0.3867
Epoch 30/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6008 - val_loss: 0.3867
Epoch 31/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6006 - val_loss: 0.3865
Epoch 32/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6004 - val_loss: 0.3868
Epoch 33/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6000 - val_loss: 0.3871
Epoch 34/68
72/72 [==============================] - 13s 175ms/step - loss: 0.5998 - val_loss: 0.3877
Epoch 35/68
72/72 [==============================] - 13s 174ms/step - loss: 0.5995 - val_loss: 0.3887
Epoch 36/68
72/72 [==============================] - 12s 172ms/step - loss: 0.5992 - val_loss: 0.3899
Epoch 37/68
72/72 [==============================] - 12s 173ms/step - loss: 0.5989 - val_loss: 0.3918
Epoch 38/68
72/72 [==============================] - 13s 179ms/step - loss: 0.5985 - val_loss: 0.3940
Epoch 39/68
72/72 [==============================] - 12s 172ms/step - loss: 0.5981 - val_loss: 0.3969
Epoch 40/68
72/72 [==============================] - 12s 172ms/step - loss: 0.5975 - val_loss: 0.4005
Epoch 41/68
72/72 [==============================] - 13s 174ms/step - loss: 0.5968 - val_loss: 0.4052
Execution time:  529.8658058643341
GRU:
Mean Absolute Error: 0.5398
Root Mean Square Error: 0.9892
Mean Square Error: 0.9786

Train RMSE: 0.989
Train MSE: 0.979
Train MAE: 0.540
###########################

MODEL:  GRU
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_153&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_69 (GRU)                 (None, 432, 55)           9570      
_________________________________________________________________
dropout_153 (Dropout)        (None, 432, 55)           0         
_________________________________________________________________
time_distributed_153 (TimeDi (None, 432, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 46s 145ms/step - loss: 0.6673 - val_loss: 0.2950
Epoch 2/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6537 - val_loss: 0.2861
Epoch 3/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6516 - val_loss: 0.2802
Epoch 4/36
317/317 [==============================] - 46s 145ms/step - loss: 0.6520 - val_loss: 0.2769
Epoch 5/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6521 - val_loss: 0.2754
Epoch 6/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6516 - val_loss: 0.2755
Epoch 7/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6506 - val_loss: 0.2861
Epoch 8/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6428 - val_loss: 0.3439
Epoch 9/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6326 - val_loss: 0.3557
Epoch 10/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6296 - val_loss: 0.3591
Epoch 11/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6286 - val_loss: 0.3600
Epoch 12/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6282 - val_loss: 0.3607
Epoch 13/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6272 - val_loss: 0.3633
Epoch 14/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6262 - val_loss: 0.3654
Epoch 15/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6268 - val_loss: 0.3650
Execution time:  684.8749639987946
GRU:
Mean Absolute Error: 0.5534
Root Mean Square Error: 0.9898
Mean Square Error: 0.9797

Train RMSE: 0.990
Train MSE: 0.980
Train MAE: 0.553
###########################

MODEL:  GRU
sequence:  3d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_154&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_70 (GRU)                 (None, 432, 40)           5160      
_________________________________________________________________
dropout_154 (Dropout)        (None, 432, 40)           0         
_________________________________________________________________
time_distributed_154 (TimeDi (None, 432, 1)            41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
72/72 [==============================] - 12s 166ms/step - loss: 0.8926 - val_loss: 1.2355
Epoch 2/68
72/72 [==============================] - 12s 171ms/step - loss: 0.8258 - val_loss: 0.9347
Epoch 3/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6957 - val_loss: 0.8156
Epoch 4/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6811 - val_loss: 0.8014
Epoch 5/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6776 - val_loss: 0.7954
Epoch 6/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6758 - val_loss: 0.7920
Epoch 7/68
72/72 [==============================] - 12s 171ms/step - loss: 0.6746 - val_loss: 0.7898
Epoch 8/68
72/72 [==============================] - 12s 174ms/step - loss: 0.6738 - val_loss: 0.7882
Epoch 9/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6733 - val_loss: 0.7870
Epoch 10/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6728 - val_loss: 0.7861
Epoch 11/68
72/72 [==============================] - 12s 170ms/step - loss: 0.6724 - val_loss: 0.7854
Epoch 12/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6721 - val_loss: 0.7848
Epoch 13/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6719 - val_loss: 0.7842
Epoch 14/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6717 - val_loss: 0.7838
Epoch 15/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6715 - val_loss: 0.7835
Epoch 16/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6713 - val_loss: 0.7831
Epoch 17/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6712 - val_loss: 0.7829
Epoch 18/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6711 - val_loss: 0.7826
Epoch 19/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6710 - val_loss: 0.7824
Epoch 20/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6709 - val_loss: 0.7822
Epoch 21/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6708 - val_loss: 0.7820
Epoch 22/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6707 - val_loss: 0.7818
Epoch 23/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6706 - val_loss: 0.7817
Epoch 24/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6706 - val_loss: 0.7816
Epoch 25/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6705 - val_loss: 0.7814
Epoch 26/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6704 - val_loss: 0.7813
Epoch 27/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6704 - val_loss: 0.7812
Epoch 28/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6703 - val_loss: 0.7811
Epoch 29/68
72/72 [==============================] - 13s 177ms/step - loss: 0.6703 - val_loss: 0.7810
Epoch 30/68
72/72 [==============================] - 12s 174ms/step - loss: 0.6703 - val_loss: 0.7810
Epoch 31/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6702 - val_loss: 0.7809
Epoch 32/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6702 - val_loss: 0.7808
Epoch 33/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 34/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6701 - val_loss: 0.7807
Epoch 35/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6701 - val_loss: 0.7806
Epoch 36/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6700 - val_loss: 0.7806
Epoch 37/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 38/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6700 - val_loss: 0.7805
Epoch 39/68
72/72 [==============================] - 12s 171ms/step - loss: 0.6700 - val_loss: 0.7804
Epoch 40/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7804
Epoch 41/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 42/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7803
Epoch 43/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 44/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 45/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6699 - val_loss: 0.7802
Epoch 46/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 47/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 48/68
72/72 [==============================] - 12s 171ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 49/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6698 - val_loss: 0.7801
Epoch 50/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 51/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 52/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6698 - val_loss: 0.7800
Epoch 53/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 54/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 55/68
72/72 [==============================] - 12s 173ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 56/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 57/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6697 - val_loss: 0.7799
Epoch 58/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 59/68
72/72 [==============================] - 13s 174ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 60/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 61/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 62/68
72/72 [==============================] - 12s 171ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 63/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 64/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6697 - val_loss: 0.7798
Epoch 65/68
72/72 [==============================] - 13s 179ms/step - loss: 0.6696 - val_loss: 0.7797
Epoch 66/68
72/72 [==============================] - 13s 175ms/step - loss: 0.6696 - val_loss: 0.7797
Epoch 67/68
72/72 [==============================] - 13s 176ms/step - loss: 0.6696 - val_loss: 0.7797
Epoch 68/68
72/72 [==============================] - 12s 172ms/step - loss: 0.6696 - val_loss: 0.7797
Execution time:  865.2246866226196
GRU:
Mean Absolute Error: 0.6862
Root Mean Square Error: 0.9968
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  GRU
sequence:  3d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_155&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_71 (GRU)                 (None, 432, 55)           9570      
_________________________________________________________________
dropout_155 (Dropout)        (None, 432, 55)           0         
_________________________________________________________________
time_distributed_155 (TimeDi (None, 432, 1)            56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
317/317 [==============================] - 46s 145ms/step - loss: 0.8161 - val_loss: 0.6198
Epoch 2/36
317/317 [==============================] - 45s 142ms/step - loss: 0.7021 - val_loss: 0.6129
Epoch 3/36
317/317 [==============================] - 45s 141ms/step - loss: 0.7006 - val_loss: 0.6110
Epoch 4/36
317/317 [==============================] - 45s 142ms/step - loss: 0.7000 - val_loss: 0.6101
Epoch 5/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6997 - val_loss: 0.6095
Epoch 6/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6995 - val_loss: 0.6091
Epoch 7/36
317/317 [==============================] - 45s 141ms/step - loss: 0.6994 - val_loss: 0.6088
Epoch 8/36
317/317 [==============================] - 45s 141ms/step - loss: 0.6993 - val_loss: 0.6086
Epoch 9/36
317/317 [==============================] - 47s 148ms/step - loss: 0.6992 - val_loss: 0.6084
Epoch 10/36
317/317 [==============================] - 46s 145ms/step - loss: 0.6991 - val_loss: 0.6082
Epoch 11/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6991 - val_loss: 0.6081
Epoch 12/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6990 - val_loss: 0.6080
Epoch 13/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6990 - val_loss: 0.6079
Epoch 14/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6078
Epoch 15/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 16/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6077
Epoch 17/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 18/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 19/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 20/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6989 - val_loss: 0.6076
Epoch 21/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6989 - val_loss: 0.6075
Epoch 22/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 23/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 24/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 25/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 26/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 27/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 28/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 29/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 30/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 31/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 32/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 33/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 34/36
317/317 [==============================] - 45s 142ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 35/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Epoch 36/36
317/317 [==============================] - 45s 143ms/step - loss: 0.6988 - val_loss: 0.6075
Execution time:  1635.3623416423798
GRU:
Mean Absolute Error: 0.6862
Root Mean Square Error: 0.9968
Mean Square Error: 0.9937

Train RMSE: 0.997
Train MSE: 0.994
Train MAE: 0.686
###########################

MODEL:  GRU
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_156&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_72 (GRU)                 (None, 1008, 40)          5160      
_________________________________________________________________
dropout_156 (Dropout)        (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_156 (TimeDi (None, 1008, 1)           41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 23s 382ms/step - loss: 0.6499 - val_loss: 0.6928
Epoch 2/68
60/60 [==============================] - 23s 384ms/step - loss: 0.5896 - val_loss: 0.5294
Epoch 3/68
60/60 [==============================] - 23s 390ms/step - loss: 0.5761 - val_loss: 0.4939
Epoch 4/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5686 - val_loss: 0.4713
Epoch 5/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5656 - val_loss: 0.4615
Epoch 6/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5634 - val_loss: 0.4542
Epoch 7/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5619 - val_loss: 0.4489
Epoch 8/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5607 - val_loss: 0.4441
Epoch 9/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5597 - val_loss: 0.4407
Epoch 10/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5589 - val_loss: 0.4381
Epoch 11/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5582 - val_loss: 0.4362
Epoch 12/68
60/60 [==============================] - 25s 409ms/step - loss: 0.5573 - val_loss: 0.4344
Epoch 13/68
60/60 [==============================] - 24s 405ms/step - loss: 0.5567 - val_loss: 0.4333
Epoch 14/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5560 - val_loss: 0.4331
Epoch 15/68
60/60 [==============================] - 24s 407ms/step - loss: 0.5552 - val_loss: 0.4334
Epoch 16/68
60/60 [==============================] - 25s 409ms/step - loss: 0.5544 - val_loss: 0.4343
Epoch 17/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5531 - val_loss: 0.4360
Epoch 18/68
60/60 [==============================] - 25s 422ms/step - loss: 0.5515 - val_loss: 0.4401
Epoch 19/68
60/60 [==============================] - 25s 415ms/step - loss: 0.5492 - val_loss: 0.4536
Epoch 20/68
60/60 [==============================] - 25s 411ms/step - loss: 0.5453 - val_loss: 0.4695
Epoch 21/68
60/60 [==============================] - 25s 409ms/step - loss: 0.5421 - val_loss: 0.4977
Epoch 22/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5393 - val_loss: 0.5125
Epoch 23/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5377 - val_loss: 0.5218
Epoch 24/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5365 - val_loss: 0.5250
Execution time:  593.1951131820679
GRU:
Mean Absolute Error: 0.6514
Root Mean Square Error: 1.1407
Mean Square Error: 1.3011

Train RMSE: 1.141
Train MSE: 1.301
Train MAE: 0.651
###########################

MODEL:  GRU
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_157&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_73 (GRU)                 (None, 1008, 55)          9570      
_________________________________________________________________
dropout_157 (Dropout)        (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_157 (TimeDi (None, 1008, 1)           56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 91s 343ms/step - loss: 0.6325 - val_loss: 0.3807
Epoch 2/36
266/266 [==============================] - 91s 341ms/step - loss: 0.6060 - val_loss: 0.3240
Epoch 3/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6010 - val_loss: 0.3170
Epoch 4/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5978 - val_loss: 0.3119
Epoch 5/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5950 - val_loss: 0.3215
Epoch 6/36
266/266 [==============================] - 91s 341ms/step - loss: 0.5934 - val_loss: 0.3213
Epoch 7/36
266/266 [==============================] - 90s 338ms/step - loss: 0.5907 - val_loss: 0.3320
Epoch 8/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5882 - val_loss: 0.3482
Epoch 9/36
266/266 [==============================] - 90s 340ms/step - loss: 0.5855 - val_loss: 0.3600
Epoch 10/36
266/266 [==============================] - 91s 340ms/step - loss: 0.5818 - val_loss: 0.3669
Epoch 11/36
266/266 [==============================] - 90s 338ms/step - loss: 0.5740 - val_loss: 0.3955
Epoch 12/36
266/266 [==============================] - 90s 340ms/step - loss: 0.5712 - val_loss: 0.4132
Epoch 13/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5692 - val_loss: 0.4284
Epoch 14/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5683 - val_loss: 0.4398
Execution time:  1273.9686002731323
GRU:
Mean Absolute Error: 0.6702
Root Mean Square Error: 1.1418
Mean Square Error: 1.3037

Train RMSE: 1.142
Train MSE: 1.304
Train MAE: 0.670
###########################

MODEL:  GRU
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adam
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_158&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_74 (GRU)                 (None, 1008, 40)          5160      
_________________________________________________________________
dropout_158 (Dropout)        (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_158 (TimeDi (None, 1008, 1)           41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 23s 388ms/step - loss: 0.9570 - val_loss: 1.2026
Epoch 2/68
60/60 [==============================] - 23s 385ms/step - loss: 0.7150 - val_loss: 0.8351
Epoch 3/68
60/60 [==============================] - 23s 387ms/step - loss: 0.6790 - val_loss: 0.8281
Epoch 4/68
60/60 [==============================] - 23s 386ms/step - loss: 0.6766 - val_loss: 0.8256
Epoch 5/68
60/60 [==============================] - 23s 386ms/step - loss: 0.6754 - val_loss: 0.8242
Epoch 6/68
60/60 [==============================] - 23s 389ms/step - loss: 0.6748 - val_loss: 0.8234
Epoch 7/68
60/60 [==============================] - 23s 385ms/step - loss: 0.6743 - val_loss: 0.8229
Epoch 8/68
60/60 [==============================] - 23s 390ms/step - loss: 0.6740 - val_loss: 0.8225
Epoch 9/68
60/60 [==============================] - 23s 386ms/step - loss: 0.6738 - val_loss: 0.8222
Epoch 10/68
60/60 [==============================] - 23s 385ms/step - loss: 0.6737 - val_loss: 0.8220
Epoch 11/68
60/60 [==============================] - 23s 382ms/step - loss: 0.6735 - val_loss: 0.8218
Epoch 12/68
60/60 [==============================] - 23s 383ms/step - loss: 0.6734 - val_loss: 0.8217
Epoch 13/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6733 - val_loss: 0.8216
Epoch 14/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6733 - val_loss: 0.8215
Epoch 15/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6732 - val_loss: 0.8214
Epoch 16/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6731 - val_loss: 0.8213
Epoch 17/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6731 - val_loss: 0.8213
Epoch 18/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6730 - val_loss: 0.8212
Epoch 19/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6730 - val_loss: 0.8212
Epoch 20/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6730 - val_loss: 0.8211
Epoch 21/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6730 - val_loss: 0.8211
Epoch 22/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6729 - val_loss: 0.8211
Epoch 23/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6729 - val_loss: 0.8210
Epoch 24/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6729 - val_loss: 0.8210
Epoch 25/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6729 - val_loss: 0.8210
Epoch 26/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 27/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 28/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 29/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 30/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 31/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 32/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6728 - val_loss: 0.8208
Epoch 33/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 34/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 35/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 36/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 37/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 38/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 39/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 40/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 41/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 42/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 43/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 44/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 45/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8207
Epoch 46/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8207
Epoch 47/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6726 - val_loss: 0.8207
Epoch 48/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6726 - val_loss: 0.8207
Epoch 49/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8207
Epoch 50/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8207
Epoch 51/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 52/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 53/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 54/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 55/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 56/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 57/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 58/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 59/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 60/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 61/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 62/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 63/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 64/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 65/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 66/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 67/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 68/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8205
Execution time:  1661.805815935135
GRU:
Mean Absolute Error: 0.6893
Root Mean Square Error: 1.0230
Mean Square Error: 1.0465

Train RMSE: 1.023
Train MSE: 1.047
Train MAE: 0.689
###########################

MODEL:  GRU
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adam
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_159&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_75 (GRU)                 (None, 1008, 55)          9570      
_________________________________________________________________
dropout_159 (Dropout)        (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_159 (TimeDi (None, 1008, 1)           56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 90s 338ms/step - loss: 0.7771 - val_loss: 0.6587
Epoch 2/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6964 - val_loss: 0.6571
Epoch 3/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6959 - val_loss: 0.6566
Epoch 4/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6957 - val_loss: 0.6564
Epoch 5/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6956 - val_loss: 0.6562
Epoch 6/36
266/266 [==============================] - 91s 341ms/step - loss: 0.6955 - val_loss: 0.6561
Epoch 7/36
266/266 [==============================] - 91s 341ms/step - loss: 0.6955 - val_loss: 0.6561
Epoch 8/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6955 - val_loss: 0.6560
Epoch 9/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 10/36
266/266 [==============================] - 90s 340ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 11/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 12/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6954 - val_loss: 0.6558
Epoch 13/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 14/36
266/266 [==============================] - 91s 340ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 15/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 16/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 17/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 18/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 19/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 20/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 21/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 22/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 23/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 24/36
266/266 [==============================] - 90s 340ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 25/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 26/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 27/36
266/266 [==============================] - 91s 341ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 28/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 29/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 30/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 31/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 32/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 33/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 34/36
266/266 [==============================] - 91s 341ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 35/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 36/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Execution time:  3257.5765478610992
GRU:
Mean Absolute Error: 0.6893
Root Mean Square Error: 1.0230
Mean Square Error: 1.0465

Train RMSE: 1.023
Train MSE: 1.046
Train MAE: 0.689
###########################

MODEL:  GRU
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_160&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_76 (GRU)                 (None, 1008, 40)          5160      
_________________________________________________________________
dropout_160 (Dropout)        (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_160 (TimeDi (None, 1008, 1)           41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 23s 381ms/step - loss: 0.6722 - val_loss: 0.8264
Epoch 2/68
60/60 [==============================] - 23s 384ms/step - loss: 0.6721 - val_loss: 0.8263
Epoch 3/68
60/60 [==============================] - 23s 382ms/step - loss: 0.6719 - val_loss: 0.8261
Epoch 4/68
60/60 [==============================] - 23s 382ms/step - loss: 0.6719 - val_loss: 0.8259
Epoch 5/68
60/60 [==============================] - 23s 379ms/step - loss: 0.6718 - val_loss: 0.8257
Epoch 6/68
60/60 [==============================] - 23s 384ms/step - loss: 0.6717 - val_loss: 0.8255
Epoch 7/68
60/60 [==============================] - 25s 412ms/step - loss: 0.6715 - val_loss: 0.8253
Epoch 8/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6714 - val_loss: 0.8251
Epoch 9/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6713 - val_loss: 0.8249
Epoch 10/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6712 - val_loss: 0.8247
Epoch 11/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6711 - val_loss: 0.8245
Epoch 12/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6709 - val_loss: 0.8243
Epoch 13/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6708 - val_loss: 0.8240
Epoch 14/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6707 - val_loss: 0.8238
Epoch 15/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6705 - val_loss: 0.8236
Epoch 16/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6704 - val_loss: 0.8233
Epoch 17/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6703 - val_loss: 0.8231
Epoch 18/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6702 - val_loss: 0.8229
Epoch 19/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6700 - val_loss: 0.8226
Epoch 20/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6699 - val_loss: 0.8224
Epoch 21/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6697 - val_loss: 0.8221
Epoch 22/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6696 - val_loss: 0.8218
Epoch 23/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6694 - val_loss: 0.8216
Epoch 24/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6693 - val_loss: 0.8213
Epoch 25/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6692 - val_loss: 0.8211
Epoch 26/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6690 - val_loss: 0.8208
Epoch 27/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6689 - val_loss: 0.8205
Epoch 28/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6687 - val_loss: 0.8203
Epoch 29/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6686 - val_loss: 0.8200
Epoch 30/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6684 - val_loss: 0.8197
Epoch 31/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6683 - val_loss: 0.8195
Epoch 32/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6681 - val_loss: 0.8192
Epoch 33/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6680 - val_loss: 0.8189
Epoch 34/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6678 - val_loss: 0.8186
Epoch 35/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6677 - val_loss: 0.8183
Epoch 36/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6675 - val_loss: 0.8181
Epoch 37/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6673 - val_loss: 0.8178
Epoch 38/68
60/60 [==============================] - 25s 409ms/step - loss: 0.6672 - val_loss: 0.8175
Epoch 39/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6670 - val_loss: 0.8172
Epoch 40/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6669 - val_loss: 0.8169
Epoch 41/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6668 - val_loss: 0.8166
Epoch 42/68
60/60 [==============================] - 25s 414ms/step - loss: 0.6666 - val_loss: 0.8163
Epoch 43/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6664 - val_loss: 0.8160
Epoch 44/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6663 - val_loss: 0.8158
Epoch 45/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6661 - val_loss: 0.8155
Epoch 46/68
60/60 [==============================] - 24s 408ms/step - loss: 0.6659 - val_loss: 0.8152
Epoch 47/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6658 - val_loss: 0.8149
Epoch 48/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6656 - val_loss: 0.8146
Epoch 49/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6654 - val_loss: 0.8143
Epoch 50/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6653 - val_loss: 0.8140
Epoch 51/68
60/60 [==============================] - 24s 403ms/step - loss: 0.6651 - val_loss: 0.8137
Epoch 52/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6650 - val_loss: 0.8134
Epoch 53/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6648 - val_loss: 0.8131
Epoch 54/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6647 - val_loss: 0.8128
Epoch 55/68
60/60 [==============================] - 25s 411ms/step - loss: 0.6645 - val_loss: 0.8125
Epoch 56/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6643 - val_loss: 0.8122
Epoch 57/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6642 - val_loss: 0.8119
Epoch 58/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6640 - val_loss: 0.8115
Epoch 59/68
60/60 [==============================] - 24s 407ms/step - loss: 0.6638 - val_loss: 0.8112
Epoch 60/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6637 - val_loss: 0.8109
Epoch 61/68
60/60 [==============================] - 24s 405ms/step - loss: 0.6635 - val_loss: 0.8106
Epoch 62/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6633 - val_loss: 0.8103
Epoch 63/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6632 - val_loss: 0.8100
Epoch 64/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6630 - val_loss: 0.8097
Epoch 65/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6629 - val_loss: 0.8094
Epoch 66/68
60/60 [==============================] - 24s 404ms/step - loss: 0.6627 - val_loss: 0.8091
Epoch 67/68
60/60 [==============================] - 25s 410ms/step - loss: 0.6625 - val_loss: 0.8088
Epoch 68/68
60/60 [==============================] - 24s 406ms/step - loss: 0.6623 - val_loss: 0.8084
Execution time:  1678.5151374340057
GRU:
Mean Absolute Error: 0.6937
Root Mean Square Error: 1.0382
Mean Square Error: 1.0780

Train RMSE: 1.038
Train MSE: 1.078
Train MAE: 0.694
###########################

MODEL:  GRU
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_161&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_77 (GRU)                 (None, 1008, 55)          9570      
_________________________________________________________________
dropout_161 (Dropout)        (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_161 (TimeDi (None, 1008, 1)           56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 92s 346ms/step - loss: 0.6945 - val_loss: 0.6588
Epoch 2/36
266/266 [==============================] - 89s 335ms/step - loss: 0.6940 - val_loss: 0.6582
Epoch 3/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6934 - val_loss: 0.6575
Epoch 4/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6927 - val_loss: 0.6568
Epoch 5/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6920 - val_loss: 0.6559
Epoch 6/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6913 - val_loss: 0.6550
Epoch 7/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6905 - val_loss: 0.6540
Epoch 8/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6897 - val_loss: 0.6529
Epoch 9/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6888 - val_loss: 0.6518
Epoch 10/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6880 - val_loss: 0.6506
Epoch 11/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6871 - val_loss: 0.6493
Epoch 12/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6862 - val_loss: 0.6480
Epoch 13/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6853 - val_loss: 0.6467
Epoch 14/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6843 - val_loss: 0.6452
Epoch 15/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6834 - val_loss: 0.6438
Epoch 16/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6824 - val_loss: 0.6423
Epoch 17/36
266/266 [==============================] - 93s 349ms/step - loss: 0.6814 - val_loss: 0.6407
Epoch 18/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6805 - val_loss: 0.6391
Epoch 19/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6795 - val_loss: 0.6375
Epoch 20/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6785 - val_loss: 0.6358
Epoch 21/36
266/266 [==============================] - 90s 336ms/step - loss: 0.6775 - val_loss: 0.6341
Epoch 22/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6765 - val_loss: 0.6324
Epoch 23/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6755 - val_loss: 0.6306
Epoch 24/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6744 - val_loss: 0.6287
Epoch 25/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6734 - val_loss: 0.6269
Epoch 26/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6723 - val_loss: 0.6250
Epoch 27/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6713 - val_loss: 0.6231
Epoch 28/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6702 - val_loss: 0.6211
Epoch 29/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6691 - val_loss: 0.6192
Epoch 30/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6681 - val_loss: 0.6172
Epoch 31/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6670 - val_loss: 0.6152
Epoch 32/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6659 - val_loss: 0.6132
Epoch 33/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6648 - val_loss: 0.6112
Epoch 34/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6637 - val_loss: 0.6092
Epoch 35/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6626 - val_loss: 0.6072
Epoch 36/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6616 - val_loss: 0.6052
Execution time:  3252.168182373047
GRU:
Mean Absolute Error: 0.6944
Root Mean Square Error: 1.0576
Mean Square Error: 1.1186

Train RMSE: 1.058
Train MSE: 1.119
Train MAE: 0.694
###########################

MODEL:  GRU
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adadelta
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_162&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_78 (GRU)                 (None, 1008, 40)          5160      
_________________________________________________________________
dropout_162 (Dropout)        (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_162 (TimeDi (None, 1008, 1)           41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 25s 415ms/step - loss: 0.9759 - val_loss: 1.3136
Epoch 2/68
60/60 [==============================] - 25s 419ms/step - loss: 0.9758 - val_loss: 1.3136
Epoch 3/68
60/60 [==============================] - 25s 413ms/step - loss: 0.9758 - val_loss: 1.3135
Epoch 4/68
60/60 [==============================] - 25s 419ms/step - loss: 0.9758 - val_loss: 1.3134
Epoch 5/68
60/60 [==============================] - 25s 417ms/step - loss: 0.9757 - val_loss: 1.3134
Epoch 6/68
60/60 [==============================] - 25s 422ms/step - loss: 0.9757 - val_loss: 1.3133
Epoch 7/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9756 - val_loss: 1.3132
Epoch 8/68
60/60 [==============================] - 27s 449ms/step - loss: 0.9756 - val_loss: 1.3132
Epoch 9/68
60/60 [==============================] - 27s 444ms/step - loss: 0.9755 - val_loss: 1.3131
Epoch 10/68
60/60 [==============================] - 27s 444ms/step - loss: 0.9754 - val_loss: 1.3130
Epoch 11/68
60/60 [==============================] - 27s 443ms/step - loss: 0.9754 - val_loss: 1.3129
Epoch 12/68
60/60 [==============================] - 26s 440ms/step - loss: 0.9753 - val_loss: 1.3129
Epoch 13/68
60/60 [==============================] - 27s 443ms/step - loss: 0.9753 - val_loss: 1.3128
Epoch 14/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9752 - val_loss: 1.3127
Epoch 15/68
60/60 [==============================] - 27s 448ms/step - loss: 0.9751 - val_loss: 1.3126
Epoch 16/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9751 - val_loss: 1.3125
Epoch 17/68
60/60 [==============================] - 26s 442ms/step - loss: 0.9750 - val_loss: 1.3124
Epoch 18/68
60/60 [==============================] - 27s 442ms/step - loss: 0.9749 - val_loss: 1.3124
Epoch 19/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9749 - val_loss: 1.3123
Epoch 20/68
60/60 [==============================] - 26s 436ms/step - loss: 0.9748 - val_loss: 1.3122
Epoch 21/68
60/60 [==============================] - 27s 442ms/step - loss: 0.9748 - val_loss: 1.3121
Epoch 22/68
60/60 [==============================] - 27s 444ms/step - loss: 0.9747 - val_loss: 1.3120
Epoch 23/68
60/60 [==============================] - 26s 435ms/step - loss: 0.9746 - val_loss: 1.3119
Epoch 24/68
60/60 [==============================] - 26s 440ms/step - loss: 0.9746 - val_loss: 1.3118
Epoch 25/68
60/60 [==============================] - 26s 439ms/step - loss: 0.9745 - val_loss: 1.3117
Epoch 26/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9744 - val_loss: 1.3116
Epoch 27/68
60/60 [==============================] - 27s 442ms/step - loss: 0.9744 - val_loss: 1.3115
Epoch 28/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9743 - val_loss: 1.3114
Epoch 29/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9742 - val_loss: 1.3113
Epoch 30/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9741 - val_loss: 1.3112
Epoch 31/68
60/60 [==============================] - 26s 439ms/step - loss: 0.9741 - val_loss: 1.3111
Epoch 32/68
60/60 [==============================] - 26s 435ms/step - loss: 0.9740 - val_loss: 1.3110
Epoch 33/68
60/60 [==============================] - 26s 440ms/step - loss: 0.9739 - val_loss: 1.3109
Epoch 34/68
60/60 [==============================] - 27s 447ms/step - loss: 0.9739 - val_loss: 1.3108
Epoch 35/68
60/60 [==============================] - 27s 445ms/step - loss: 0.9738 - val_loss: 1.3107
Epoch 36/68
60/60 [==============================] - 26s 440ms/step - loss: 0.9737 - val_loss: 1.3106
Epoch 37/68
60/60 [==============================] - 26s 439ms/step - loss: 0.9736 - val_loss: 1.3105
Epoch 38/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9736 - val_loss: 1.3104
Epoch 39/68
60/60 [==============================] - 26s 433ms/step - loss: 0.9735 - val_loss: 1.3103
Epoch 40/68
60/60 [==============================] - 26s 440ms/step - loss: 0.9734 - val_loss: 1.3102
Epoch 41/68
60/60 [==============================] - 26s 436ms/step - loss: 0.9733 - val_loss: 1.3101
Epoch 42/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9733 - val_loss: 1.3100
Epoch 43/68
60/60 [==============================] - 26s 440ms/step - loss: 0.9732 - val_loss: 1.3099
Epoch 44/68
60/60 [==============================] - 26s 439ms/step - loss: 0.9731 - val_loss: 1.3098
Epoch 45/68
60/60 [==============================] - 26s 436ms/step - loss: 0.9730 - val_loss: 1.3097
Epoch 46/68
60/60 [==============================] - 26s 433ms/step - loss: 0.9730 - val_loss: 1.3095
Epoch 47/68
60/60 [==============================] - 26s 436ms/step - loss: 0.9729 - val_loss: 1.3094
Epoch 48/68
60/60 [==============================] - 27s 442ms/step - loss: 0.9728 - val_loss: 1.3093
Epoch 49/68
60/60 [==============================] - 26s 434ms/step - loss: 0.9727 - val_loss: 1.3092
Epoch 50/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9727 - val_loss: 1.3091
Epoch 51/68
60/60 [==============================] - 26s 438ms/step - loss: 0.9726 - val_loss: 1.3090
Epoch 52/68
60/60 [==============================] - 26s 439ms/step - loss: 0.9725 - val_loss: 1.3089
Epoch 53/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9724 - val_loss: 1.3088
Epoch 54/68
60/60 [==============================] - 26s 434ms/step - loss: 0.9723 - val_loss: 1.3087
Epoch 55/68
60/60 [==============================] - 26s 441ms/step - loss: 0.9723 - val_loss: 1.3085
Epoch 56/68
60/60 [==============================] - 26s 438ms/step - loss: 0.9722 - val_loss: 1.3084
Epoch 57/68
60/60 [==============================] - 26s 434ms/step - loss: 0.9721 - val_loss: 1.3083
Epoch 58/68
60/60 [==============================] - 26s 434ms/step - loss: 0.9720 - val_loss: 1.3082
Epoch 59/68
60/60 [==============================] - 26s 432ms/step - loss: 0.9720 - val_loss: 1.3081
Epoch 60/68
60/60 [==============================] - 26s 440ms/step - loss: 0.9719 - val_loss: 1.3080
Epoch 61/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9718 - val_loss: 1.3078
Epoch 62/68
60/60 [==============================] - 26s 436ms/step - loss: 0.9717 - val_loss: 1.3077
Epoch 63/68
60/60 [==============================] - 26s 433ms/step - loss: 0.9716 - val_loss: 1.3076
Epoch 64/68
60/60 [==============================] - 26s 437ms/step - loss: 0.9715 - val_loss: 1.3075
Epoch 65/68
60/60 [==============================] - 26s 433ms/step - loss: 0.9714 - val_loss: 1.3074
Epoch 66/68
60/60 [==============================] - 27s 445ms/step - loss: 0.9714 - val_loss: 1.3073
Epoch 67/68
60/60 [==============================] - 26s 436ms/step - loss: 0.9713 - val_loss: 1.3071
Epoch 68/68
60/60 [==============================] - 27s 442ms/step - loss: 0.9712 - val_loss: 1.3070
Execution time:  1814.725310087204
GRU:
Mean Absolute Error: 0.9029
Root Mean Square Error: 1.1217
Mean Square Error: 1.2581

Train RMSE: 1.122
Train MSE: 1.258
Train MAE: 0.903
###########################

MODEL:  GRU
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adadelta
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_163&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_79 (GRU)                 (None, 1008, 55)          9570      
_________________________________________________________________
dropout_163 (Dropout)        (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_163 (TimeDi (None, 1008, 1)           56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 92s 344ms/step - loss: 0.9761 - val_loss: 1.1464
Epoch 2/36
266/266 [==============================] - 90s 340ms/step - loss: 0.9759 - val_loss: 1.1461
Epoch 3/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9756 - val_loss: 1.1458
Epoch 4/36
266/266 [==============================] - 89s 335ms/step - loss: 0.9753 - val_loss: 1.1454
Epoch 5/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9750 - val_loss: 1.1449
Epoch 6/36
266/266 [==============================] - 89s 336ms/step - loss: 0.9746 - val_loss: 1.1445
Epoch 7/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9742 - val_loss: 1.1440
Epoch 8/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9738 - val_loss: 1.1434
Epoch 9/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9734 - val_loss: 1.1429
Epoch 10/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9730 - val_loss: 1.1423
Epoch 11/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9725 - val_loss: 1.1417
Epoch 12/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9721 - val_loss: 1.1411
Epoch 13/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9716 - val_loss: 1.1404
Epoch 14/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9711 - val_loss: 1.1397
Epoch 15/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9706 - val_loss: 1.1390
Epoch 16/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9701 - val_loss: 1.1383
Epoch 17/36
266/266 [==============================] - 91s 341ms/step - loss: 0.9695 - val_loss: 1.1376
Epoch 18/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9690 - val_loss: 1.1368
Epoch 19/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9684 - val_loss: 1.1360
Epoch 20/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9679 - val_loss: 1.1352
Epoch 21/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9673 - val_loss: 1.1343
Epoch 22/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9666 - val_loss: 1.1335
Epoch 23/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9660 - val_loss: 1.1326
Epoch 24/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9654 - val_loss: 1.1316
Epoch 25/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9647 - val_loss: 1.1307
Epoch 26/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9640 - val_loss: 1.1297
Epoch 27/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9633 - val_loss: 1.1287
Epoch 28/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9626 - val_loss: 1.1276
Epoch 29/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9618 - val_loss: 1.1265
Epoch 30/36
266/266 [==============================] - 90s 340ms/step - loss: 0.9611 - val_loss: 1.1254
Epoch 31/36
266/266 [==============================] - 90s 339ms/step - loss: 0.9603 - val_loss: 1.1243
Epoch 32/36
266/266 [==============================] - 90s 338ms/step - loss: 0.9595 - val_loss: 1.1231
Epoch 33/36
266/266 [==============================] - 91s 341ms/step - loss: 0.9587 - val_loss: 1.1219
Epoch 34/36
266/266 [==============================] - 90s 340ms/step - loss: 0.9578 - val_loss: 1.1206
Epoch 35/36
266/266 [==============================] - 90s 340ms/step - loss: 0.9569 - val_loss: 1.1193
Epoch 36/36
266/266 [==============================] - 90s 337ms/step - loss: 0.9560 - val_loss: 1.1180
Execution time:  3256.1397848129272
GRU:
Mean Absolute Error: 0.9021
Root Mean Square Error: 1.1249
Mean Square Error: 1.2654

Train RMSE: 1.125
Train MSE: 1.265
Train MAE: 0.902
###########################

MODEL:  GRU
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: tanh
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_164&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_80 (GRU)                 (None, 1008, 40)          5160      
_________________________________________________________________
dropout_164 (Dropout)        (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_164 (TimeDi (None, 1008, 1)           41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 23s 377ms/step - loss: 0.6520 - val_loss: 0.7382
Epoch 2/68
60/60 [==============================] - 23s 380ms/step - loss: 0.5899 - val_loss: 0.5312
Epoch 3/68
60/60 [==============================] - 23s 382ms/step - loss: 0.5653 - val_loss: 0.4690
Epoch 4/68
60/60 [==============================] - 23s 380ms/step - loss: 0.5641 - val_loss: 0.4613
Epoch 5/68
60/60 [==============================] - 23s 376ms/step - loss: 0.5627 - val_loss: 0.4556
Epoch 6/68
60/60 [==============================] - 23s 380ms/step - loss: 0.5617 - val_loss: 0.4508
Epoch 7/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5609 - val_loss: 0.4470
Epoch 8/68
60/60 [==============================] - 24s 393ms/step - loss: 0.5602 - val_loss: 0.4431
Epoch 9/68
60/60 [==============================] - 24s 395ms/step - loss: 0.5596 - val_loss: 0.4404
Epoch 10/68
60/60 [==============================] - 24s 392ms/step - loss: 0.5590 - val_loss: 0.4379
Epoch 11/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5586 - val_loss: 0.4356
Epoch 12/68
60/60 [==============================] - 24s 394ms/step - loss: 0.5581 - val_loss: 0.4333
Epoch 13/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5577 - val_loss: 0.4313
Epoch 14/68
60/60 [==============================] - 24s 396ms/step - loss: 0.5572 - val_loss: 0.4296
Epoch 15/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5568 - val_loss: 0.4280
Epoch 16/68
60/60 [==============================] - 24s 398ms/step - loss: 0.5565 - val_loss: 0.4264
Epoch 17/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5562 - val_loss: 0.4252
Epoch 18/68
60/60 [==============================] - 24s 393ms/step - loss: 0.5558 - val_loss: 0.4239
Epoch 19/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5555 - val_loss: 0.4229
Epoch 20/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5552 - val_loss: 0.4217
Epoch 21/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5549 - val_loss: 0.4206
Epoch 22/68
60/60 [==============================] - 24s 406ms/step - loss: 0.5546 - val_loss: 0.4198
Epoch 23/68
60/60 [==============================] - 24s 394ms/step - loss: 0.5543 - val_loss: 0.4190
Epoch 24/68
60/60 [==============================] - 24s 404ms/step - loss: 0.5540 - val_loss: 0.4182
Epoch 25/68
60/60 [==============================] - 24s 394ms/step - loss: 0.5538 - val_loss: 0.4175
Epoch 26/68
60/60 [==============================] - 24s 403ms/step - loss: 0.5535 - val_loss: 0.4170
Epoch 27/68
60/60 [==============================] - 24s 395ms/step - loss: 0.5532 - val_loss: 0.4166
Epoch 28/68
60/60 [==============================] - 24s 407ms/step - loss: 0.5530 - val_loss: 0.4163
Epoch 29/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5527 - val_loss: 0.4161
Epoch 30/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5524 - val_loss: 0.4159
Epoch 31/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5522 - val_loss: 0.4159
Epoch 32/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5519 - val_loss: 0.4160
Epoch 33/68
60/60 [==============================] - 24s 399ms/step - loss: 0.5516 - val_loss: 0.4162
Epoch 34/68
60/60 [==============================] - 24s 400ms/step - loss: 0.5514 - val_loss: 0.4167
Epoch 35/68
60/60 [==============================] - 24s 402ms/step - loss: 0.5510 - val_loss: 0.4174
Epoch 36/68
60/60 [==============================] - 24s 397ms/step - loss: 0.5507 - val_loss: 0.4180
Epoch 37/68
60/60 [==============================] - 24s 395ms/step - loss: 0.5504 - val_loss: 0.4188
Epoch 38/68
60/60 [==============================] - 24s 393ms/step - loss: 0.5500 - val_loss: 0.4198
Epoch 39/68
60/60 [==============================] - 24s 394ms/step - loss: 0.5497 - val_loss: 0.4208
Epoch 40/68
60/60 [==============================] - 24s 401ms/step - loss: 0.5493 - val_loss: 0.4221
Execution time:  968.2555317878723
GRU:
Mean Absolute Error: 0.6504
Root Mean Square Error: 1.1419
Mean Square Error: 1.3039

Train RMSE: 1.142
Train MSE: 1.304
Train MAE: 0.650
###########################

MODEL:  GRU
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: tanh
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_165&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_81 (GRU)                 (None, 1008, 55)          9570      
_________________________________________________________________
dropout_165 (Dropout)        (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_165 (TimeDi (None, 1008, 1)           56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 91s 343ms/step - loss: 0.6483 - val_loss: 0.3268
Epoch 2/36
266/266 [==============================] - 90s 340ms/step - loss: 0.5990 - val_loss: 0.3076
Epoch 3/36
266/266 [==============================] - 90s 338ms/step - loss: 0.5956 - val_loss: 0.3015
Epoch 4/36
266/266 [==============================] - 90s 337ms/step - loss: 0.5946 - val_loss: 0.3086
Epoch 5/36
266/266 [==============================] - 90s 338ms/step - loss: 0.5937 - val_loss: 0.3147
Epoch 6/36
266/266 [==============================] - 90s 338ms/step - loss: 0.5924 - val_loss: 0.3189
Epoch 7/36
266/266 [==============================] - 90s 340ms/step - loss: 0.5914 - val_loss: 0.3197
Epoch 8/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5903 - val_loss: 0.3176
Epoch 9/36
266/266 [==============================] - 90s 337ms/step - loss: 0.5896 - val_loss: 0.3184
Epoch 10/36
266/266 [==============================] - 90s 339ms/step - loss: 0.5895 - val_loss: 0.3173
Epoch 11/36
266/266 [==============================] - 90s 338ms/step - loss: 0.5885 - val_loss: 0.3160
Epoch 12/36
266/266 [==============================] - 90s 337ms/step - loss: 0.5879 - val_loss: 0.3176
Epoch 13/36
266/266 [==============================] - 90s 337ms/step - loss: 0.5875 - val_loss: 0.3156
Execution time:  1178.3893194198608
GRU:
Mean Absolute Error: 0.6568
Root Mean Square Error: 1.1633
Mean Square Error: 1.3533

Train RMSE: 1.163
Train MSE: 1.353
Train MAE: 0.657
###########################

MODEL:  GRU
sequence:  7d
units:  40
dropout1:  0.40519643149940265
optimizer: adamax
activationDense: sigmoid
epochs: 68
batchsize: 45
validation_split: 0.1
Model: &#34;sequential_166&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_82 (GRU)                 (None, 1008, 40)          5160      
_________________________________________________________________
dropout_166 (Dropout)        (None, 1008, 40)          0         
_________________________________________________________________
time_distributed_166 (TimeDi (None, 1008, 1)           41        
=================================================================
Total params: 5,201
Trainable params: 5,201
Non-trainable params: 0
_________________________________________________________________
Epoch 1/68
60/60 [==============================] - 22s 374ms/step - loss: 0.9662 - val_loss: 1.2776
Epoch 2/68
60/60 [==============================] - 23s 379ms/step - loss: 0.9005 - val_loss: 1.0340
Epoch 3/68
60/60 [==============================] - 23s 385ms/step - loss: 0.7193 - val_loss: 0.8563
Epoch 4/68
60/60 [==============================] - 24s 393ms/step - loss: 0.6882 - val_loss: 0.8404
Epoch 5/68
60/60 [==============================] - 23s 390ms/step - loss: 0.6828 - val_loss: 0.8343
Epoch 6/68
60/60 [==============================] - 23s 392ms/step - loss: 0.6802 - val_loss: 0.8310
Epoch 7/68
60/60 [==============================] - 24s 393ms/step - loss: 0.6786 - val_loss: 0.8289
Epoch 8/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6776 - val_loss: 0.8274
Epoch 9/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6768 - val_loss: 0.8264
Epoch 10/68
60/60 [==============================] - 23s 388ms/step - loss: 0.6763 - val_loss: 0.8256
Epoch 11/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6758 - val_loss: 0.8249
Epoch 12/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6754 - val_loss: 0.8244
Epoch 13/68
60/60 [==============================] - 23s 389ms/step - loss: 0.6751 - val_loss: 0.8240
Epoch 14/68
60/60 [==============================] - 23s 389ms/step - loss: 0.6749 - val_loss: 0.8236
Epoch 15/68
60/60 [==============================] - 23s 385ms/step - loss: 0.6747 - val_loss: 0.8233
Epoch 16/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6745 - val_loss: 0.8231
Epoch 17/68
60/60 [==============================] - 24s 392ms/step - loss: 0.6743 - val_loss: 0.8229
Epoch 18/68
60/60 [==============================] - 23s 389ms/step - loss: 0.6742 - val_loss: 0.8227
Epoch 19/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6741 - val_loss: 0.8225
Epoch 20/68
60/60 [==============================] - 23s 387ms/step - loss: 0.6740 - val_loss: 0.8223
Epoch 21/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6738 - val_loss: 0.8222
Epoch 22/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6738 - val_loss: 0.8221
Epoch 23/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6737 - val_loss: 0.8220
Epoch 24/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6736 - val_loss: 0.8219
Epoch 25/68
60/60 [==============================] - 23s 386ms/step - loss: 0.6735 - val_loss: 0.8218
Epoch 26/68
60/60 [==============================] - 23s 392ms/step - loss: 0.6735 - val_loss: 0.8217
Epoch 27/68
60/60 [==============================] - 23s 392ms/step - loss: 0.6734 - val_loss: 0.8216
Epoch 28/68
60/60 [==============================] - 23s 391ms/step - loss: 0.6734 - val_loss: 0.8215
Epoch 29/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6733 - val_loss: 0.8215
Epoch 30/68
60/60 [==============================] - 27s 454ms/step - loss: 0.6733 - val_loss: 0.8214
Epoch 31/68
60/60 [==============================] - 27s 442ms/step - loss: 0.6732 - val_loss: 0.8214
Epoch 32/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6732 - val_loss: 0.8213
Epoch 33/68
60/60 [==============================] - 24s 392ms/step - loss: 0.6731 - val_loss: 0.8213
Epoch 34/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6731 - val_loss: 0.8212
Epoch 35/68
60/60 [==============================] - 24s 392ms/step - loss: 0.6731 - val_loss: 0.8212
Epoch 36/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6731 - val_loss: 0.8211
Epoch 37/68
60/60 [==============================] - 23s 390ms/step - loss: 0.6730 - val_loss: 0.8211
Epoch 38/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6730 - val_loss: 0.8211
Epoch 39/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6730 - val_loss: 0.8210
Epoch 40/68
60/60 [==============================] - 24s 393ms/step - loss: 0.6729 - val_loss: 0.8210
Epoch 41/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6729 - val_loss: 0.8210
Epoch 42/68
60/60 [==============================] - 23s 391ms/step - loss: 0.6729 - val_loss: 0.8210
Epoch 43/68
60/60 [==============================] - 23s 390ms/step - loss: 0.6729 - val_loss: 0.8209
Epoch 44/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6729 - val_loss: 0.8209
Epoch 45/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6729 - val_loss: 0.8209
Epoch 46/68
60/60 [==============================] - 24s 393ms/step - loss: 0.6728 - val_loss: 0.8209
Epoch 47/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6728 - val_loss: 0.8208
Epoch 48/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6728 - val_loss: 0.8208
Epoch 49/68
60/60 [==============================] - 24s 398ms/step - loss: 0.6728 - val_loss: 0.8208
Epoch 50/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6728 - val_loss: 0.8208
Epoch 51/68
60/60 [==============================] - 23s 391ms/step - loss: 0.6728 - val_loss: 0.8208
Epoch 52/68
60/60 [==============================] - 24s 393ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 53/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6727 - val_loss: 0.8208
Epoch 54/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 55/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 56/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 57/68
60/60 [==============================] - 24s 397ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 58/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 59/68
60/60 [==============================] - 24s 394ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 60/68
60/60 [==============================] - 24s 399ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 61/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 62/68
60/60 [==============================] - 24s 396ms/step - loss: 0.6727 - val_loss: 0.8207
Epoch 63/68
60/60 [==============================] - 24s 400ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 64/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 65/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 66/68
60/60 [==============================] - 24s 395ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 67/68
60/60 [==============================] - 24s 401ms/step - loss: 0.6726 - val_loss: 0.8206
Epoch 68/68
60/60 [==============================] - 24s 402ms/step - loss: 0.6726 - val_loss: 0.8206
Execution time:  1643.7634372711182
GRU:
Mean Absolute Error: 0.6893
Root Mean Square Error: 1.0230
Mean Square Error: 1.0465

Train RMSE: 1.023
Train MSE: 1.046
Train MAE: 0.689
###########################

MODEL:  GRU
sequence:  7d
units:  55
dropout1:  0.11814836227952394
optimizer: adamax
activationDense: sigmoid
epochs: 36
batchsize: 9
validation_split: 0.2
Model: &#34;sequential_167&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_83 (GRU)                 (None, 1008, 55)          9570      
_________________________________________________________________
dropout_167 (Dropout)        (None, 1008, 55)          0         
_________________________________________________________________
time_distributed_167 (TimeDi (None, 1008, 1)           56        
=================================================================
Total params: 9,626
Trainable params: 9,626
Non-trainable params: 0
_________________________________________________________________
Epoch 1/36
266/266 [==============================] - 91s 341ms/step - loss: 0.8342 - val_loss: 0.6673
Epoch 2/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6990 - val_loss: 0.6602
Epoch 3/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6971 - val_loss: 0.6584
Epoch 4/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6965 - val_loss: 0.6576
Epoch 5/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6961 - val_loss: 0.6571
Epoch 6/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6959 - val_loss: 0.6568
Epoch 7/36
266/266 [==============================] - 89s 336ms/step - loss: 0.6958 - val_loss: 0.6565
Epoch 8/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6957 - val_loss: 0.6564
Epoch 9/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6956 - val_loss: 0.6562
Epoch 10/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6956 - val_loss: 0.6561
Epoch 11/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6955 - val_loss: 0.6561
Epoch 12/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6955 - val_loss: 0.6560
Epoch 13/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 14/36
266/266 [==============================] - 90s 340ms/step - loss: 0.6954 - val_loss: 0.6559
Epoch 15/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6954 - val_loss: 0.6558
Epoch 16/36
266/266 [==============================] - 91s 340ms/step - loss: 0.6953 - val_loss: 0.6558
Epoch 17/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 18/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 19/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 20/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 21/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6557
Epoch 22/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 23/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 24/36
266/266 [==============================] - 90s 337ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 25/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 26/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 27/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 28/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 29/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 30/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 31/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 32/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 33/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 34/36
266/266 [==============================] - 90s 339ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 35/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Epoch 36/36
266/266 [==============================] - 90s 338ms/step - loss: 0.6953 - val_loss: 0.6556
Execution time:  3255.470086336136
GRU:
Mean Absolute Error: 0.6893
Root Mean Square Error: 1.0230
Mean Square Error: 1.0465

Train RMSE: 1.023
Train MSE: 1.046
Train MAE: 0.689
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">list_results</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;resultats-cerca-optim-lstm-una-capa.csv&quot;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">list_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[19]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>sequence</th>
      <th>activationDense</th>
      <th>optimizer</th>
      <th>dropout1</th>
      <th>units</th>
      <th>epochs</th>
      <th>batchsize</th>
      <th>validation_split</th>
      <th>RMSE</th>
      <th>MSE</th>
      <th>MAE</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.581774</td>
      <td>0.338461</td>
      <td>0.159277</td>
      <td>27.412817</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.581916</td>
      <td>0.338626</td>
      <td>0.151122</td>
      <td>51.123055</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.752527</td>
      <td>0.566297</td>
      <td>0.483684</td>
      <td>28.099232</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.758053</td>
      <td>0.574645</td>
      <td>0.483228</td>
      <td>35.801286</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.958890</td>
      <td>0.919471</td>
      <td>0.669135</td>
      <td>27.464905</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.932710</td>
      <td>0.869947</td>
      <td>0.639812</td>
      <td>51.920173</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adadelta</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>1.118657</td>
      <td>1.251392</td>
      <td>0.923693</td>
      <td>29.166857</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adadelta</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>1.103161</td>
      <td>1.216963</td>
      <td>0.908986</td>
      <td>52.628776</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.584879</td>
      <td>0.342083</td>
      <td>0.161601</td>
      <td>27.472851</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.582802</td>
      <td>0.339658</td>
      <td>0.153627</td>
      <td>51.269109</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adamax</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.752643</td>
      <td>0.566471</td>
      <td>0.484742</td>
      <td>28.038204</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>sigmoid</td>
      <td>adamax</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.751879</td>
      <td>0.565323</td>
      <td>0.482665</td>
      <td>52.554424</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.586655</td>
      <td>0.344164</td>
      <td>0.171474</td>
      <td>34.469355</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.585573</td>
      <td>0.342896</td>
      <td>0.165658</td>
      <td>103.103569</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.757094</td>
      <td>0.573191</td>
      <td>0.495606</td>
      <td>50.677530</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>sigmoid</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.766000</td>
      <td>0.586757</td>
      <td>0.496910</td>
      <td>104.571928</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.966205</td>
      <td>0.933551</td>
      <td>0.670099</td>
      <td>50.471102</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>tanh</td>
      <td>adadelta</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.894303</td>
      <td>0.799777</td>
      <td>0.593011</td>
      <td>103.204283</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>sigmoid</td>
      <td>adadelta</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>1.119369</td>
      <td>1.252988</td>
      <td>0.924787</td>
      <td>51.198771</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>3h</td>
      <td>sigmoid</td>
      <td>adadelta</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>1.093491</td>
      <td>1.195723</td>
      <td>0.899046</td>
      <td>102.731944</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">list_results</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;RMSE&#39;</span><span class="p">,</span> <span class="s1">&#39;sequence&#39;</span><span class="p">])</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[22]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>sequence</th>
      <th>activationDense</th>
      <th>optimizer</th>
      <th>dropout1</th>
      <th>units</th>
      <th>epochs</th>
      <th>batchsize</th>
      <th>validation_split</th>
      <th>RMSE</th>
      <th>MSE</th>
      <th>MAE</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>GRU</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.581650</td>
      <td>0.338317</td>
      <td>0.154136</td>
      <td>58.817262</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.581774</td>
      <td>0.338461</td>
      <td>0.159277</td>
      <td>27.412817</td>
    </tr>
    <tr>
      <th>0</th>
      <td>GRU</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.581867</td>
      <td>0.338569</td>
      <td>0.154247</td>
      <td>58.510861</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>0.581916</td>
      <td>0.338626</td>
      <td>0.151122</td>
      <td>51.123055</td>
    </tr>
    <tr>
      <th>0</th>
      <td>GRU</td>
      <td>1h</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>0.582345</td>
      <td>0.339125</td>
      <td>0.160290</td>
      <td>30.322881</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>7d</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>1.146864</td>
      <td>1.315297</td>
      <td>0.643848</td>
      <td>1682.198702</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>7d</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.405196</td>
      <td>40</td>
      <td>68</td>
      <td>45</td>
      <td>0.1</td>
      <td>1.154867</td>
      <td>1.333718</td>
      <td>0.664537</td>
      <td>1262.185469</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>7d</td>
      <td>tanh</td>
      <td>adam</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>1.158497</td>
      <td>1.342116</td>
      <td>0.648923</td>
      <td>2014.745057</td>
    </tr>
    <tr>
      <th>0</th>
      <td>LSTM</td>
      <td>7d</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>1.160135</td>
      <td>1.345913</td>
      <td>0.653456</td>
      <td>1424.883327</td>
    </tr>
    <tr>
      <th>0</th>
      <td>GRU</td>
      <td>7d</td>
      <td>tanh</td>
      <td>adamax</td>
      <td>0.118148</td>
      <td>55</td>
      <td>36</td>
      <td>9</td>
      <td>0.2</td>
      <td>1.163293</td>
      <td>1.353250</td>
      <td>0.656830</td>
      <td>1178.389319</td>
    </tr>
  </tbody>
</table>
<p>168 rows × 13 columns</p>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[&nbsp;]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[&nbsp;]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div>
</body>







</html>
