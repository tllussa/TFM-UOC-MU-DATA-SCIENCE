{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cercador model òptim de DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lMCuRV3SJK6w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dj_kr\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from time import time\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d9z8yxsJK6x"
   },
   "source": [
    "## Càrrega de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "seq5Q3gjJK6y",
    "outputId": "72e82bf7-9c19-4d0d-da4a-42c4aff349b3"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/SentDATA.csv')\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df = df.set_index('Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSvxlfJ5JK6y"
   },
   "source": [
    "## Anàlisis estadístic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VLv_DJhJK6y"
   },
   "source": [
    "## Transformació de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NUx3mg0RJK6y"
   },
   "outputs": [],
   "source": [
    "columns = ['PM1','PM25','PM10','PM1ATM','PM25ATM','PM10ATM']\n",
    "\n",
    "df1 = df.copy();\n",
    "\n",
    "df1 = df1.rename(columns={\"PM 1\":\"PM1\",\"PM 2.5\":\"PM25\",\"PM 10\":\"PM10\",\"PM 1 ATM\":\"PM1ATM\",\"PM 2.5 ATM\":\"PM25ATM\",\"PM 10 ATM\":\"PM10ATM\"})\n",
    "\n",
    "df1['PM1'] = df['PM 1'].astype(np.float32)\n",
    "df1['PM25'] = df['PM 2.5'].astype(np.float32)\n",
    "df1['PM10'] = df['PM 10'].astype(np.float32)\n",
    "df1['PM1ATM'] = df['PM 1 ATM'].astype(np.float32)\n",
    "df1['PM25ATM'] = df['PM 2.5 ATM'].astype(np.float32)\n",
    "df1['PM10ATM'] = df['PM 10 ATM'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eh0Xz9k-IHtg"
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLJVxlmlJK6z"
   },
   "source": [
    "## Crear dades d'entrenament i de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1Dwm-E9JK6z",
    "outputId": "620f3cbe-6354-40b1-88c3-a80206b571f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3991, 7), (998, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(df2) * 0.8)\n",
    "test_size = len(df2) - train_size\n",
    "train, test = df2.iloc[0:train_size], df2.iloc[train_size:len(df2)]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuMi9G1LJK6z"
   },
   "source": [
    "## Normalitzar les dades d'entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqcC5lWvJK6z",
    "outputId": "ca0f0c98-e1f3-4e86-dc5f-65dcb992a209"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c8a1383fd1da>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[col] = scaler.fit_transform(train[[col]])\n",
      "<ipython-input-6-c8a1383fd1da>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[col] = scaler.fit_transform(train[[col]])\n",
      "<ipython-input-6-c8a1383fd1da>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[col] = scaler.fit_transform(train[[col]])\n",
      "<ipython-input-6-c8a1383fd1da>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[col] = scaler.fit_transform(train[[col]])\n",
      "<ipython-input-6-c8a1383fd1da>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[col] = scaler.fit_transform(train[[col]])\n",
      "<ipython-input-6-c8a1383fd1da>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[col] = scaler.fit_transform(train[[col]])\n"
     ]
    }
   ],
   "source": [
    "#Standardize the data\n",
    "for col in columns:\n",
    "    scaler = MinMaxScaler()\n",
    "    train[col] = scaler.fit_transform(train[[col]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LsNLCHSJK6z"
   },
   "source": [
    "## Crear finestra de temps PM 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEH4EmxcJK6z",
    "outputId": "b8c5f2c4-adca-47ef-fb6b-c734f0160dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1h shape: (3847, 144, 1)\n",
      "X_train3d shape: (3973, 18, 1)\n",
      "X_train6h shape: (3955, 36, 1)\n",
      "X_train12h shape: (3919, 72, 1)\n",
      "X_train1d shape: (3847, 144, 1)\n",
      "X_train3d shape: (3559, 432, 1)\n",
      "X_train7d shape: (2983, 1008, 1)\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS=144 #6 registres hora x 24h x 3 --> equival a una finestra d'un dia\n",
    "\n",
    "def create_sequences(X, y, time_steps=TIME_STEPS):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        Xs.append(X.iloc[i:(i+time_steps)].values)\n",
    "        ys.append(y.iloc[i+time_steps])\n",
    "    \n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train1h, y_train1h = create_sequences(train[[columns[1]]], train[columns[1]], 6) #1 hour\n",
    "\n",
    "X_train3h, y_train3h = create_sequences(train[[columns[1]]], train[columns[1]], 18) #3 hours\n",
    "\n",
    "X_train6h, y_train6h = create_sequences(train[[columns[1]]], train[columns[1]], 36) #6 hours\n",
    "\n",
    "X_train12h, y_train12h = create_sequences(train[[columns[1]]], train[columns[1]], 72) #12 hours\n",
    "\n",
    "X_train1d, y_train1d = create_sequences(train[[columns[1]]], train[columns[1]], 144) #1 day\n",
    "\n",
    "X_train3d, y_train3d = create_sequences(train[[columns[1]]], train[columns[1]], 432) #3 days\n",
    "\n",
    "X_train7d, y_train7d = create_sequences(train[[columns[1]]], train[columns[1]], 1008) #7 days\n",
    "#X_test, y_test = create_sequences(test[[columns[1]]], test[columns[1]])\n",
    "\n",
    "print(f'X_train1h shape: {X_train1d.shape}')\n",
    "print(f'X_train3d shape: {X_train3h.shape}')\n",
    "print(f'X_train6h shape: {X_train6h.shape}')\n",
    "print(f'X_train12h shape: {X_train12h.shape}')\n",
    "print(f'X_train1d shape: {X_train1d.shape}')\n",
    "print(f'X_train3d shape: {X_train3d.shape}')\n",
    "print(f'X_train7d shape: {X_train7d.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "\n",
    "    print(model_name + ':')\n",
    "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
    "    print('Root Mean Square Error: {:.4f}'.format(rmse))\n",
    "    print('Mean Square Error: {:.4f}'.format(mse))\n",
    "    print('')\n",
    "    return mae,rmse,mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9caadjyJK6z"
   },
   "source": [
    "## Cerca dels models òptims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7znr_i72OjVb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 6, 97)             194       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6, 16)             1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      " 1/84 [..............................] - ETA: 0s - loss: 0.4239WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0147s). Check your callbacks.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2356 - val_loss: 0.0544\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1553 - val_loss: 0.0700\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.0451\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0901 - val_loss: 0.0229\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0587 - val_loss: 0.0059\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0107\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0145\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0140\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0142\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0147\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0148\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0145\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0147\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0147\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0148\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0148\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0150\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0148\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0149\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0150\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0159\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0153\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0162\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0156\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0159\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0155\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0158\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0153\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0143\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0155\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0154\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0145\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0144\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0145\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0151\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0149\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0151\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0146\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0143\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0143\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0145\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0142\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0142\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0142\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0140\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0140\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0139\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0142\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0132\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0129\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0124\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0137\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0135\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0141\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0136\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0132\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0138\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0132\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0135\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0133\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0131\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0134\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0131\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0133\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0133\n",
      "Epoch 70/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0139\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0134\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0140\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0134\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0137\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0138\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0136\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0135\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0156\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0129\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0118\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0132\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0117\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0120\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0118\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0113\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0114\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0112\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0113\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0105\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0114\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0115\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0116\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0114\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0113\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0105\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0104\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0106\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0104\n",
      "Execution time:  17.585649251937866\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0190\n",
      "Root Mean Square Error: 0.0301\n",
      "Mean Square Error: 0.0009\n",
      "\n",
      "Train RMSE: 0.030\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.019\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 6, 22)             44        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6, 16)             368       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "139/139 [==============================] - 0s 3ms/step - loss: 0.2045 - val_loss: 0.0861\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1164 - val_loss: 0.0387\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.0189\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0161\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0301 - val_loss: 0.0161\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 0.0164\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0305 - val_loss: 0.0170\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0301 - val_loss: 0.0168\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0162\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0299 - val_loss: 0.0170\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0299 - val_loss: 0.0169\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0298 - val_loss: 0.0170\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0170\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0170\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0170\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0169\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0290 - val_loss: 0.0168\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0290 - val_loss: 0.0174\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0168\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0168\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0168\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0166\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0165\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0163\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0162\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0154\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0159\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0262 - val_loss: 0.0164\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0155\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0153\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0147\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0257 - val_loss: 0.0145\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0142\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0143\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0249 - val_loss: 0.0142\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0144\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0145\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0142\n",
      "Epoch 41/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0142\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0141\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0144\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0142\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0140\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0142\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0140\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0140\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0141\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0139\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0139\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0250 - val_loss: 0.0142\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0241 - val_loss: 0.0138\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0247 - val_loss: 0.0141\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0141\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0150\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0139\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0141\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0138\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0141\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0139\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0139\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0246 - val_loss: 0.0141\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0240 - val_loss: 0.0139\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0140\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0247 - val_loss: 0.0141\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0138\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0138\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0138\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0246 - val_loss: 0.0141\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0139\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0140\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0141\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0140\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0140\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0139\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0140\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0139\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 0.0139\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0139\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0140\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0240 - val_loss: 0.0142\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0241 - val_loss: 0.0137\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0140\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0139\n",
      "Execution time:  19.619282722473145\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0220\n",
      "Root Mean Square Error: 0.0347\n",
      "Mean Square Error: 0.0012\n",
      "\n",
      "Train RMSE: 0.035\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.022\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 6, 97)             194       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6, 16)             1568      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0658 - val_loss: 0.0574\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.0501\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0491 - val_loss: 0.0446\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0403\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0356\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0329\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0311\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0254\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0252\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0231\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0209\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0198\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0197\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0186\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0183\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0172\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0158\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0164\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0164\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0156\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0164\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0150\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0165\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0163\n",
      "Epoch 25/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0153\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0165\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0150\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0150\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0155\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0158\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0155\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0158\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0157\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0159\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0158\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0158\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0159\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0157\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0162\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0160\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0163\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0163\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0166\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0158\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0165\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0164\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0163\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0164\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0166\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0165\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0167\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0164\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0167\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0160\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0162\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0155\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0161\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0157\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0162\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0166\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0162\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0156\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0158\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0162\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0161\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0150\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0156\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0158\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0160\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0146\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0163\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0157\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0159\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0162\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0161\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0158\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0162\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0163\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0158\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0165\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0164\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0156\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0159\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0163\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0160\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0159\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0172\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0154\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0161\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0164\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0159\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0155\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0156\n",
      "Execution time:  16.172645568847656\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0201\n",
      "Root Mean Square Error: 0.0331\n",
      "Mean Square Error: 0.0011\n",
      "\n",
      "Train RMSE: 0.033\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.020\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 6, 22)             44        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6, 16)             368       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/139 [..............................] - ETA: 0s - loss: 0.0709WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "139/139 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.0327\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0270\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0432 - val_loss: 0.0284\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0382 - val_loss: 0.0247\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 0.0235\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0322 - val_loss: 0.0247\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0297 - val_loss: 0.0200\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0293 - val_loss: 0.0211\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0287 - val_loss: 0.0196\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0285 - val_loss: 0.0192\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0286 - val_loss: 0.0195\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0183\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0281 - val_loss: 0.0184\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Execution time:  18.85319757461548\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0262\n",
      "Root Mean Square Error: 0.0431\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 6, 97)             194       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 6, 16)             1568      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4446 - val_loss: 0.4052\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4455 - val_loss: 0.4033\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4408 - val_loss: 0.4013\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4395 - val_loss: 0.3991\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4374 - val_loss: 0.3969\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4356 - val_loss: 0.3945\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4309 - val_loss: 0.3920\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4290 - val_loss: 0.3895\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4286 - val_loss: 0.3868\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4249 - val_loss: 0.3841\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4210 - val_loss: 0.3813\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4186 - val_loss: 0.3784\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4162 - val_loss: 0.3755\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4117 - val_loss: 0.3724\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.3688\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4059 - val_loss: 0.3646\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4006 - val_loss: 0.3605\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3969 - val_loss: 0.3564\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3917 - val_loss: 0.3521\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3872 - val_loss: 0.3479\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3436\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3789 - val_loss: 0.3392\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3752 - val_loss: 0.3348\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3698 - val_loss: 0.3303\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3651 - val_loss: 0.3257\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3221\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3570 - val_loss: 0.3185\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3532 - val_loss: 0.3148\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3485 - val_loss: 0.3111\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3449 - val_loss: 0.3074\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3411 - val_loss: 0.3036\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.2998\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3336 - val_loss: 0.2959\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3301 - val_loss: 0.2920\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3253 - val_loss: 0.2882\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3217 - val_loss: 0.2843\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3187 - val_loss: 0.2804\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3130 - val_loss: 0.2765\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3103 - val_loss: 0.2726\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3049 - val_loss: 0.2687\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3027 - val_loss: 0.2648\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2992 - val_loss: 0.2609\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2952 - val_loss: 0.2571\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2898 - val_loss: 0.2533\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2878 - val_loss: 0.2496\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2842 - val_loss: 0.2459\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2786 - val_loss: 0.2427\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2760 - val_loss: 0.2398\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2748 - val_loss: 0.2371\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2702 - val_loss: 0.2345\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2700 - val_loss: 0.2320\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2685 - val_loss: 0.2297\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2661 - val_loss: 0.2274\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2637 - val_loss: 0.2252\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2633 - val_loss: 0.2233\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2627 - val_loss: 0.2215\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2614 - val_loss: 0.2199\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2597 - val_loss: 0.2182\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2599 - val_loss: 0.2167\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2588 - val_loss: 0.2151\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2586 - val_loss: 0.2136\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2577 - val_loss: 0.2121\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2564 - val_loss: 0.2106\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2540 - val_loss: 0.2091\n",
      "Epoch 65/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2542 - val_loss: 0.2076\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2510 - val_loss: 0.2060\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2531 - val_loss: 0.2045\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2518 - val_loss: 0.2030\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2511 - val_loss: 0.2015\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2506 - val_loss: 0.2000\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2496 - val_loss: 0.1985\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2488 - val_loss: 0.1969\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2481 - val_loss: 0.1953\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2489 - val_loss: 0.1938\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2474 - val_loss: 0.1921\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2456 - val_loss: 0.1905\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2457 - val_loss: 0.1888\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2435 - val_loss: 0.1871\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2447 - val_loss: 0.1854\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2437 - val_loss: 0.1837\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2410 - val_loss: 0.1818\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2424 - val_loss: 0.1801\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2409 - val_loss: 0.1783\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2410 - val_loss: 0.1764\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2373 - val_loss: 0.1746\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2370 - val_loss: 0.1727\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2358 - val_loss: 0.1709\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2370 - val_loss: 0.1690\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2344 - val_loss: 0.1672\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2330 - val_loss: 0.1652\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2319 - val_loss: 0.1633\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2302 - val_loss: 0.1614\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2322 - val_loss: 0.1597\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2296 - val_loss: 0.1579\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2283 - val_loss: 0.1562\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2293 - val_loss: 0.1544\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2264 - val_loss: 0.1528\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1511\n",
      "Execution time:  15.930931329727173\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1752\n",
      "Root Mean Square Error: 0.1779\n",
      "Mean Square Error: 0.0317\n",
      "\n",
      "Train RMSE: 0.178\n",
      "Train MSE: 0.032\n",
      "Train MAE: 0.175\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 6, 22)             44        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 6, 16)             368       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "139/139 [==============================] - 1s 4ms/step - loss: 0.5810 - val_loss: 0.5671\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.5636\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.5753 - val_loss: 0.5598\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.5742 - val_loss: 0.5557\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.5658 - val_loss: 0.5514\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5671 - val_loss: 0.5468\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5633 - val_loss: 0.5421\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5591 - val_loss: 0.5371\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.5320\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5511 - val_loss: 0.5268\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5437 - val_loss: 0.5214\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5400 - val_loss: 0.5159\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5356 - val_loss: 0.5103\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.5046\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 0.4987\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5215 - val_loss: 0.4927\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.5159 - val_loss: 0.4866\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5103 - val_loss: 0.4805\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.5025 - val_loss: 0.4742\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4988 - val_loss: 0.4680\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.4619\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4902 - val_loss: 0.4560\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4846 - val_loss: 0.4501\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4823 - val_loss: 0.4442\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4733 - val_loss: 0.4382\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4663 - val_loss: 0.4324\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4606 - val_loss: 0.4266\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4583 - val_loss: 0.4208\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4543 - val_loss: 0.4150\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4495 - val_loss: 0.4093\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4441 - val_loss: 0.4035\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.3978\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4320 - val_loss: 0.3920\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.3862\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.3805\n",
      "Epoch 36/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4188 - val_loss: 0.3748\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4152 - val_loss: 0.3689\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4117 - val_loss: 0.3631\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.4023 - val_loss: 0.3574\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4028 - val_loss: 0.3518\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3969 - val_loss: 0.3462\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3884 - val_loss: 0.3406\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.3352\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.3298\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.3245\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3712 - val_loss: 0.3193\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3144\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3629 - val_loss: 0.3097\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3053\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.3013\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.2976\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.2938\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.2902\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3391 - val_loss: 0.2867\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3394 - val_loss: 0.2832\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3355 - val_loss: 0.2798\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3327 - val_loss: 0.2764\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3302 - val_loss: 0.2730\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3280 - val_loss: 0.2696\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.2662\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.2628\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3175 - val_loss: 0.2595\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3141 - val_loss: 0.2562\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3103 - val_loss: 0.2530\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.2496\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3048 - val_loss: 0.2464\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.3020 - val_loss: 0.2432\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2993 - val_loss: 0.2398\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2973 - val_loss: 0.2363\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2931 - val_loss: 0.2326\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2912 - val_loss: 0.2290\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.2253\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2822 - val_loss: 0.2215\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2811 - val_loss: 0.2178\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2773 - val_loss: 0.2141\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2766 - val_loss: 0.2103\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2701 - val_loss: 0.2067\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2698 - val_loss: 0.2031\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2651 - val_loss: 0.1995\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2600 - val_loss: 0.1960\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2582 - val_loss: 0.1925\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2567 - val_loss: 0.1890\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2516 - val_loss: 0.1856\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2516 - val_loss: 0.1823\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.2466 - val_loss: 0.1791\n",
      "Execution time:  18.863140106201172\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1957\n",
      "Root Mean Square Error: 0.1995\n",
      "Mean Square Error: 0.0398\n",
      "\n",
      "Train RMSE: 0.200\n",
      "Train MSE: 0.040\n",
      "Train MAE: 0.196\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 6, 97)             194       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 6, 16)             1568      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1222 - val_loss: 0.1534\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.1526\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.1518\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.1510\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1186 - val_loss: 0.1501\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1173 - val_loss: 0.1493\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1167 - val_loss: 0.1486\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1165 - val_loss: 0.1480\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1152 - val_loss: 0.1473\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1153 - val_loss: 0.1466\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1140 - val_loss: 0.1459\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1134 - val_loss: 0.1452\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.1444\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1121 - val_loss: 0.1437\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 0.1429\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 0.1421\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1103 - val_loss: 0.1413\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1093 - val_loss: 0.1405\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1083 - val_loss: 0.1397\n",
      "Epoch 20/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1075 - val_loss: 0.1389\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1067 - val_loss: 0.1381\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.1372\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1364\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.1356\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.1347\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1026 - val_loss: 0.1339\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1019 - val_loss: 0.1330\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1010 - val_loss: 0.1322\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1003 - val_loss: 0.1313\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.1304\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.1296\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0979 - val_loss: 0.1287\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0967 - val_loss: 0.1278\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0960 - val_loss: 0.1270\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0952 - val_loss: 0.1261\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0942 - val_loss: 0.1252\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0938 - val_loss: 0.1244\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0930 - val_loss: 0.1235\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0922 - val_loss: 0.1227\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0914 - val_loss: 0.1218\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0905 - val_loss: 0.1210\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0903 - val_loss: 0.1204\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0897 - val_loss: 0.1197\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.1191\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0879 - val_loss: 0.1184\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0878 - val_loss: 0.1178\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0867 - val_loss: 0.1172\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.1168\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.1164\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.1159\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0852 - val_loss: 0.1155\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0850 - val_loss: 0.1151\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0849 - val_loss: 0.1147\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.1143\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0839 - val_loss: 0.1139\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0839 - val_loss: 0.1135\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0833 - val_loss: 0.1131\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0830 - val_loss: 0.1127\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.1123\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0824 - val_loss: 0.1119\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0823 - val_loss: 0.1115\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0816 - val_loss: 0.1111\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0818 - val_loss: 0.1107\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0811 - val_loss: 0.1103\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0809 - val_loss: 0.1099\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0809 - val_loss: 0.1095\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0802 - val_loss: 0.1091\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.1089\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0803 - val_loss: 0.1086\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0795 - val_loss: 0.1084\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0794 - val_loss: 0.1082\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0791 - val_loss: 0.1079\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0789 - val_loss: 0.1077\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0787 - val_loss: 0.1075\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.1073\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0788 - val_loss: 0.1071\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0787 - val_loss: 0.1069\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0789 - val_loss: 0.1067\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0785 - val_loss: 0.1065\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0785 - val_loss: 0.1063\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0783 - val_loss: 0.1060\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0781 - val_loss: 0.1058\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.1056\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0774 - val_loss: 0.1054\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.1052\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0777 - val_loss: 0.1050\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.1049\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.1047\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.1045\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0770 - val_loss: 0.1043\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.1041\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0764 - val_loss: 0.1039\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0770 - val_loss: 0.1037\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0768 - val_loss: 0.1036\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0766 - val_loss: 0.1034\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0765 - val_loss: 0.1033\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0762 - val_loss: 0.1031\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0763 - val_loss: 0.1030\n",
      "Execution time:  16.35512399673462\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0736\n",
      "Root Mean Square Error: 0.0810\n",
      "Mean Square Error: 0.0066\n",
      "\n",
      "Train RMSE: 0.081\n",
      "Train MSE: 0.007\n",
      "Train MAE: 0.074\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 6, 22)             44        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 6, 16)             368       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 0.1071\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0860 - val_loss: 0.1068\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0853 - val_loss: 0.1064\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0852 - val_loss: 0.1061\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0847 - val_loss: 0.1057\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0844 - val_loss: 0.1054\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.1050\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0838 - val_loss: 0.1046\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.1042\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0830 - val_loss: 0.1038\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0828 - val_loss: 0.1033\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.1029\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.1025\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.1021\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0812 - val_loss: 0.1016\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.1012\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.1007\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.1003\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 0.0998\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0794 - val_loss: 0.0994\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0788 - val_loss: 0.0990\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0788 - val_loss: 0.0985\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0781 - val_loss: 0.0981\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0779 - val_loss: 0.0977\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0771 - val_loss: 0.0972\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0769 - val_loss: 0.0968\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0767 - val_loss: 0.0964\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0767 - val_loss: 0.0960\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0760 - val_loss: 0.0957\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0757 - val_loss: 0.0953\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0754 - val_loss: 0.0949\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0753 - val_loss: 0.0945\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0750 - val_loss: 0.0942\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.0938\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0747 - val_loss: 0.0934\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0742 - val_loss: 0.0931\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0740 - val_loss: 0.0927\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0735 - val_loss: 0.0924\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0731 - val_loss: 0.0921\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0735 - val_loss: 0.0917\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0726 - val_loss: 0.0914\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0723 - val_loss: 0.0911\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0723 - val_loss: 0.0907\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0720 - val_loss: 0.0904\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0722 - val_loss: 0.0901\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0716 - val_loss: 0.0898\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0718 - val_loss: 0.0895\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0711 - val_loss: 0.0892\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0710 - val_loss: 0.0889\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0713 - val_loss: 0.0886\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0707 - val_loss: 0.0883\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0707 - val_loss: 0.0880\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0702 - val_loss: 0.0877\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0695 - val_loss: 0.0874\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0698 - val_loss: 0.0871\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0696 - val_loss: 0.0868\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0695 - val_loss: 0.0866\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0696 - val_loss: 0.0863\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0691 - val_loss: 0.0860\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0693 - val_loss: 0.0857\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0686 - val_loss: 0.0855\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0686 - val_loss: 0.0852\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0682 - val_loss: 0.0850\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0684 - val_loss: 0.0847\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0683 - val_loss: 0.0845\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0680 - val_loss: 0.0842\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0679 - val_loss: 0.0840\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0675 - val_loss: 0.0838\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0675 - val_loss: 0.0835\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0671 - val_loss: 0.0833\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0670 - val_loss: 0.0831\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0669 - val_loss: 0.0828\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0671 - val_loss: 0.0826\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0667 - val_loss: 0.0824\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0668 - val_loss: 0.0822\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0665 - val_loss: 0.0819\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0660 - val_loss: 0.0817\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0664 - val_loss: 0.0815\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0666 - val_loss: 0.0813\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0660 - val_loss: 0.0811\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0658 - val_loss: 0.0808\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0659 - val_loss: 0.0806\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0657 - val_loss: 0.0804\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.0802\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.0800\n",
      "Execution time:  18.562690019607544\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0632\n",
      "Root Mean Square Error: 0.0711\n",
      "Mean Square Error: 0.0051\n",
      "\n",
      "Train RMSE: 0.071\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.063\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 6, 97)             194       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 6, 16)             1568      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2388 - val_loss: 0.0687\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1715 - val_loss: 0.0562\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1427 - val_loss: 0.0687\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1175 - val_loss: 0.0500\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.0326\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0732 - val_loss: 0.0151\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0055\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0076\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0117\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0156\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0152\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0167\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0154\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0160\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0162\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0164\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0157\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0156\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0159\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0152\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0160\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0157\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0156\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0162\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0156\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0161\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0158\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0160\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0161\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0161\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0161\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0159\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0161\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0156\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0158\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0158\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0153\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0156\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0152\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0145\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0142\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0140\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0140\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0134\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0137\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0124\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0116\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0121\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0119\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0114\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0114\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0115\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0113\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0121\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0114\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0111\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0112\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0110\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0112\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0115\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0111\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0112\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0106\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0105\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0105\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0106\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0111\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0110\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0112\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0111\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0109\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0109\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0111\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0110\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0116\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0122\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0113\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0114\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0114\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0111\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0109\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0108\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0111\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0108\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0108\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0109\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0111\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0108\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0113\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0113\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0111\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0109\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0108\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0106\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0107\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0105\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0105\n",
      "Execution time:  16.440959215164185\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0161\n",
      "Root Mean Square Error: 0.0263\n",
      "Mean Square Error: 0.0007\n",
      "\n",
      "Train RMSE: 0.026\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.016\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 6, 22)             44        \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 6, 16)             368       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "139/139 [==============================] - 0s 3ms/step - loss: 0.2141 - val_loss: 0.0603\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.1474 - val_loss: 0.0282\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.1061 - val_loss: 0.0192\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0623 - val_loss: 0.0148\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 0.0157\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0294 - val_loss: 0.0165\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0292 - val_loss: 0.0156\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0291 - val_loss: 0.0165\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0286 - val_loss: 0.0168\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0289 - val_loss: 0.0168\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0289 - val_loss: 0.0169\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0287 - val_loss: 0.0161\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0287 - val_loss: 0.0169\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0279 - val_loss: 0.0162\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0285 - val_loss: 0.0161\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0166\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0278 - val_loss: 0.0166\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0276 - val_loss: 0.0150\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0272 - val_loss: 0.0162\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0260 - val_loss: 0.0153\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0262 - val_loss: 0.0155\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0263 - val_loss: 0.0153\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0255 - val_loss: 0.0148\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0250 - val_loss: 0.0147\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0253 - val_loss: 0.0145\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0246 - val_loss: 0.0144\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0142\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0143\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0143\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0140\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0140\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0136\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0136\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0137\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0137\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0134\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0132\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0134\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0132\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0131\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0128\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0130\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0128\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0126\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0130\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0131\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0128\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0131\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0130\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0130\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0126\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0127\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0129\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0128\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0128\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0126\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0129\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0131\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0127\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0129\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0127\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0127\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0129\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0130\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0129\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.0128\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0126\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0129\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0129\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0126\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0125\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0124\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.0129\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0129\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0126\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0126\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0128\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0129\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0127\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0127\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0125\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0123\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0125\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0126\n",
      "Execution time:  18.20118761062622\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0183\n",
      "Root Mean Square Error: 0.0281\n",
      "Mean Square Error: 0.0008\n",
      "\n",
      "Train RMSE: 0.028\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.018\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 6, 97)             194       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 6, 16)             1568      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0660 - val_loss: 0.0694\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0586\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0524 - val_loss: 0.0510\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0484 - val_loss: 0.0464\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0448 - val_loss: 0.0429\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0390\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0353\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.0330\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.0304\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 0.0288\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0287\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0261\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0234\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0244\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0243\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0252\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0231\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0218\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0206\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0193\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0191\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0190\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0190\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0189\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0189\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0189\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0189\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0187\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0188\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0186\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0187\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0187\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0186\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0184\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0185\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0184\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0185\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0185\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0182\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0180\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0180\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0177\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0175\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0170\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0156\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0148\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0147\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0146\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0145\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0144\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0145\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0143\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0142\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0142\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0141\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0140\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0140\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0139\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0139\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0138\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0139\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0136\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0136\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0137\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0136\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0136\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0135\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0136\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0138\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0135\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0135\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0137\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0139\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0138\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0141\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0138\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0140\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0138\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0143\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0142\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0142\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0136\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0148\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0152\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0144\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0147\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0149\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0150\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0151\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0144\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0149\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0142\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0145\n",
      "Execution time:  16.130773067474365\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0163\n",
      "Root Mean Square Error: 0.0260\n",
      "Mean Square Error: 0.0007\n",
      "\n",
      "Train RMSE: 0.026\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.016\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 6, 22)             44        \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 6, 16)             368       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 6, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 6, 1)              17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 3ms/step - loss: 0.0657 - val_loss: 0.0542\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0435\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0433 - val_loss: 0.0384\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0394 - val_loss: 0.0346\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 0.0318\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0341 - val_loss: 0.0284\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0327 - val_loss: 0.0265\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0312 - val_loss: 0.0242\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0304 - val_loss: 0.0225\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0297 - val_loss: 0.0219\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0290 - val_loss: 0.0206\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0287 - val_loss: 0.0199\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0192\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0189\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0281 - val_loss: 0.0186\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0280 - val_loss: 0.0183\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0280 - val_loss: 0.0187\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0277 - val_loss: 0.0184\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0278 - val_loss: 0.0182\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0277 - val_loss: 0.0181\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0276 - val_loss: 0.0181\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0180\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0274 - val_loss: 0.0179\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0273 - val_loss: 0.0179\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0272 - val_loss: 0.0179\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0270 - val_loss: 0.0178\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0269 - val_loss: 0.0177\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0268 - val_loss: 0.0176\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0266 - val_loss: 0.0175\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0264 - val_loss: 0.0175\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0264 - val_loss: 0.0173\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.0172\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0259 - val_loss: 0.0171\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0256 - val_loss: 0.0169\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0251 - val_loss: 0.0167\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0250 - val_loss: 0.0165\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0248 - val_loss: 0.0165\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0164\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0163\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0162\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0235 - val_loss: 0.0161\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0159\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0159\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0157\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0156\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0155\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0153\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0151\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0150\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0210 - val_loss: 0.0148\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0145\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0143\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0139\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0138\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0136\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0134\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0134\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0200 - val_loss: 0.0133\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0131\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0132\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.0130\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0127\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0126\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0126\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 0.0125\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0123\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.0123\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0125\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0123\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0123\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.0125\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0118\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0123\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0122\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0124\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0123\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0124\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0121\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0118\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0120\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.0120\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0121\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 0.0123\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 1ms/step - loss: 0.0194 - val_loss: 0.0121\n",
      "Execution time:  18.778048515319824\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0141\n",
      "Root Mean Square Error: 0.0226\n",
      "Mean Square Error: 0.0005\n",
      "\n",
      "Train RMSE: 0.023\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.014\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 18, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 18, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2964 - val_loss: 0.1550\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1993 - val_loss: 0.1037\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1407 - val_loss: 0.0506\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0977 - val_loss: 0.0137\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.0053\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.0138\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0164\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0151\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0149\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0148\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0154\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0152\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0161\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0159\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0157\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0156\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0157\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0157\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0156\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0156\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0156\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0156\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Execution time:  22.761476516723633\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0263\n",
      "Root Mean Square Error: 0.0445\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.045\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 18, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 18, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "139/139 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 0.0858\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1213 - val_loss: 0.0567\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0734 - val_loss: 0.0227\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0159\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0165\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0163\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0165\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0167\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0168\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0169\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0166\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0168\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0170\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0167\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0168\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0170\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0169\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0169\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0170\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0170\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0169\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0169\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0170\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0170\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0169\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0169\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0167\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0167\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0160\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0160\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0157\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0156\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0157\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0154\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0154\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0156\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0156\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0156\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0155\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0154\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0151\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0154\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0154\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0154\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0154\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0152\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0154\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0153\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0151\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0154\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0152\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0151\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0153\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0152\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0151\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0153\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0154\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0152\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0152\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0152\n",
      "Execution time:  22.262794971466064\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0241\n",
      "Root Mean Square Error: 0.0405\n",
      "Mean Square Error: 0.0016\n",
      "\n",
      "Train RMSE: 0.040\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.024\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 18, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 18, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0646 - val_loss: 0.0559\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.0500\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.0446\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.0393\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0354\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0322\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0283\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0253\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0248\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0251\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0240\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0210\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0213\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0203\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0184\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0187\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0184\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0185\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0185\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0184\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0181\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0185\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0181\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0181\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0180\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0180\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0179\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0179\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0178\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0177\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0174\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0175\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0175\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0172\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0166\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0164\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0173\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0175\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0175\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0175\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0173\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0172\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0170\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0168\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0163\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0173\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0173\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0173\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0174\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0170\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0173\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0174\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0171\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0174\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0172\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0170\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0170\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0171\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0170\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0173\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0173\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0171\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0176\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0175\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0175\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0176\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0172\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0172\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0173\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0173\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0172\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0176\n",
      "Execution time:  22.712627410888672\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0254\n",
      "Root Mean Square Error: 0.0426\n",
      "Mean Square Error: 0.0018\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 18, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 18, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0396\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0343\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0306\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.0272\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0244\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0229\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0202\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0200\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0205\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0195\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0196\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0193\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0190\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0191\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0189\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0189\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0178\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0188\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0181\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0185\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0186\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0186\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0184\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0184\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0184\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0184\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0184\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0181\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0180\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0181\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0179\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0177\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0182\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0178\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0180\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0179\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0173\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0167\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0181\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0176\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0177\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0176\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0176\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0176\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0175\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0175\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0175\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0175\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0175\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0173\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0172\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0172\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0170\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0170\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0170\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0170\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0169\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0170\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0169\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0170\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0172\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0171\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0171\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0171\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0171\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0171\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0172\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0171\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0172\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.029 - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Execution time:  22.448381662368774\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0235\n",
      "Root Mean Square Error: 0.0399\n",
      "Mean Square Error: 0.0016\n",
      "\n",
      "Train RMSE: 0.040\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.023\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 18, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 18, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3447 - val_loss: 0.3090\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3408 - val_loss: 0.3064\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3396 - val_loss: 0.3037\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3364 - val_loss: 0.3008\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3341 - val_loss: 0.2978\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3320 - val_loss: 0.2946\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3287 - val_loss: 0.2910\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3247 - val_loss: 0.2873\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3213 - val_loss: 0.2835\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3185 - val_loss: 0.2796\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3157 - val_loss: 0.2754\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3134 - val_loss: 0.2711\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3087 - val_loss: 0.2669\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3043 - val_loss: 0.2626\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3009 - val_loss: 0.2584\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2980 - val_loss: 0.2542\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2946 - val_loss: 0.2502\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2915 - val_loss: 0.2462\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 0.2421\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2862 - val_loss: 0.2380\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 0.2342\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 0.2304\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2769 - val_loss: 0.2266\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2746 - val_loss: 0.2229\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2720 - val_loss: 0.2191\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2675 - val_loss: 0.2155\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 0.2117\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2625 - val_loss: 0.2082\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2600 - val_loss: 0.2047\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2577 - val_loss: 0.2013\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2557 - val_loss: 0.1979\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2522 - val_loss: 0.1944\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2508 - val_loss: 0.1909\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2492 - val_loss: 0.1875\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2464 - val_loss: 0.1841\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 0.1807\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2410 - val_loss: 0.1773\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 0.1741\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2368 - val_loss: 0.1708\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2352 - val_loss: 0.1676\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2329 - val_loss: 0.1645\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2309 - val_loss: 0.1614\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2284 - val_loss: 0.1584\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2274 - val_loss: 0.1556\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2261 - val_loss: 0.1530\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2237 - val_loss: 0.1509\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2232 - val_loss: 0.1488\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2222 - val_loss: 0.1469\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2208 - val_loss: 0.1450\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2202 - val_loss: 0.1432\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2174 - val_loss: 0.1414\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2184 - val_loss: 0.1397\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2169 - val_loss: 0.1380\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2159 - val_loss: 0.1363\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2153 - val_loss: 0.1345\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2145 - val_loss: 0.1327\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2131 - val_loss: 0.1309\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 0.1291\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2107 - val_loss: 0.1274\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2105 - val_loss: 0.1258\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 0.1244\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2089 - val_loss: 0.1229\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2081 - val_loss: 0.1215\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2076 - val_loss: 0.1205\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2072 - val_loss: 0.1195\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2072 - val_loss: 0.1185\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2058 - val_loss: 0.1175\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2055 - val_loss: 0.1166\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2041 - val_loss: 0.1157\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2047 - val_loss: 0.1148\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2033 - val_loss: 0.1139\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 0.1129\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2029 - val_loss: 0.1121\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2026 - val_loss: 0.1111\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2012 - val_loss: 0.1102\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2019 - val_loss: 0.1094\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1993 - val_loss: 0.1086\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1994 - val_loss: 0.1077\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 0.1069\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 0.1061\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 0.1053\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1980 - val_loss: 0.1047\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1973 - val_loss: 0.1040\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1970 - val_loss: 0.1034\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1961 - val_loss: 0.1031\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1960 - val_loss: 0.1027\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1948 - val_loss: 0.1024\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 0.1022\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1943 - val_loss: 0.1020\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1935 - val_loss: 0.1019\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1926 - val_loss: 0.1018\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1933 - val_loss: 0.1017\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 0.1016\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 0.1015\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 0.1015\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1912 - val_loss: 0.1016\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1898 - val_loss: 0.1017\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1908 - val_loss: 0.1018\n",
      "Execution time:  22.73730778694153\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1209\n",
      "Root Mean Square Error: 0.1234\n",
      "Mean Square Error: 0.0152\n",
      "\n",
      "Train RMSE: 0.123\n",
      "Train MSE: 0.015\n",
      "Train MAE: 0.121\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 18, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 18, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "  1/139 [..............................] - ETA: 0s - loss: 0.4974WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0103s). Check your callbacks.\n",
      "139/139 [==============================] - 0s 3ms/step - loss: 0.4809 - val_loss: 0.4549\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4777 - val_loss: 0.4524\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4751 - val_loss: 0.4497\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4734 - val_loss: 0.4468\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4693 - val_loss: 0.4437\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4666 - val_loss: 0.4404\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4633 - val_loss: 0.4371\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4603 - val_loss: 0.4342\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4567 - val_loss: 0.4311\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4538 - val_loss: 0.4279\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4505 - val_loss: 0.4246\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4479 - val_loss: 0.4212\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4441 - val_loss: 0.4176\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4401 - val_loss: 0.4139\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4367 - val_loss: 0.4101\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4330 - val_loss: 0.4062\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4282 - val_loss: 0.4022\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4249 - val_loss: 0.3982\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4200 - val_loss: 0.3940\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4163 - val_loss: 0.3897\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4124 - val_loss: 0.3853\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4076 - val_loss: 0.3808\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.4024 - val_loss: 0.3762\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3716\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3929 - val_loss: 0.3668\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3878 - val_loss: 0.3620\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3838 - val_loss: 0.3571\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3790 - val_loss: 0.3521\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3740 - val_loss: 0.3470\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3420\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.370 - 0s 2ms/step - loss: 0.3639 - val_loss: 0.3372\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3588 - val_loss: 0.3323\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3538 - val_loss: 0.3274\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3487 - val_loss: 0.3223\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3440 - val_loss: 0.3173\n",
      "Epoch 36/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3381 - val_loss: 0.3122\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3339 - val_loss: 0.3071\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.3020\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3235 - val_loss: 0.2968\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3195 - val_loss: 0.2917\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3131 - val_loss: 0.2866\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3093 - val_loss: 0.2820\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3059 - val_loss: 0.2779\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.3017 - val_loss: 0.2739\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2976 - val_loss: 0.2699\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2945 - val_loss: 0.2660\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2921 - val_loss: 0.2621\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2879 - val_loss: 0.2581\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2850 - val_loss: 0.2542\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2820 - val_loss: 0.2502\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2780 - val_loss: 0.2464\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2753 - val_loss: 0.2424\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2717 - val_loss: 0.2386\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2688 - val_loss: 0.2347\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2651 - val_loss: 0.2308\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2631 - val_loss: 0.2270\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2605 - val_loss: 0.2234\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2585 - val_loss: 0.2197\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2554 - val_loss: 0.2159\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2529 - val_loss: 0.2122\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2490 - val_loss: 0.2084\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2473 - val_loss: 0.2048\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2442 - val_loss: 0.2013\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2430 - val_loss: 0.1977\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2406 - val_loss: 0.1942\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2376 - val_loss: 0.1908\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2343 - val_loss: 0.1876\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2336 - val_loss: 0.1844\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2306 - val_loss: 0.1812\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2299 - val_loss: 0.1782\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2270 - val_loss: 0.1753\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2260 - val_loss: 0.1725\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2248 - val_loss: 0.1697\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2227 - val_loss: 0.1670\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2214 - val_loss: 0.1645\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2191 - val_loss: 0.1619\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2180 - val_loss: 0.1595\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2167 - val_loss: 0.1571\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2151 - val_loss: 0.1548\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2144 - val_loss: 0.1525\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2124 - val_loss: 0.1503\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2114 - val_loss: 0.1482\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2098 - val_loss: 0.1462\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2089 - val_loss: 0.1442\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.2079 - val_loss: 0.1424\n",
      "Execution time:  22.1005539894104\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1577\n",
      "Root Mean Square Error: 0.1616\n",
      "Mean Square Error: 0.0261\n",
      "\n",
      "Train RMSE: 0.162\n",
      "Train MSE: 0.026\n",
      "Train MAE: 0.158\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 18, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 18, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1164 - val_loss: 0.1488\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 0.1482\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1152 - val_loss: 0.1475\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1146 - val_loss: 0.1467\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1138 - val_loss: 0.1459\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1129 - val_loss: 0.1451\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1122 - val_loss: 0.1442\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 0.1433\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 0.1424\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1093 - val_loss: 0.1414\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1084 - val_loss: 0.1405\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.1394\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.1384\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 0.1374\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1043 - val_loss: 0.1363\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1352\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1022 - val_loss: 0.1342\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1012 - val_loss: 0.1330\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 0.1319\n",
      "Epoch 20/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 0.1308\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0979 - val_loss: 0.1296\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 0.1285\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0956 - val_loss: 0.1273\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0945 - val_loss: 0.1262\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0936 - val_loss: 0.1250\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.1238\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0913 - val_loss: 0.1226\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.1214\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.1202\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0880 - val_loss: 0.1191\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0870 - val_loss: 0.1179\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 0.1167\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0851 - val_loss: 0.1156\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.1144\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 0.1133\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0821 - val_loss: 0.1121\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0813 - val_loss: 0.1110\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0803 - val_loss: 0.1099\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.1088\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0786 - val_loss: 0.1078\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 0.1067\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0769 - val_loss: 0.1057\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 0.1046\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 0.1037\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0747 - val_loss: 0.1028\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0739 - val_loss: 0.1020\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0734 - val_loss: 0.1012\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 0.1005\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0997\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.0990\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0713 - val_loss: 0.0984\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.0978\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0971\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0701 - val_loss: 0.0965\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0698 - val_loss: 0.0959\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0692 - val_loss: 0.0953\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.0948\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.0942\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.0937\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0684 - val_loss: 0.0932\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0677 - val_loss: 0.0926\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0675 - val_loss: 0.0921\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.0916\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.0912\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.0908\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.0905\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0664 - val_loss: 0.0901\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.0897\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.0894\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0659 - val_loss: 0.0890\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0657 - val_loss: 0.0887\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.0883\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0655 - val_loss: 0.0880\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0655 - val_loss: 0.0877\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0653 - val_loss: 0.0873\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0650 - val_loss: 0.0870\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.0867\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.0864\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.0861\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.0858\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0645 - val_loss: 0.0855\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.0852\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0647 - val_loss: 0.0849\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0643 - val_loss: 0.0846\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0642 - val_loss: 0.0844\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.0841\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.0839\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0640 - val_loss: 0.0836\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0834\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0831\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0637 - val_loss: 0.0829\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0634 - val_loss: 0.0827\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.0824\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.0822\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.0820\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.0818\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.0816\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0631 - val_loss: 0.0814\n",
      "Execution time:  22.97955298423767\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0558\n",
      "Root Mean Square Error: 0.0638\n",
      "Mean Square Error: 0.0041\n",
      "\n",
      "Train RMSE: 0.064\n",
      "Train MSE: 0.004\n",
      "Train MAE: 0.056\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 18, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 18, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/139 [..............................] - ETA: 0s - loss: 0.0917WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0078s). Check your callbacks.\n",
      "139/139 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 0.1269\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1083 - val_loss: 0.1264\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1080 - val_loss: 0.1258\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1070 - val_loss: 0.1251\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1066 - val_loss: 0.1245\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1065 - val_loss: 0.1238\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.1230\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1048 - val_loss: 0.1223\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.1215\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.1208\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1027 - val_loss: 0.1200\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.1192\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1013 - val_loss: 0.1184\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1005 - val_loss: 0.1176\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0996 - val_loss: 0.1167\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.1159\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.1151\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0973 - val_loss: 0.1143\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0970 - val_loss: 0.1135\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0960 - val_loss: 0.1128\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0953 - val_loss: 0.1121\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0947 - val_loss: 0.1114\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0946 - val_loss: 0.1106\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0934 - val_loss: 0.1099\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0933 - val_loss: 0.1092\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0927 - val_loss: 0.1085\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0917 - val_loss: 0.1078\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0910 - val_loss: 0.1071\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0905 - val_loss: 0.1065\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0901 - val_loss: 0.1058\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0897 - val_loss: 0.1051\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0891 - val_loss: 0.1044\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.1038\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.1031\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0871 - val_loss: 0.1024\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.1018\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0862 - val_loss: 0.1012\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.1005\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0854 - val_loss: 0.0999\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0845 - val_loss: 0.0993\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0987\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0839 - val_loss: 0.0981\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0832 - val_loss: 0.0975\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0829 - val_loss: 0.0969\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0824 - val_loss: 0.0963\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0823 - val_loss: 0.0957\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0814 - val_loss: 0.0952\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0810 - val_loss: 0.0946\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0805 - val_loss: 0.0941\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.0936\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0798 - val_loss: 0.0930\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0794 - val_loss: 0.0925\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0795 - val_loss: 0.0920\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0787 - val_loss: 0.0915\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0783 - val_loss: 0.0910\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0782 - val_loss: 0.0905\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0774 - val_loss: 0.0900\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.0895\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0763 - val_loss: 0.0891\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0766 - val_loss: 0.0886\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0765 - val_loss: 0.0882\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0760 - val_loss: 0.0878\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.0874\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0754 - val_loss: 0.0871\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0867\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0751 - val_loss: 0.0863\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.0860\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0742 - val_loss: 0.0856\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0741 - val_loss: 0.0853\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 0.0849\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0737 - val_loss: 0.0845\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0731 - val_loss: 0.0842\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0729 - val_loss: 0.0839\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0728 - val_loss: 0.0835\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0724 - val_loss: 0.0832\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0724 - val_loss: 0.0828\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0721 - val_loss: 0.0825\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.0822\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.0819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0712 - val_loss: 0.0816\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0812\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0809\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0708 - val_loss: 0.0806\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.0804\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0801\n",
      "Execution time:  21.981473207473755\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0635\n",
      "Root Mean Square Error: 0.0713\n",
      "Mean Square Error: 0.0051\n",
      "\n",
      "Train RMSE: 0.071\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.063\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 18, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 18, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2964 - val_loss: 0.1843\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2059 - val_loss: 0.1284\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1661 - val_loss: 0.0970\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1289 - val_loss: 0.0645\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0928 - val_loss: 0.0347\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.0051\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0115\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0179\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0167\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0158\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0157\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0159\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0158\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0168\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0160\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0155\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 20/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0156\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0160\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0160\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0164\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0163\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0164\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0160\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0166\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0165\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0161\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0163\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0160\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0169\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0157\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0170\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0155\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0164\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0157\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0162\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0164\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0163\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0161\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0160\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0163\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0159\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0164\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0163\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0164\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0164\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0164\n",
      "Epoch 65/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0162\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0162\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0162\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0161\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0163\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0162\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0164\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0163\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0163\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0162\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0161\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0163\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0161\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0160\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0163\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0161\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0162\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0160\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0163\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0162\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0162\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0160\n",
      "Execution time:  22.698858499526978\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0228\n",
      "Root Mean Square Error: 0.0370\n",
      "Mean Square Error: 0.0014\n",
      "\n",
      "Train RMSE: 0.037\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.023\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 18, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 18, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "139/139 [==============================] - 0s 3ms/step - loss: 0.2242 - val_loss: 0.0739\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1503 - val_loss: 0.0389\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.1043 - val_loss: 0.0173\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0607 - val_loss: 0.0177\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0157\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0168\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0170\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0173\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0171\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0169\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0170\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0165\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0168\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0168\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0168\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0168\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0167\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0168\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0167\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0163\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0163\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0164\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0162\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0161\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0161\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0156\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0157\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0157\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0157\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0155\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0153\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0151\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0153\n",
      "Epoch 36/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0151\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0151\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0149\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0148\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0146\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0147\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0144\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0146\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0146\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0146\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0143\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0145\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0144\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0143\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0147\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0143\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0146\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0145\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0144\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0142\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0146\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0142\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0146\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0144\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0145\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0142\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0145\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0143\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0143\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0142\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0145\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0143\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0142\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0144\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0143\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0143\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0141\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0144\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0142\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0145\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0144\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0142\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0145\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0142\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0142\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0143\n",
      "Epoch 82/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0141\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0141\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0144\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0144\n",
      "Execution time:  22.32919669151306\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0218\n",
      "Root Mean Square Error: 0.0356\n",
      "Mean Square Error: 0.0013\n",
      "\n",
      "Train RMSE: 0.036\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.022\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 18, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 18, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0761 - val_loss: 0.0790\n",
      "Epoch 2/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0585 - val_loss: 0.0613\n",
      "Epoch 3/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.0547\n",
      "Epoch 4/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.0490\n",
      "Epoch 5/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0436\n",
      "Epoch 6/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0390\n",
      "Epoch 7/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.0350\n",
      "Epoch 8/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0321\n",
      "Epoch 9/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0294\n",
      "Epoch 10/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0265\n",
      "Epoch 11/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0267\n",
      "Epoch 12/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0245\n",
      "Epoch 13/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0218\n",
      "Epoch 14/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0193\n",
      "Epoch 15/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0209\n",
      "Epoch 16/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0190\n",
      "Epoch 17/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0180\n",
      "Epoch 18/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0187\n",
      "Epoch 19/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0204\n",
      "Epoch 20/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0192\n",
      "Epoch 21/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0186\n",
      "Epoch 22/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0177\n",
      "Epoch 23/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0198\n",
      "Epoch 24/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0196\n",
      "Epoch 25/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0195\n",
      "Epoch 26/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0195\n",
      "Epoch 27/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0194\n",
      "Epoch 28/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0193\n",
      "Epoch 29/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 30/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 31/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 32/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 33/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 34/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 35/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0198\n",
      "Epoch 36/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0199\n",
      "Epoch 37/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0195\n",
      "Epoch 38/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0195\n",
      "Epoch 39/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0194\n",
      "Epoch 40/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0193\n",
      "Epoch 41/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0193\n",
      "Epoch 42/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 43/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 44/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 45/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 46/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 47/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 48/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0181\n",
      "Epoch 49/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0167\n",
      "Epoch 50/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0177\n",
      "Epoch 51/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 52/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0166\n",
      "Epoch 53/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0162\n",
      "Epoch 54/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 55/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 56/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0159\n",
      "Epoch 57/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0162\n",
      "Epoch 58/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 59/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0160\n",
      "Epoch 60/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0160\n",
      "Epoch 61/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0160\n",
      "Epoch 62/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0164\n",
      "Epoch 63/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 64/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0165\n",
      "Epoch 65/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0163\n",
      "Epoch 66/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 67/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 68/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 69/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 70/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0160\n",
      "Epoch 71/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0158\n",
      "Epoch 72/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0159\n",
      "Epoch 73/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 74/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0158\n",
      "Epoch 75/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0147\n",
      "Epoch 76/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0162\n",
      "Epoch 77/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0164\n",
      "Epoch 78/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0162\n",
      "Epoch 79/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0162\n",
      "Epoch 80/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0161\n",
      "Epoch 81/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0162\n",
      "Epoch 82/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0150\n",
      "Epoch 83/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0155\n",
      "Epoch 84/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0155\n",
      "Epoch 85/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0157\n",
      "Epoch 86/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0157\n",
      "Epoch 87/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0158\n",
      "Epoch 88/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0158\n",
      "Epoch 89/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0152\n",
      "Epoch 90/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0159\n",
      "Epoch 91/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0158\n",
      "Epoch 92/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0159\n",
      "Epoch 93/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0159\n",
      "Epoch 94/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0160\n",
      "Epoch 95/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0151\n",
      "Epoch 96/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0152\n",
      "Epoch 97/98\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0149\n",
      "Epoch 98/98\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0155\n",
      "Execution time:  22.81311845779419\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0238\n",
      "Root Mean Square Error: 0.0407\n",
      "Mean Square Error: 0.0017\n",
      "\n",
      "Train RMSE: 0.041\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.024\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 18, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 18, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 18, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 3ms/step - loss: 0.0664 - val_loss: 0.0481\n",
      "Epoch 2/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0420\n",
      "Epoch 3/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0378\n",
      "Epoch 4/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.0330\n",
      "Epoch 5/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0295\n",
      "Epoch 6/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.0260\n",
      "Epoch 7/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 0.0241\n",
      "Epoch 8/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0221\n",
      "Epoch 9/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.0229\n",
      "Epoch 10/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0212\n",
      "Epoch 11/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0199\n",
      "Epoch 12/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0200\n",
      "Epoch 13/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0192\n",
      "Epoch 14/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0188\n",
      "Epoch 15/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0185\n",
      "Epoch 16/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0183\n",
      "Epoch 17/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0182\n",
      "Epoch 18/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0181\n",
      "Epoch 19/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0181\n",
      "Epoch 20/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0181\n",
      "Epoch 21/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0181\n",
      "Epoch 22/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0180\n",
      "Epoch 23/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0179\n",
      "Epoch 24/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0178\n",
      "Epoch 25/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0177\n",
      "Epoch 26/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0177\n",
      "Epoch 27/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0175\n",
      "Epoch 28/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0175\n",
      "Epoch 29/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0174\n",
      "Epoch 30/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0173\n",
      "Epoch 31/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0171\n",
      "Epoch 32/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0170\n",
      "Epoch 33/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0169\n",
      "Epoch 34/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0168\n",
      "Epoch 35/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0167\n",
      "Epoch 36/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0166\n",
      "Epoch 37/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0165\n",
      "Epoch 38/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0163\n",
      "Epoch 39/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0162\n",
      "Epoch 40/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0162\n",
      "Epoch 41/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0161\n",
      "Epoch 42/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0161\n",
      "Epoch 43/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0160\n",
      "Epoch 44/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0160\n",
      "Epoch 45/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0159\n",
      "Epoch 46/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0158\n",
      "Epoch 47/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0158\n",
      "Epoch 48/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0157\n",
      "Epoch 49/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0157\n",
      "Epoch 50/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0156\n",
      "Epoch 51/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0156\n",
      "Epoch 52/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0156\n",
      "Epoch 53/85\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.026 - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0155\n",
      "Epoch 54/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0155\n",
      "Epoch 55/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0155\n",
      "Epoch 56/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0154\n",
      "Epoch 57/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0155\n",
      "Epoch 58/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0154\n",
      "Epoch 59/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0153\n",
      "Epoch 60/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0153\n",
      "Epoch 61/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0152\n",
      "Epoch 62/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0153\n",
      "Epoch 63/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 64/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0152\n",
      "Epoch 65/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0153\n",
      "Epoch 66/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0152\n",
      "Epoch 67/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 68/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 69/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 70/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0152\n",
      "Epoch 71/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0152\n",
      "Epoch 72/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0153\n",
      "Epoch 73/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0152\n",
      "Epoch 74/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 75/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0151\n",
      "Epoch 76/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0151\n",
      "Epoch 77/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0152\n",
      "Epoch 78/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0152\n",
      "Epoch 79/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 80/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 81/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0152\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0152\n",
      "Epoch 83/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0152\n",
      "Epoch 84/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0152\n",
      "Epoch 85/85\n",
      "139/139 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0152\n",
      "Execution time:  22.03658413887024\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0200\n",
      "Root Mean Square Error: 0.0320\n",
      "Mean Square Error: 0.0010\n",
      "\n",
      "Train RMSE: 0.032\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.020\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 36, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 36, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2262 - val_loss: 0.0915\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1532 - val_loss: 0.0760\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.0404\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 0.0216\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.0053\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0105\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0139\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0143\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0147\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0150\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0150\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0151\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0150\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0151\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0150\n",
      "Epoch 22/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0151\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0151\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0158\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0157\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0158\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0157\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0158\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0157\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0158\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0158\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0157\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0156\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0155\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0154\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0152\n",
      "Execution time:  26.200271606445312\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0264\n",
      "Root Mean Square Error: 0.0447\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.045\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 36, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 36, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 0.1984 - val_loss: 0.0864\n",
      "Epoch 2/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.0562\n",
      "Epoch 3/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.0215\n",
      "Epoch 4/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0160\n",
      "Epoch 5/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0160\n",
      "Epoch 6/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0166\n",
      "Epoch 7/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0170\n",
      "Epoch 8/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0169\n",
      "Epoch 9/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0171\n",
      "Epoch 10/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0172\n",
      "Epoch 11/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0171\n",
      "Epoch 12/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0172\n",
      "Epoch 13/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0173\n",
      "Epoch 14/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0171\n",
      "Epoch 15/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0172\n",
      "Epoch 16/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0172\n",
      "Epoch 17/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0172\n",
      "Epoch 18/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0174\n",
      "Epoch 19/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0172\n",
      "Epoch 20/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0173\n",
      "Epoch 21/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0174\n",
      "Epoch 22/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0174\n",
      "Epoch 23/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0174\n",
      "Epoch 24/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0174\n",
      "Epoch 25/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0172\n",
      "Epoch 26/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0174\n",
      "Epoch 27/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0173\n",
      "Epoch 28/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0173\n",
      "Epoch 29/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0176\n",
      "Epoch 30/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0173\n",
      "Epoch 31/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0175\n",
      "Epoch 32/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0170\n",
      "Epoch 33/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0173\n",
      "Epoch 34/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0174\n",
      "Epoch 35/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0175\n",
      "Epoch 36/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 37/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0174\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0174\n",
      "Epoch 39/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 40/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0175\n",
      "Epoch 41/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 42/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 43/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0176\n",
      "Epoch 44/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0175\n",
      "Epoch 45/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0173\n",
      "Epoch 46/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0173\n",
      "Epoch 47/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0171\n",
      "Epoch 48/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0170\n",
      "Epoch 49/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 50/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0164\n",
      "Epoch 51/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0161\n",
      "Epoch 52/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0159\n",
      "Epoch 53/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0160\n",
      "Epoch 54/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0158\n",
      "Epoch 55/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 56/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0155\n",
      "Epoch 57/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0156\n",
      "Epoch 58/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0150\n",
      "Epoch 59/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0154\n",
      "Epoch 60/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0155\n",
      "Epoch 61/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0154\n",
      "Epoch 62/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0150\n",
      "Epoch 63/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0152\n",
      "Epoch 64/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0151\n",
      "Epoch 65/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0154\n",
      "Epoch 66/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0151\n",
      "Epoch 67/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0150\n",
      "Epoch 68/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0150\n",
      "Epoch 69/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0150\n",
      "Epoch 70/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0152\n",
      "Epoch 71/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0150\n",
      "Epoch 72/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0153\n",
      "Epoch 73/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0150\n",
      "Epoch 74/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0150\n",
      "Epoch 75/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0151\n",
      "Epoch 76/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0154\n",
      "Epoch 77/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0163\n",
      "Epoch 78/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0149\n",
      "Epoch 79/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0149\n",
      "Epoch 80/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0153\n",
      "Epoch 81/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0149\n",
      "Epoch 82/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0150\n",
      "Epoch 83/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0153\n",
      "Epoch 84/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0148\n",
      "Epoch 85/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0151\n",
      "Execution time:  27.05190086364746\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0222\n",
      "Root Mean Square Error: 0.0370\n",
      "Mean Square Error: 0.0014\n",
      "\n",
      "Train RMSE: 0.037\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.022\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 36, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 36, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.0733 - val_loss: 0.0743\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0665\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.0574\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0484\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.0408\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0354\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0349\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0299\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0263\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0234\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0217\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0202\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0200\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0181\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0195\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0194\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0193\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0193\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0192\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0192\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0191\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0191\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 67/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0189\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0188\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Execution time:  26.309404134750366\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0262\n",
      "Root Mean Square Error: 0.0433\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_87 (Dense)             (None, 36, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 36, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0321\n",
      "Epoch 2/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0275\n",
      "Epoch 3/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0293\n",
      "Epoch 4/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0259\n",
      "Epoch 5/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0232\n",
      "Epoch 6/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0255\n",
      "Epoch 7/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0214\n",
      "Epoch 8/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0239\n",
      "Epoch 9/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0206\n",
      "Epoch 10/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0197\n",
      "Epoch 11/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0198\n",
      "Epoch 12/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0195\n",
      "Epoch 13/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 14/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 15/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 16/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0191\n",
      "Epoch 17/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 18/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 19/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 20/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 21/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 22/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 23/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 24/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 25/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 26/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 27/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0191\n",
      "Epoch 28/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 29/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 30/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 31/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0190\n",
      "Epoch 32/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0189\n",
      "Epoch 33/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0189\n",
      "Epoch 34/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0189\n",
      "Epoch 35/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 36/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 37/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0187\n",
      "Epoch 38/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0185\n",
      "Epoch 39/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 40/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0188\n",
      "Epoch 41/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0187\n",
      "Epoch 42/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0185\n",
      "Epoch 43/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0187\n",
      "Epoch 44/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0187\n",
      "Epoch 45/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0186\n",
      "Epoch 46/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0187\n",
      "Epoch 47/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0182\n",
      "Epoch 48/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0185\n",
      "Epoch 49/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0184\n",
      "Epoch 50/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0182\n",
      "Epoch 51/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0184\n",
      "Epoch 52/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0180\n",
      "Epoch 53/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 54/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0179\n",
      "Epoch 55/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 56/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0180\n",
      "Epoch 57/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0183\n",
      "Epoch 58/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0179\n",
      "Epoch 59/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0181\n",
      "Epoch 60/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0180\n",
      "Epoch 61/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0179\n",
      "Epoch 62/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0180\n",
      "Epoch 63/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0179\n",
      "Epoch 64/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0179\n",
      "Epoch 65/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0177\n",
      "Epoch 66/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0177\n",
      "Epoch 67/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0177\n",
      "Epoch 68/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0176\n",
      "Epoch 69/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0177\n",
      "Epoch 70/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0176\n",
      "Epoch 71/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0176\n",
      "Epoch 72/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0175\n",
      "Epoch 73/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0171\n",
      "Epoch 74/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0176\n",
      "Epoch 75/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0176\n",
      "Epoch 76/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0175\n",
      "Epoch 77/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0175\n",
      "Epoch 78/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0175\n",
      "Epoch 79/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0175\n",
      "Epoch 80/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0175\n",
      "Epoch 81/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0176\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0176\n",
      "Epoch 83/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0176\n",
      "Epoch 84/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0176\n",
      "Epoch 85/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0176\n",
      "Execution time:  27.262611150741577\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0247\n",
      "Root Mean Square Error: 0.0418\n",
      "Mean Square Error: 0.0017\n",
      "\n",
      "Train RMSE: 0.042\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 36, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 36, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4144 - val_loss: 0.3761\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.4118 - val_loss: 0.3737\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.4090 - val_loss: 0.3710\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.4065 - val_loss: 0.3682\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.4028 - val_loss: 0.3653\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.4003 - val_loss: 0.3623\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3972 - val_loss: 0.3591\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3938 - val_loss: 0.3557\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3902 - val_loss: 0.3523\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3863 - val_loss: 0.3487\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3827 - val_loss: 0.3451\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3788 - val_loss: 0.3416\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3754 - val_loss: 0.3379\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3717 - val_loss: 0.3341\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3671 - val_loss: 0.3303\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3637 - val_loss: 0.3264\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3596 - val_loss: 0.3224\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3551 - val_loss: 0.3183\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3507 - val_loss: 0.3141\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3469 - val_loss: 0.3099\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3427 - val_loss: 0.3055\n",
      "Epoch 22/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3376 - val_loss: 0.3012\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3341 - val_loss: 0.2967\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3289 - val_loss: 0.2922\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 0.2877\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3197 - val_loss: 0.2831\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3152 - val_loss: 0.2786\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3108 - val_loss: 0.2743\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3071 - val_loss: 0.2700\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.3036 - val_loss: 0.2657\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2989 - val_loss: 0.2615\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2959 - val_loss: 0.2572\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2913 - val_loss: 0.2530\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2881 - val_loss: 0.2489\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2848 - val_loss: 0.2447\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2804 - val_loss: 0.2406\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2777 - val_loss: 0.2369\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2742 - val_loss: 0.2333\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2717 - val_loss: 0.2296\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2682 - val_loss: 0.2260\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2654 - val_loss: 0.2223\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2629 - val_loss: 0.2188\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2600 - val_loss: 0.2153\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2573 - val_loss: 0.2118\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2559 - val_loss: 0.2084\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2532 - val_loss: 0.2051\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 0.2018\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2497 - val_loss: 0.1986\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2469 - val_loss: 0.1956\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2455 - val_loss: 0.1925\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2433 - val_loss: 0.1896\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2422 - val_loss: 0.1867\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2408 - val_loss: 0.1842\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2393 - val_loss: 0.1818\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2374 - val_loss: 0.1794\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2366 - val_loss: 0.1770\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2356 - val_loss: 0.1747\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2346 - val_loss: 0.1724\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2328 - val_loss: 0.1702\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2318 - val_loss: 0.1681\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2314 - val_loss: 0.1662\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2303 - val_loss: 0.1643\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2293 - val_loss: 0.1627\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2284 - val_loss: 0.1611\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2274 - val_loss: 0.1597\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2273 - val_loss: 0.1582\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2266 - val_loss: 0.1569\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2255 - val_loss: 0.1555\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2249 - val_loss: 0.1542\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2246 - val_loss: 0.1529\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2240 - val_loss: 0.1517\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2231 - val_loss: 0.1505\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2227 - val_loss: 0.1493\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2217 - val_loss: 0.1482\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2217 - val_loss: 0.1470\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2213 - val_loss: 0.1458\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2208 - val_loss: 0.1447\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2208 - val_loss: 0.1436\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2195 - val_loss: 0.1424\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2189 - val_loss: 0.1413\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2182 - val_loss: 0.1402\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 0.1391\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2165 - val_loss: 0.1379\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2165 - val_loss: 0.1368\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2163 - val_loss: 0.1357\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2153 - val_loss: 0.1346\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2155 - val_loss: 0.1335\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2146 - val_loss: 0.1324\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2142 - val_loss: 0.1313\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2134 - val_loss: 0.1303\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2122 - val_loss: 0.1294\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 0.1285\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2120 - val_loss: 0.1277\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2117 - val_loss: 0.1269\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2105 - val_loss: 0.1262\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2105 - val_loss: 0.1257\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2104 - val_loss: 0.1250\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.2102 - val_loss: 0.1245\n",
      "Execution time:  25.566434621810913\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1463\n",
      "Root Mean Square Error: 0.1490\n",
      "Mean Square Error: 0.0222\n",
      "\n",
      "Train RMSE: 0.149\n",
      "Train MSE: 0.022\n",
      "Train MAE: 0.146\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_93 (Dense)             (None, 36, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 36, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 0.2991 - val_loss: 0.2608\n",
      "Epoch 2/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2978 - val_loss: 0.2583\n",
      "Epoch 3/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2967 - val_loss: 0.2557\n",
      "Epoch 4/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2932 - val_loss: 0.2528\n",
      "Epoch 5/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2922 - val_loss: 0.2499\n",
      "Epoch 6/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2896 - val_loss: 0.2468\n",
      "Epoch 7/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2872 - val_loss: 0.2435\n",
      "Epoch 8/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2860 - val_loss: 0.2402\n",
      "Epoch 9/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2830 - val_loss: 0.2368\n",
      "Epoch 10/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2808 - val_loss: 0.2333\n",
      "Epoch 11/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2784 - val_loss: 0.2297\n",
      "Epoch 12/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2759 - val_loss: 0.2261\n",
      "Epoch 13/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2737 - val_loss: 0.2224\n",
      "Epoch 14/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2705 - val_loss: 0.2187\n",
      "Epoch 15/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2678 - val_loss: 0.2151\n",
      "Epoch 16/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2655 - val_loss: 0.2115\n",
      "Epoch 17/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2645 - val_loss: 0.2080\n",
      "Epoch 18/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2624 - val_loss: 0.2044\n",
      "Epoch 19/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2592 - val_loss: 0.2009\n",
      "Epoch 20/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2574 - val_loss: 0.1974\n",
      "Epoch 21/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2550 - val_loss: 0.1939\n",
      "Epoch 22/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 0.1904\n",
      "Epoch 23/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2510 - val_loss: 0.1869\n",
      "Epoch 24/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.1834\n",
      "Epoch 25/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2464 - val_loss: 0.1800\n",
      "Epoch 26/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2449 - val_loss: 0.1766\n",
      "Epoch 27/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2431 - val_loss: 0.1733\n",
      "Epoch 28/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2400 - val_loss: 0.1700\n",
      "Epoch 29/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2390 - val_loss: 0.1669\n",
      "Epoch 30/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2372 - val_loss: 0.1638\n",
      "Epoch 31/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2347 - val_loss: 0.1608\n",
      "Epoch 32/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2340 - val_loss: 0.1578\n",
      "Epoch 33/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2319 - val_loss: 0.1550\n",
      "Epoch 34/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2303 - val_loss: 0.1522\n",
      "Epoch 35/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2278 - val_loss: 0.1495\n",
      "Epoch 36/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2271 - val_loss: 0.1469\n",
      "Epoch 37/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2247 - val_loss: 0.1444\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2235 - val_loss: 0.1419\n",
      "Epoch 39/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2216 - val_loss: 0.1395\n",
      "Epoch 40/85\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.223 - 0s 2ms/step - loss: 0.2203 - val_loss: 0.1372\n",
      "Epoch 41/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2197 - val_loss: 0.1351\n",
      "Epoch 42/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2181 - val_loss: 0.1330\n",
      "Epoch 43/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2167 - val_loss: 0.1310\n",
      "Epoch 44/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2139 - val_loss: 0.1290\n",
      "Epoch 45/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2137 - val_loss: 0.1271\n",
      "Epoch 46/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2115 - val_loss: 0.1253\n",
      "Epoch 47/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2103 - val_loss: 0.1235\n",
      "Epoch 48/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2097 - val_loss: 0.1218\n",
      "Epoch 49/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2087 - val_loss: 0.1202\n",
      "Epoch 50/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2073 - val_loss: 0.1186\n",
      "Epoch 51/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2063 - val_loss: 0.1170\n",
      "Epoch 52/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2051 - val_loss: 0.1155\n",
      "Epoch 53/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2037 - val_loss: 0.1140\n",
      "Epoch 54/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2029 - val_loss: 0.1125\n",
      "Epoch 55/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2015 - val_loss: 0.1112\n",
      "Epoch 56/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.2005 - val_loss: 0.1100\n",
      "Epoch 57/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1992 - val_loss: 0.1090\n",
      "Epoch 58/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1982 - val_loss: 0.1081\n",
      "Epoch 59/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1972 - val_loss: 0.1074\n",
      "Epoch 60/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1965 - val_loss: 0.1066\n",
      "Epoch 61/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1966 - val_loss: 0.1058\n",
      "Epoch 62/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1961 - val_loss: 0.1051\n",
      "Epoch 63/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1945 - val_loss: 0.1045\n",
      "Epoch 64/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1949 - val_loss: 0.1040\n",
      "Epoch 65/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1929 - val_loss: 0.1034\n",
      "Epoch 66/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1931 - val_loss: 0.1029\n",
      "Epoch 67/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1925 - val_loss: 0.1024\n",
      "Epoch 68/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1926 - val_loss: 0.1018\n",
      "Epoch 69/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1913 - val_loss: 0.1012\n",
      "Epoch 70/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1904 - val_loss: 0.1006\n",
      "Epoch 71/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 0.1001\n",
      "Epoch 72/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1894 - val_loss: 0.0997\n",
      "Epoch 73/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1889 - val_loss: 0.0993\n",
      "Epoch 74/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1883 - val_loss: 0.0988\n",
      "Epoch 75/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1877 - val_loss: 0.0984\n",
      "Epoch 76/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 0.0980\n",
      "Epoch 77/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1870 - val_loss: 0.0976\n",
      "Epoch 78/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 0.0972\n",
      "Epoch 79/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 0.0968\n",
      "Epoch 80/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1848 - val_loss: 0.0964\n",
      "Epoch 81/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1852 - val_loss: 0.0961\n",
      "Epoch 82/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1834 - val_loss: 0.0957\n",
      "Epoch 83/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1829 - val_loss: 0.0954\n",
      "Epoch 84/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 0.0951\n",
      "Epoch 85/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1821 - val_loss: 0.0948\n",
      "Execution time:  27.253050565719604\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1037\n",
      "Root Mean Square Error: 0.1058\n",
      "Mean Square Error: 0.0112\n",
      "\n",
      "Train RMSE: 0.106\n",
      "Train MSE: 0.011\n",
      "Train MAE: 0.104\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 36, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 36, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0937 - val_loss: 0.1271\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 0.1266\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0930 - val_loss: 0.1261\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.1256\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0919 - val_loss: 0.1251\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 0.1245\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0909 - val_loss: 0.1240\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 0.1234\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 0.1228\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 0.1222\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 0.1216\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 0.1210\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 0.1204\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0871 - val_loss: 0.1197\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0865 - val_loss: 0.1191\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 0.1184\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 0.1178\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0848 - val_loss: 0.1171\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0843 - val_loss: 0.1165\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.1158\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0833 - val_loss: 0.1152\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0826 - val_loss: 0.1145\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0823 - val_loss: 0.1139\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0816 - val_loss: 0.1132\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 0.1126\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0805 - val_loss: 0.1119\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0800 - val_loss: 0.1113\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.1107\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 0.1101\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0786 - val_loss: 0.1095\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.1090\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 0.1085\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0775 - val_loss: 0.1080\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0770 - val_loss: 0.1076\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0767 - val_loss: 0.1071\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.1066\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 0.1061\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 0.1057\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.1052\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0750 - val_loss: 0.1048\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 0.1043\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 0.1039\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 0.1034\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 0.1030\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.1026\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0734 - val_loss: 0.1022\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0732 - val_loss: 0.1017\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 0.1013\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.1009\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0723 - val_loss: 0.1005\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0721 - val_loss: 0.1001\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0997\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0993\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0990\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0986\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.0982\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0708 - val_loss: 0.0979\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0706 - val_loss: 0.0975\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0703 - val_loss: 0.0971\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.0968\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.0964\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 0.0961\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0695 - val_loss: 0.0958\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0693 - val_loss: 0.0954\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.0951\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.0948\n",
      "Epoch 67/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.0945\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.0941\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0684 - val_loss: 0.0938\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.0935\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0682 - val_loss: 0.0932\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0680 - val_loss: 0.0929\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.0926\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0677 - val_loss: 0.0923\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.0920\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0674 - val_loss: 0.0918\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0674 - val_loss: 0.0915\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.0912\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.0909\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.0907\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.0904\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.0901\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.0899\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.0896\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0664 - val_loss: 0.0894\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.0892\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.0889\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0659 - val_loss: 0.0887\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.0885\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0657 - val_loss: 0.0883\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0655 - val_loss: 0.0881\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.0879\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0655 - val_loss: 0.0877\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0655 - val_loss: 0.0875\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0652 - val_loss: 0.0873\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.0871\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.0869\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0652 - val_loss: 0.0867\n",
      "Execution time:  25.825891971588135\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0600\n",
      "Root Mean Square Error: 0.0679\n",
      "Mean Square Error: 0.0046\n",
      "\n",
      "Train RMSE: 0.068\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.060\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 36, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 36, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 3ms/step - loss: 0.1219 - val_loss: 0.1390\n",
      "Epoch 2/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1217 - val_loss: 0.1383\n",
      "Epoch 3/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1208 - val_loss: 0.1376\n",
      "Epoch 4/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1202 - val_loss: 0.1368\n",
      "Epoch 5/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1195 - val_loss: 0.1360\n",
      "Epoch 6/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1185 - val_loss: 0.1351\n",
      "Epoch 7/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1176 - val_loss: 0.1342\n",
      "Epoch 8/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 0.1333\n",
      "Epoch 9/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1160 - val_loss: 0.1324\n",
      "Epoch 10/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1151 - val_loss: 0.1314\n",
      "Epoch 11/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1141 - val_loss: 0.1305\n",
      "Epoch 12/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1132 - val_loss: 0.1295\n",
      "Epoch 13/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1120 - val_loss: 0.1285\n",
      "Epoch 14/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 0.1274\n",
      "Epoch 15/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1101 - val_loss: 0.1264\n",
      "Epoch 16/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1093 - val_loss: 0.1253\n",
      "Epoch 17/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1085 - val_loss: 0.1243\n",
      "Epoch 18/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1074 - val_loss: 0.1232\n",
      "Epoch 19/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.1221\n",
      "Epoch 20/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1052 - val_loss: 0.1211\n",
      "Epoch 21/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.1200\n",
      "Epoch 22/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.1189\n",
      "Epoch 23/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1021 - val_loss: 0.1178\n",
      "Epoch 24/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1009 - val_loss: 0.1167\n",
      "Epoch 25/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1002 - val_loss: 0.1156\n",
      "Epoch 26/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.1145\n",
      "Epoch 27/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0980 - val_loss: 0.1134\n",
      "Epoch 28/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0971 - val_loss: 0.1123\n",
      "Epoch 29/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0959 - val_loss: 0.1112\n",
      "Epoch 30/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0950 - val_loss: 0.1102\n",
      "Epoch 31/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.1091\n",
      "Epoch 32/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.1080\n",
      "Epoch 33/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0923 - val_loss: 0.1069\n",
      "Epoch 34/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0912 - val_loss: 0.1059\n",
      "Epoch 35/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0902 - val_loss: 0.1049\n",
      "Epoch 36/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0895 - val_loss: 0.1038\n",
      "Epoch 37/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.1028\n",
      "Epoch 38/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0876 - val_loss: 0.1018\n",
      "Epoch 39/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0866 - val_loss: 0.1008\n",
      "Epoch 40/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0858 - val_loss: 0.0998\n",
      "Epoch 41/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0847 - val_loss: 0.0988\n",
      "Epoch 42/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0840 - val_loss: 0.0978\n",
      "Epoch 43/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.0969\n",
      "Epoch 44/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0824 - val_loss: 0.0959\n",
      "Epoch 45/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0814 - val_loss: 0.0950\n",
      "Epoch 46/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0807 - val_loss: 0.0941\n",
      "Epoch 47/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.0932\n",
      "Epoch 48/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0792 - val_loss: 0.0923\n",
      "Epoch 49/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0785 - val_loss: 0.0915\n",
      "Epoch 50/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0779 - val_loss: 0.0908\n",
      "Epoch 51/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.0900\n",
      "Epoch 52/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0766 - val_loss: 0.0893\n",
      "Epoch 53/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0759 - val_loss: 0.0886\n",
      "Epoch 54/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0754 - val_loss: 0.0879\n",
      "Epoch 55/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0746 - val_loss: 0.0872\n",
      "Epoch 56/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0745 - val_loss: 0.0865\n",
      "Epoch 57/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0739 - val_loss: 0.0858\n",
      "Epoch 58/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0731 - val_loss: 0.0852\n",
      "Epoch 59/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0728 - val_loss: 0.0845\n",
      "Epoch 60/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0723 - val_loss: 0.0839\n",
      "Epoch 61/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.0833\n",
      "Epoch 62/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0715 - val_loss: 0.0827\n",
      "Epoch 63/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.0822\n",
      "Epoch 64/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0706 - val_loss: 0.0817\n",
      "Epoch 65/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0812\n",
      "Epoch 66/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0694 - val_loss: 0.0807\n",
      "Epoch 67/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0694 - val_loss: 0.0803\n",
      "Epoch 68/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0690 - val_loss: 0.0798\n",
      "Epoch 69/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0689 - val_loss: 0.0794\n",
      "Epoch 70/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.0789\n",
      "Epoch 71/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0680 - val_loss: 0.0785\n",
      "Epoch 72/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0679 - val_loss: 0.0781\n",
      "Epoch 73/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0678 - val_loss: 0.0777\n",
      "Epoch 74/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0674 - val_loss: 0.0773\n",
      "Epoch 75/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0668 - val_loss: 0.0769\n",
      "Epoch 76/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0666 - val_loss: 0.0765\n",
      "Epoch 77/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0666 - val_loss: 0.0761\n",
      "Epoch 78/85\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.0757\n",
      "Epoch 79/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0660 - val_loss: 0.0754\n",
      "Epoch 80/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0656 - val_loss: 0.0750\n",
      "Epoch 81/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0654 - val_loss: 0.0746\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0653 - val_loss: 0.0743\n",
      "Epoch 83/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0652 - val_loss: 0.0740\n",
      "Epoch 84/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0648 - val_loss: 0.0736\n",
      "Epoch 85/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0646 - val_loss: 0.0734\n",
      "Execution time:  27.3598895072937\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0576\n",
      "Root Mean Square Error: 0.0655\n",
      "Mean Square Error: 0.0043\n",
      "\n",
      "Train RMSE: 0.066\n",
      "Train MSE: 0.004\n",
      "Train MAE: 0.058\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 36, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 36, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.1941 - val_loss: 0.0763\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1533 - val_loss: 0.0445\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1342 - val_loss: 0.0339\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.1161 - val_loss: 0.0281\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0235\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 0.0128\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0055\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.0086\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0094\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0115\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0159\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0170\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0165\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0167\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0163\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0165\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0166\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0161\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0166\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0162\n",
      "Epoch 22/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0161\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0165\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0161\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0160\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0163\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0160\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0163\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0160\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0163\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0163\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0162\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0163\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0162\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0162\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0164\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0159\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0159\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0160\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0159\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0159\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0159\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0159\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0160\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0160\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0161\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0160\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0161\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0160\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0164\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0163\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0164\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0166\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0161\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0164\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0163\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0172\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0155\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0168\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0162\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0169\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0156\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0167\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0158\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0167\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0157\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0168\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0160\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0164\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0167\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0157\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0168\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0157\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0168\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0158\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0167\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0159\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0168\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0157\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0167\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0156\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0166\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0158\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0169\n",
      "Execution time:  26.066433668136597\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0232\n",
      "Root Mean Square Error: 0.0378\n",
      "Mean Square Error: 0.0014\n",
      "\n",
      "Train RMSE: 0.038\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.023\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_105 (Dense)            (None, 36, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 36, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 0.1878 - val_loss: 0.0499\n",
      "Epoch 2/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.1257 - val_loss: 0.0285\n",
      "Epoch 3/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0997 - val_loss: 0.0278\n",
      "Epoch 4/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0721 - val_loss: 0.0272\n",
      "Epoch 5/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0178\n",
      "Epoch 6/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.0159\n",
      "Epoch 7/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0157\n",
      "Epoch 8/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0161\n",
      "Epoch 9/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0169\n",
      "Epoch 10/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0171\n",
      "Epoch 11/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0172\n",
      "Epoch 12/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 13/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0170\n",
      "Epoch 14/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 15/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 16/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 17/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 18/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0171\n",
      "Epoch 19/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0171\n",
      "Epoch 20/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0171\n",
      "Epoch 21/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 22/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 23/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0171\n",
      "Epoch 24/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 25/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 26/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 27/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 28/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0171\n",
      "Epoch 29/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0170\n",
      "Epoch 30/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0170\n",
      "Epoch 31/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0171\n",
      "Epoch 32/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0170\n",
      "Epoch 33/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0170\n",
      "Epoch 34/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0169\n",
      "Epoch 35/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0170\n",
      "Epoch 36/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0169\n",
      "Epoch 37/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0169\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0169\n",
      "Epoch 39/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0169\n",
      "Epoch 40/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0169\n",
      "Epoch 41/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0169\n",
      "Epoch 42/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0168\n",
      "Epoch 43/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0167\n",
      "Epoch 44/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0167\n",
      "Epoch 45/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0167\n",
      "Epoch 46/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0166\n",
      "Epoch 47/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0166\n",
      "Epoch 48/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0165\n",
      "Epoch 49/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0164\n",
      "Epoch 50/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0163\n",
      "Epoch 51/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0162\n",
      "Epoch 52/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0163\n",
      "Epoch 53/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0162\n",
      "Epoch 54/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0161\n",
      "Epoch 55/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0161\n",
      "Epoch 56/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0161\n",
      "Epoch 57/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0161\n",
      "Epoch 58/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0160\n",
      "Epoch 59/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0159\n",
      "Epoch 60/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0159\n",
      "Epoch 61/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0161\n",
      "Epoch 62/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 63/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0160\n",
      "Epoch 64/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0159\n",
      "Epoch 65/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0159\n",
      "Epoch 66/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0157\n",
      "Epoch 67/85\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0157\n",
      "Epoch 68/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0156\n",
      "Epoch 69/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0154\n",
      "Epoch 70/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0150\n",
      "Epoch 71/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0152\n",
      "Epoch 72/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0148\n",
      "Epoch 73/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0148\n",
      "Epoch 74/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0148\n",
      "Epoch 75/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0149\n",
      "Epoch 76/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0148\n",
      "Epoch 77/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0148\n",
      "Epoch 78/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0146\n",
      "Epoch 79/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0147\n",
      "Epoch 80/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0147\n",
      "Epoch 81/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0147\n",
      "Epoch 82/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0146\n",
      "Epoch 83/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0147\n",
      "Epoch 84/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0146\n",
      "Epoch 85/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0148\n",
      "Execution time:  27.321958541870117\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0204\n",
      "Root Mean Square Error: 0.0333\n",
      "Mean Square Error: 0.0011\n",
      "\n",
      "Train RMSE: 0.033\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.020\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_108 (Dense)            (None, 36, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 36, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0670 - val_loss: 0.0678\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.0566\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.0514\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.0477\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.0442\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0413\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0381\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0354\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0353\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0327\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0298\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0274\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0253\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0223\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0228\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0217\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0208\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0208\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0213\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0205\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0200\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0172\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0192\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0192\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0190\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0189\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0189\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0188\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0188\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0187\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0187\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0186\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0186\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0185\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0185\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0184\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0185\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0185\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0184\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0184\n",
      "Epoch 67/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0183\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0183\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0183\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0182\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0184\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0188\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0184\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0183\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0182\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0182\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0180\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0179\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0178\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0177\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0175\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0180\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0177\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0176\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0174\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0175\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0175\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0173\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0173\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0173\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0172\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0176\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0174\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0172\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0171\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0170\n",
      "Execution time:  26.197134733200073\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0255\n",
      "Root Mean Square Error: 0.0428\n",
      "Mean Square Error: 0.0018\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  6h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_111 (Dense)            (None, 36, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 36, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 36, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 36, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/85\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 0.0591 - val_loss: 0.0420\n",
      "Epoch 2/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0508 - val_loss: 0.0368\n",
      "Epoch 3/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.0330\n",
      "Epoch 4/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0294\n",
      "Epoch 5/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.0260\n",
      "Epoch 6/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0255\n",
      "Epoch 7/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0255\n",
      "Epoch 8/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0255\n",
      "Epoch 9/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0252\n",
      "Epoch 10/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0230\n",
      "Epoch 11/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0227\n",
      "Epoch 12/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0221\n",
      "Epoch 13/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0202\n",
      "Epoch 14/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0203\n",
      "Epoch 15/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0195\n",
      "Epoch 16/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0193\n",
      "Epoch 17/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 18/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0192\n",
      "Epoch 19/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0193\n",
      "Epoch 20/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0194\n",
      "Epoch 21/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 22/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 23/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0193\n",
      "Epoch 24/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 25/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0191\n",
      "Epoch 26/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 27/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 28/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 29/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 30/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0190\n",
      "Epoch 31/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0191\n",
      "Epoch 32/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0191\n",
      "Epoch 33/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 34/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 35/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 36/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0192\n",
      "Epoch 37/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0191\n",
      "Epoch 38/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0191\n",
      "Epoch 39/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0191\n",
      "Epoch 40/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0190\n",
      "Epoch 41/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0190\n",
      "Epoch 42/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0190\n",
      "Epoch 43/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0189\n",
      "Epoch 44/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0189\n",
      "Epoch 45/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0189\n",
      "Epoch 46/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0189\n",
      "Epoch 47/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0190\n",
      "Epoch 48/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0189\n",
      "Epoch 49/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0186\n",
      "Epoch 50/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0189\n",
      "Epoch 51/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0180\n",
      "Epoch 52/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0182\n",
      "Epoch 53/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0188\n",
      "Epoch 54/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0183\n",
      "Epoch 55/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0174\n",
      "Epoch 56/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0181\n",
      "Epoch 57/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0179\n",
      "Epoch 58/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0174\n",
      "Epoch 59/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0174\n",
      "Epoch 60/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0166\n",
      "Epoch 61/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0181\n",
      "Epoch 62/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0172\n",
      "Epoch 63/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0167\n",
      "Epoch 64/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0177\n",
      "Epoch 65/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0171\n",
      "Epoch 66/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0165\n",
      "Epoch 67/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0166\n",
      "Epoch 68/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 69/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 70/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 71/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0167\n",
      "Epoch 72/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0168\n",
      "Epoch 73/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0168\n",
      "Epoch 74/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0166\n",
      "Epoch 75/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0164\n",
      "Epoch 76/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0164\n",
      "Epoch 77/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0164\n",
      "Epoch 78/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0165\n",
      "Epoch 79/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0164\n",
      "Epoch 80/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0163\n",
      "Epoch 81/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0163\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0163\n",
      "Epoch 83/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0164\n",
      "Epoch 84/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0164\n",
      "Epoch 85/85\n",
      "138/138 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0163\n",
      "Execution time:  27.489278316497803\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0229\n",
      "Root Mean Square Error: 0.0381\n",
      "Mean Square Error: 0.0015\n",
      "\n",
      "Train RMSE: 0.038\n",
      "Train MSE: 0.001\n",
      "Train MAE: 0.023\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 72, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 72, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2561 - val_loss: 0.0807\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.1689 - val_loss: 0.0779\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.1307 - val_loss: 0.0389\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0906 - val_loss: 0.0172\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0604 - val_loss: 0.0072\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0377 - val_loss: 0.0097\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0285 - val_loss: 0.0131\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0280 - val_loss: 0.0144\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0278 - val_loss: 0.0150\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0152\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0147\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0153\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0155\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0149\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0154\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0150\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0155\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0151\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0155\n",
      "Epoch 22/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0158\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0157\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0159\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0159\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0156\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0159\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0152\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0158\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0159\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0153\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0157\n",
      "Execution time:  44.70770335197449\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0263\n",
      "Root Mean Square Error: 0.0445\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.045\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_117 (Dense)            (None, 72, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 72, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2183 - val_loss: 0.0686\n",
      "Epoch 2/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1142 - val_loss: 0.0253\n",
      "Epoch 3/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0757 - val_loss: 0.0225\n",
      "Epoch 4/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0163\n",
      "Epoch 5/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0162\n",
      "Epoch 6/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0161\n",
      "Epoch 7/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0167\n",
      "Epoch 8/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0169\n",
      "Epoch 9/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0171\n",
      "Epoch 10/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0172\n",
      "Epoch 11/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0172\n",
      "Epoch 12/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 13/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 14/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 15/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 16/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 17/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0172\n",
      "Epoch 18/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0173\n",
      "Epoch 19/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0173\n",
      "Epoch 20/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0173\n",
      "Epoch 21/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0173\n",
      "Epoch 22/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0173\n",
      "Epoch 23/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0180\n",
      "Epoch 24/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0173\n",
      "Epoch 25/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 26/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 27/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0173\n",
      "Epoch 28/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 29/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0173\n",
      "Epoch 30/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 31/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 32/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 33/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0172\n",
      "Epoch 34/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0172\n",
      "Epoch 35/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0171\n",
      "Epoch 36/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0174\n",
      "Epoch 37/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0172\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0174\n",
      "Epoch 39/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0171\n",
      "Epoch 40/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0173\n",
      "Epoch 41/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0171\n",
      "Epoch 42/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0173\n",
      "Epoch 43/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0173\n",
      "Epoch 44/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0174\n",
      "Epoch 45/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0173\n",
      "Epoch 46/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0172\n",
      "Epoch 47/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0172\n",
      "Epoch 48/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0173\n",
      "Epoch 49/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0172\n",
      "Epoch 50/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0173\n",
      "Epoch 51/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0172\n",
      "Epoch 52/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0172\n",
      "Epoch 53/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0172\n",
      "Epoch 54/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0172\n",
      "Epoch 55/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0172\n",
      "Epoch 56/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0171\n",
      "Epoch 57/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0171\n",
      "Epoch 58/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0171\n",
      "Epoch 59/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0172\n",
      "Epoch 60/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0173\n",
      "Epoch 61/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0172\n",
      "Epoch 62/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0172\n",
      "Epoch 63/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0171\n",
      "Epoch 64/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0171\n",
      "Epoch 65/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0170\n",
      "Epoch 66/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0170\n",
      "Epoch 67/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0170\n",
      "Epoch 68/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0170\n",
      "Epoch 69/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 70/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0171\n",
      "Epoch 71/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0170\n",
      "Epoch 72/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0171\n",
      "Epoch 73/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0170\n",
      "Epoch 74/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0170\n",
      "Epoch 75/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0169\n",
      "Epoch 76/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0170\n",
      "Epoch 77/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0172\n",
      "Epoch 78/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0171\n",
      "Epoch 79/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0171\n",
      "Epoch 80/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0173\n",
      "Epoch 81/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0173\n",
      "Epoch 82/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0174\n",
      "Epoch 83/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0175\n",
      "Epoch 84/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0175\n",
      "Epoch 85/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0176\n",
      "Execution time:  29.87058973312378\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0262\n",
      "Root Mean Square Error: 0.0442\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.044\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_120 (Dense)            (None, 72, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 72, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0686 - val_loss: 0.0571\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0562 - val_loss: 0.0472\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0492 - val_loss: 0.0427\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0443 - val_loss: 0.0381\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0405 - val_loss: 0.0386\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0366 - val_loss: 0.0383\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0310 - val_loss: 0.0293\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0285 - val_loss: 0.0239\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0224\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0210\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0201\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0194\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0187\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0268 - val_loss: 0.0185\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0268 - val_loss: 0.0187\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0185\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0268 - val_loss: 0.0190\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0188\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0188\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0188\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 67/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0188\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0188\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0190\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0189\n",
      "Execution time:  45.05456233024597\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0262\n",
      "Root Mean Square Error: 0.0433\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_123 (Dense)            (None, 72, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 72, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/137 [=========================>....] - ETA: 0s - loss: 0.0581WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0156s). Check your callbacks.\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0579 - val_loss: 0.0389\n",
      "Epoch 2/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0303\n",
      "Epoch 3/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0284\n",
      "Epoch 4/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0214\n",
      "Epoch 5/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0267\n",
      "Epoch 6/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0231\n",
      "Epoch 7/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0233\n",
      "Epoch 8/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 9/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0196\n",
      "Epoch 10/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0199\n",
      "Epoch 11/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0187\n",
      "Epoch 12/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0193\n",
      "Epoch 13/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0193\n",
      "Epoch 14/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0193\n",
      "Epoch 15/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0192\n",
      "Epoch 16/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 17/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 18/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 19/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 20/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 21/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 22/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 23/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 24/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 25/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 26/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 27/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 28/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 29/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 30/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 31/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 32/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 33/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 34/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 35/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 36/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 37/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 38/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 39/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 40/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 41/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 42/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 43/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 44/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 45/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 46/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 47/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 48/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 49/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 50/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 51/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 52/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 53/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 54/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 55/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 56/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 57/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 58/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 59/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 60/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 61/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0179\n",
      "Epoch 62/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0192\n",
      "Epoch 63/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0193\n",
      "Epoch 64/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 65/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 66/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 67/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 68/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 69/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 70/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 71/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 72/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 73/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 74/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 75/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 76/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 77/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 78/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 79/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 81/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 82/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 83/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 84/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 85/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Execution time:  29.312489986419678\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0262\n",
      "Root Mean Square Error: 0.0434\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_126 (Dense)            (None, 72, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 72, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.4219 - val_loss: 0.3841\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4197 - val_loss: 0.3821\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4178 - val_loss: 0.3800\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4157 - val_loss: 0.3778\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4134 - val_loss: 0.3755\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4110 - val_loss: 0.3731\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4088 - val_loss: 0.3706\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4063 - val_loss: 0.3680\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4033 - val_loss: 0.3654\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.4002 - val_loss: 0.3627\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3975 - val_loss: 0.3599\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3951 - val_loss: 0.3570\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3920 - val_loss: 0.3540\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3887 - val_loss: 0.3510\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3852 - val_loss: 0.3479\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3829 - val_loss: 0.3448\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3790 - val_loss: 0.3416\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3760 - val_loss: 0.3384\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3723 - val_loss: 0.3351\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3692 - val_loss: 0.3317\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3659 - val_loss: 0.3283\n",
      "Epoch 22/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3626 - val_loss: 0.3249\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.3587 - val_loss: 0.3215\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3557 - val_loss: 0.3180\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3517 - val_loss: 0.3150\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3484 - val_loss: 0.3121\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.3459 - val_loss: 0.3093\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.3436 - val_loss: 0.3065\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.3411 - val_loss: 0.3037\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.3389 - val_loss: 0.3009\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3356 - val_loss: 0.2981\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3332 - val_loss: 0.2952\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3309 - val_loss: 0.2924\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3289 - val_loss: 0.2896\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3257 - val_loss: 0.2869\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3239 - val_loss: 0.2841\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3217 - val_loss: 0.2813\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.3199 - val_loss: 0.2785\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3178 - val_loss: 0.2756\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3152 - val_loss: 0.2728\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3130 - val_loss: 0.2699\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3104 - val_loss: 0.2670\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3082 - val_loss: 0.2640\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3062 - val_loss: 0.2611\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3036 - val_loss: 0.2581\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.3017 - val_loss: 0.2551\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2993 - val_loss: 0.2520\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2971 - val_loss: 0.2489\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2946 - val_loss: 0.2458\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2922 - val_loss: 0.2427\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2899 - val_loss: 0.2396\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2878 - val_loss: 0.2365\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2856 - val_loss: 0.2335\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2829 - val_loss: 0.2309\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2808 - val_loss: 0.2285\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2798 - val_loss: 0.2261\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2780 - val_loss: 0.2238\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2760 - val_loss: 0.2216\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2753 - val_loss: 0.2198\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2735 - val_loss: 0.2183\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2725 - val_loss: 0.2170\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2712 - val_loss: 0.2157\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.2704 - val_loss: 0.2145\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.2696 - val_loss: 0.2132\n",
      "Epoch 65/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 6ms/step - loss: 0.2689 - val_loss: 0.2120\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2674 - val_loss: 0.2108\n",
      "Epoch 67/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2664 - val_loss: 0.2095\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2653 - val_loss: 0.2083\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2646 - val_loss: 0.2071\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2639 - val_loss: 0.2058\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2632 - val_loss: 0.2047\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2623 - val_loss: 0.2035\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2616 - val_loss: 0.2023\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2601 - val_loss: 0.2012\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2603 - val_loss: 0.2000\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2588 - val_loss: 0.1989\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2585 - val_loss: 0.1979\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2578 - val_loss: 0.1969\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2577 - val_loss: 0.1960\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2573 - val_loss: 0.1952\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2559 - val_loss: 0.1946\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2568 - val_loss: 0.1940\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2557 - val_loss: 0.1936\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2556 - val_loss: 0.1934\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2553 - val_loss: 0.1932\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2549 - val_loss: 0.1931\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2554 - val_loss: 0.1929\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2546 - val_loss: 0.1927\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2544 - val_loss: 0.1926\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2544 - val_loss: 0.1924\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2541 - val_loss: 0.1922\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2545 - val_loss: 0.1921\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2539 - val_loss: 0.1919\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2541 - val_loss: 0.1918\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2535 - val_loss: 0.1916\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2540 - val_loss: 0.1915\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2535 - val_loss: 0.1913\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.2530 - val_loss: 0.1912\n",
      "Execution time:  44.82381892204285\n",
      "DNN:\n",
      "Mean Absolute Error: 0.2155\n",
      "Root Mean Square Error: 0.2178\n",
      "Mean Square Error: 0.0474\n",
      "\n",
      "Train RMSE: 0.218\n",
      "Train MSE: 0.047\n",
      "Train MAE: 0.216\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_129 (Dense)            (None, 72, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 72, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.3184 - val_loss: 0.2972\n",
      "Epoch 2/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3161 - val_loss: 0.2946\n",
      "Epoch 3/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3133 - val_loss: 0.2919\n",
      "Epoch 4/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3113 - val_loss: 0.2889\n",
      "Epoch 5/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3086 - val_loss: 0.2858\n",
      "Epoch 6/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3052 - val_loss: 0.2825\n",
      "Epoch 7/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3026 - val_loss: 0.2791\n",
      "Epoch 8/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2993 - val_loss: 0.2755\n",
      "Epoch 9/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2965 - val_loss: 0.2718\n",
      "Epoch 10/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2931 - val_loss: 0.2679\n",
      "Epoch 11/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2901 - val_loss: 0.2640\n",
      "Epoch 12/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2868 - val_loss: 0.2601\n",
      "Epoch 13/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2841 - val_loss: 0.2561\n",
      "Epoch 14/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2804 - val_loss: 0.2521\n",
      "Epoch 15/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2774 - val_loss: 0.2480\n",
      "Epoch 16/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2735 - val_loss: 0.2438\n",
      "Epoch 17/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2709 - val_loss: 0.2396\n",
      "Epoch 18/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2678 - val_loss: 0.2353\n",
      "Epoch 19/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2637 - val_loss: 0.2311\n",
      "Epoch 20/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2611 - val_loss: 0.2273\n",
      "Epoch 21/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2582 - val_loss: 0.2237\n",
      "Epoch 22/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2567 - val_loss: 0.2201\n",
      "Epoch 23/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2530 - val_loss: 0.2165\n",
      "Epoch 24/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2507 - val_loss: 0.2130\n",
      "Epoch 25/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2495 - val_loss: 0.2099\n",
      "Epoch 26/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2475 - val_loss: 0.2068\n",
      "Epoch 27/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2451 - val_loss: 0.2039\n",
      "Epoch 28/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2434 - val_loss: 0.2010\n",
      "Epoch 29/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2420 - val_loss: 0.1982\n",
      "Epoch 30/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2406 - val_loss: 0.1954\n",
      "Epoch 31/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2387 - val_loss: 0.1926\n",
      "Epoch 32/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2372 - val_loss: 0.1898\n",
      "Epoch 33/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2354 - val_loss: 0.1872\n",
      "Epoch 34/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2338 - val_loss: 0.1846\n",
      "Epoch 35/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2328 - val_loss: 0.1820\n",
      "Epoch 36/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2316 - val_loss: 0.1795\n",
      "Epoch 37/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2303 - val_loss: 0.1772\n",
      "Epoch 38/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2295 - val_loss: 0.1750\n",
      "Epoch 39/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2284 - val_loss: 0.1728\n",
      "Epoch 40/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2270 - val_loss: 0.1706\n",
      "Epoch 41/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1685\n",
      "Epoch 42/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2251 - val_loss: 0.1664\n",
      "Epoch 43/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2246 - val_loss: 0.1643\n",
      "Epoch 44/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2234 - val_loss: 0.1623\n",
      "Epoch 45/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 0.1602\n",
      "Epoch 46/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.2210 - val_loss: 0.1582\n",
      "Epoch 47/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 0.1561\n",
      "Epoch 48/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2193 - val_loss: 0.1541\n",
      "Epoch 49/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2179 - val_loss: 0.1520\n",
      "Epoch 50/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2171 - val_loss: 0.1499\n",
      "Epoch 51/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2159 - val_loss: 0.1479\n",
      "Epoch 52/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2158 - val_loss: 0.1459\n",
      "Epoch 53/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2141 - val_loss: 0.1439\n",
      "Epoch 54/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2131 - val_loss: 0.1418\n",
      "Epoch 55/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2121 - val_loss: 0.1398\n",
      "Epoch 56/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2108 - val_loss: 0.1377\n",
      "Epoch 57/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2102 - val_loss: 0.1357\n",
      "Epoch 58/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2091 - val_loss: 0.1337\n",
      "Epoch 59/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2081 - val_loss: 0.1317\n",
      "Epoch 60/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2076 - val_loss: 0.1298\n",
      "Epoch 61/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 0.1278\n",
      "Epoch 62/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2053 - val_loss: 0.1259\n",
      "Epoch 63/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2050 - val_loss: 0.1240\n",
      "Epoch 64/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2034 - val_loss: 0.1221\n",
      "Epoch 65/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.1203\n",
      "Epoch 66/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2020 - val_loss: 0.1185\n",
      "Epoch 67/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.2005 - val_loss: 0.1168\n",
      "Epoch 68/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1998 - val_loss: 0.1150\n",
      "Epoch 69/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1992 - val_loss: 0.1133\n",
      "Epoch 70/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 0.1118\n",
      "Epoch 71/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1977 - val_loss: 0.1103\n",
      "Epoch 72/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1964 - val_loss: 0.1089\n",
      "Epoch 73/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1963 - val_loss: 0.1077\n",
      "Epoch 74/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1953 - val_loss: 0.1065\n",
      "Epoch 75/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1946 - val_loss: 0.1053\n",
      "Epoch 76/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1939 - val_loss: 0.1042\n",
      "Epoch 77/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1935 - val_loss: 0.1031\n",
      "Epoch 78/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 0.1021\n",
      "Epoch 79/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1916 - val_loss: 0.1011\n",
      "Epoch 80/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.1001\n",
      "Epoch 81/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1904 - val_loss: 0.0991\n",
      "Epoch 82/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 0.0982\n",
      "Epoch 83/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1887 - val_loss: 0.0973\n",
      "Epoch 84/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1888 - val_loss: 0.0965\n",
      "Epoch 85/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1880 - val_loss: 0.0956\n",
      "Execution time:  29.695478200912476\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1068\n",
      "Root Mean Square Error: 0.1098\n",
      "Mean Square Error: 0.0121\n",
      "\n",
      "Train RMSE: 0.110\n",
      "Train MSE: 0.012\n",
      "Train MAE: 0.107\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 72, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 72, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.0922 - val_loss: 0.1231\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0919 - val_loss: 0.1227\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0915 - val_loss: 0.1223\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0912 - val_loss: 0.1219\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0909 - val_loss: 0.1214\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0906 - val_loss: 0.1210\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0902 - val_loss: 0.1205\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0898 - val_loss: 0.1201\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0895 - val_loss: 0.1196\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0891 - val_loss: 0.1192\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0887 - val_loss: 0.1187\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0884 - val_loss: 0.1183\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0880 - val_loss: 0.1178\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0878 - val_loss: 0.1173\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0873 - val_loss: 0.1168\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0869 - val_loss: 0.1164\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.1159\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.1154\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0858 - val_loss: 0.1150\n",
      "Epoch 20/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0855 - val_loss: 0.1145\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0852 - val_loss: 0.1140\n",
      "Epoch 22/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0847 - val_loss: 0.1135\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0843 - val_loss: 0.1131\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0841 - val_loss: 0.1126\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0837 - val_loss: 0.1122\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0834 - val_loss: 0.1117\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.1112\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0828 - val_loss: 0.1108\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0822 - val_loss: 0.1104\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0822 - val_loss: 0.1099\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0819 - val_loss: 0.1095\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0815 - val_loss: 0.1091\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0812 - val_loss: 0.1086\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0810 - val_loss: 0.1082\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0805 - val_loss: 0.1078\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0803 - val_loss: 0.1074\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0801 - val_loss: 0.1069\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0798 - val_loss: 0.1065\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0796 - val_loss: 0.1062\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0791 - val_loss: 0.1058\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.1054\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0788 - val_loss: 0.1051\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0785 - val_loss: 0.1048\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0784 - val_loss: 0.1044\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0781 - val_loss: 0.1041\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.1038\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0777 - val_loss: 0.1035\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0775 - val_loss: 0.1031\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0774 - val_loss: 0.1028\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.1025\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0769 - val_loss: 0.1022\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0767 - val_loss: 0.1019\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0766 - val_loss: 0.1016\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0765 - val_loss: 0.1013\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0763 - val_loss: 0.1010\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0761 - val_loss: 0.1007\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0759 - val_loss: 0.1004\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0757 - val_loss: 0.1001\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0755 - val_loss: 0.0998\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0754 - val_loss: 0.0995\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0751 - val_loss: 0.0993\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0750 - val_loss: 0.0990\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0748 - val_loss: 0.0987\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0748 - val_loss: 0.0984\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0746 - val_loss: 0.0982\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0746 - val_loss: 0.0979\n",
      "Epoch 67/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0744 - val_loss: 0.0976\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0742 - val_loss: 0.0974\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0741 - val_loss: 0.0971\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0739 - val_loss: 0.0969\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0966\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0964\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0961\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0959\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0731 - val_loss: 0.0956\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0731 - val_loss: 0.0954\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0730 - val_loss: 0.0952\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0728 - val_loss: 0.0950\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0948\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0947\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0945\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0943\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0941\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.0723 - val_loss: 0.0940\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0938\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0936\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0934\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0933\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0931\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0929\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0928\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0926\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0925\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0715 - val_loss: 0.0923\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0715 - val_loss: 0.0921\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0713 - val_loss: 0.0920\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0713 - val_loss: 0.0918\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0711 - val_loss: 0.0917\n",
      "Execution time:  44.758739709854126\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0639\n",
      "Root Mean Square Error: 0.0719\n",
      "Mean Square Error: 0.0052\n",
      "\n",
      "Train RMSE: 0.072\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.064\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_135 (Dense)            (None, 72, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 72, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 1s 4ms/step - loss: 0.1218 - val_loss: 0.1333\n",
      "Epoch 2/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1214 - val_loss: 0.1327\n",
      "Epoch 3/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1207 - val_loss: 0.1320\n",
      "Epoch 4/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1201 - val_loss: 0.1313\n",
      "Epoch 5/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.1305\n",
      "Epoch 6/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.1297\n",
      "Epoch 7/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1181 - val_loss: 0.1289\n",
      "Epoch 8/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1173 - val_loss: 0.1281\n",
      "Epoch 9/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.1272\n",
      "Epoch 10/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.1160 - val_loss: 0.1264\n",
      "Epoch 11/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.1152 - val_loss: 0.1255\n",
      "Epoch 12/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.1145 - val_loss: 0.1246\n",
      "Epoch 13/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.1135 - val_loss: 0.1236\n",
      "Epoch 14/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.1227\n",
      "Epoch 15/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1118 - val_loss: 0.1218\n",
      "Epoch 16/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 0.1208\n",
      "Epoch 17/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1103 - val_loss: 0.1199\n",
      "Epoch 18/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1094 - val_loss: 0.1189\n",
      "Epoch 19/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.1180\n",
      "Epoch 20/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1078 - val_loss: 0.1170\n",
      "Epoch 21/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.1161\n",
      "Epoch 22/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.1151\n",
      "Epoch 23/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1142\n",
      "Epoch 24/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.1132\n",
      "Epoch 25/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1033 - val_loss: 0.1123\n",
      "Epoch 26/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1027 - val_loss: 0.1113\n",
      "Epoch 27/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 0.1104\n",
      "Epoch 28/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1010 - val_loss: 0.1096\n",
      "Epoch 29/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1002 - val_loss: 0.1087\n",
      "Epoch 30/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0993 - val_loss: 0.1079\n",
      "Epoch 31/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0987 - val_loss: 0.1071\n",
      "Epoch 32/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.1062\n",
      "Epoch 33/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0969 - val_loss: 0.1054\n",
      "Epoch 34/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.1046\n",
      "Epoch 35/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0957 - val_loss: 0.1038\n",
      "Epoch 36/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0949 - val_loss: 0.1030\n",
      "Epoch 37/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.1022\n",
      "Epoch 38/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 0.1015\n",
      "Epoch 39/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0928 - val_loss: 0.1007\n",
      "Epoch 40/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0922 - val_loss: 0.0999\n",
      "Epoch 41/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0915 - val_loss: 0.0992\n",
      "Epoch 42/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0908 - val_loss: 0.0984\n",
      "Epoch 43/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0900 - val_loss: 0.0977\n",
      "Epoch 44/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0896 - val_loss: 0.0970\n",
      "Epoch 45/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0890 - val_loss: 0.0962\n",
      "Epoch 46/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.0955\n",
      "Epoch 47/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0878 - val_loss: 0.0948\n",
      "Epoch 48/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0871 - val_loss: 0.0941\n",
      "Epoch 49/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0865 - val_loss: 0.0935\n",
      "Epoch 50/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.0928\n",
      "Epoch 51/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0851 - val_loss: 0.0921\n",
      "Epoch 52/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0848 - val_loss: 0.0915\n",
      "Epoch 53/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0842 - val_loss: 0.0909\n",
      "Epoch 54/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.0902\n",
      "Epoch 55/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.0896\n",
      "Epoch 56/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0824 - val_loss: 0.0890\n",
      "Epoch 57/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 0.0885\n",
      "Epoch 58/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.0879\n",
      "Epoch 59/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 0.0874\n",
      "Epoch 60/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0806 - val_loss: 0.0868\n",
      "Epoch 61/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0800 - val_loss: 0.0863\n",
      "Epoch 62/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0796 - val_loss: 0.0858\n",
      "Epoch 63/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0792 - val_loss: 0.0853\n",
      "Epoch 64/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0787 - val_loss: 0.0848\n",
      "Epoch 65/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.0843\n",
      "Epoch 66/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.0839\n",
      "Epoch 67/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0774 - val_loss: 0.0834\n",
      "Epoch 68/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.0830\n",
      "Epoch 69/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0770 - val_loss: 0.0826\n",
      "Epoch 70/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0766 - val_loss: 0.0822\n",
      "Epoch 71/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0761 - val_loss: 0.0818\n",
      "Epoch 72/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0760 - val_loss: 0.0815\n",
      "Epoch 73/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0757 - val_loss: 0.0811\n",
      "Epoch 74/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0752 - val_loss: 0.0807\n",
      "Epoch 75/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0804\n",
      "Epoch 76/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0745 - val_loss: 0.0801\n",
      "Epoch 77/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0744 - val_loss: 0.0797\n",
      "Epoch 78/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0741 - val_loss: 0.0794\n",
      "Epoch 79/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0739 - val_loss: 0.0791\n",
      "Epoch 80/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0735 - val_loss: 0.0788\n",
      "Epoch 81/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0734 - val_loss: 0.0785\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0732 - val_loss: 0.0782\n",
      "Epoch 83/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0728 - val_loss: 0.0779\n",
      "Epoch 84/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.0776\n",
      "Epoch 85/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0723 - val_loss: 0.0773\n",
      "Execution time:  29.741265773773193\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0613\n",
      "Root Mean Square Error: 0.0691\n",
      "Mean Square Error: 0.0048\n",
      "\n",
      "Train RMSE: 0.069\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.061\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_138 (Dense)            (None, 72, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 72, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.2638 - val_loss: 0.1115\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.1747 - val_loss: 0.0319\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.1396 - val_loss: 0.0685\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.1163 - val_loss: 0.0495\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0938 - val_loss: 0.0328\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0148\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0535 - val_loss: 0.0061\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0392 - val_loss: 0.0072\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0307 - val_loss: 0.0114\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0284 - val_loss: 0.0156\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0161\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0160\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0162\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 22/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0168\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0163\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0168\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0163\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0168\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0163\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0168\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0163\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0161\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0163\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0168\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0163\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0165\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0160\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.0164\n",
      "Execution time:  44.771260261535645\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0263\n",
      "Root Mean Square Error: 0.0443\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.044\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_141 (Dense)            (None, 72, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 72, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2488 - val_loss: 0.0872\n",
      "Epoch 2/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1536 - val_loss: 0.0647\n",
      "Epoch 3/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.1070 - val_loss: 0.0406\n",
      "Epoch 4/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0630 - val_loss: 0.0202\n",
      "Epoch 5/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0163\n",
      "Epoch 6/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0179\n",
      "Epoch 7/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 8/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 9/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 10/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 11/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 12/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 13/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 14/85\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 15/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 16/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 17/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 18/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 19/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 20/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 21/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 22/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 23/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 24/85\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 25/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 26/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 27/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 28/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 29/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 30/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 31/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 32/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 33/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 34/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 35/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 36/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 37/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 39/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 40/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 41/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 42/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 43/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 44/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 45/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 46/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 47/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 48/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 49/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 50/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 51/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 52/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 53/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 54/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 55/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 56/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 57/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 58/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 59/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 60/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 61/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 62/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 63/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 64/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 65/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 66/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 67/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 68/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 69/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 70/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 71/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 72/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 73/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 74/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 75/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 76/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 77/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 78/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 79/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 80/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Epoch 81/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 82/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 83/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 84/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0179\n",
      "Epoch 85/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0179\n",
      "Execution time:  33.11192226409912\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0263\n",
      "Root Mean Square Error: 0.0446\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.045\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_144 (Dense)            (None, 72, 97)            194       \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 72, 16)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0753 - val_loss: 0.0816\n",
      "Epoch 2/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0609 - val_loss: 0.0706\n",
      "Epoch 3/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0541 - val_loss: 0.0627\n",
      "Epoch 4/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0484 - val_loss: 0.0550\n",
      "Epoch 5/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0436 - val_loss: 0.0486\n",
      "Epoch 6/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0397 - val_loss: 0.0436\n",
      "Epoch 7/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0367 - val_loss: 0.0390\n",
      "Epoch 8/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0347 - val_loss: 0.0398\n",
      "Epoch 9/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0322 - val_loss: 0.0353\n",
      "Epoch 10/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0304 - val_loss: 0.0311\n",
      "Epoch 11/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0292 - val_loss: 0.0282\n",
      "Epoch 12/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.0255\n",
      "Epoch 13/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0278 - val_loss: 0.0242\n",
      "Epoch 14/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0225\n",
      "Epoch 15/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.0211\n",
      "Epoch 16/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0270 - val_loss: 0.0203\n",
      "Epoch 17/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0195\n",
      "Epoch 18/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0194\n",
      "Epoch 19/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0191\n",
      "Epoch 20/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0191\n",
      "Epoch 21/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 23/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 24/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 25/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 26/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 27/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 28/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 29/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 30/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 31/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 32/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 33/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 34/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 35/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 36/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 37/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 38/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 39/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 40/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0190\n",
      "Epoch 41/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0192\n",
      "Epoch 42/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 43/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 44/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 45/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 46/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 47/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 48/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 49/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 50/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0190\n",
      "Epoch 51/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0192\n",
      "Epoch 52/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 53/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 54/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 55/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 56/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 57/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 58/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 59/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 60/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0190\n",
      "Epoch 61/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0192\n",
      "Epoch 62/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 63/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 64/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 65/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 66/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 67/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 68/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 69/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 70/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0190\n",
      "Epoch 71/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0192\n",
      "Epoch 72/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 73/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0191\n",
      "Epoch 74/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 75/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 76/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 77/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 78/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 79/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 80/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 81/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 82/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 83/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 84/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0190\n",
      "Epoch 85/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0192\n",
      "Epoch 86/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 87/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0191\n",
      "Epoch 88/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 89/98\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 90/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 91/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 92/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 93/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0191\n",
      "Epoch 94/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 95/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 96/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 97/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 98/98\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Execution time:  45.26902914047241\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0262\n",
      "Root Mean Square Error: 0.0433\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  12h\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_147 (Dense)            (None, 72, 22)            44        \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 72, 16)            368       \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 72, 1)             17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0710 - val_loss: 0.0585\n",
      "Epoch 2/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0460\n",
      "Epoch 3/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0441 - val_loss: 0.0387\n",
      "Epoch 4/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.0332\n",
      "Epoch 5/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0292\n",
      "Epoch 6/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0259\n",
      "Epoch 7/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0232\n",
      "Epoch 8/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0234\n",
      "Epoch 9/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0215\n",
      "Epoch 10/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0227\n",
      "Epoch 11/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0208\n",
      "Epoch 12/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0199\n",
      "Epoch 13/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 14/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 15/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 16/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 17/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 18/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 19/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 20/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0193\n",
      "Epoch 21/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 22/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 23/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 24/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 25/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 26/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 27/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 28/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 29/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 30/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 31/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 32/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 33/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 34/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 35/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 36/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 37/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 38/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0192\n",
      "Epoch 39/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 40/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 41/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 42/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 43/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 44/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 45/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 46/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 47/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 48/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 49/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 50/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0191\n",
      "Epoch 51/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 52/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0191\n",
      "Epoch 53/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 54/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0191\n",
      "Epoch 55/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 56/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0191\n",
      "Epoch 57/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0192\n",
      "Epoch 58/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0191\n",
      "Epoch 59/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0192\n",
      "Epoch 60/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0192\n",
      "Epoch 61/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0191\n",
      "Epoch 62/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0191\n",
      "Epoch 63/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0191\n",
      "Epoch 64/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0191\n",
      "Epoch 65/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0191\n",
      "Epoch 66/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0191\n",
      "Epoch 67/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0191\n",
      "Epoch 68/85\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.030 - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0191\n",
      "Epoch 69/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0191\n",
      "Epoch 70/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0191\n",
      "Epoch 71/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0191\n",
      "Epoch 72/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0191\n",
      "Epoch 73/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0190\n",
      "Epoch 74/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0190\n",
      "Epoch 75/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0190\n",
      "Epoch 76/85\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0190\n",
      "Epoch 77/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0190\n",
      "Epoch 78/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0191\n",
      "Epoch 79/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0190\n",
      "Epoch 80/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0190\n",
      "Epoch 81/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0191\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0190\n",
      "Epoch 83/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0190\n",
      "Epoch 84/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 85/85\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0190\n",
      "Execution time:  29.852935075759888\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0249\n",
      "Root Mean Square Error: 0.0406\n",
      "Mean Square Error: 0.0016\n",
      "\n",
      "Train RMSE: 0.041\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_150 (Dense)            (None, 144, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 144, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2205 - val_loss: 0.0856\n",
      "Epoch 2/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1550 - val_loss: 0.0504\n",
      "Epoch 3/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1171 - val_loss: 0.0426\n",
      "Epoch 4/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0883 - val_loss: 0.0153\n",
      "Epoch 5/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0609 - val_loss: 0.0063\n",
      "Epoch 6/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0393 - val_loss: 0.0105\n",
      "Epoch 7/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0282 - val_loss: 0.0133\n",
      "Epoch 8/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0275 - val_loss: 0.0143\n",
      "Epoch 9/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0274 - val_loss: 0.0151\n",
      "Epoch 10/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0273 - val_loss: 0.0152\n",
      "Epoch 11/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0273 - val_loss: 0.0158\n",
      "Epoch 12/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0272 - val_loss: 0.0156\n",
      "Epoch 13/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0272 - val_loss: 0.0159\n",
      "Epoch 14/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0156\n",
      "Epoch 15/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0272 - val_loss: 0.0160\n",
      "Epoch 16/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0157\n",
      "Epoch 17/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0160\n",
      "Epoch 18/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0157\n",
      "Epoch 19/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0160\n",
      "Epoch 20/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0157\n",
      "Epoch 21/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0160\n",
      "Epoch 22/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0157\n",
      "Epoch 23/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 24/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0159\n",
      "Epoch 25/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 26/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 27/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 28/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 29/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 30/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 31/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0160\n",
      "Epoch 32/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 33/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 34/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 35/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 36/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 37/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 38/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 39/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 40/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 41/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 42/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 43/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 44/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 45/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 46/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 47/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 48/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 49/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 50/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 51/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 52/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 53/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 54/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 55/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 56/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 57/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 58/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 59/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 60/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 61/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 62/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 63/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 64/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 65/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 66/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 68/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 69/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 70/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 71/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 72/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 73/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 74/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 75/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 76/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 77/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 78/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 79/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 80/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 81/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 82/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 83/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 84/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 85/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 86/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 87/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 88/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 89/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 90/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Epoch 91/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0163\n",
      "Epoch 92/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 93/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 94/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 95/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0162\n",
      "Epoch 96/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0270 - val_loss: 0.0159\n",
      "Epoch 97/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0161\n",
      "Epoch 98/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0270 - val_loss: 0.0158\n",
      "Execution time:  70.84632587432861\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0261\n",
      "Root Mean Square Error: 0.0444\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.044\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_153 (Dense)            (None, 144, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 144, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "134/134 [==============================] - 1s 5ms/step - loss: 0.2323 - val_loss: 0.0928\n",
      "Epoch 2/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1483 - val_loss: 0.0592\n",
      "Epoch 3/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.0336\n",
      "Epoch 4/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0166\n",
      "Epoch 5/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0170\n",
      "Epoch 6/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0176\n",
      "Epoch 7/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0175\n",
      "Epoch 8/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0175\n",
      "Epoch 9/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0176\n",
      "Epoch 10/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0179\n",
      "Epoch 11/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0180\n",
      "Epoch 12/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0176\n",
      "Epoch 13/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0182\n",
      "Epoch 14/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0175\n",
      "Epoch 15/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0179\n",
      "Epoch 16/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0181\n",
      "Epoch 17/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0177\n",
      "Epoch 18/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 19/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0181\n",
      "Epoch 20/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0177\n",
      "Epoch 21/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 22/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0181\n",
      "Epoch 23/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0177\n",
      "Epoch 24/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 25/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0182\n",
      "Epoch 26/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 27/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 28/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0182\n",
      "Epoch 29/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 30/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0179\n",
      "Epoch 31/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0179\n",
      "Epoch 32/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0181\n",
      "Epoch 33/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0177\n",
      "Epoch 34/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 35/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0182\n",
      "Epoch 36/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 37/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0182\n",
      "Epoch 39/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 40/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 41/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 42/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 43/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 44/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 45/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 46/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 47/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 48/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 49/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 50/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 51/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 52/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 53/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 54/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 55/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 56/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 57/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 58/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 59/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 60/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 61/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 62/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 63/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 64/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 65/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 66/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 67/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 68/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 69/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 70/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 71/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 72/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 73/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 74/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 75/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 76/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 77/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 78/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 79/85\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 80/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 81/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 82/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Epoch 83/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0182\n",
      "Epoch 84/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Epoch 85/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0180\n",
      "Execution time:  39.93927454948425\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0262\n",
      "Root Mean Square Error: 0.0448\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.045\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_156 (Dense)            (None, 144, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 144, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "81/81 [==============================] - 1s 10ms/step - loss: 0.0683 - val_loss: 0.0597\n",
      "Epoch 2/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0550 - val_loss: 0.0506\n",
      "Epoch 3/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0498 - val_loss: 0.0461\n",
      "Epoch 4/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0448 - val_loss: 0.0414\n",
      "Epoch 5/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0404 - val_loss: 0.0379\n",
      "Epoch 6/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0369 - val_loss: 0.0347\n",
      "Epoch 7/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0342 - val_loss: 0.0316\n",
      "Epoch 8/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0321 - val_loss: 0.0280\n",
      "Epoch 9/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0305 - val_loss: 0.0251\n",
      "Epoch 10/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0293 - val_loss: 0.0231\n",
      "Epoch 11/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0284 - val_loss: 0.0215\n",
      "Epoch 12/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0278 - val_loss: 0.0199\n",
      "Epoch 13/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0274 - val_loss: 0.0204\n",
      "Epoch 14/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0196\n",
      "Epoch 15/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0267 - val_loss: 0.0192\n",
      "Epoch 16/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0266 - val_loss: 0.0188\n",
      "Epoch 17/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0264 - val_loss: 0.0193\n",
      "Epoch 18/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 19/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 20/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 21/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0188\n",
      "Epoch 23/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 24/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0188\n",
      "Epoch 25/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 26/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 27/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 28/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 29/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 30/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 31/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 32/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 33/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 34/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 35/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 36/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 37/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 38/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0189\n",
      "Epoch 39/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 40/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 41/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 42/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 43/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 44/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 45/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 46/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 47/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 48/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 49/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 50/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 51/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 52/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 53/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 54/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 55/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 56/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 57/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 58/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 59/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 60/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 61/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 62/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 63/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 64/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 65/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 66/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 67/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 68/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 69/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 70/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 71/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 72/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 73/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 74/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 75/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 76/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 77/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 78/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 79/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 80/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 81/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 82/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 83/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 84/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 85/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 86/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 87/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 88/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 89/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 90/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 91/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 92/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 93/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 94/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 95/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 96/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Epoch 97/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0189\n",
      "Epoch 98/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0190\n",
      "Execution time:  70.98528146743774\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0260\n",
      "Root Mean Square Error: 0.0433\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_159 (Dense)            (None, 144, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 144, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 1s 5ms/step - loss: 0.0560 - val_loss: 0.0275\n",
      "Epoch 2/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.0264\n",
      "Epoch 3/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.0248\n",
      "Epoch 4/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0247\n",
      "Epoch 5/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0230\n",
      "Epoch 6/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0248\n",
      "Epoch 7/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0214\n",
      "Epoch 8/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0303 - val_loss: 0.0210\n",
      "Epoch 9/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0200\n",
      "Epoch 10/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0200\n",
      "Epoch 11/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0196\n",
      "Epoch 12/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0279 - val_loss: 0.0195\n",
      "Epoch 13/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0190\n",
      "Epoch 14/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0193\n",
      "Epoch 15/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0194\n",
      "Epoch 16/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0194\n",
      "Epoch 17/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0194\n",
      "Epoch 18/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0195\n",
      "Epoch 19/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 20/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 21/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 22/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 23/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 24/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 25/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 26/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 27/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 28/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 29/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 30/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 31/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 32/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 33/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 34/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 35/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 36/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0195\n",
      "Epoch 37/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 38/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 39/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 40/85\n",
      "134/134 [==============================] - ETA: 0s - loss: 0.028 - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 41/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 42/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 43/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 44/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 45/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 46/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 47/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 48/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 49/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 50/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 51/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 52/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 53/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 54/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 55/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 56/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 57/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0193\n",
      "Epoch 58/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 59/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 60/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 61/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 62/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 63/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 64/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 65/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 66/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 67/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 68/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 69/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 70/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 71/85\n",
      "134/134 [==============================] - ETA: 0s - loss: 0.028 - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 72/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 73/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 74/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 75/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0193\n",
      "Epoch 76/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 77/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 78/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 79/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 80/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 81/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 83/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 84/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Epoch 85/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0195\n",
      "Execution time:  39.954674243927\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0259\n",
      "Root Mean Square Error: 0.0432\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_162 (Dense)            (None, 144, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 144, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.4588 - val_loss: 0.4195\n",
      "Epoch 2/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4575 - val_loss: 0.4182\n",
      "Epoch 3/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4558 - val_loss: 0.4168\n",
      "Epoch 4/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4545 - val_loss: 0.4152\n",
      "Epoch 5/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4526 - val_loss: 0.4136\n",
      "Epoch 6/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4510 - val_loss: 0.4120\n",
      "Epoch 7/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4495 - val_loss: 0.4103\n",
      "Epoch 8/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4477 - val_loss: 0.4085\n",
      "Epoch 9/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4459 - val_loss: 0.4066\n",
      "Epoch 10/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4439 - val_loss: 0.4047\n",
      "Epoch 11/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4420 - val_loss: 0.4027\n",
      "Epoch 12/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4399 - val_loss: 0.4006\n",
      "Epoch 13/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4375 - val_loss: 0.3986\n",
      "Epoch 14/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4357 - val_loss: 0.3964\n",
      "Epoch 15/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4332 - val_loss: 0.3943\n",
      "Epoch 16/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4311 - val_loss: 0.3920\n",
      "Epoch 17/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4290 - val_loss: 0.3901\n",
      "Epoch 18/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4269 - val_loss: 0.3882\n",
      "Epoch 19/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4247 - val_loss: 0.3864\n",
      "Epoch 20/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4231 - val_loss: 0.3845\n",
      "Epoch 21/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4211 - val_loss: 0.3826\n",
      "Epoch 22/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4192 - val_loss: 0.3806\n",
      "Epoch 23/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4172 - val_loss: 0.3787\n",
      "Epoch 24/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4150 - val_loss: 0.3768\n",
      "Epoch 25/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4132 - val_loss: 0.3748\n",
      "Epoch 26/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4111 - val_loss: 0.3728\n",
      "Epoch 27/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4091 - val_loss: 0.3708\n",
      "Epoch 28/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4069 - val_loss: 0.3688\n",
      "Epoch 29/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4051 - val_loss: 0.3668\n",
      "Epoch 30/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4030 - val_loss: 0.3647\n",
      "Epoch 31/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4008 - val_loss: 0.3628\n",
      "Epoch 32/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3989 - val_loss: 0.3612\n",
      "Epoch 33/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3967 - val_loss: 0.3595\n",
      "Epoch 34/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3954 - val_loss: 0.3578\n",
      "Epoch 35/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3937 - val_loss: 0.3562\n",
      "Epoch 36/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3919 - val_loss: 0.3545\n",
      "Epoch 37/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3902 - val_loss: 0.3527\n",
      "Epoch 38/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3884 - val_loss: 0.3510\n",
      "Epoch 39/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3865 - val_loss: 0.3493\n",
      "Epoch 40/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3847 - val_loss: 0.3475\n",
      "Epoch 41/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3829 - val_loss: 0.3457\n",
      "Epoch 42/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3808 - val_loss: 0.3439\n",
      "Epoch 43/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3791 - val_loss: 0.3420\n",
      "Epoch 44/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3774 - val_loss: 0.3402\n",
      "Epoch 45/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3752 - val_loss: 0.3383\n",
      "Epoch 46/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3732 - val_loss: 0.3364\n",
      "Epoch 47/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3715 - val_loss: 0.3345\n",
      "Epoch 48/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3694 - val_loss: 0.3326\n",
      "Epoch 49/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3675 - val_loss: 0.3307\n",
      "Epoch 50/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3656 - val_loss: 0.3287\n",
      "Epoch 51/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3637 - val_loss: 0.3267\n",
      "Epoch 52/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3616 - val_loss: 0.3247\n",
      "Epoch 53/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3597 - val_loss: 0.3227\n",
      "Epoch 54/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3576 - val_loss: 0.3206\n",
      "Epoch 55/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3554 - val_loss: 0.3185\n",
      "Epoch 56/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3532 - val_loss: 0.3164\n",
      "Epoch 57/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3512 - val_loss: 0.3142\n",
      "Epoch 58/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3491 - val_loss: 0.3121\n",
      "Epoch 59/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3472 - val_loss: 0.3099\n",
      "Epoch 60/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3448 - val_loss: 0.3076\n",
      "Epoch 61/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3422 - val_loss: 0.3054\n",
      "Epoch 62/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3402 - val_loss: 0.3031\n",
      "Epoch 63/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3382 - val_loss: 0.3008\n",
      "Epoch 64/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3358 - val_loss: 0.2985\n",
      "Epoch 65/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3336 - val_loss: 0.2962\n",
      "Epoch 66/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3314 - val_loss: 0.2939\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3291 - val_loss: 0.2915\n",
      "Epoch 68/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3267 - val_loss: 0.2892\n",
      "Epoch 69/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3249 - val_loss: 0.2869\n",
      "Epoch 70/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3231 - val_loss: 0.2846\n",
      "Epoch 71/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3206 - val_loss: 0.2824\n",
      "Epoch 72/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3188 - val_loss: 0.2802\n",
      "Epoch 73/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3170 - val_loss: 0.2782\n",
      "Epoch 74/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3154 - val_loss: 0.2762\n",
      "Epoch 75/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3139 - val_loss: 0.2744\n",
      "Epoch 76/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3122 - val_loss: 0.2728\n",
      "Epoch 77/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3109 - val_loss: 0.2715\n",
      "Epoch 78/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3100 - val_loss: 0.2702\n",
      "Epoch 79/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3085 - val_loss: 0.2690\n",
      "Epoch 80/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3072 - val_loss: 0.2678\n",
      "Epoch 81/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3064 - val_loss: 0.2666\n",
      "Epoch 82/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3055 - val_loss: 0.2655\n",
      "Epoch 83/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3041 - val_loss: 0.2644\n",
      "Epoch 84/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3033 - val_loss: 0.2638\n",
      "Epoch 85/98\n",
      "81/81 [==============================] - 1s 10ms/step - loss: 0.3026 - val_loss: 0.2634\n",
      "Epoch 86/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3018 - val_loss: 0.2631\n",
      "Epoch 87/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3012 - val_loss: 0.2628\n",
      "Epoch 88/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3011 - val_loss: 0.2625\n",
      "Epoch 89/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3006 - val_loss: 0.2621\n",
      "Epoch 90/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3005 - val_loss: 0.2618\n",
      "Epoch 91/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.3001 - val_loss: 0.2615\n",
      "Epoch 92/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.2994 - val_loss: 0.2612\n",
      "Epoch 93/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.2992 - val_loss: 0.2609\n",
      "Epoch 94/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.2988 - val_loss: 0.2606\n",
      "Epoch 95/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.2984 - val_loss: 0.2602\n",
      "Epoch 96/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.2979 - val_loss: 0.2599\n",
      "Epoch 97/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.2980 - val_loss: 0.2596\n",
      "Epoch 98/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.2976 - val_loss: 0.2593\n",
      "Execution time:  70.4985499382019\n",
      "DNN:\n",
      "Mean Absolute Error: 0.2886\n",
      "Root Mean Square Error: 0.2911\n",
      "Mean Square Error: 0.0848\n",
      "\n",
      "Train RMSE: 0.291\n",
      "Train MSE: 0.085\n",
      "Train MAE: 0.289\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_165 (Dense)            (None, 144, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 144, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "134/134 [==============================] - 1s 5ms/step - loss: 0.3997 - val_loss: 0.3784\n",
      "Epoch 2/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3983 - val_loss: 0.3769\n",
      "Epoch 3/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3965 - val_loss: 0.3753\n",
      "Epoch 4/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3947 - val_loss: 0.3735\n",
      "Epoch 5/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3930 - val_loss: 0.3716\n",
      "Epoch 6/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3907 - val_loss: 0.3695\n",
      "Epoch 7/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3886 - val_loss: 0.3673\n",
      "Epoch 8/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3865 - val_loss: 0.3651\n",
      "Epoch 9/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3841 - val_loss: 0.3627\n",
      "Epoch 10/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.3818 - val_loss: 0.3602\n",
      "Epoch 11/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.3790 - val_loss: 0.3576\n",
      "Epoch 12/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.3767 - val_loss: 0.3549\n",
      "Epoch 13/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3739 - val_loss: 0.3522\n",
      "Epoch 14/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3714 - val_loss: 0.3493\n",
      "Epoch 15/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3681 - val_loss: 0.3463\n",
      "Epoch 16/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3650 - val_loss: 0.3433\n",
      "Epoch 17/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3621 - val_loss: 0.3401\n",
      "Epoch 18/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3593 - val_loss: 0.3369\n",
      "Epoch 19/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3555 - val_loss: 0.3336\n",
      "Epoch 20/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 0.3302\n",
      "Epoch 21/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3487 - val_loss: 0.3267\n",
      "Epoch 22/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3454 - val_loss: 0.3232\n",
      "Epoch 23/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3423 - val_loss: 0.3196\n",
      "Epoch 24/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3386 - val_loss: 0.3159\n",
      "Epoch 25/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3349 - val_loss: 0.3122\n",
      "Epoch 26/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3312 - val_loss: 0.3084\n",
      "Epoch 27/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3280 - val_loss: 0.3045\n",
      "Epoch 28/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3237 - val_loss: 0.3006\n",
      "Epoch 29/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3202 - val_loss: 0.2966\n",
      "Epoch 30/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3164 - val_loss: 0.2926\n",
      "Epoch 31/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 0.2885\n",
      "Epoch 32/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3088 - val_loss: 0.2843\n",
      "Epoch 33/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3046 - val_loss: 0.2801\n",
      "Epoch 34/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.3010 - val_loss: 0.2759\n",
      "Epoch 35/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2976 - val_loss: 0.2717\n",
      "Epoch 36/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 0.2676\n",
      "Epoch 37/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2901 - val_loss: 0.2635\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2864 - val_loss: 0.2596\n",
      "Epoch 39/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.2834 - val_loss: 0.2559\n",
      "Epoch 40/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2808 - val_loss: 0.2524\n",
      "Epoch 41/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2780 - val_loss: 0.2490\n",
      "Epoch 42/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2752 - val_loss: 0.2458\n",
      "Epoch 43/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2733 - val_loss: 0.2426\n",
      "Epoch 44/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2707 - val_loss: 0.2395\n",
      "Epoch 45/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.2681 - val_loss: 0.2364\n",
      "Epoch 46/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.2663 - val_loss: 0.2334\n",
      "Epoch 47/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2639 - val_loss: 0.2304\n",
      "Epoch 48/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2619 - val_loss: 0.2276\n",
      "Epoch 49/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2596 - val_loss: 0.2252\n",
      "Epoch 50/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2583 - val_loss: 0.2229\n",
      "Epoch 51/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2571 - val_loss: 0.2207\n",
      "Epoch 52/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 0.2186\n",
      "Epoch 53/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2545 - val_loss: 0.2165\n",
      "Epoch 54/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2530 - val_loss: 0.2144\n",
      "Epoch 55/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2518 - val_loss: 0.2123\n",
      "Epoch 56/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2503 - val_loss: 0.2103\n",
      "Epoch 57/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2497 - val_loss: 0.2083\n",
      "Epoch 58/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2480 - val_loss: 0.2063\n",
      "Epoch 59/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2473 - val_loss: 0.2043\n",
      "Epoch 60/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2459 - val_loss: 0.2023\n",
      "Epoch 61/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2448 - val_loss: 0.2002\n",
      "Epoch 62/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2438 - val_loss: 0.1982\n",
      "Epoch 63/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2422 - val_loss: 0.1962\n",
      "Epoch 64/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2416 - val_loss: 0.1941\n",
      "Epoch 65/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2400 - val_loss: 0.1920\n",
      "Epoch 66/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2392 - val_loss: 0.1898\n",
      "Epoch 67/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2381 - val_loss: 0.1877\n",
      "Epoch 68/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2367 - val_loss: 0.1855\n",
      "Epoch 69/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2359 - val_loss: 0.1833\n",
      "Epoch 70/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2342 - val_loss: 0.1810\n",
      "Epoch 71/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2333 - val_loss: 0.1788\n",
      "Epoch 72/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2321 - val_loss: 0.1765\n",
      "Epoch 73/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2307 - val_loss: 0.1741\n",
      "Epoch 74/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 0.1718\n",
      "Epoch 75/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2282 - val_loss: 0.1694\n",
      "Epoch 76/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2270 - val_loss: 0.1670\n",
      "Epoch 77/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2262 - val_loss: 0.1646\n",
      "Epoch 78/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2251 - val_loss: 0.1622\n",
      "Epoch 79/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 0.1597\n",
      "Epoch 80/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.2226 - val_loss: 0.1573\n",
      "Epoch 81/85\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.2212 - val_loss: 0.1549\n",
      "Epoch 82/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2201 - val_loss: 0.1525\n",
      "Epoch 83/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.2192 - val_loss: 0.1502\n",
      "Epoch 84/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.2181 - val_loss: 0.1479\n",
      "Epoch 85/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.2169 - val_loss: 0.1457\n",
      "Execution time:  40.04785752296448\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1596\n",
      "Root Mean Square Error: 0.1629\n",
      "Mean Square Error: 0.0265\n",
      "\n",
      "Train RMSE: 0.163\n",
      "Train MSE: 0.027\n",
      "Train MAE: 0.160\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_168 (Dense)            (None, 144, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 144, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.1108 - val_loss: 0.1422\n",
      "Epoch 2/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1103 - val_loss: 0.1417\n",
      "Epoch 3/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1098 - val_loss: 0.1413\n",
      "Epoch 4/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1093 - val_loss: 0.1407\n",
      "Epoch 5/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1087 - val_loss: 0.1402\n",
      "Epoch 6/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1082 - val_loss: 0.1396\n",
      "Epoch 7/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.1390\n",
      "Epoch 8/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1070 - val_loss: 0.1384\n",
      "Epoch 9/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1064 - val_loss: 0.1378\n",
      "Epoch 10/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1058 - val_loss: 0.1372\n",
      "Epoch 11/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1051 - val_loss: 0.1365\n",
      "Epoch 12/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1044 - val_loss: 0.1358\n",
      "Epoch 13/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1038 - val_loss: 0.1351\n",
      "Epoch 14/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1031 - val_loss: 0.1344\n",
      "Epoch 15/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1024 - val_loss: 0.1337\n",
      "Epoch 16/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1016 - val_loss: 0.1330\n",
      "Epoch 17/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1010 - val_loss: 0.1325\n",
      "Epoch 18/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1004 - val_loss: 0.1321\n",
      "Epoch 19/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1000 - val_loss: 0.1317\n",
      "Epoch 20/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0997 - val_loss: 0.1314\n",
      "Epoch 21/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0994 - val_loss: 0.1311\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0990 - val_loss: 0.1307\n",
      "Epoch 23/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0987 - val_loss: 0.1304\n",
      "Epoch 24/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0983 - val_loss: 0.1300\n",
      "Epoch 25/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0980 - val_loss: 0.1297\n",
      "Epoch 26/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0976 - val_loss: 0.1293\n",
      "Epoch 27/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0974 - val_loss: 0.1290\n",
      "Epoch 28/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0970 - val_loss: 0.1286\n",
      "Epoch 29/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0966 - val_loss: 0.1282\n",
      "Epoch 30/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0963 - val_loss: 0.1279\n",
      "Epoch 31/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0959 - val_loss: 0.1275\n",
      "Epoch 32/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0956 - val_loss: 0.1271\n",
      "Epoch 33/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0952 - val_loss: 0.1267\n",
      "Epoch 34/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0949 - val_loss: 0.1263\n",
      "Epoch 35/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0946 - val_loss: 0.1260\n",
      "Epoch 36/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0942 - val_loss: 0.1256\n",
      "Epoch 37/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0939 - val_loss: 0.1252\n",
      "Epoch 38/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0935 - val_loss: 0.1248\n",
      "Epoch 39/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0932 - val_loss: 0.1244\n",
      "Epoch 40/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0929 - val_loss: 0.1240\n",
      "Epoch 41/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0925 - val_loss: 0.1237\n",
      "Epoch 42/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0922 - val_loss: 0.1233\n",
      "Epoch 43/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0919 - val_loss: 0.1229\n",
      "Epoch 44/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0916 - val_loss: 0.1225\n",
      "Epoch 45/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0913 - val_loss: 0.1222\n",
      "Epoch 46/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0910 - val_loss: 0.1218\n",
      "Epoch 47/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0907 - val_loss: 0.1214\n",
      "Epoch 48/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0903 - val_loss: 0.1211\n",
      "Epoch 49/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0900 - val_loss: 0.1207\n",
      "Epoch 50/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0897 - val_loss: 0.1203\n",
      "Epoch 51/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0895 - val_loss: 0.1200\n",
      "Epoch 52/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0892 - val_loss: 0.1196\n",
      "Epoch 53/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0889 - val_loss: 0.1193\n",
      "Epoch 54/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0887 - val_loss: 0.1190\n",
      "Epoch 55/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0884 - val_loss: 0.1186\n",
      "Epoch 56/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0882 - val_loss: 0.1183\n",
      "Epoch 57/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0880 - val_loss: 0.1181\n",
      "Epoch 58/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0877 - val_loss: 0.1178\n",
      "Epoch 59/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0875 - val_loss: 0.1175\n",
      "Epoch 60/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0873 - val_loss: 0.1173\n",
      "Epoch 61/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0872 - val_loss: 0.1170\n",
      "Epoch 62/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0870 - val_loss: 0.1168\n",
      "Epoch 63/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0868 - val_loss: 0.1165\n",
      "Epoch 64/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0867 - val_loss: 0.1163\n",
      "Epoch 65/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0865 - val_loss: 0.1160\n",
      "Epoch 66/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0863 - val_loss: 0.1158\n",
      "Epoch 67/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0861 - val_loss: 0.1155\n",
      "Epoch 68/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0860 - val_loss: 0.1153\n",
      "Epoch 69/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0859 - val_loss: 0.1151\n",
      "Epoch 70/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0858 - val_loss: 0.1148\n",
      "Epoch 71/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0856 - val_loss: 0.1146\n",
      "Epoch 72/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0854 - val_loss: 0.1144\n",
      "Epoch 73/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0853 - val_loss: 0.1142\n",
      "Epoch 74/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0852 - val_loss: 0.1139\n",
      "Epoch 75/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0850 - val_loss: 0.1137\n",
      "Epoch 76/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0850 - val_loss: 0.1135\n",
      "Epoch 77/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0848 - val_loss: 0.1133\n",
      "Epoch 78/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0846 - val_loss: 0.1130\n",
      "Epoch 79/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0845 - val_loss: 0.1128\n",
      "Epoch 80/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0844 - val_loss: 0.1126\n",
      "Epoch 81/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0843 - val_loss: 0.1124\n",
      "Epoch 82/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0842 - val_loss: 0.1122\n",
      "Epoch 83/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0840 - val_loss: 0.1120\n",
      "Epoch 84/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0839 - val_loss: 0.1118\n",
      "Epoch 85/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0838 - val_loss: 0.1116\n",
      "Epoch 86/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0837 - val_loss: 0.1114\n",
      "Epoch 87/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0835 - val_loss: 0.1112\n",
      "Epoch 88/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0834 - val_loss: 0.1110\n",
      "Epoch 89/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0833 - val_loss: 0.1108\n",
      "Epoch 90/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0833 - val_loss: 0.1106\n",
      "Epoch 91/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0830 - val_loss: 0.1104\n",
      "Epoch 92/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0830 - val_loss: 0.1102\n",
      "Epoch 93/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0830 - val_loss: 0.1100\n",
      "Epoch 94/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0829 - val_loss: 0.1098\n",
      "Epoch 95/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0827 - val_loss: 0.1097\n",
      "Epoch 96/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0828 - val_loss: 0.1095\n",
      "Epoch 97/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0826 - val_loss: 0.1093\n",
      "Epoch 98/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0825 - val_loss: 0.1091\n",
      "Execution time:  70.73938226699829\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0799\n",
      "Root Mean Square Error: 0.0870\n",
      "Mean Square Error: 0.0076\n",
      "\n",
      "Train RMSE: 0.087\n",
      "Train MSE: 0.008\n",
      "Train MAE: 0.080\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_171 (Dense)            (None, 144, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 144, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 1s 5ms/step - loss: 0.1071 - val_loss: 0.1244\n",
      "Epoch 2/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.1238\n",
      "Epoch 3/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 0.1231\n",
      "Epoch 4/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 0.1225\n",
      "Epoch 5/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1047 - val_loss: 0.1218\n",
      "Epoch 6/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 0.1210\n",
      "Epoch 7/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1203\n",
      "Epoch 8/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 0.1195\n",
      "Epoch 9/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 0.1187\n",
      "Epoch 10/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.1011 - val_loss: 0.1179\n",
      "Epoch 11/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.1171\n",
      "Epoch 12/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0996 - val_loss: 0.1163\n",
      "Epoch 13/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0987 - val_loss: 0.1154\n",
      "Epoch 14/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 0.1146\n",
      "Epoch 15/85\n",
      "134/134 [==============================] - ETA: 0s - loss: 0.095 - 0s 3ms/step - loss: 0.0971 - val_loss: 0.1138\n",
      "Epoch 16/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 0.1129\n",
      "Epoch 17/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 0.1121\n",
      "Epoch 18/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0948 - val_loss: 0.1112\n",
      "Epoch 19/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 0.1103\n",
      "Epoch 20/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 0.1095\n",
      "Epoch 21/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.1086\n",
      "Epoch 22/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.1078\n",
      "Epoch 23/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 0.1069\n",
      "Epoch 24/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 0.1061\n",
      "Epoch 25/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 0.1053\n",
      "Epoch 26/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 0.1044\n",
      "Epoch 27/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 0.1036\n",
      "Epoch 28/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 0.1028\n",
      "Epoch 29/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 0.1020\n",
      "Epoch 30/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 0.1012\n",
      "Epoch 31/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0853 - val_loss: 0.1005\n",
      "Epoch 32/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0848 - val_loss: 0.0999\n",
      "Epoch 33/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0842 - val_loss: 0.0993\n",
      "Epoch 34/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 0.0987\n",
      "Epoch 35/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0833 - val_loss: 0.0982\n",
      "Epoch 36/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 0.0977\n",
      "Epoch 37/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 0.0971\n",
      "Epoch 38/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 0.0966\n",
      "Epoch 39/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0816 - val_loss: 0.0961\n",
      "Epoch 40/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.0956\n",
      "Epoch 41/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 0.0951\n",
      "Epoch 42/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0805 - val_loss: 0.0946\n",
      "Epoch 43/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0802 - val_loss: 0.0941\n",
      "Epoch 44/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 0.0937\n",
      "Epoch 45/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 0.0932\n",
      "Epoch 46/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0791 - val_loss: 0.0928\n",
      "Epoch 47/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 0.0923\n",
      "Epoch 48/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0919\n",
      "Epoch 49/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0780 - val_loss: 0.0914\n",
      "Epoch 50/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 0.0910\n",
      "Epoch 51/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0775 - val_loss: 0.0906\n",
      "Epoch 52/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.0902\n",
      "Epoch 53/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 0.0898\n",
      "Epoch 54/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0767 - val_loss: 0.0893\n",
      "Epoch 55/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0890\n",
      "Epoch 56/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 0.0886\n",
      "Epoch 57/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 0.0882\n",
      "Epoch 58/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.0878\n",
      "Epoch 59/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0753 - val_loss: 0.0874\n",
      "Epoch 60/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.0871\n",
      "Epoch 61/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0747 - val_loss: 0.0867\n",
      "Epoch 62/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 0.0863\n",
      "Epoch 63/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.0860\n",
      "Epoch 64/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.0856\n",
      "Epoch 65/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.0853\n",
      "Epoch 66/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0850\n",
      "Epoch 67/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0846\n",
      "Epoch 68/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0843\n",
      "Epoch 69/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0840\n",
      "Epoch 70/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0837\n",
      "Epoch 71/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0834\n",
      "Epoch 72/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0830\n",
      "Epoch 73/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0827\n",
      "Epoch 74/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0824\n",
      "Epoch 75/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0822\n",
      "Epoch 76/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0819\n",
      "Epoch 77/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0816\n",
      "Epoch 78/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0813\n",
      "Epoch 79/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.0810\n",
      "Epoch 80/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0707 - val_loss: 0.0807\n",
      "Epoch 81/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0705 - val_loss: 0.0805\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0703 - val_loss: 0.0802\n",
      "Epoch 83/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.0799\n",
      "Epoch 84/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0701 - val_loss: 0.0797\n",
      "Epoch 85/85\n",
      "134/134 [==============================] - ETA: 0s - loss: 0.068 - 0s 3ms/step - loss: 0.0700 - val_loss: 0.0795\n",
      "Execution time:  39.990984201431274\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0635\n",
      "Root Mean Square Error: 0.0713\n",
      "Mean Square Error: 0.0051\n",
      "\n",
      "Train RMSE: 0.071\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.063\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_174 (Dense)            (None, 144, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 144, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.2407 - val_loss: 0.1137\n",
      "Epoch 2/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1977 - val_loss: 0.0928\n",
      "Epoch 3/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.1672 - val_loss: 0.0714\n",
      "Epoch 4/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1345 - val_loss: 0.0489\n",
      "Epoch 5/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.1018 - val_loss: 0.0287\n",
      "Epoch 6/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0723 - val_loss: 0.0139\n",
      "Epoch 7/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0473 - val_loss: 0.0081\n",
      "Epoch 8/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0296 - val_loss: 0.0139\n",
      "Epoch 9/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0271 - val_loss: 0.0174\n",
      "Epoch 10/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0268 - val_loss: 0.0164\n",
      "Epoch 11/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 12/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0164\n",
      "Epoch 13/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 14/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 15/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 16/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 17/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 18/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 19/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 20/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 21/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 22/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 23/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 24/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 25/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 26/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 27/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 28/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 29/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 30/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 31/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 32/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 33/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 34/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 35/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 36/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 37/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 38/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 39/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 40/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 41/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 42/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 43/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 44/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 45/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 46/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 47/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 48/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 49/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 50/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 51/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 52/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 53/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 54/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 55/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 56/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0164\n",
      "Epoch 57/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 58/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 59/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 60/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0164\n",
      "Epoch 61/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 62/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 63/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 64/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 65/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 66/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 67/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 68/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 69/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 70/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 71/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 72/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 73/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 74/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 75/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 76/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 77/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 78/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 79/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 80/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 81/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 82/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 83/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 84/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 85/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 86/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 87/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 88/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 89/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 90/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 91/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 92/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 93/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Epoch 94/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 95/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0166\n",
      "Epoch 96/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0168\n",
      "Epoch 97/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0165\n",
      "Epoch 98/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0167\n",
      "Execution time:  70.42355418205261\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0261\n",
      "Root Mean Square Error: 0.0441\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.044\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_177 (Dense)            (None, 144, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 144, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.1965 - val_loss: 0.0618\n",
      "Epoch 2/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.1298 - val_loss: 0.0425\n",
      "Epoch 3/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0995 - val_loss: 0.0264\n",
      "Epoch 4/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0259\n",
      "Epoch 5/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.0191\n",
      "Epoch 6/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0168\n",
      "Epoch 7/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0162\n",
      "Epoch 8/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0167\n",
      "Epoch 9/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0174\n",
      "Epoch 10/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0175\n",
      "Epoch 11/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.0175\n",
      "Epoch 12/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 13/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 14/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 15/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 16/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 17/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 18/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 19/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 20/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 21/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 22/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0175\n",
      "Epoch 23/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0175\n",
      "Epoch 24/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 25/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 26/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 27/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 28/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 29/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 30/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 31/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 32/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 33/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 34/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 35/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 36/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 37/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 38/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 39/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 40/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 41/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 42/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 43/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 44/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 45/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 46/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 47/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 48/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 49/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 50/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0173\n",
      "Epoch 51/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 52/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0172\n",
      "Epoch 53/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0175\n",
      "Epoch 54/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 55/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 56/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0171\n",
      "Epoch 57/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0175\n",
      "Epoch 58/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 59/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0174\n",
      "Epoch 60/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0171\n",
      "Epoch 61/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0175\n",
      "Epoch 62/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 63/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0176\n",
      "Epoch 64/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 65/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0175\n",
      "Epoch 66/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 67/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0176\n",
      "Epoch 68/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0172\n",
      "Epoch 69/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0176\n",
      "Epoch 70/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 71/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0176\n",
      "Epoch 72/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 73/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0175\n",
      "Epoch 74/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 75/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0175\n",
      "Epoch 76/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 77/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0175\n",
      "Epoch 78/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0177\n",
      "Epoch 79/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 80/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0177\n",
      "Epoch 81/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0176\n",
      "Epoch 82/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 83/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0176\n",
      "Epoch 84/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0174\n",
      "Epoch 85/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0178\n",
      "Execution time:  40.264103412628174\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0258\n",
      "Root Mean Square Error: 0.0440\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.044\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_180 (Dense)            (None, 144, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 144, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0600 - val_loss: 0.0531\n",
      "Epoch 2/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0478\n",
      "Epoch 3/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0493 - val_loss: 0.0432\n",
      "Epoch 4/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0461 - val_loss: 0.0392\n",
      "Epoch 5/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0437 - val_loss: 0.0364\n",
      "Epoch 6/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0417 - val_loss: 0.0346\n",
      "Epoch 7/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0399 - val_loss: 0.0335\n",
      "Epoch 8/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0382 - val_loss: 0.0317\n",
      "Epoch 9/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0366 - val_loss: 0.0294\n",
      "Epoch 10/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0353 - val_loss: 0.0278\n",
      "Epoch 11/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0341 - val_loss: 0.0257\n",
      "Epoch 12/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0333 - val_loss: 0.0267\n",
      "Epoch 13/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0320 - val_loss: 0.0248\n",
      "Epoch 14/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0312 - val_loss: 0.0233\n",
      "Epoch 15/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0305 - val_loss: 0.0238\n",
      "Epoch 16/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0295 - val_loss: 0.0223\n",
      "Epoch 17/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0289 - val_loss: 0.0208\n",
      "Epoch 18/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0284 - val_loss: 0.0185\n",
      "Epoch 19/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0280 - val_loss: 0.0190\n",
      "Epoch 20/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0277 - val_loss: 0.0182\n",
      "Epoch 21/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0274 - val_loss: 0.0175\n",
      "Epoch 22/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0272 - val_loss: 0.0179\n",
      "Epoch 23/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0175\n",
      "Epoch 24/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0269 - val_loss: 0.0196\n",
      "Epoch 25/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0266 - val_loss: 0.0206\n",
      "Epoch 26/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0194\n",
      "Epoch 27/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 28/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0190\n",
      "Epoch 29/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 30/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 31/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 32/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 33/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 34/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0193\n",
      "Epoch 35/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0182\n",
      "Epoch 36/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0263 - val_loss: 0.0191\n",
      "Epoch 37/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 38/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 39/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 40/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 41/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 42/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0190\n",
      "Epoch 43/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 44/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 45/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 46/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 47/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 48/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 49/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 50/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 51/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 52/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 53/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 54/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 55/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 56/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 57/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 58/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 59/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 60/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 61/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 62/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 63/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 64/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 65/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 66/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 67/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 68/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 69/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 70/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0190\n",
      "Epoch 71/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 72/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 73/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 74/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 75/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 76/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 77/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 78/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 79/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 80/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 81/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0190\n",
      "Epoch 82/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 83/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0190\n",
      "Epoch 84/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 85/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 86/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 87/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 88/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 89/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 90/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 91/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 92/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 93/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Epoch 94/98\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0190\n",
      "Epoch 95/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 96/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 97/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0191\n",
      "Epoch 98/98\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0192\n",
      "Execution time:  70.89094686508179\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0260\n",
      "Root Mean Square Error: 0.0432\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  1d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_183 (Dense)            (None, 144, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 144, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 144, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 144, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 1s 5ms/step - loss: 0.0680 - val_loss: 0.0601\n",
      "Epoch 2/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.0449\n",
      "Epoch 3/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.0388\n",
      "Epoch 4/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0336\n",
      "Epoch 5/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0292\n",
      "Epoch 6/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0255\n",
      "Epoch 7/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0295\n",
      "Epoch 8/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0250\n",
      "Epoch 9/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0226\n",
      "Epoch 10/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0212\n",
      "Epoch 11/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0204\n",
      "Epoch 12/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0199\n",
      "Epoch 13/85\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0274 - val_loss: 0.0197\n",
      "Epoch 14/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 15/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 16/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 17/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 18/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 19/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 20/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 21/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 22/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 23/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 24/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 25/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 26/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 27/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 28/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 29/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 30/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 31/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 32/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 33/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 34/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 35/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 36/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 37/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 38/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 39/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 40/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 41/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 42/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 43/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 44/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 45/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 46/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 47/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 48/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 49/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 50/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 51/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 52/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 53/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 54/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 55/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 56/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 57/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 58/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 59/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 60/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0185\n",
      "Epoch 61/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 62/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 63/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 64/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 65/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 66/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 67/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 68/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 69/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 70/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 71/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 72/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 73/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 74/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 75/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 76/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 77/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 78/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 79/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 80/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Epoch 81/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 83/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 84/85\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0196\n",
      "Epoch 85/85\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0195\n",
      "Execution time:  40.00578212738037\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0260\n",
      "Root Mean Square Error: 0.0434\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_186 (Dense)            (None, 432, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 432, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "75/75 [==============================] - 2s 29ms/step - loss: 0.1866 - val_loss: 0.0372\n",
      "Epoch 2/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1411 - val_loss: 0.0373\n",
      "Epoch 3/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1159 - val_loss: 0.0140\n",
      "Epoch 4/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0890 - val_loss: 0.0065\n",
      "Epoch 5/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0643 - val_loss: 0.0067\n",
      "Epoch 6/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0451 - val_loss: 0.0072\n",
      "Epoch 7/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0311 - val_loss: 0.0120\n",
      "Epoch 8/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0264 - val_loss: 0.0141\n",
      "Epoch 9/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0261 - val_loss: 0.0153\n",
      "Epoch 10/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0260 - val_loss: 0.0160\n",
      "Epoch 11/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0259 - val_loss: 0.0162\n",
      "Epoch 12/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0259 - val_loss: 0.0165\n",
      "Epoch 13/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0258 - val_loss: 0.0161\n",
      "Epoch 14/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0258 - val_loss: 0.0166\n",
      "Epoch 15/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0162\n",
      "Epoch 16/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0258 - val_loss: 0.0166\n",
      "Epoch 17/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0164\n",
      "Epoch 18/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0258 - val_loss: 0.0168\n",
      "Epoch 19/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0164\n",
      "Epoch 20/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0168\n",
      "Epoch 21/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0165\n",
      "Epoch 22/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0168\n",
      "Epoch 23/98\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 0.0257 - val_loss: 0.0165\n",
      "Epoch 24/98\n",
      "75/75 [==============================] - 2s 27ms/step - loss: 0.0257 - val_loss: 0.0168\n",
      "Epoch 25/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0165\n",
      "Epoch 26/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0257 - val_loss: 0.0168\n",
      "Epoch 27/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0165\n",
      "Epoch 28/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0168\n",
      "Epoch 29/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 30/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 31/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 32/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 33/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 34/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 35/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0256 - val_loss: 0.0165\n",
      "Epoch 36/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 37/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0165\n",
      "Epoch 38/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0168\n",
      "Epoch 39/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 40/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 41/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 42/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 43/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 44/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 45/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 46/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 47/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0167\n",
      "Epoch 48/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 49/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 50/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 51/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 52/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 53/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 54/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 55/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0165\n",
      "Epoch 56/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 57/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0167\n",
      "Epoch 58/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 59/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 60/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 61/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 62/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 63/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 64/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 65/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0167\n",
      "Epoch 66/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 67/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 68/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 69/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 70/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 71/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 72/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 73/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0256 - val_loss: 0.0167\n",
      "Epoch 74/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 75/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 76/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 77/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 78/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 79/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 80/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 81/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 82/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0169\n",
      "Epoch 83/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0167\n",
      "Epoch 84/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 85/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 86/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 87/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 88/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 89/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 90/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 91/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 92/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 93/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 94/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 95/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 96/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Epoch 97/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0256 - val_loss: 0.0166\n",
      "Epoch 98/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0257 - val_loss: 0.0170\n",
      "Execution time:  175.17628717422485\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0252\n",
      "Root Mean Square Error: 0.0435\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.044\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_189 (Dense)            (None, 432, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 432, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.2095 - val_loss: 0.0412\n",
      "Epoch 2/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0307\n",
      "Epoch 3/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0886 - val_loss: 0.0141\n",
      "Epoch 4/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0526 - val_loss: 0.0108\n",
      "Epoch 5/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0310 - val_loss: 0.0109\n",
      "Epoch 6/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0110\n",
      "Epoch 7/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0289 - val_loss: 0.0110\n",
      "Epoch 8/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0114\n",
      "Epoch 9/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0116\n",
      "Epoch 10/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0117\n",
      "Epoch 11/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0287 - val_loss: 0.0119\n",
      "Epoch 12/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0119\n",
      "Epoch 13/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0119\n",
      "Epoch 14/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0120\n",
      "Epoch 15/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0120\n",
      "Epoch 16/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0120\n",
      "Epoch 17/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0120\n",
      "Epoch 18/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0120\n",
      "Epoch 19/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0121\n",
      "Epoch 20/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0121\n",
      "Epoch 21/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0121\n",
      "Epoch 22/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0121\n",
      "Epoch 23/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0121\n",
      "Epoch 24/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0121\n",
      "Epoch 25/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0121\n",
      "Epoch 26/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 27/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 28/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 29/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 30/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 31/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 32/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 33/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 34/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 35/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0120\n",
      "Epoch 36/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0122\n",
      "Epoch 37/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 38/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 39/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 40/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 41/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 42/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 43/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 44/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 45/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 46/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 47/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 48/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0122\n",
      "Epoch 49/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 50/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 51/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 52/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 53/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 54/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 55/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 56/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 57/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 58/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 59/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 60/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0122\n",
      "Epoch 61/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 62/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 63/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 64/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 65/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 66/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 67/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 68/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 69/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 70/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 71/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 72/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0122\n",
      "Epoch 73/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 74/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0122\n",
      "Epoch 75/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 76/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 77/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 78/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 79/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 80/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 81/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 82/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0285 - val_loss: 0.0122\n",
      "Epoch 83/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 84/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Epoch 85/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0284 - val_loss: 0.0121\n",
      "Execution time:  64.7225935459137\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0255\n",
      "Root Mean Square Error: 0.0452\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.045\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_192 (Dense)            (None, 432, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 432, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0685 - val_loss: 0.0438\n",
      "Epoch 2/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0521 - val_loss: 0.0394\n",
      "Epoch 3/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0478 - val_loss: 0.0333\n",
      "Epoch 4/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0439 - val_loss: 0.0319\n",
      "Epoch 5/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0408 - val_loss: 0.0306\n",
      "Epoch 6/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0380 - val_loss: 0.0289\n",
      "Epoch 7/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0356 - val_loss: 0.0269\n",
      "Epoch 8/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0336 - val_loss: 0.0253\n",
      "Epoch 9/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0319 - val_loss: 0.0237\n",
      "Epoch 10/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0305 - val_loss: 0.0223\n",
      "Epoch 11/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0294 - val_loss: 0.0210\n",
      "Epoch 12/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0284 - val_loss: 0.0198\n",
      "Epoch 13/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0277 - val_loss: 0.0190\n",
      "Epoch 14/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0271 - val_loss: 0.0183\n",
      "Epoch 15/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0266 - val_loss: 0.0178\n",
      "Epoch 16/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0263 - val_loss: 0.0176\n",
      "Epoch 17/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0260 - val_loss: 0.0178\n",
      "Epoch 18/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0257 - val_loss: 0.0178\n",
      "Epoch 19/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0176\n",
      "Epoch 20/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0253 - val_loss: 0.0181\n",
      "Epoch 21/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0252 - val_loss: 0.0184\n",
      "Epoch 22/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0251 - val_loss: 0.0182\n",
      "Epoch 23/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0250 - val_loss: 0.0188\n",
      "Epoch 24/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0249 - val_loss: 0.0186\n",
      "Epoch 25/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0249 - val_loss: 0.0188\n",
      "Epoch 26/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0249 - val_loss: 0.0188\n",
      "Epoch 27/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 28/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 29/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 30/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 31/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 32/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 33/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 34/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 35/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 36/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 37/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 38/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 39/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 40/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0188\n",
      "Epoch 41/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 42/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 43/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 44/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 45/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 46/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 47/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 48/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 49/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 50/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 51/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 52/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 53/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 54/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 55/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 56/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 57/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 58/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 59/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 60/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 61/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 62/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 63/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 64/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 65/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 66/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 67/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 68/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 69/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 70/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 71/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 72/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 73/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 74/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189: 0s - l\n",
      "Epoch 75/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 76/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0184\n",
      "Epoch 77/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0249 - val_loss: 0.0189\n",
      "Epoch 78/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 79/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 80/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 81/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 82/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 83/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 84/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 85/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 86/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 87/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 88/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 89/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 90/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 91/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 92/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 93/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 94/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 95/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 96/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 97/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 98/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Execution time:  173.42216730117798\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0251\n",
      "Root Mean Square Error: 0.0429\n",
      "Mean Square Error: 0.0018\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_195 (Dense)            (None, 432, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 432, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0575 - val_loss: 0.0165\n",
      "Epoch 2/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0487 - val_loss: 0.0194\n",
      "Epoch 3/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0439 - val_loss: 0.0187\n",
      "Epoch 4/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0397 - val_loss: 0.0152\n",
      "Epoch 5/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0365 - val_loss: 0.0162\n",
      "Epoch 6/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0350 - val_loss: 0.0165\n",
      "Epoch 7/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0327 - val_loss: 0.0156\n",
      "Epoch 8/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0308 - val_loss: 0.0146\n",
      "Epoch 9/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0297 - val_loss: 0.0145\n",
      "Epoch 10/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0140\n",
      "Epoch 11/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0137\n",
      "Epoch 12/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0138\n",
      "Epoch 13/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0275 - val_loss: 0.0137\n",
      "Epoch 14/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0271 - val_loss: 0.0138\n",
      "Epoch 15/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0145\n",
      "Epoch 16/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 17/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 18/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 19/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 20/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 21/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 22/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 23/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 24/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 25/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 26/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 27/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 28/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 29/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 30/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 31/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 32/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 33/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 34/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 35/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 36/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 37/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 38/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 39/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 40/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0133\n",
      "Epoch 41/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0144\n",
      "Epoch 42/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 43/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 44/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 45/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 46/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 47/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 48/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 49/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 50/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 51/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 52/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 53/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 54/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 55/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 56/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 57/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 58/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 59/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 60/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 61/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 62/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 63/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 64/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 65/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 66/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 67/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 68/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 69/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 70/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 71/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 72/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 73/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 74/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 75/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 76/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 77/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 78/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 79/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 80/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 81/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 83/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 84/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Epoch 85/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0144\n",
      "Execution time:  64.34763741493225\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0251\n",
      "Root Mean Square Error: 0.0431\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_198 (Dense)            (None, 432, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 432, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.2293 - val_loss: 0.1708\n",
      "Epoch 2/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2281 - val_loss: 0.1683\n",
      "Epoch 3/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2268 - val_loss: 0.1659\n",
      "Epoch 4/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2251 - val_loss: 0.1634\n",
      "Epoch 5/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2239 - val_loss: 0.1609\n",
      "Epoch 6/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2224 - val_loss: 0.1585\n",
      "Epoch 7/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2210 - val_loss: 0.1560\n",
      "Epoch 8/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.2198 - val_loss: 0.1535\n",
      "Epoch 9/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.2183 - val_loss: 0.1510\n",
      "Epoch 10/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2167 - val_loss: 0.1485\n",
      "Epoch 11/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2158 - val_loss: 0.1460\n",
      "Epoch 12/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2142 - val_loss: 0.1435\n",
      "Epoch 13/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2130 - val_loss: 0.1411\n",
      "Epoch 14/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2114 - val_loss: 0.1386\n",
      "Epoch 15/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.2103 - val_loss: 0.1362\n",
      "Epoch 16/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2087 - val_loss: 0.1338\n",
      "Epoch 17/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.2077 - val_loss: 0.1315\n",
      "Epoch 18/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.2065 - val_loss: 0.1292\n",
      "Epoch 19/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2052 - val_loss: 0.1269\n",
      "Epoch 20/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2039 - val_loss: 0.1247\n",
      "Epoch 21/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2028 - val_loss: 0.1226\n",
      "Epoch 22/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2017 - val_loss: 0.1208\n",
      "Epoch 23/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2009 - val_loss: 0.1191\n",
      "Epoch 24/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2000 - val_loss: 0.1175\n",
      "Epoch 25/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1991 - val_loss: 0.1160\n",
      "Epoch 26/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.1984 - val_loss: 0.1145\n",
      "Epoch 27/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1975 - val_loss: 0.1130\n",
      "Epoch 28/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1968 - val_loss: 0.1116\n",
      "Epoch 29/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1960 - val_loss: 0.1102\n",
      "Epoch 30/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1955 - val_loss: 0.1088\n",
      "Epoch 31/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1947 - val_loss: 0.1075\n",
      "Epoch 32/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1940 - val_loss: 0.1062\n",
      "Epoch 33/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1934 - val_loss: 0.1050\n",
      "Epoch 34/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1926 - val_loss: 0.1038\n",
      "Epoch 35/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.1921 - val_loss: 0.1026\n",
      "Epoch 36/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1913 - val_loss: 0.1014\n",
      "Epoch 37/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1909 - val_loss: 0.1004\n",
      "Epoch 38/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1902 - val_loss: 0.0993\n",
      "Epoch 39/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1896 - val_loss: 0.0982\n",
      "Epoch 40/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1888 - val_loss: 0.0972\n",
      "Epoch 41/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1885 - val_loss: 0.0962\n",
      "Epoch 42/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1880 - val_loss: 0.0952\n",
      "Epoch 43/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1874 - val_loss: 0.0942\n",
      "Epoch 44/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1869 - val_loss: 0.0932\n",
      "Epoch 45/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1864 - val_loss: 0.0922\n",
      "Epoch 46/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1859 - val_loss: 0.0913\n",
      "Epoch 47/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1855 - val_loss: 0.0904\n",
      "Epoch 48/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1850 - val_loss: 0.0895\n",
      "Epoch 49/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1845 - val_loss: 0.0886\n",
      "Epoch 50/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1841 - val_loss: 0.0878\n",
      "Epoch 51/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1835 - val_loss: 0.0870\n",
      "Epoch 52/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1833 - val_loss: 0.0862\n",
      "Epoch 53/98\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 0.1828 - val_loss: 0.0855\n",
      "Epoch 54/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.1826 - val_loss: 0.0848\n",
      "Epoch 55/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1820 - val_loss: 0.0842\n",
      "Epoch 56/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1817 - val_loss: 0.0836\n",
      "Epoch 57/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1813 - val_loss: 0.0830\n",
      "Epoch 58/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1811 - val_loss: 0.0824\n",
      "Epoch 59/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1807 - val_loss: 0.0818\n",
      "Epoch 60/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1803 - val_loss: 0.0812\n",
      "Epoch 61/98\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.180 - 2s 23ms/step - loss: 0.1803 - val_loss: 0.0807\n",
      "Epoch 62/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.1799 - val_loss: 0.0802\n",
      "Epoch 63/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1796 - val_loss: 0.0798\n",
      "Epoch 64/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1793 - val_loss: 0.0793\n",
      "Epoch 65/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1790 - val_loss: 0.0789\n",
      "Epoch 66/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1785 - val_loss: 0.0784\n",
      "Epoch 67/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1785 - val_loss: 0.0780\n",
      "Epoch 68/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1783 - val_loss: 0.0776\n",
      "Epoch 69/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1779 - val_loss: 0.0772\n",
      "Epoch 70/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1777 - val_loss: 0.0767\n",
      "Epoch 71/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.1774 - val_loss: 0.0763\n",
      "Epoch 72/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1772 - val_loss: 0.0759\n",
      "Epoch 73/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1770 - val_loss: 0.0755\n",
      "Epoch 74/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1771 - val_loss: 0.0752\n",
      "Epoch 75/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1769 - val_loss: 0.0749\n",
      "Epoch 76/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1767 - val_loss: 0.0745\n",
      "Epoch 77/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1765 - val_loss: 0.0742\n",
      "Epoch 78/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1763 - val_loss: 0.0740\n",
      "Epoch 79/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1761 - val_loss: 0.0738\n",
      "Epoch 80/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1758 - val_loss: 0.0735\n",
      "Epoch 81/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1755 - val_loss: 0.0733\n",
      "Epoch 82/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1754 - val_loss: 0.0731\n",
      "Epoch 83/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1754 - val_loss: 0.0728\n",
      "Epoch 84/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1750 - val_loss: 0.0727\n",
      "Epoch 85/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1751 - val_loss: 0.0725\n",
      "Epoch 86/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1751 - val_loss: 0.0723\n",
      "Epoch 87/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1747 - val_loss: 0.0722\n",
      "Epoch 88/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1745 - val_loss: 0.0720\n",
      "Epoch 89/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.1744 - val_loss: 0.0718\n",
      "Epoch 90/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1743 - val_loss: 0.0717\n",
      "Epoch 91/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1741 - val_loss: 0.0715\n",
      "Epoch 92/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1740 - val_loss: 0.0713\n",
      "Epoch 93/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1737 - val_loss: 0.0711\n",
      "Epoch 94/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1736 - val_loss: 0.0710\n",
      "Epoch 95/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1734 - val_loss: 0.0708\n",
      "Epoch 96/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1734 - val_loss: 0.0706\n",
      "Epoch 97/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1732 - val_loss: 0.0704\n",
      "Epoch 98/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1733 - val_loss: 0.0703\n",
      "Execution time:  174.513765335083\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0892\n",
      "Root Mean Square Error: 0.0909\n",
      "Mean Square Error: 0.0083\n",
      "\n",
      "Train RMSE: 0.091\n",
      "Train MSE: 0.008\n",
      "Train MAE: 0.089\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_201 (Dense)            (None, 432, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 432, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.4712 - val_loss: 0.4446\n",
      "Epoch 2/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4684 - val_loss: 0.4414\n",
      "Epoch 3/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4655 - val_loss: 0.4380\n",
      "Epoch 4/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4621 - val_loss: 0.4343\n",
      "Epoch 5/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4593 - val_loss: 0.4304\n",
      "Epoch 6/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4552 - val_loss: 0.4262\n",
      "Epoch 7/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4516 - val_loss: 0.4218\n",
      "Epoch 8/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4473 - val_loss: 0.4172\n",
      "Epoch 9/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4432 - val_loss: 0.4125\n",
      "Epoch 10/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4385 - val_loss: 0.4075\n",
      "Epoch 11/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4341 - val_loss: 0.4024\n",
      "Epoch 12/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4293 - val_loss: 0.3971\n",
      "Epoch 13/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4251 - val_loss: 0.3917\n",
      "Epoch 14/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4203 - val_loss: 0.3861\n",
      "Epoch 15/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4151 - val_loss: 0.3804\n",
      "Epoch 16/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4103 - val_loss: 0.3745\n",
      "Epoch 17/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.4051 - val_loss: 0.3685\n",
      "Epoch 18/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3997 - val_loss: 0.3624\n",
      "Epoch 19/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3948 - val_loss: 0.3562\n",
      "Epoch 20/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3894 - val_loss: 0.3500\n",
      "Epoch 21/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3842 - val_loss: 0.3436\n",
      "Epoch 22/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3790 - val_loss: 0.3371\n",
      "Epoch 23/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3735 - val_loss: 0.3306\n",
      "Epoch 24/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3679 - val_loss: 0.3240\n",
      "Epoch 25/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3626 - val_loss: 0.3174\n",
      "Epoch 26/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3575 - val_loss: 0.3107\n",
      "Epoch 27/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3520 - val_loss: 0.3040\n",
      "Epoch 28/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3466 - val_loss: 0.2973\n",
      "Epoch 29/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3410 - val_loss: 0.2906\n",
      "Epoch 30/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3365 - val_loss: 0.2839\n",
      "Epoch 31/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3311 - val_loss: 0.2772\n",
      "Epoch 32/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3261 - val_loss: 0.2705\n",
      "Epoch 33/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3215 - val_loss: 0.2638\n",
      "Epoch 34/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3161 - val_loss: 0.2572\n",
      "Epoch 35/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3116 - val_loss: 0.2507\n",
      "Epoch 36/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3068 - val_loss: 0.2442\n",
      "Epoch 37/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 6ms/step - loss: 0.3023 - val_loss: 0.2377\n",
      "Epoch 38/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2979 - val_loss: 0.2316\n",
      "Epoch 39/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2935 - val_loss: 0.2260\n",
      "Epoch 40/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2899 - val_loss: 0.2206\n",
      "Epoch 41/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2860 - val_loss: 0.2154\n",
      "Epoch 42/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2827 - val_loss: 0.2102\n",
      "Epoch 43/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2790 - val_loss: 0.2052\n",
      "Epoch 44/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2762 - val_loss: 0.2002\n",
      "Epoch 45/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2728 - val_loss: 0.1954\n",
      "Epoch 46/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2699 - val_loss: 0.1907\n",
      "Epoch 47/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2668 - val_loss: 0.1862\n",
      "Epoch 48/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2637 - val_loss: 0.1820\n",
      "Epoch 49/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2611 - val_loss: 0.1780\n",
      "Epoch 50/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2586 - val_loss: 0.1740\n",
      "Epoch 51/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2563 - val_loss: 0.1702\n",
      "Epoch 52/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2539 - val_loss: 0.1666\n",
      "Epoch 53/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2514 - val_loss: 0.1632\n",
      "Epoch 54/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2495 - val_loss: 0.1599\n",
      "Epoch 55/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2475 - val_loss: 0.1566\n",
      "Epoch 56/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2455 - val_loss: 0.1535\n",
      "Epoch 57/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2438 - val_loss: 0.1504\n",
      "Epoch 58/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2416 - val_loss: 0.1474\n",
      "Epoch 59/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2398 - val_loss: 0.1445\n",
      "Epoch 60/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2381 - val_loss: 0.1417\n",
      "Epoch 61/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2365 - val_loss: 0.1389\n",
      "Epoch 62/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2344 - val_loss: 0.1362\n",
      "Epoch 63/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2327 - val_loss: 0.1336\n",
      "Epoch 64/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2309 - val_loss: 0.1310\n",
      "Epoch 65/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2293 - val_loss: 0.1286\n",
      "Epoch 66/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2279 - val_loss: 0.1262\n",
      "Epoch 67/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2265 - val_loss: 0.1239\n",
      "Epoch 68/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2251 - val_loss: 0.1217\n",
      "Epoch 69/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2237 - val_loss: 0.1195\n",
      "Epoch 70/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2218 - val_loss: 0.1174\n",
      "Epoch 71/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2204 - val_loss: 0.1154\n",
      "Epoch 72/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2191 - val_loss: 0.1134\n",
      "Epoch 73/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2176 - val_loss: 0.1115\n",
      "Epoch 74/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2164 - val_loss: 0.1097\n",
      "Epoch 75/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2148 - val_loss: 0.1080\n",
      "Epoch 76/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2140 - val_loss: 0.1063\n",
      "Epoch 77/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2127 - val_loss: 0.1047\n",
      "Epoch 78/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2115 - val_loss: 0.1031\n",
      "Epoch 79/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2101 - val_loss: 0.1016\n",
      "Epoch 80/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2088 - val_loss: 0.1001\n",
      "Epoch 81/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2077 - val_loss: 0.0987\n",
      "Epoch 82/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2065 - val_loss: 0.0974\n",
      "Epoch 83/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2054 - val_loss: 0.0960\n",
      "Epoch 84/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2041 - val_loss: 0.0948\n",
      "Epoch 85/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.2030 - val_loss: 0.0936\n",
      "Execution time:  64.61616849899292\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1129\n",
      "Root Mean Square Error: 0.1165\n",
      "Mean Square Error: 0.0136\n",
      "\n",
      "Train RMSE: 0.116\n",
      "Train MSE: 0.014\n",
      "Train MAE: 0.113\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_204 (Dense)            (None, 432, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 432, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0901 - val_loss: 0.1197\n",
      "Epoch 2/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0899 - val_loss: 0.1193\n",
      "Epoch 3/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0895 - val_loss: 0.1189\n",
      "Epoch 4/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0892 - val_loss: 0.1185\n",
      "Epoch 5/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0888 - val_loss: 0.1181\n",
      "Epoch 6/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0885 - val_loss: 0.1177\n",
      "Epoch 7/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0881 - val_loss: 0.1173\n",
      "Epoch 8/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0877 - val_loss: 0.1168\n",
      "Epoch 9/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0873 - val_loss: 0.1163\n",
      "Epoch 10/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0869 - val_loss: 0.1159\n",
      "Epoch 11/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0865 - val_loss: 0.1154\n",
      "Epoch 12/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0860 - val_loss: 0.1149\n",
      "Epoch 13/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0856 - val_loss: 0.1144\n",
      "Epoch 14/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0852 - val_loss: 0.1139\n",
      "Epoch 15/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0848 - val_loss: 0.1133\n",
      "Epoch 16/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0843 - val_loss: 0.1128\n",
      "Epoch 17/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0839 - val_loss: 0.1123\n",
      "Epoch 18/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0835 - val_loss: 0.1118\n",
      "Epoch 19/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0831 - val_loss: 0.1112\n",
      "Epoch 20/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0827 - val_loss: 0.1107\n",
      "Epoch 21/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0822 - val_loss: 0.1102\n",
      "Epoch 22/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0819 - val_loss: 0.1096\n",
      "Epoch 23/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0814 - val_loss: 0.1091\n",
      "Epoch 24/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0810 - val_loss: 0.1086\n",
      "Epoch 25/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0807 - val_loss: 0.1081\n",
      "Epoch 26/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0803 - val_loss: 0.1075\n",
      "Epoch 27/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0799 - val_loss: 0.1070\n",
      "Epoch 28/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0795 - val_loss: 0.1065\n",
      "Epoch 29/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0792 - val_loss: 0.1060\n",
      "Epoch 30/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0788 - val_loss: 0.1055\n",
      "Epoch 31/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0785 - val_loss: 0.1050\n",
      "Epoch 32/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0781 - val_loss: 0.1045\n",
      "Epoch 33/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0777 - val_loss: 0.1040\n",
      "Epoch 34/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0774 - val_loss: 0.1035\n",
      "Epoch 35/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0771 - val_loss: 0.1030\n",
      "Epoch 36/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0768 - val_loss: 0.1026\n",
      "Epoch 37/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0765 - val_loss: 0.1021\n",
      "Epoch 38/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0762 - val_loss: 0.1016\n",
      "Epoch 39/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0759 - val_loss: 0.1012\n",
      "Epoch 40/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0755 - val_loss: 0.1007\n",
      "Epoch 41/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0753 - val_loss: 0.1003\n",
      "Epoch 42/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0751 - val_loss: 0.0998\n",
      "Epoch 43/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0748 - val_loss: 0.0994\n",
      "Epoch 44/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0746 - val_loss: 0.0990\n",
      "Epoch 45/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0743 - val_loss: 0.0985\n",
      "Epoch 46/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0740 - val_loss: 0.0981\n",
      "Epoch 47/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0738 - val_loss: 0.0977\n",
      "Epoch 48/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0736 - val_loss: 0.0973\n",
      "Epoch 49/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0733 - val_loss: 0.0969\n",
      "Epoch 50/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0731 - val_loss: 0.0965\n",
      "Epoch 51/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0729 - val_loss: 0.0962\n",
      "Epoch 52/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0727 - val_loss: 0.0958\n",
      "Epoch 53/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0726 - val_loss: 0.0954\n",
      "Epoch 54/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0723 - val_loss: 0.0950\n",
      "Epoch 55/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0722 - val_loss: 0.0947\n",
      "Epoch 56/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0719 - val_loss: 0.0943\n",
      "Epoch 57/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0718 - val_loss: 0.0940\n",
      "Epoch 58/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0716 - val_loss: 0.0936\n",
      "Epoch 59/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0714 - val_loss: 0.0933\n",
      "Epoch 60/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0713 - val_loss: 0.0930\n",
      "Epoch 61/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0712 - val_loss: 0.0927\n",
      "Epoch 62/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0710 - val_loss: 0.0924\n",
      "Epoch 63/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0709 - val_loss: 0.0921\n",
      "Epoch 64/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0708 - val_loss: 0.0919\n",
      "Epoch 65/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0706 - val_loss: 0.0916\n",
      "Epoch 66/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0705 - val_loss: 0.0913\n",
      "Epoch 67/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0703 - val_loss: 0.0910\n",
      "Epoch 68/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0702 - val_loss: 0.0908\n",
      "Epoch 69/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0701 - val_loss: 0.0905\n",
      "Epoch 70/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0700 - val_loss: 0.0902\n",
      "Epoch 71/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0699 - val_loss: 0.0900\n",
      "Epoch 72/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0698 - val_loss: 0.0897\n",
      "Epoch 73/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0696 - val_loss: 0.0895\n",
      "Epoch 74/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0696 - val_loss: 0.0893\n",
      "Epoch 75/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0695 - val_loss: 0.0890\n",
      "Epoch 76/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0693 - val_loss: 0.0888\n",
      "Epoch 77/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0692 - val_loss: 0.0886\n",
      "Epoch 78/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0692 - val_loss: 0.0884\n",
      "Epoch 79/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0691 - val_loss: 0.0882\n",
      "Epoch 80/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0690 - val_loss: 0.0879\n",
      "Epoch 81/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0689 - val_loss: 0.0877\n",
      "Epoch 82/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0688 - val_loss: 0.0875\n",
      "Epoch 83/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0687 - val_loss: 0.0873\n",
      "Epoch 84/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0687 - val_loss: 0.0871\n",
      "Epoch 85/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0686 - val_loss: 0.0870\n",
      "Epoch 86/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0685 - val_loss: 0.0868\n",
      "Epoch 87/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0684 - val_loss: 0.0866\n",
      "Epoch 88/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0683 - val_loss: 0.0864\n",
      "Epoch 89/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0681 - val_loss: 0.0862\n",
      "Epoch 90/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0682 - val_loss: 0.0860\n",
      "Epoch 91/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0681 - val_loss: 0.0858\n",
      "Epoch 92/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0680 - val_loss: 0.0857\n",
      "Epoch 93/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0679 - val_loss: 0.0855\n",
      "Epoch 94/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0678 - val_loss: 0.0853\n",
      "Epoch 95/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0678 - val_loss: 0.0851\n",
      "Epoch 96/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0677 - val_loss: 0.0850\n",
      "Epoch 97/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0676 - val_loss: 0.0848\n",
      "Epoch 98/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0675 - val_loss: 0.0847\n",
      "Execution time:  172.99929404258728\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0596\n",
      "Root Mean Square Error: 0.0675\n",
      "Mean Square Error: 0.0046\n",
      "\n",
      "Train RMSE: 0.067\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.060\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_207 (Dense)            (None, 432, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 432, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0839 - val_loss: 0.1047\n",
      "Epoch 2/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0838 - val_loss: 0.1045\n",
      "Epoch 3/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0837 - val_loss: 0.1043\n",
      "Epoch 4/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0835 - val_loss: 0.1040\n",
      "Epoch 5/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0833 - val_loss: 0.1038\n",
      "Epoch 6/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0830 - val_loss: 0.1035\n",
      "Epoch 7/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0829 - val_loss: 0.1032\n",
      "Epoch 8/85\n",
      "124/124 [==============================] - 1s 8ms/step - loss: 0.0827 - val_loss: 0.1029\n",
      "Epoch 9/85\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.0824 - val_loss: 0.1026\n",
      "Epoch 10/85\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.0822 - val_loss: 0.1023\n",
      "Epoch 11/85\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.0820 - val_loss: 0.1020\n",
      "Epoch 12/85\n",
      "124/124 [==============================] - 1s 8ms/step - loss: 0.0819 - val_loss: 0.1017\n",
      "Epoch 13/85\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.0816 - val_loss: 0.1014\n",
      "Epoch 14/85\n",
      "124/124 [==============================] - 1s 8ms/step - loss: 0.0814 - val_loss: 0.1011\n",
      "Epoch 15/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0811 - val_loss: 0.1008\n",
      "Epoch 16/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0809 - val_loss: 0.1005\n",
      "Epoch 17/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0808 - val_loss: 0.1002\n",
      "Epoch 18/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0806 - val_loss: 0.0999\n",
      "Epoch 19/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0803 - val_loss: 0.0996\n",
      "Epoch 20/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0801 - val_loss: 0.0992\n",
      "Epoch 21/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0799 - val_loss: 0.0989\n",
      "Epoch 22/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0797 - val_loss: 0.0986\n",
      "Epoch 23/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0795 - val_loss: 0.0983\n",
      "Epoch 24/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0793 - val_loss: 0.0980\n",
      "Epoch 25/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0791 - val_loss: 0.0977\n",
      "Epoch 26/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0789 - val_loss: 0.0974\n",
      "Epoch 27/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0787 - val_loss: 0.0970\n",
      "Epoch 28/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.0967\n",
      "Epoch 29/85\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.078 - 1s 7ms/step - loss: 0.0782 - val_loss: 0.0964\n",
      "Epoch 30/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0781 - val_loss: 0.0961\n",
      "Epoch 31/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0779 - val_loss: 0.0958\n",
      "Epoch 32/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0777 - val_loss: 0.0955\n",
      "Epoch 33/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0775 - val_loss: 0.0952\n",
      "Epoch 34/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0773 - val_loss: 0.0949\n",
      "Epoch 35/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0771 - val_loss: 0.0946\n",
      "Epoch 36/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0770 - val_loss: 0.0943\n",
      "Epoch 37/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0768 - val_loss: 0.0940\n",
      "Epoch 38/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0767 - val_loss: 0.0938\n",
      "Epoch 39/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0765 - val_loss: 0.0935\n",
      "Epoch 40/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0762 - val_loss: 0.0932\n",
      "Epoch 41/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0761 - val_loss: 0.0929\n",
      "Epoch 42/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0760 - val_loss: 0.0926\n",
      "Epoch 43/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0758 - val_loss: 0.0924\n",
      "Epoch 44/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0757 - val_loss: 0.0921\n",
      "Epoch 45/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0756 - val_loss: 0.0918\n",
      "Epoch 46/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0754 - val_loss: 0.0916\n",
      "Epoch 47/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0753 - val_loss: 0.0913\n",
      "Epoch 48/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0751 - val_loss: 0.0911\n",
      "Epoch 49/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0750 - val_loss: 0.0908\n",
      "Epoch 50/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0749 - val_loss: 0.0905\n",
      "Epoch 51/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0747 - val_loss: 0.0903\n",
      "Epoch 52/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0745 - val_loss: 0.0900\n",
      "Epoch 53/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0744 - val_loss: 0.0898\n",
      "Epoch 54/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0743 - val_loss: 0.0895\n",
      "Epoch 55/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0742 - val_loss: 0.0893\n",
      "Epoch 56/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0741 - val_loss: 0.0890\n",
      "Epoch 57/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0739 - val_loss: 0.0888\n",
      "Epoch 58/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0738 - val_loss: 0.0885\n",
      "Epoch 59/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0737 - val_loss: 0.0883\n",
      "Epoch 60/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0736 - val_loss: 0.0880\n",
      "Epoch 61/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0734 - val_loss: 0.0878\n",
      "Epoch 62/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0733 - val_loss: 0.0876\n",
      "Epoch 63/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0731 - val_loss: 0.0873\n",
      "Epoch 64/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0731 - val_loss: 0.0871\n",
      "Epoch 65/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0729 - val_loss: 0.0868\n",
      "Epoch 66/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0729 - val_loss: 0.0866\n",
      "Epoch 67/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0727 - val_loss: 0.0864\n",
      "Epoch 68/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0726 - val_loss: 0.0862\n",
      "Epoch 69/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0725 - val_loss: 0.0859\n",
      "Epoch 70/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0724 - val_loss: 0.0857\n",
      "Epoch 71/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0723 - val_loss: 0.0855\n",
      "Epoch 72/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0722 - val_loss: 0.0853\n",
      "Epoch 73/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0721 - val_loss: 0.0851\n",
      "Epoch 74/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0719 - val_loss: 0.0849\n",
      "Epoch 75/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0718 - val_loss: 0.0847\n",
      "Epoch 76/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0717 - val_loss: 0.0844\n",
      "Epoch 77/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0716 - val_loss: 0.0842\n",
      "Epoch 78/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0716 - val_loss: 0.0840\n",
      "Epoch 79/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0715 - val_loss: 0.0838\n",
      "Epoch 80/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0713 - val_loss: 0.0836\n",
      "Epoch 81/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0713 - val_loss: 0.0834\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0712 - val_loss: 0.0832\n",
      "Epoch 83/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0711 - val_loss: 0.0830\n",
      "Epoch 84/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0710 - val_loss: 0.0829\n",
      "Epoch 85/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0709 - val_loss: 0.0827\n",
      "Execution time:  71.30935168266296\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0648\n",
      "Root Mean Square Error: 0.0726\n",
      "Mean Square Error: 0.0053\n",
      "\n",
      "Train RMSE: 0.073\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.065\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_210 (Dense)            (None, 432, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 432, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.2330 - val_loss: 0.0558\n",
      "Epoch 2/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1890 - val_loss: 0.0464\n",
      "Epoch 3/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.1652 - val_loss: 0.0396\n",
      "Epoch 4/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1393 - val_loss: 0.0304\n",
      "Epoch 5/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.1113 - val_loss: 0.0212\n",
      "Epoch 6/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0838 - val_loss: 0.0161\n",
      "Epoch 7/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0607 - val_loss: 0.0074\n",
      "Epoch 8/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0439 - val_loss: 0.0070\n",
      "Epoch 9/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0295 - val_loss: 0.0114\n",
      "Epoch 10/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0260 - val_loss: 0.0165\n",
      "Epoch 11/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0170\n",
      "Epoch 12/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 13/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 14/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 15/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 16/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 17/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 18/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 19/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 20/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 21/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 22/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 23/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 24/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 25/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 26/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 27/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 28/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 29/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 30/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 31/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 32/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 33/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 34/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 35/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 36/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 37/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 38/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 39/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 40/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 41/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 42/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 43/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 44/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 45/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 46/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 47/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 48/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 49/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 50/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 51/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 52/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 53/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 54/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 55/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 56/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 57/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 58/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 59/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 60/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 61/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 62/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 63/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 64/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 65/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 66/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 67/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 68/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 69/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 70/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 71/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 72/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 73/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 74/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 75/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 76/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 77/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 78/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 79/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 80/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0185\n",
      "Epoch 81/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0253 - val_loss: 0.0160\n",
      "Epoch 82/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0168\n",
      "Epoch 83/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 84/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 85/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 86/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 87/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 88/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 89/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 90/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 91/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 92/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 93/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 94/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Epoch 95/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 96/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 97/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0255 - val_loss: 0.0174\n",
      "Epoch 98/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0254 - val_loss: 0.0172\n",
      "Execution time:  173.19090485572815\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0252\n",
      "Root Mean Square Error: 0.0434\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_213 (Dense)            (None, 432, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 432, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.2385 - val_loss: 0.0787\n",
      "Epoch 2/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.1375 - val_loss: 0.0402\n",
      "Epoch 3/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0296\n",
      "Epoch 4/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0832 - val_loss: 0.0236\n",
      "Epoch 5/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0602 - val_loss: 0.0131\n",
      "Epoch 6/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0415 - val_loss: 0.0113\n",
      "Epoch 7/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0110\n",
      "Epoch 8/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0122\n",
      "Epoch 9/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0283 - val_loss: 0.0125\n",
      "Epoch 10/85\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 11/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 12/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 13/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 14/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 15/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 16/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 17/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 18/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 19/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 20/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 21/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 22/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 23/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 24/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 25/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 26/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 27/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 28/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 29/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 30/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 31/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 32/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 33/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0124\n",
      "Epoch 34/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 35/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 36/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 37/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 38/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 39/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 40/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 41/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 42/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 43/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 44/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 45/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 46/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 47/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 48/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 49/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 50/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 51/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 52/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 53/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 54/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 55/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 56/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 57/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 58/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 59/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 60/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 61/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 62/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 63/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 64/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 65/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 66/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 67/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 68/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 69/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 70/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 71/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 72/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 73/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 74/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 75/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 76/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 77/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 78/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 79/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 80/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 81/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 82/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 83/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Epoch 84/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0124\n",
      "Epoch 85/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0125\n",
      "Execution time:  64.9166271686554\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0254\n",
      "Root Mean Square Error: 0.0447\n",
      "Mean Square Error: 0.0020\n",
      "\n",
      "Train RMSE: 0.045\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_216 (Dense)            (None, 432, 97)           194       \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 432, 16)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0738 - val_loss: 0.0624\n",
      "Epoch 2/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0530 - val_loss: 0.0472\n",
      "Epoch 3/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0478 - val_loss: 0.0425\n",
      "Epoch 4/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0451 - val_loss: 0.0397\n",
      "Epoch 5/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0429 - val_loss: 0.0371\n",
      "Epoch 6/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0408 - val_loss: 0.0348\n",
      "Epoch 7/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0389 - val_loss: 0.0330\n",
      "Epoch 8/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0372 - val_loss: 0.0309\n",
      "Epoch 9/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0357 - val_loss: 0.0291\n",
      "Epoch 10/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0343 - val_loss: 0.0275\n",
      "Epoch 11/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0331 - val_loss: 0.0287\n",
      "Epoch 12/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0316 - val_loss: 0.0270\n",
      "Epoch 13/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0305 - val_loss: 0.0252\n",
      "Epoch 14/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0296 - val_loss: 0.0235\n",
      "Epoch 15/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0287 - val_loss: 0.0221\n",
      "Epoch 16/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0281 - val_loss: 0.0209\n",
      "Epoch 17/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0276 - val_loss: 0.0216\n",
      "Epoch 18/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0269 - val_loss: 0.0208\n",
      "Epoch 19/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0264 - val_loss: 0.0198\n",
      "Epoch 20/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0262 - val_loss: 0.0196\n",
      "Epoch 21/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0258 - val_loss: 0.0192\n",
      "Epoch 22/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0255 - val_loss: 0.0195\n",
      "Epoch 23/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0252 - val_loss: 0.0189\n",
      "Epoch 24/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0252 - val_loss: 0.0193\n",
      "Epoch 25/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0249 - val_loss: 0.0188\n",
      "Epoch 26/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0249 - val_loss: 0.0188\n",
      "Epoch 27/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0171\n",
      "Epoch 28/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0249 - val_loss: 0.0189\n",
      "Epoch 29/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 30/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 31/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 32/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 33/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 34/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 35/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 36/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 37/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 38/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 39/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 40/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 41/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 42/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 43/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 44/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 45/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 46/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 47/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 48/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 49/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 50/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 51/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 52/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 53/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 54/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 55/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 56/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 57/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 58/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 59/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 60/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 61/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 62/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 63/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 64/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 65/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 66/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 67/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 68/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 69/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 70/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 71/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 72/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 73/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 74/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 75/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 76/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 77/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0185\n",
      "Epoch 78/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 79/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 80/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 81/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 82/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 83/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 84/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 85/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 86/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 87/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 88/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 89/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 90/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 91/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 92/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 93/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 94/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 95/98\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Epoch 96/98\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 97/98\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0248 - val_loss: 0.0191\n",
      "Epoch 98/98\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.0248 - val_loss: 0.0190\n",
      "Execution time:  173.98967957496643\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0251\n",
      "Root Mean Square Error: 0.0429\n",
      "Mean Square Error: 0.0018\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  3d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_219 (Dense)            (None, 432, 22)           44        \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 432, 16)           368       \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 432, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 432, 1)            17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 7ms/step - loss: 0.0714 - val_loss: 0.0565\n",
      "Epoch 2/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0533 - val_loss: 0.0452\n",
      "Epoch 3/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0453 - val_loss: 0.0354\n",
      "Epoch 4/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0397 - val_loss: 0.0275\n",
      "Epoch 5/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0368 - val_loss: 0.0240\n",
      "Epoch 6/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0344 - val_loss: 0.0210\n",
      "Epoch 7/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0327 - val_loss: 0.0185\n",
      "Epoch 8/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0314 - val_loss: 0.0167\n",
      "Epoch 9/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.0167\n",
      "Epoch 10/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0292 - val_loss: 0.0169\n",
      "Epoch 11/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0281 - val_loss: 0.0153\n",
      "Epoch 12/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.0158\n",
      "Epoch 13/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0272 - val_loss: 0.0149\n",
      "Epoch 14/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0145\n",
      "Epoch 15/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0143\n",
      "Epoch 16/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0143\n",
      "Epoch 17/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0145\n",
      "Epoch 18/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0145\n",
      "Epoch 19/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 20/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 21/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 22/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 23/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 24/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 25/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 26/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0144\n",
      "Epoch 27/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 28/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 29/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 30/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0139\n",
      "Epoch 31/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0145\n",
      "Epoch 32/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 33/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 34/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 35/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 36/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 37/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 38/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 39/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 40/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 41/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 42/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 43/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 44/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 45/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 46/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 47/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 48/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 49/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 50/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 51/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 52/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 53/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 54/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 55/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 56/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 57/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 58/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 59/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 60/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 61/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 62/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0138\n",
      "Epoch 63/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0145\n",
      "Epoch 64/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 65/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 66/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 67/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 68/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 69/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 70/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 71/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 72/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 73/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 74/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 75/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 76/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 77/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 78/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 79/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 80/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 81/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 82/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 83/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 84/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Epoch 85/85\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.0268 - val_loss: 0.0145\n",
      "Execution time:  64.558354139328\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0251\n",
      "Root Mean Square Error: 0.0431\n",
      "Mean Square Error: 0.0019\n",
      "\n",
      "Train RMSE: 0.043\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.025\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_222 (Dense)            (None, 1008, 97)          194       \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 1008, 16)          1568      \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.1754 - val_loss: 0.0535\n",
      "Epoch 2/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.1379 - val_loss: 0.0402\n",
      "Epoch 3/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.1173 - val_loss: 0.0151\n",
      "Epoch 4/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0958 - val_loss: 0.0085\n",
      "Epoch 5/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0749 - val_loss: 0.0062\n",
      "Epoch 6/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0560 - val_loss: 0.0052\n",
      "Epoch 7/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0408 - val_loss: 0.0089\n",
      "Epoch 8/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0275 - val_loss: 0.0154\n",
      "Epoch 9/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0237 - val_loss: 0.0137\n",
      "Epoch 10/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0238 - val_loss: 0.0158\n",
      "Epoch 11/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0237 - val_loss: 0.0145\n",
      "Epoch 12/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0236 - val_loss: 0.0148\n",
      "Epoch 13/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0236 - val_loss: 0.0148\n",
      "Epoch 14/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0236 - val_loss: 0.0153\n",
      "Epoch 15/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 16/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0236 - val_loss: 0.0151\n",
      "Epoch 17/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0236 - val_loss: 0.0152\n",
      "Epoch 18/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0235 - val_loss: 0.0141\n",
      "Epoch 19/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0145\n",
      "Epoch 20/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0236 - val_loss: 0.0150\n",
      "Epoch 21/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0142\n",
      "Epoch 22/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0236 - val_loss: 0.0154\n",
      "Epoch 23/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0234 - val_loss: 0.0129\n",
      "Epoch 24/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0235 - val_loss: 0.0150\n",
      "Epoch 25/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0138\n",
      "Epoch 26/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0234 - val_loss: 0.0144\n",
      "Epoch 27/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0146\n",
      "Epoch 28/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0235 - val_loss: 0.0151\n",
      "Epoch 29/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0127\n",
      "Epoch 30/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0158\n",
      "Epoch 31/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0131\n",
      "Epoch 32/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0154\n",
      "Epoch 33/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0233 - val_loss: 0.0128\n",
      "Epoch 34/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0149\n",
      "Epoch 35/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0137\n",
      "Epoch 36/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 37/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0148\n",
      "Epoch 38/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0234 - val_loss: 0.0140\n",
      "Epoch 39/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0235 - val_loss: 0.0152\n",
      "Epoch 40/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0128\n",
      "Epoch 41/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0149\n",
      "Epoch 42/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0137\n",
      "Epoch 43/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 44/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 45/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0140\n",
      "Epoch 46/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0152\n",
      "Epoch 47/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0129\n",
      "Epoch 48/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0235 - val_loss: 0.0162\n",
      "Epoch 49/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0136\n",
      "Epoch 50/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 51/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 52/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 53/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0233 - val_loss: 0.0143\n",
      "Epoch 54/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 55/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0140\n",
      "Epoch 56/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0152\n",
      "Epoch 57/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0233 - val_loss: 0.0129\n",
      "Epoch 58/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0235 - val_loss: 0.0161\n",
      "Epoch 59/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0136\n",
      "Epoch 60/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0233 - val_loss: 0.0143\n",
      "Epoch 61/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 62/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 63/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0233 - val_loss: 0.0143\n",
      "Epoch 64/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0234 - val_loss: 0.0147\n",
      "Epoch 65/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 66/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0143\n",
      "Epoch 67/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0234 - val_loss: 0.0147\n",
      "Epoch 68/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0233 - val_loss: 0.0139\n",
      "Epoch 69/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0235 - val_loss: 0.0152\n",
      "Epoch 70/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0129\n",
      "Epoch 71/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0235 - val_loss: 0.0161\n",
      "Epoch 72/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0135\n",
      "Epoch 73/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 74/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 75/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0137\n",
      "Epoch 76/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 77/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 78/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0233 - val_loss: 0.0136\n",
      "Epoch 79/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 80/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 81/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0233 - val_loss: 0.0136\n",
      "Epoch 82/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 83/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 84/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0136\n",
      "Epoch 85/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Epoch 86/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0234 - val_loss: 0.0148\n",
      "Epoch 87/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 88/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0233 - val_loss: 0.0143\n",
      "Epoch 89/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0234 - val_loss: 0.0147\n",
      "Epoch 90/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 91/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0233 - val_loss: 0.0142\n",
      "Epoch 92/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0234 - val_loss: 0.0147\n",
      "Epoch 93/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0233 - val_loss: 0.0139\n",
      "Epoch 94/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0235 - val_loss: 0.0151\n",
      "Epoch 95/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0129\n",
      "Epoch 96/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0235 - val_loss: 0.0161\n",
      "Epoch 97/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0135\n",
      "Epoch 98/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0234 - val_loss: 0.0143\n",
      "Execution time:  313.3182635307312\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0258\n",
      "Root Mean Square Error: 0.0464\n",
      "Mean Square Error: 0.0021\n",
      "\n",
      "Train RMSE: 0.046\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_225 (Dense)            (None, 1008, 22)          44        \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 1008, 16)          368       \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.2420 - val_loss: 0.0437\n",
      "Epoch 2/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.1428 - val_loss: 0.0590\n",
      "Epoch 3/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.1087 - val_loss: 0.0351\n",
      "Epoch 4/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0749 - val_loss: 0.0116\n",
      "Epoch 5/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0440 - val_loss: 0.0106\n",
      "Epoch 6/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0278 - val_loss: 0.0106\n",
      "Epoch 7/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0256 - val_loss: 0.0105\n",
      "Epoch 8/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0251 - val_loss: 0.0105\n",
      "Epoch 9/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0251 - val_loss: 0.0106\n",
      "Epoch 10/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0106\n",
      "Epoch 11/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0108\n",
      "Epoch 12/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0109\n",
      "Epoch 13/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0111\n",
      "Epoch 14/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0254 - val_loss: 0.0113\n",
      "Epoch 15/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0114\n",
      "Epoch 16/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0112\n",
      "Epoch 17/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0253 - val_loss: 0.0113\n",
      "Epoch 18/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0253 - val_loss: 0.0112\n",
      "Epoch 19/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0253 - val_loss: 0.0113\n",
      "Epoch 20/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0112\n",
      "Epoch 21/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0113\n",
      "Epoch 22/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0113\n",
      "Epoch 23/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 24/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0113\n",
      "Epoch 25/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0112\n",
      "Epoch 26/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 27/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0120\n",
      "Epoch 28/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0255 - val_loss: 0.0117\n",
      "Epoch 29/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0109\n",
      "Epoch 30/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0122\n",
      "Epoch 31/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0110\n",
      "Epoch 32/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0118\n",
      "Epoch 33/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0111\n",
      "Epoch 34/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 35/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 36/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 37/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 38/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 39/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 40/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 41/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 42/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 43/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 44/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 45/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 46/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 47/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 48/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 49/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 50/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 51/85\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.025 - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 52/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 53/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 54/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 55/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 56/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 57/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 58/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 59/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 60/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 61/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 62/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 63/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 64/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 65/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 66/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 67/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 68/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 69/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 70/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 71/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 72/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 73/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 74/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 75/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 76/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 77/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0112\n",
      "Epoch 78/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0114\n",
      "Epoch 79/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 80/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0116\n",
      "Epoch 81/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0120\n",
      "Epoch 82/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0253 - val_loss: 0.0106\n",
      "Epoch 83/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0117\n",
      "Epoch 84/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0110\n",
      "Epoch 85/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Execution time:  138.09410881996155\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0260\n",
      "Root Mean Square Error: 0.0472\n",
      "Mean Square Error: 0.0022\n",
      "\n",
      "Train RMSE: 0.047\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_228 (Dense)            (None, 1008, 97)          194       \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 1008, 16)          1568      \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "63/63 [==============================] - 4s 57ms/step - loss: 0.0702 - val_loss: 0.0564\n",
      "Epoch 2/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0566 - val_loss: 0.0490\n",
      "Epoch 3/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0518 - val_loss: 0.0452\n",
      "Epoch 4/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0476 - val_loss: 0.0417\n",
      "Epoch 5/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0436 - val_loss: 0.0384\n",
      "Epoch 6/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0393 - val_loss: 0.0372\n",
      "Epoch 7/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0361 - val_loss: 0.0320\n",
      "Epoch 8/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0335 - val_loss: 0.0266\n",
      "Epoch 9/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0311 - val_loss: 0.0287\n",
      "Epoch 10/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0290 - val_loss: 0.0262\n",
      "Epoch 11/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0269 - val_loss: 0.0235\n",
      "Epoch 12/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0254 - val_loss: 0.0215\n",
      "Epoch 13/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0244 - val_loss: 0.0200\n",
      "Epoch 14/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0238 - val_loss: 0.0192\n",
      "Epoch 15/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0179\n",
      "Epoch 16/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0230 - val_loss: 0.0179\n",
      "Epoch 17/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0229 - val_loss: 0.0170\n",
      "Epoch 18/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0228 - val_loss: 0.0160\n",
      "Epoch 19/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0228 - val_loss: 0.0161\n",
      "Epoch 20/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0227 - val_loss: 0.0155\n",
      "Epoch 21/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0227 - val_loss: 0.0150\n",
      "Epoch 22/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0226 - val_loss: 0.0146\n",
      "Epoch 23/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0226 - val_loss: 0.0142\n",
      "Epoch 24/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0226 - val_loss: 0.0139\n",
      "Epoch 25/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0226 - val_loss: 0.0135\n",
      "Epoch 26/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0226 - val_loss: 0.0132\n",
      "Epoch 27/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0226 - val_loss: 0.0130\n",
      "Epoch 28/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0226 - val_loss: 0.0129\n",
      "Epoch 29/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 30/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 31/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 32/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0225 - val_loss: 0.0122\n",
      "Epoch 33/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0226 - val_loss: 0.0126\n",
      "Epoch 34/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0120\n",
      "Epoch 35/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 36/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0225 - val_loss: 0.0119\n",
      "Epoch 37/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0226 - val_loss: 0.0129\n",
      "Epoch 38/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 39/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0119\n",
      "Epoch 40/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 41/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 42/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0226 - val_loss: 0.0127\n",
      "Epoch 43/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 44/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0226 - val_loss: 0.0127\n",
      "Epoch 45/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 46/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 47/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0119\n",
      "Epoch 48/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0226 - val_loss: 0.0129\n",
      "Epoch 49/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 50/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0119\n",
      "Epoch 51/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0226 - val_loss: 0.0129\n",
      "Epoch 52/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 53/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0119\n",
      "Epoch 54/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0127\n",
      "Epoch 55/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0122\n",
      "Epoch 56/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0125\n",
      "Epoch 57/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0119\n",
      "Epoch 58/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 59/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 60/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 61/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 62/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 63/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 64/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 65/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 66/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 67/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 68/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 69/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 70/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 71/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 72/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 73/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 74/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 75/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 76/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 77/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 78/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 79/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0122\n",
      "Epoch 80/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0125\n",
      "Epoch 81/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0120\n",
      "Epoch 82/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 83/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 84/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 85/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 86/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 87/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 88/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 89/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0119\n",
      "Epoch 90/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0226 - val_loss: 0.0128\n",
      "Epoch 91/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 92/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 93/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 94/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 95/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 96/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 97/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 98/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0122\n",
      "Execution time:  315.68565702438354\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0261\n",
      "Root Mean Square Error: 0.0473\n",
      "Mean Square Error: 0.0022\n",
      "\n",
      "Train RMSE: 0.047\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adam\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_231 (Dense)            (None, 1008, 22)          44        \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 1008, 16)          368       \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_233 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 18ms/step - loss: 0.0675 - val_loss: 0.0310\n",
      "Epoch 2/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0487 - val_loss: 0.0272\n",
      "Epoch 3/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0439 - val_loss: 0.0242\n",
      "Epoch 4/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0397 - val_loss: 0.0212\n",
      "Epoch 5/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0361 - val_loss: 0.0186\n",
      "Epoch 6/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0330 - val_loss: 0.0166\n",
      "Epoch 7/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0307 - val_loss: 0.0166\n",
      "Epoch 8/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0285 - val_loss: 0.0161\n",
      "Epoch 9/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0270 - val_loss: 0.0145\n",
      "Epoch 10/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0258 - val_loss: 0.0143\n",
      "Epoch 11/85\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0253- ETA: 0s - loss: 0. - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0142\n",
      "Epoch 12/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0246 - val_loss: 0.0143\n",
      "Epoch 13/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0245 - val_loss: 0.0136\n",
      "Epoch 14/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0245 - val_loss: 0.0133\n",
      "Epoch 15/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0245 - val_loss: 0.0131\n",
      "Epoch 16/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0129\n",
      "Epoch 17/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0128\n",
      "Epoch 18/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0127\n",
      "Epoch 19/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0126\n",
      "Epoch 20/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0124\n",
      "Epoch 21/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0123\n",
      "Epoch 22/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0122\n",
      "Epoch 23/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0120\n",
      "Epoch 24/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0119\n",
      "Epoch 25/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0118\n",
      "Epoch 26/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0117\n",
      "Epoch 27/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0116\n",
      "Epoch 28/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0115\n",
      "Epoch 29/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0114\n",
      "Epoch 30/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 31/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 32/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 33/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 34/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 35/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 36/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 37/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 38/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 39/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 40/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 41/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 42/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 43/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 44/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 45/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 46/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 47/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 48/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 49/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 50/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 51/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 52/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 53/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 54/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 55/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 56/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 57/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 58/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 59/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 60/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 61/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 62/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 63/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 64/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 65/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 66/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 67/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 68/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 69/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 70/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 71/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 72/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 73/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 74/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 75/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 76/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 77/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 78/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 79/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 80/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 81/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 82/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 83/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 84/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 85/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Execution time:  137.95492219924927\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0261\n",
      "Root Mean Square Error: 0.0472\n",
      "Mean Square Error: 0.0022\n",
      "\n",
      "Train RMSE: 0.047\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_234 (Dense)            (None, 1008, 97)          194       \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 1008, 16)          1568      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "63/63 [==============================] - 4s 57ms/step - loss: 0.4292 - val_loss: 0.3972\n",
      "Epoch 2/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.4275 - val_loss: 0.3955\n",
      "Epoch 3/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.4257 - val_loss: 0.3938\n",
      "Epoch 4/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.4239 - val_loss: 0.3919\n",
      "Epoch 5/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.4219 - val_loss: 0.3899\n",
      "Epoch 6/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.4199 - val_loss: 0.3878\n",
      "Epoch 7/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.4178 - val_loss: 0.3857\n",
      "Epoch 8/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.4155 - val_loss: 0.3835\n",
      "Epoch 9/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.4133 - val_loss: 0.3812\n",
      "Epoch 10/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.4108 - val_loss: 0.3788\n",
      "Epoch 11/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.4085 - val_loss: 0.3764\n",
      "Epoch 12/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.4060 - val_loss: 0.3739\n",
      "Epoch 13/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.4034 - val_loss: 0.3714\n",
      "Epoch 14/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.4009 - val_loss: 0.3688\n",
      "Epoch 15/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3981 - val_loss: 0.3661\n",
      "Epoch 16/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3954 - val_loss: 0.3634\n",
      "Epoch 17/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3926 - val_loss: 0.3607\n",
      "Epoch 18/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.3899 - val_loss: 0.3581\n",
      "Epoch 19/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.3873 - val_loss: 0.3554\n",
      "Epoch 20/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3845 - val_loss: 0.3527\n",
      "Epoch 21/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3818 - val_loss: 0.3500\n",
      "Epoch 22/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3791 - val_loss: 0.3478\n",
      "Epoch 23/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.3768 - val_loss: 0.3456\n",
      "Epoch 24/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.3746 - val_loss: 0.3434\n",
      "Epoch 25/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3722 - val_loss: 0.3411\n",
      "Epoch 26/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3698 - val_loss: 0.3387\n",
      "Epoch 27/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3674 - val_loss: 0.3363\n",
      "Epoch 28/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.3649 - val_loss: 0.3339\n",
      "Epoch 29/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.3624 - val_loss: 0.3314\n",
      "Epoch 30/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3599 - val_loss: 0.3289\n",
      "Epoch 31/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3573 - val_loss: 0.3264\n",
      "Epoch 32/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3547 - val_loss: 0.3238\n",
      "Epoch 33/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.3522 - val_loss: 0.3212\n",
      "Epoch 34/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.3494 - val_loss: 0.3185\n",
      "Epoch 35/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3466 - val_loss: 0.3158\n",
      "Epoch 36/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3440 - val_loss: 0.3130\n",
      "Epoch 37/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3412 - val_loss: 0.3102\n",
      "Epoch 38/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.3384 - val_loss: 0.3074\n",
      "Epoch 39/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.3355 - val_loss: 0.3046\n",
      "Epoch 40/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3327 - val_loss: 0.3017\n",
      "Epoch 41/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3298 - val_loss: 0.2988\n",
      "Epoch 42/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3268 - val_loss: 0.2958\n",
      "Epoch 43/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.3239 - val_loss: 0.2928\n",
      "Epoch 44/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.3208 - val_loss: 0.2898\n",
      "Epoch 45/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.3178 - val_loss: 0.2867\n",
      "Epoch 46/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3146 - val_loss: 0.2836\n",
      "Epoch 47/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3117 - val_loss: 0.2808\n",
      "Epoch 48/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.3090 - val_loss: 0.2782\n",
      "Epoch 49/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.3065 - val_loss: 0.2755\n",
      "Epoch 50/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3041 - val_loss: 0.2729\n",
      "Epoch 51/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.3019 - val_loss: 0.2704\n",
      "Epoch 52/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2997 - val_loss: 0.2679\n",
      "Epoch 53/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2974 - val_loss: 0.2655\n",
      "Epoch 54/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2956 - val_loss: 0.2632\n",
      "Epoch 55/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2938 - val_loss: 0.2609\n",
      "Epoch 56/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2922 - val_loss: 0.2586\n",
      "Epoch 57/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.2903 - val_loss: 0.2563\n",
      "Epoch 58/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.2888 - val_loss: 0.2540\n",
      "Epoch 59/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.2868 - val_loss: 0.2516\n",
      "Epoch 60/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.2853 - val_loss: 0.2493\n",
      "Epoch 61/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2834 - val_loss: 0.2469\n",
      "Epoch 62/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.2818 - val_loss: 0.2444\n",
      "Epoch 63/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.2800 - val_loss: 0.2420\n",
      "Epoch 64/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2782 - val_loss: 0.2395\n",
      "Epoch 65/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 48ms/step - loss: 0.2765 - val_loss: 0.2370\n",
      "Epoch 66/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2748 - val_loss: 0.2345\n",
      "Epoch 67/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2730 - val_loss: 0.2319\n",
      "Epoch 68/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2713 - val_loss: 0.2294\n",
      "Epoch 69/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2696 - val_loss: 0.2268\n",
      "Epoch 70/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.2679 - val_loss: 0.2243\n",
      "Epoch 71/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.2663 - val_loss: 0.2218\n",
      "Epoch 72/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.2645 - val_loss: 0.2193\n",
      "Epoch 73/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.2630 - val_loss: 0.2168\n",
      "Epoch 74/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2616 - val_loss: 0.2144\n",
      "Epoch 75/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2600 - val_loss: 0.2120\n",
      "Epoch 76/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2586 - val_loss: 0.2096\n",
      "Epoch 77/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2571 - val_loss: 0.2072\n",
      "Epoch 78/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2560 - val_loss: 0.2049\n",
      "Epoch 79/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2548 - val_loss: 0.2027\n",
      "Epoch 80/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2538 - val_loss: 0.2007\n",
      "Epoch 81/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2528 - val_loss: 0.1986\n",
      "Epoch 82/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2520 - val_loss: 0.1967\n",
      "Epoch 83/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2512 - val_loss: 0.1949\n",
      "Epoch 84/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2501 - val_loss: 0.1931\n",
      "Epoch 85/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.2494 - val_loss: 0.1913\n",
      "Epoch 86/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.2486 - val_loss: 0.1896\n",
      "Epoch 87/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2479 - val_loss: 0.1879\n",
      "Epoch 88/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.2472 - val_loss: 0.1862\n",
      "Epoch 89/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.2466 - val_loss: 0.1846\n",
      "Epoch 90/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2457 - val_loss: 0.1831\n",
      "Epoch 91/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2452 - val_loss: 0.1815\n",
      "Epoch 92/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2445 - val_loss: 0.1801\n",
      "Epoch 93/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2439 - val_loss: 0.1787\n",
      "Epoch 94/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.2435 - val_loss: 0.1773\n",
      "Epoch 95/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2429 - val_loss: 0.1761\n",
      "Epoch 96/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2423 - val_loss: 0.1748\n",
      "Epoch 97/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.2417 - val_loss: 0.1737\n",
      "Epoch 98/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.2416 - val_loss: 0.1726\n",
      "Execution time:  314.88357043266296\n",
      "DNN:\n",
      "Mean Absolute Error: 0.2025\n",
      "Root Mean Square Error: 0.2050\n",
      "Mean Square Error: 0.0420\n",
      "\n",
      "Train RMSE: 0.205\n",
      "Train MSE: 0.042\n",
      "Train MAE: 0.202\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_237 (Dense)            (None, 1008, 22)          44        \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 1008, 16)          368       \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.4052 - val_loss: 0.3796\n",
      "Epoch 2/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.4035 - val_loss: 0.3778\n",
      "Epoch 3/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.4018 - val_loss: 0.3759\n",
      "Epoch 4/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3997 - val_loss: 0.3739\n",
      "Epoch 5/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3978 - val_loss: 0.3717\n",
      "Epoch 6/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3955 - val_loss: 0.3694\n",
      "Epoch 7/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3930 - val_loss: 0.3670\n",
      "Epoch 8/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3907 - val_loss: 0.3644\n",
      "Epoch 9/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.3880 - val_loss: 0.3618\n",
      "Epoch 10/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3855 - val_loss: 0.3591\n",
      "Epoch 11/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3828 - val_loss: 0.3563\n",
      "Epoch 12/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3799 - val_loss: 0.3534\n",
      "Epoch 13/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3769 - val_loss: 0.3504\n",
      "Epoch 14/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3740 - val_loss: 0.3473\n",
      "Epoch 15/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3708 - val_loss: 0.3441\n",
      "Epoch 16/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3680 - val_loss: 0.3409\n",
      "Epoch 17/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3648 - val_loss: 0.3376\n",
      "Epoch 18/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3617 - val_loss: 0.3347\n",
      "Epoch 19/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.3588 - val_loss: 0.3318\n",
      "Epoch 20/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3560 - val_loss: 0.3289\n",
      "Epoch 21/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3532 - val_loss: 0.3260\n",
      "Epoch 22/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3503 - val_loss: 0.3230\n",
      "Epoch 23/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3474 - val_loss: 0.3199\n",
      "Epoch 24/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3446 - val_loss: 0.3168\n",
      "Epoch 25/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3417 - val_loss: 0.3136\n",
      "Epoch 26/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3386 - val_loss: 0.3104\n",
      "Epoch 27/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3356 - val_loss: 0.3071\n",
      "Epoch 28/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3326 - val_loss: 0.3038\n",
      "Epoch 29/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.3296 - val_loss: 0.3005\n",
      "Epoch 30/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.3268 - val_loss: 0.2975\n",
      "Epoch 31/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3237 - val_loss: 0.2945\n",
      "Epoch 32/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3210 - val_loss: 0.2914\n",
      "Epoch 33/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3184 - val_loss: 0.2883\n",
      "Epoch 34/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3159 - val_loss: 0.2852\n",
      "Epoch 35/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3130 - val_loss: 0.2821\n",
      "Epoch 36/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3102 - val_loss: 0.2789\n",
      "Epoch 37/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3077 - val_loss: 0.2757\n",
      "Epoch 38/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.3048 - val_loss: 0.2725\n",
      "Epoch 39/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.3018 - val_loss: 0.2692\n",
      "Epoch 40/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2993 - val_loss: 0.2660\n",
      "Epoch 41/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2966 - val_loss: 0.2627\n",
      "Epoch 42/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2939 - val_loss: 0.2594\n",
      "Epoch 43/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2910 - val_loss: 0.2561\n",
      "Epoch 44/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2883 - val_loss: 0.2527\n",
      "Epoch 45/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2857 - val_loss: 0.2494\n",
      "Epoch 46/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2830 - val_loss: 0.2461\n",
      "Epoch 47/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2804 - val_loss: 0.2428\n",
      "Epoch 48/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2777 - val_loss: 0.2394\n",
      "Epoch 49/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.2751 - val_loss: 0.2361\n",
      "Epoch 50/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2725 - val_loss: 0.2328\n",
      "Epoch 51/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2702 - val_loss: 0.2295\n",
      "Epoch 52/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2676 - val_loss: 0.2262\n",
      "Epoch 53/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2652 - val_loss: 0.2229\n",
      "Epoch 54/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2627 - val_loss: 0.2197\n",
      "Epoch 55/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2605 - val_loss: 0.2165\n",
      "Epoch 56/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2582 - val_loss: 0.2133\n",
      "Epoch 57/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2560 - val_loss: 0.2102\n",
      "Epoch 58/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2538 - val_loss: 0.2071\n",
      "Epoch 59/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.2516 - val_loss: 0.2041\n",
      "Epoch 60/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2499 - val_loss: 0.2011\n",
      "Epoch 61/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2475 - val_loss: 0.1981\n",
      "Epoch 62/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2455 - val_loss: 0.1953\n",
      "Epoch 63/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2436 - val_loss: 0.1924\n",
      "Epoch 64/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2417 - val_loss: 0.1896\n",
      "Epoch 65/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2400 - val_loss: 0.1869\n",
      "Epoch 66/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2382 - val_loss: 0.1842\n",
      "Epoch 67/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2365 - val_loss: 0.1815\n",
      "Epoch 68/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2349 - val_loss: 0.1789\n",
      "Epoch 69/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.2331 - val_loss: 0.1763\n",
      "Epoch 70/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2316 - val_loss: 0.1738\n",
      "Epoch 71/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2301 - val_loss: 0.1713\n",
      "Epoch 72/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2287 - val_loss: 0.1689\n",
      "Epoch 73/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2271 - val_loss: 0.1666\n",
      "Epoch 74/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2257 - val_loss: 0.1642\n",
      "Epoch 75/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2244 - val_loss: 0.1620\n",
      "Epoch 76/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2230 - val_loss: 0.1597\n",
      "Epoch 77/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2214 - val_loss: 0.1576\n",
      "Epoch 78/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2201 - val_loss: 0.1554\n",
      "Epoch 79/85\n",
      "104/104 [==============================] - 2s 18ms/step - loss: 0.2188 - val_loss: 0.1533\n",
      "Epoch 80/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2176 - val_loss: 0.1512\n",
      "Epoch 81/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2162 - val_loss: 0.1491\n",
      "Epoch 82/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2151 - val_loss: 0.1470\n",
      "Epoch 83/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2137 - val_loss: 0.1450\n",
      "Epoch 84/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2127 - val_loss: 0.1431\n",
      "Epoch 85/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.2115 - val_loss: 0.1411\n",
      "Execution time:  137.03246068954468\n",
      "DNN:\n",
      "Mean Absolute Error: 0.1640\n",
      "Root Mean Square Error: 0.1667\n",
      "Mean Square Error: 0.0278\n",
      "\n",
      "Train RMSE: 0.167\n",
      "Train MSE: 0.028\n",
      "Train MAE: 0.164\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_240 (Dense)            (None, 1008, 97)          194       \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 1008, 16)          1568      \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0934 - val_loss: 0.1186\n",
      "Epoch 2/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0930 - val_loss: 0.1182\n",
      "Epoch 3/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0926 - val_loss: 0.1178\n",
      "Epoch 4/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0923 - val_loss: 0.1174\n",
      "Epoch 5/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0919 - val_loss: 0.1169\n",
      "Epoch 6/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0915 - val_loss: 0.1165\n",
      "Epoch 7/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0911 - val_loss: 0.1160\n",
      "Epoch 8/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0907 - val_loss: 0.1156\n",
      "Epoch 9/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0904 - val_loss: 0.1151\n",
      "Epoch 10/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0899 - val_loss: 0.1146\n",
      "Epoch 11/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0895 - val_loss: 0.1141\n",
      "Epoch 12/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0892 - val_loss: 0.1137\n",
      "Epoch 13/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0888 - val_loss: 0.1133\n",
      "Epoch 14/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0885 - val_loss: 0.1129\n",
      "Epoch 15/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0882 - val_loss: 0.1125\n",
      "Epoch 16/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0878 - val_loss: 0.1120\n",
      "Epoch 17/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0875 - val_loss: 0.1116\n",
      "Epoch 18/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0872 - val_loss: 0.1112\n",
      "Epoch 19/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0869 - val_loss: 0.1108\n",
      "Epoch 20/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0865 - val_loss: 0.1103\n",
      "Epoch 21/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0862 - val_loss: 0.1099\n",
      "Epoch 22/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0859 - val_loss: 0.1095\n",
      "Epoch 23/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0856 - val_loss: 0.1090\n",
      "Epoch 24/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0853 - val_loss: 0.1086\n",
      "Epoch 25/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0850 - val_loss: 0.1082\n",
      "Epoch 26/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0846 - val_loss: 0.1077\n",
      "Epoch 27/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0844 - val_loss: 0.1073\n",
      "Epoch 28/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0841 - val_loss: 0.1069\n",
      "Epoch 29/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0838 - val_loss: 0.1065\n",
      "Epoch 30/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0835 - val_loss: 0.1060\n",
      "Epoch 31/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0832 - val_loss: 0.1056\n",
      "Epoch 32/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0830 - val_loss: 0.1052\n",
      "Epoch 33/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0826 - val_loss: 0.1048\n",
      "Epoch 34/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0823 - val_loss: 0.1044\n",
      "Epoch 35/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0821 - val_loss: 0.1039\n",
      "Epoch 36/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0819 - val_loss: 0.1035\n",
      "Epoch 37/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0816 - val_loss: 0.1031\n",
      "Epoch 38/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0813 - val_loss: 0.1027\n",
      "Epoch 39/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0811 - val_loss: 0.1023\n",
      "Epoch 40/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0808 - val_loss: 0.1019\n",
      "Epoch 41/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0806 - val_loss: 0.1015\n",
      "Epoch 42/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0803 - val_loss: 0.1011\n",
      "Epoch 43/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0801 - val_loss: 0.1007\n",
      "Epoch 44/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0799 - val_loss: 0.1003\n",
      "Epoch 45/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0796 - val_loss: 0.0999\n",
      "Epoch 46/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0794 - val_loss: 0.0995\n",
      "Epoch 47/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0792 - val_loss: 0.0991\n",
      "Epoch 48/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0789 - val_loss: 0.0986\n",
      "Epoch 49/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0786 - val_loss: 0.0981\n",
      "Epoch 50/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0784 - val_loss: 0.0977\n",
      "Epoch 51/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0781 - val_loss: 0.0972\n",
      "Epoch 52/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0778 - val_loss: 0.0968\n",
      "Epoch 53/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0776 - val_loss: 0.0963\n",
      "Epoch 54/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0774 - val_loss: 0.0959\n",
      "Epoch 55/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0770 - val_loss: 0.0954\n",
      "Epoch 56/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0768 - val_loss: 0.0950\n",
      "Epoch 57/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0766 - val_loss: 0.0946\n",
      "Epoch 58/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0764 - val_loss: 0.0942\n",
      "Epoch 59/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0761 - val_loss: 0.0938\n",
      "Epoch 60/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0759 - val_loss: 0.0933\n",
      "Epoch 61/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0758 - val_loss: 0.0929\n",
      "Epoch 62/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0755 - val_loss: 0.0925\n",
      "Epoch 63/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0753 - val_loss: 0.0922\n",
      "Epoch 64/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0751 - val_loss: 0.0918\n",
      "Epoch 65/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0749 - val_loss: 0.0914\n",
      "Epoch 66/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0748 - val_loss: 0.0910\n",
      "Epoch 67/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0746 - val_loss: 0.0906\n",
      "Epoch 68/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0744 - val_loss: 0.0903\n",
      "Epoch 69/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0742 - val_loss: 0.0899\n",
      "Epoch 70/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0741 - val_loss: 0.0896\n",
      "Epoch 71/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0739 - val_loss: 0.0892\n",
      "Epoch 72/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0737 - val_loss: 0.0889\n",
      "Epoch 73/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0735 - val_loss: 0.0886\n",
      "Epoch 74/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0734 - val_loss: 0.0882\n",
      "Epoch 75/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0732 - val_loss: 0.0879\n",
      "Epoch 76/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0731 - val_loss: 0.0876\n",
      "Epoch 77/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0729 - val_loss: 0.0873\n",
      "Epoch 78/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0728 - val_loss: 0.0870\n",
      "Epoch 79/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0727 - val_loss: 0.0867\n",
      "Epoch 80/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0725 - val_loss: 0.0864\n",
      "Epoch 81/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0724 - val_loss: 0.0861\n",
      "Epoch 82/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0723 - val_loss: 0.0859\n",
      "Epoch 83/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0722 - val_loss: 0.0856\n",
      "Epoch 84/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0721 - val_loss: 0.0854\n",
      "Epoch 85/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0720 - val_loss: 0.0851\n",
      "Epoch 86/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0719 - val_loss: 0.0849\n",
      "Epoch 87/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0717 - val_loss: 0.0846\n",
      "Epoch 88/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0716 - val_loss: 0.0844\n",
      "Epoch 89/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0716 - val_loss: 0.0842\n",
      "Epoch 90/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0714 - val_loss: 0.0839\n",
      "Epoch 91/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0713 - val_loss: 0.0837\n",
      "Epoch 92/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0712 - val_loss: 0.0835\n",
      "Epoch 93/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0711 - val_loss: 0.0833\n",
      "Epoch 94/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0711 - val_loss: 0.0831\n",
      "Epoch 95/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0710 - val_loss: 0.0829\n",
      "Epoch 96/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0709 - val_loss: 0.0828\n",
      "Epoch 97/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0708 - val_loss: 0.0826\n",
      "Epoch 98/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0708 - val_loss: 0.0824\n",
      "Execution time:  314.3203213214874\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0566\n",
      "Root Mean Square Error: 0.0653\n",
      "Mean Square Error: 0.0043\n",
      "\n",
      "Train RMSE: 0.065\n",
      "Train MSE: 0.004\n",
      "Train MAE: 0.057\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adadelta\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_243 (Dense)            (None, 1008, 22)          44        \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 1008, 16)          368       \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0928 - val_loss: 0.1135\n",
      "Epoch 2/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0925 - val_loss: 0.1131\n",
      "Epoch 3/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0922 - val_loss: 0.1128\n",
      "Epoch 4/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0918 - val_loss: 0.1124\n",
      "Epoch 5/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0914 - val_loss: 0.1120\n",
      "Epoch 6/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0911 - val_loss: 0.1115\n",
      "Epoch 7/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0907 - val_loss: 0.1111\n",
      "Epoch 8/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0903 - val_loss: 0.1106\n",
      "Epoch 9/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0899 - val_loss: 0.1102\n",
      "Epoch 10/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0895 - val_loss: 0.1098\n",
      "Epoch 11/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0892 - val_loss: 0.1094\n",
      "Epoch 12/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0888 - val_loss: 0.1090\n",
      "Epoch 13/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0885 - val_loss: 0.1086\n",
      "Epoch 14/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0881 - val_loss: 0.1082\n",
      "Epoch 15/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0877 - val_loss: 0.1077\n",
      "Epoch 16/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0873 - val_loss: 0.1073\n",
      "Epoch 17/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0870 - val_loss: 0.1068\n",
      "Epoch 18/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0866 - val_loss: 0.1064\n",
      "Epoch 19/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0862 - val_loss: 0.1059\n",
      "Epoch 20/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0857 - val_loss: 0.1054\n",
      "Epoch 21/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0853 - val_loss: 0.1050\n",
      "Epoch 22/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0849 - val_loss: 0.1045\n",
      "Epoch 23/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0846 - val_loss: 0.1040\n",
      "Epoch 24/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0841 - val_loss: 0.1035\n",
      "Epoch 25/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0837 - val_loss: 0.1030\n",
      "Epoch 26/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0833 - val_loss: 0.1025- \n",
      "Epoch 27/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0829 - val_loss: 0.1020\n",
      "Epoch 28/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0825 - val_loss: 0.1015\n",
      "Epoch 29/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0821 - val_loss: 0.1010\n",
      "Epoch 30/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0817 - val_loss: 0.1006\n",
      "Epoch 31/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0813 - val_loss: 0.1002\n",
      "Epoch 32/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0809 - val_loss: 0.0997\n",
      "Epoch 33/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0806 - val_loss: 0.0993\n",
      "Epoch 34/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0802 - val_loss: 0.0989\n",
      "Epoch 35/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0799 - val_loss: 0.0984\n",
      "Epoch 36/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0796 - val_loss: 0.0980\n",
      "Epoch 37/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0792 - val_loss: 0.0976\n",
      "Epoch 38/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0789 - val_loss: 0.0971\n",
      "Epoch 39/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0785 - val_loss: 0.0967\n",
      "Epoch 40/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0782 - val_loss: 0.0962\n",
      "Epoch 41/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0778 - val_loss: 0.0958\n",
      "Epoch 42/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0775 - val_loss: 0.0954\n",
      "Epoch 43/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0772 - val_loss: 0.0949\n",
      "Epoch 44/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0768 - val_loss: 0.0945\n",
      "Epoch 45/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0765 - val_loss: 0.0940\n",
      "Epoch 46/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0761 - val_loss: 0.0936\n",
      "Epoch 47/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0758 - val_loss: 0.0931\n",
      "Epoch 48/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0755 - val_loss: 0.0927\n",
      "Epoch 49/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0752 - val_loss: 0.0923\n",
      "Epoch 50/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0749 - val_loss: 0.0918\n",
      "Epoch 51/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0746 - val_loss: 0.0914\n",
      "Epoch 52/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0742 - val_loss: 0.0910\n",
      "Epoch 53/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0739 - val_loss: 0.0905\n",
      "Epoch 54/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0736 - val_loss: 0.0901\n",
      "Epoch 55/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0733 - val_loss: 0.0897\n",
      "Epoch 56/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0730 - val_loss: 0.0892\n",
      "Epoch 57/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0727 - val_loss: 0.0888\n",
      "Epoch 58/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0725 - val_loss: 0.0884\n",
      "Epoch 59/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0722 - val_loss: 0.0880\n",
      "Epoch 60/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0719 - val_loss: 0.0875\n",
      "Epoch 61/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0716 - val_loss: 0.0871\n",
      "Epoch 62/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0713 - val_loss: 0.0867\n",
      "Epoch 63/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0710 - val_loss: 0.0863\n",
      "Epoch 64/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0708 - val_loss: 0.0859\n",
      "Epoch 65/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0705 - val_loss: 0.0855\n",
      "Epoch 66/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0703 - val_loss: 0.0851\n",
      "Epoch 67/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0700 - val_loss: 0.0847\n",
      "Epoch 68/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0698 - val_loss: 0.0843\n",
      "Epoch 69/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0695 - val_loss: 0.0839\n",
      "Epoch 70/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0693 - val_loss: 0.0835\n",
      "Epoch 71/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0690 - val_loss: 0.0831\n",
      "Epoch 72/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0688 - val_loss: 0.0827\n",
      "Epoch 73/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0686 - val_loss: 0.0823\n",
      "Epoch 74/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0683 - val_loss: 0.0819\n",
      "Epoch 75/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0681 - val_loss: 0.0816\n",
      "Epoch 76/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0680 - val_loss: 0.0812\n",
      "Epoch 77/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0677 - val_loss: 0.0808\n",
      "Epoch 78/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0675 - val_loss: 0.0805\n",
      "Epoch 79/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0673 - val_loss: 0.0801\n",
      "Epoch 80/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0671 - val_loss: 0.0798\n",
      "Epoch 81/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0668 - val_loss: 0.0794\n",
      "Epoch 82/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0667 - val_loss: 0.0791\n",
      "Epoch 83/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0665 - val_loss: 0.0787\n",
      "Epoch 84/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0663 - val_loss: 0.0784\n",
      "Epoch 85/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0661 - val_loss: 0.0780\n",
      "Execution time:  137.80939745903015\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0590\n",
      "Root Mean Square Error: 0.0674\n",
      "Mean Square Error: 0.0045\n",
      "\n",
      "Train RMSE: 0.067\n",
      "Train MSE: 0.005\n",
      "Train MAE: 0.059\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_246 (Dense)            (None, 1008, 97)          194       \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 1008, 16)          1568      \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.2025 - val_loss: 0.0822\n",
      "Epoch 2/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.1645 - val_loss: 0.0707\n",
      "Epoch 3/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.1453 - val_loss: 0.0483\n",
      "Epoch 4/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.1299 - val_loss: 0.0406\n",
      "Epoch 5/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.1163 - val_loss: 0.0357\n",
      "Epoch 6/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.1027 - val_loss: 0.0312\n",
      "Epoch 7/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0897 - val_loss: 0.0262\n",
      "Epoch 8/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0773 - val_loss: 0.0199\n",
      "Epoch 9/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0658 - val_loss: 0.0126\n",
      "Epoch 10/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0541 - val_loss: 0.0047\n",
      "Epoch 11/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0449 - val_loss: 0.0074\n",
      "Epoch 12/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0372 - val_loss: 0.0075\n",
      "Epoch 13/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0311 - val_loss: 0.0084\n",
      "Epoch 14/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0253 - val_loss: 0.0125\n",
      "Epoch 15/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0234 - val_loss: 0.0140\n",
      "Epoch 16/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0143\n",
      "Epoch 17/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0127\n",
      "Epoch 18/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0148\n",
      "Epoch 19/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 20/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 21/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 22/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 23/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 24/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0233 - val_loss: 0.0153\n",
      "Epoch 25/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 26/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 27/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 28/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 29/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 30/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0233 - val_loss: 0.0153\n",
      "Epoch 31/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 32/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0148\n",
      "Epoch 33/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 34/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 35/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 36/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0154\n",
      "Epoch 37/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 38/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 39/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 40/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0233 - val_loss: 0.0153\n",
      "Epoch 41/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0134\n",
      "Epoch 42/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0137\n",
      "Epoch 43/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0142\n",
      "Epoch 44/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0128\n",
      "Epoch 45/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0232 - val_loss: 0.0149\n",
      "Epoch 46/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0131\n",
      "Epoch 47/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0148\n",
      "Epoch 48/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 49/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 50/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 51/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 52/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 53/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0153\n",
      "Epoch 54/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 55/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 56/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 57/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 58/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 59/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0233 - val_loss: 0.0154\n",
      "Epoch 60/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 61/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0148\n",
      "Epoch 62/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 63/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 64/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 65/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0233 - val_loss: 0.0154\n",
      "Epoch 66/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 67/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 68/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 69/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0233 - val_loss: 0.0153\n",
      "Epoch 70/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0134\n",
      "Epoch 71/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0138\n",
      "Epoch 72/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0142\n",
      "Epoch 73/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0128\n",
      "Epoch 74/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0232 - val_loss: 0.0148\n",
      "Epoch 75/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 76/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 77/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 78/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 79/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 80/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0233 - val_loss: 0.0153\n",
      "Epoch 81/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 82/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 83/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 84/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 85/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 86/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0154\n",
      "Epoch 87/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 88/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0232 - val_loss: 0.0148\n",
      "Epoch 89/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0231 - val_loss: 0.0130\n",
      "Epoch 90/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 91/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 92/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0233 - val_loss: 0.0154\n",
      "Epoch 93/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0135\n",
      "Epoch 94/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0232 - val_loss: 0.0147\n",
      "Epoch 95/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0231 - val_loss: 0.0129\n",
      "Epoch 96/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0233 - val_loss: 0.0153\n",
      "Epoch 97/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0134\n",
      "Epoch 98/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0231 - val_loss: 0.0138\n",
      "Execution time:  315.8593010902405\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0259\n",
      "Root Mean Square Error: 0.0466\n",
      "Mean Square Error: 0.0022\n",
      "\n",
      "Train RMSE: 0.047\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: tanh\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_249 (Dense)            (None, 1008, 22)          44        \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 1008, 16)          368       \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.2534 - val_loss: 0.1098\n",
      "Epoch 2/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.1683 - val_loss: 0.0609\n",
      "Epoch 3/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.1299 - val_loss: 0.0288\n",
      "Epoch 4/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0942 - val_loss: 0.0197\n",
      "Epoch 5/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0646 - val_loss: 0.0109\n",
      "Epoch 6/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0406 - val_loss: 0.0102\n",
      "Epoch 7/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0275 - val_loss: 0.0107\n",
      "Epoch 8/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0107\n",
      "Epoch 9/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0111\n",
      "Epoch 10/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0110\n",
      "Epoch 11/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0251 - val_loss: 0.0112\n",
      "Epoch 12/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0252 - val_loss: 0.0111\n",
      "Epoch 13/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0112\n",
      "Epoch 14/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0112\n",
      "Epoch 15/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0112\n",
      "Epoch 16/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0114\n",
      "Epoch 17/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0112\n",
      "Epoch 18/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 19/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0251 - val_loss: 0.0112\n",
      "Epoch 20/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 21/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 22/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 23/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 24/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 25/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 26/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 27/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 28/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 29/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 30/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 31/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 32/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 33/85\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.025 - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 34/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 35/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 37/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 38/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 39/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 40/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 41/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 42/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 43/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 44/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 45/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 46/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 47/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 48/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 49/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 50/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 51/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0252 - val_loss: 0.0107\n",
      "Epoch 52/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0118\n",
      "Epoch 53/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0110\n",
      "Epoch 54/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 55/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 56/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 57/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 58/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 59/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 60/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 61/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 62/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 63/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 64/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 65/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 66/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 67/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 68/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 69/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 70/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 71/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 72/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 73/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 74/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 75/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 76/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 77/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 78/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 79/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 80/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 81/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Epoch 82/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0115\n",
      "Epoch 83/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0112\n",
      "Epoch 84/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0114\n",
      "Epoch 85/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0250 - val_loss: 0.0111\n",
      "Execution time:  138.19904589653015\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0261\n",
      "Root Mean Square Error: 0.0475\n",
      "Mean Square Error: 0.0023\n",
      "\n",
      "Train RMSE: 0.048\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  97\n",
      "dropout1:  0.795696407994891\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 98\n",
      "batchsize: 43\n",
      "validation_split: 0.1\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_252 (Dense)            (None, 1008, 97)          194       \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 1008, 16)          1568      \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 1,779\n",
      "Trainable params: 1,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0815 - val_loss: 0.0725\n",
      "Epoch 2/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0643 - val_loss: 0.0647\n",
      "Epoch 3/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0582 - val_loss: 0.0621\n",
      "Epoch 4/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0519 - val_loss: 0.0511\n",
      "Epoch 5/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0473 - val_loss: 0.0459\n",
      "Epoch 6/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0440 - val_loss: 0.0426\n",
      "Epoch 7/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0409 - val_loss: 0.0395\n",
      "Epoch 8/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0382 - val_loss: 0.0366\n",
      "Epoch 9/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0358 - val_loss: 0.0334\n",
      "Epoch 10/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0337 - val_loss: 0.0307\n",
      "Epoch 11/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0318 - val_loss: 0.0281\n",
      "Epoch 12/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0302 - val_loss: 0.0262\n",
      "Epoch 13/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0289 - val_loss: 0.0269\n",
      "Epoch 14/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0274 - val_loss: 0.0246\n",
      "Epoch 15/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0263 - val_loss: 0.0221\n",
      "Epoch 16/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0252 - val_loss: 0.0213\n",
      "Epoch 17/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0247 - val_loss: 0.0230\n",
      "Epoch 18/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0237 - val_loss: 0.0199\n",
      "Epoch 19/98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0230 - val_loss: 0.0189\n",
      "Epoch 20/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0228 - val_loss: 0.0183\n",
      "Epoch 21/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0227 - val_loss: 0.0175\n",
      "Epoch 22/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0227 - val_loss: 0.0168\n",
      "Epoch 23/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0226 - val_loss: 0.0161\n",
      "Epoch 24/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0226 - val_loss: 0.0157\n",
      "Epoch 25/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0226 - val_loss: 0.0153\n",
      "Epoch 26/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0149\n",
      "Epoch 27/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0146\n",
      "Epoch 28/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0143\n",
      "Epoch 29/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0140\n",
      "Epoch 30/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0137\n",
      "Epoch 31/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0134\n",
      "Epoch 32/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0131\n",
      "Epoch 33/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0128\n",
      "Epoch 34/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0126\n",
      "Epoch 35/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 36/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 37/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 38/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 39/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 40/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Epoch 41/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 42/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 43/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 44/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0120\n",
      "Epoch 45/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0122\n",
      "Epoch 46/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Epoch 47/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0125\n",
      "Epoch 48/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0120\n",
      "Epoch 49/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 50/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0120\n",
      "Epoch 51/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 52/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 53/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 54/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 55/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Epoch 56/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 57/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 58/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 59/98\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 60/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 61/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 62/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 63/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 64/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 65/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 66/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 67/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 68/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 69/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 70/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 71/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 72/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 73/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 74/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 75/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Epoch 76/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 77/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 78/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 79/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 80/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Epoch 81/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 82/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 83/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 84/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 85/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Epoch 86/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 87/98\n",
      "63/63 [==============================] - 3s 54ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 88/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 89/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0225 - val_loss: 0.0120\n",
      "Epoch 90/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0122\n",
      "Epoch 91/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Epoch 92/98\n",
      "63/63 [==============================] - 3s 53ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 93/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0224 - val_loss: 0.0120\n",
      "Epoch 94/98\n",
      "63/63 [==============================] - 3s 50ms/step - loss: 0.0225 - val_loss: 0.0124\n",
      "Epoch 95/98\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 96/98\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.0225 - val_loss: 0.0121\n",
      "Epoch 97/98\n",
      "63/63 [==============================] - 3s 55ms/step - loss: 0.0225 - val_loss: 0.0123\n",
      "Epoch 98/98\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.0224 - val_loss: 0.0118\n",
      "Execution time:  316.2880046367645\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0261\n",
      "Root Mean Square Error: 0.0475\n",
      "Mean Square Error: 0.0023\n",
      "\n",
      "Train RMSE: 0.047\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n",
      "###########################\n",
      "\n",
      "MODEL:  DNN\n",
      "sequence:  7d\n",
      "units:  22\n",
      "dropout1:  0.7794062193640215\n",
      "optimizer: adamax\n",
      "activationDense: sigmoid\n",
      "epochs: 85\n",
      "batchsize: 23\n",
      "validation_split: 0.2\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_255 (Dense)            (None, 1008, 22)          44        \n",
      "_________________________________________________________________\n",
      "dense_256 (Dense)            (None, 1008, 16)          368       \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 1008, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 1008, 1)           17        \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0777 - val_loss: 0.0685\n",
      "Epoch 2/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0621 - val_loss: 0.0568\n",
      "Epoch 3/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0539 - val_loss: 0.0497\n",
      "Epoch 4/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0468 - val_loss: 0.0426\n",
      "Epoch 5/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0407 - val_loss: 0.0362\n",
      "Epoch 6/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0357 - val_loss: 0.0287\n",
      "Epoch 7/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0311 - val_loss: 0.0196\n",
      "Epoch 8/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0285 - val_loss: 0.0197\n",
      "Epoch 9/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0269 - val_loss: 0.0171\n",
      "Epoch 10/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0256 - val_loss: 0.0156\n",
      "Epoch 11/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0248 - val_loss: 0.0149\n",
      "Epoch 12/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0245 - val_loss: 0.0146\n",
      "Epoch 13/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0244 - val_loss: 0.0143\n",
      "Epoch 14/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0243 - val_loss: 0.0137\n",
      "Epoch 15/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0133\n",
      "Epoch 16/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0131\n",
      "Epoch 17/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0130\n",
      "Epoch 18/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0128\n",
      "Epoch 19/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0126\n",
      "Epoch 20/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0124\n",
      "Epoch 21/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0123\n",
      "Epoch 22/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0122\n",
      "Epoch 23/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0119\n",
      "Epoch 24/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0119\n",
      "Epoch 25/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0118\n",
      "Epoch 26/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0117\n",
      "Epoch 27/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0116\n",
      "Epoch 28/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0120\n",
      "Epoch 29/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0121\n",
      "Epoch 30/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0120\n",
      "Epoch 31/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0119\n",
      "Epoch 32/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0118\n",
      "Epoch 33/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0117\n",
      "Epoch 34/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0116\n",
      "Epoch 35/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0115\n",
      "Epoch 36/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0114\n",
      "Epoch 37/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0114\n",
      "Epoch 38/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0113\n",
      "Epoch 39/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 40/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 41/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0113\n",
      "Epoch 42/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 43/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0111\n",
      "Epoch 44/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 45/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 46/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 47/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 48/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 49/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 50/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 51/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0110\n",
      "Epoch 52/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 53/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 54/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 55/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0242 - val_loss: 0.0111\n",
      "Epoch 56/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 57/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 58/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 59/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0111\n",
      "Epoch 60/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 61/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 62/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 63/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0111\n",
      "Epoch 64/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 65/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 66/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 67/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 68/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0110\n",
      "Epoch 69/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0115\n",
      "Epoch 70/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0114\n",
      "Epoch 71/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0113\n",
      "Epoch 72/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0113\n",
      "Epoch 73/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 74/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 75/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 76/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 77/85\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 78/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 79/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0113\n",
      "Epoch 80/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 81/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 82/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 83/85\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0242 - val_loss: 0.0112\n",
      "Epoch 84/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0110\n",
      "Epoch 85/85\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0242 - val_loss: 0.0113\n",
      "Execution time:  138.2881453037262\n",
      "DNN:\n",
      "Mean Absolute Error: 0.0260\n",
      "Root Mean Square Error: 0.0472\n",
      "Mean Square Error: 0.0022\n",
      "\n",
      "Train RMSE: 0.047\n",
      "Train MSE: 0.002\n",
      "Train MAE: 0.026\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _model(units,activationDense,dropout1,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(rate=dropout1))\n",
    "    model.add(Dense(X_train.shape[2],activation=activationDense))\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "sequences = [\"1h\",\"3h\",\"6h\",\"12h\",\"1d\", \"3d\", \"7d\"]\n",
    "X_trains = [X_train1h,X_train3h, X_train6h, X_train12h,X_train1d,X_train3d, X_train7d]\n",
    "y_trains= [y_train1h,y_train3h, y_train6h,y_train12h,y_train1d,y_train3d, y_train7d]\n",
    "activationsDense = ['tanh','sigmoid']\n",
    "optimizers = ['adam','adadelta','adamax']\n",
    "list_validationSplit = [0.1,0.2]\n",
    "list_dropout1 =  np.random.uniform(0.1,0.8,5)   #generació de llista de 10 learning rate (entre 0.01 i 0.2)\n",
    "list_units = np.random.randint(6,high=100, size=5) #generació de llista de 10 iteracions (entre 20 i 500)\n",
    "list_epochs = np.random.randint(5,high=100, size=5) #generació de llista de 10 iteracions (entre 20 i 500)  \n",
    "list_batchsize = np.random.randint(6,high=64, size=5) #generació de llista de 10 iteracions (entre 20 i 500)    \n",
    "\n",
    "list_results = pd.DataFrame()\n",
    "\n",
    "for X_train, y_train,sequence in zip(X_trains,y_trains,sequences):\n",
    "    for optimizer in optimizers:\n",
    "        #for activation in activations:\n",
    "            for activationDense in activationsDense:\n",
    "                for units,epochs,batchsize,dropout1,validationsplit in zip(list_units,list_epochs,list_batchsize,list_dropout1,list_validationSplit): \n",
    "                    start = time()\n",
    "                    print(\"###########################\\n\")\n",
    "                    print(\"MODEL: \", \"DNN\")\n",
    "                    print('sequence: ',sequence)\n",
    "                    print('units: ',units)\n",
    "                    print('dropout1: ',dropout1)\n",
    "                    print('optimizer:',optimizer)\n",
    "                    #print('activation:',activation)\n",
    "                    print('activationDense:',activationDense)\n",
    "                    print('epochs:',epochs)\n",
    "                    print('batchsize:',batchsize)\n",
    "                    print('validation_split:',validationsplit)\n",
    "\n",
    "                    model = _model(units,activationDense,dropout1,optimizer)\n",
    "                    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batchsize, validation_split=validationsplit, shuffle=False)\n",
    "                    end = time()\n",
    "                    totalTime = end-start\n",
    "                    print ('Execution time: ',totalTime)\n",
    "\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    plt.plot(history.history['loss'], label='Training loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "                    plt.legend();\n",
    "\n",
    "                    X_train_pred = model.predict(X_train, verbose=0)\n",
    "\n",
    "                    mae,rmse,mse = evaluate_prediction(X_train_pred, X_train,\"DNN\")\n",
    "\n",
    "                    print('Train RMSE: %.3f' % rmse);\n",
    "                    print('Train MSE: %.3f' % mse);\n",
    "                    print('Train MAE: %.3f' % mae);\n",
    "\n",
    "                    result = pd.DataFrame({\n",
    "                                           #'activation':[activation],\n",
    "                                           'model':[\"DNN\"],\n",
    "                                           'sequence':[sequence],\n",
    "                                           'activationDense':[activationDense],\n",
    "                                           'optimizer':[optimizer],\n",
    "                                           'dropout1':[dropout1],\n",
    "                                           'units':[units],\n",
    "                                           'epochs':[epochs],\n",
    "                                           'batchsize':[batchsize],\n",
    "                                           'validation_split':[validationsplit],\n",
    "\n",
    "                                           'RMSE':[rmse],\n",
    "                                           'MSE':[mse],\n",
    "                                           'MAE':[mae],                            \n",
    "                                           'Time':[totalTime]})\n",
    "                    list_results = list_results.append(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_results.to_csv(\"resultats-cerca-optim-dnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sequence</th>\n",
       "      <th>activationDense</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout1</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.030132</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>17.585649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.034743</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>19.619283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>16.172646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.043117</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.026169</td>\n",
       "      <td>18.853198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.177944</td>\n",
       "      <td>0.031664</td>\n",
       "      <td>0.175224</td>\n",
       "      <td>15.930931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.199517</td>\n",
       "      <td>0.039807</td>\n",
       "      <td>0.195685</td>\n",
       "      <td>18.863140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.081038</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.073569</td>\n",
       "      <td>16.355124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.071137</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>0.063235</td>\n",
       "      <td>18.562690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.026339</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>16.440959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.028083</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.018252</td>\n",
       "      <td>18.201188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.025958</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>16.130773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.014066</td>\n",
       "      <td>18.778049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.044534</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.026330</td>\n",
       "      <td>22.761477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.040479</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.024087</td>\n",
       "      <td>22.262795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.042585</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>22.712627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.039892</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>22.448382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.123372</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.120948</td>\n",
       "      <td>22.737308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.161561</td>\n",
       "      <td>0.026102</td>\n",
       "      <td>0.157683</td>\n",
       "      <td>22.100554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.063788</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>0.055766</td>\n",
       "      <td>22.979553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>3h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.071259</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.063450</td>\n",
       "      <td>21.981473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model sequence activationDense optimizer  dropout1  units  epochs  \\\n",
       "0   DNN       1h            tanh      adam  0.795696     97      98   \n",
       "0   DNN       1h            tanh      adam  0.779406     22      85   \n",
       "0   DNN       1h         sigmoid      adam  0.795696     97      98   \n",
       "0   DNN       1h         sigmoid      adam  0.779406     22      85   \n",
       "0   DNN       1h            tanh  adadelta  0.795696     97      98   \n",
       "0   DNN       1h            tanh  adadelta  0.779406     22      85   \n",
       "0   DNN       1h         sigmoid  adadelta  0.795696     97      98   \n",
       "0   DNN       1h         sigmoid  adadelta  0.779406     22      85   \n",
       "0   DNN       1h            tanh    adamax  0.795696     97      98   \n",
       "0   DNN       1h            tanh    adamax  0.779406     22      85   \n",
       "0   DNN       1h         sigmoid    adamax  0.795696     97      98   \n",
       "0   DNN       1h         sigmoid    adamax  0.779406     22      85   \n",
       "0   DNN       3h            tanh      adam  0.795696     97      98   \n",
       "0   DNN       3h            tanh      adam  0.779406     22      85   \n",
       "0   DNN       3h         sigmoid      adam  0.795696     97      98   \n",
       "0   DNN       3h         sigmoid      adam  0.779406     22      85   \n",
       "0   DNN       3h            tanh  adadelta  0.795696     97      98   \n",
       "0   DNN       3h            tanh  adadelta  0.779406     22      85   \n",
       "0   DNN       3h         sigmoid  adadelta  0.795696     97      98   \n",
       "0   DNN       3h         sigmoid  adadelta  0.779406     22      85   \n",
       "\n",
       "   batchsize  validation_split      RMSE       MSE       MAE       Time  \n",
       "0         43               0.1  0.030132  0.000908  0.018967  17.585649  \n",
       "0         23               0.2  0.034743  0.001207  0.022050  19.619283  \n",
       "0         43               0.1  0.033080  0.001094  0.020072  16.172646  \n",
       "0         23               0.2  0.043117  0.001859  0.026169  18.853198  \n",
       "0         43               0.1  0.177944  0.031664  0.175224  15.930931  \n",
       "0         23               0.2  0.199517  0.039807  0.195685  18.863140  \n",
       "0         43               0.1  0.081038  0.006567  0.073569  16.355124  \n",
       "0         23               0.2  0.071137  0.005060  0.063235  18.562690  \n",
       "0         43               0.1  0.026339  0.000694  0.016100  16.440959  \n",
       "0         23               0.2  0.028083  0.000789  0.018252  18.201188  \n",
       "0         43               0.1  0.025958  0.000674  0.016329  16.130773  \n",
       "0         23               0.2  0.022632  0.000512  0.014066  18.778049  \n",
       "0         43               0.1  0.044534  0.001983  0.026330  22.761477  \n",
       "0         23               0.2  0.040479  0.001639  0.024087  22.262795  \n",
       "0         43               0.1  0.042585  0.001813  0.025410  22.712627  \n",
       "0         23               0.2  0.039892  0.001591  0.023473  22.448382  \n",
       "0         43               0.1  0.123372  0.015221  0.120948  22.737308  \n",
       "0         23               0.2  0.161561  0.026102  0.157683  22.100554  \n",
       "0         43               0.1  0.063788  0.004069  0.055766  22.979553  \n",
       "0         23               0.2  0.071259  0.005078  0.063450  21.981473  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sequence</th>\n",
       "      <th>activationDense</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout1</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.014066</td>\n",
       "      <td>18.778049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.025958</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>16.130773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.026339</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>16.440959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adamax</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.028083</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.018252</td>\n",
       "      <td>18.201188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.030132</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>17.585649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.177944</td>\n",
       "      <td>0.031664</td>\n",
       "      <td>0.175224</td>\n",
       "      <td>15.930931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>22</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.199517</td>\n",
       "      <td>0.039807</td>\n",
       "      <td>0.195685</td>\n",
       "      <td>18.863140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>7d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.204984</td>\n",
       "      <td>0.042018</td>\n",
       "      <td>0.202476</td>\n",
       "      <td>314.883570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>12h</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.217763</td>\n",
       "      <td>0.047421</td>\n",
       "      <td>0.215512</td>\n",
       "      <td>44.823819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>1d</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.795696</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.291126</td>\n",
       "      <td>0.084754</td>\n",
       "      <td>0.288634</td>\n",
       "      <td>70.498550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model sequence activationDense optimizer  dropout1  units  epochs  \\\n",
       "0    DNN       1h         sigmoid    adamax  0.779406     22      85   \n",
       "0    DNN       1h         sigmoid    adamax  0.795696     97      98   \n",
       "0    DNN       1h            tanh    adamax  0.795696     97      98   \n",
       "0    DNN       1h            tanh    adamax  0.779406     22      85   \n",
       "0    DNN       1h            tanh      adam  0.795696     97      98   \n",
       "..   ...      ...             ...       ...       ...    ...     ...   \n",
       "0    DNN       1h            tanh  adadelta  0.795696     97      98   \n",
       "0    DNN       1h            tanh  adadelta  0.779406     22      85   \n",
       "0    DNN       7d            tanh  adadelta  0.795696     97      98   \n",
       "0    DNN      12h            tanh  adadelta  0.795696     97      98   \n",
       "0    DNN       1d            tanh  adadelta  0.795696     97      98   \n",
       "\n",
       "    batchsize  validation_split      RMSE       MSE       MAE        Time  \n",
       "0          23               0.2  0.022632  0.000512  0.014066   18.778049  \n",
       "0          43               0.1  0.025958  0.000674  0.016329   16.130773  \n",
       "0          43               0.1  0.026339  0.000694  0.016100   16.440959  \n",
       "0          23               0.2  0.028083  0.000789  0.018252   18.201188  \n",
       "0          43               0.1  0.030132  0.000908  0.018967   17.585649  \n",
       "..        ...               ...       ...       ...       ...         ...  \n",
       "0          43               0.1  0.177944  0.031664  0.175224   15.930931  \n",
       "0          23               0.2  0.199517  0.039807  0.195685   18.863140  \n",
       "0          43               0.1  0.204984  0.042018  0.202476  314.883570  \n",
       "0          43               0.1  0.217763  0.047421  0.215512   44.823819  \n",
       "0          43               0.1  0.291126  0.084754  0.288634   70.498550  \n",
       "\n",
       "[84 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results.sort_values(by=['RMSE', 'sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PAC3_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
